{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=500_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=500.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=500_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]_result.json', stream_id=3, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4080, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["The Jones et al. and Briffa reconstructions", "help transfer and dissipate excess energy", "Historical and Critical Dictionary", "human settlement and development of the land", "over three days", "eight", "polynomial algebra", "60 days", "American Association of University Women", "mid-Cambrian period", "1926", "The outcome of most votes can be predicted beforehand", "Newton", "a disaster", "between the 1960s and 1990s", "James Hutton", "British", "growth and investment", "Each step had to be successfully accomplished before the next ones could be performed", "Downtown Riverside", "plague of Athens in 430 BC", "expelled Jews", "interleukin 1", "DuMont Television Network", "The city of Fresno", "1762", "colloblasts", "self", "Alta California", "300 km long and up to 40 km wide", "two-phased system", "On the Councils and the Church", "Super Bowl LI", "1968", "\"right\", \"just\", or \"true\"", "pastors", "meritocracy", "Vistula River", "arrows, swords, and leather shields", "23 November 1963", "since the 1960s", "zero net force", "2012", "CD4", "income inequality", "Tuition Fee Supplement", "April 1523", "the California State Automobile Association and the Automobile Club of Southern California", "Von Miller", "lymphocytes or an antibody-based humoral response", "unsuccessful", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Percy Shelley", "education, sanitation, and traffic control", "six divisions", "six series of theses", "Pi\u0142sudski", "Disney\u2013ABC Television Group", "SAP Center", "Genghis Khan", "materials melted near an impact crater", "William the Silent", "182 million tons", "John G. Trump"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8592347756410257}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4191", "mrqa_squad-validation-9426", "mrqa_squad-validation-368", "mrqa_squad-validation-3776", "mrqa_squad-validation-4769", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_squad-validation-2812", "mrqa_squad-validation-6559", "mrqa_squad-validation-2153", "mrqa_squad-validation-7246"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["occupational stress", "Sunni Arabs from Iraq and Syria", "The Lower Rhine", "Mike Tolbert", "silt up the lake", "cigarette advertising from all television and radio networks", "Dorothy and Michael Hintze", "climate change", "northwest", "Turkey", "Economist", "The Tyneside flat", "73", "Derek Jacobi", "ten minutes", "Sydney", "Basel", "ideal strings", "unstable molecules", "lasting damage", "December 2014", "other locations throughout Scotland", "eastern", "Newton", "Westwood One", "2008", "the colonies of British America and New France", "early 1546", "lower bounds", "2011", "became the University of Northumbria at Newcastle", "between 1859 and 1865", "8\u20134\u20134 system", "a green algal derived chloroplast", "over 200", "William the Conqueror", "The Service Module was discarded", "18 million volumes", "north", "detention", "collenchyma tissue", "Louis Adamic", "cytokine T IGF-\u03b2", "England", "Yuri Gagarin", "antigenic variation", "Knaurs Lexikon", "Chester, South Carolina", "1992", "three", "9.1 million", "rich and well socially standing", "pharynx", "1969", "Wardenclyffe Tower", "Tower Theatre", "700,000", "Pittsburgh", "jeopardy/2516_Qs.txt at master  jedoublen/jeopardy", "Heroes struggle... Animated, Action, Adventure, Fantasy, Sci-Fi.... Lego Star Wars: The Yoda Ch", "American baseball, until the late 1940s, excluded, with some big exceptions in... The color line was broken for good when Jackie Robinson signed with the Brooklyn", "the hand of cards which he supposedly held at the time of his death... killed by the assassin Jack McCall in Deadwood, Black Hills, August 2, 1876", "His... (initial capital letter) a German-built enciphering machine developed for commercial", "Europe"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8128918650793651}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.16, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9752", "mrqa_squad-validation-5549", "mrqa_squad-validation-8718", "mrqa_squad-validation-8891", "mrqa_squad-validation-9744", "mrqa_squad-validation-5337", "mrqa_squad-validation-3885", "mrqa_squad-validation-1568", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-4596"], "SR": 0.78125, "CSR": 0.8046875, "EFR": 1.0, "Overall": 0.90234375}, {"timecode": 2, "before_eval_results": {"predictions": ["John Sutcliffe", "can also concentrate wealth, pass environmental costs on to society, and abuse workers and consumers", "6800", "medical treatment for the sick and wounded French soldiers", "Ticonderoga Point", "October 16, 1973", "1980s", "seven", "books and articles for magazines and journals", "Roger NFL", "the oceans and seas", "60,000 European settlers", "by over 100%", "1350", "North America", "Euclid's fundamental theorem of arithmetic", "Maria de la Queillerie", "to encourage investment", "Julia Butterfly Hill", "farming assets, ranging from mobile phones to productive land and livestock, and is opening pathways for them to move out of poverty", "587,000 square kilometres", "March 1974", "quickly", "end of the 19th century", "DeMarcus Ware", "University of North Florida team", "motivated students", "platyctenids", "elementary school education certificate", "1220", "two", "30.0%", "Raghuram Rajan", "VHF channel 7", "The Pink Triangle", "ideological", "Methodist institutions", "Ticonderoga", "six daughters", "Los Angeles Dodgers", "19th", "a course of study and lesson plan", "a delay costs money", "Funchess", "Watt", "1,000 m3/s (35,000 cu ft/s)", "Thuringia", "rivers", "anti-Semitic policies of the National Socialists", "visor helmet", "Catholic", "Hollywood", "Long Island Sound", "Sweden's", "an invaluable service as usurers in medieval society", "an African American", "first woman governor", "an athlete who plays cricket", "conifer", "Alaska", "an Austrian and American film actress and inventor", "Daniel Defoe", "the Association of American Universities", "50JJB Sports Fitness Clubs"], "metric_results": {"EM": 0.625, "QA-F1": 0.7018229166666666}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.6, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665]}}, "before_error_ids": ["mrqa_squad-validation-7525", "mrqa_squad-validation-10258", "mrqa_squad-validation-1565", "mrqa_squad-validation-85", "mrqa_squad-validation-802", "mrqa_squad-validation-9061", "mrqa_squad-validation-8322", "mrqa_squad-validation-4257", "mrqa_squad-validation-1960", "mrqa_squad-validation-5678", "mrqa_squad-validation-6101", "mrqa_squad-validation-2786", "mrqa_squad-validation-1877", "mrqa_squad-validation-2497", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-13452", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-5603"], "SR": 0.625, "CSR": 0.7447916666666667, "EFR": 0.9166666666666666, "Overall": 0.8307291666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["public schools", "connection id in a table", "males", "electrical, water, sewage, phone, and cable facilities", "level of the top tax rate", "\"Wise up or die.\"", "VideoGuard UK", "highly-paid", "pump this into the mesoglea", "Fred Silverman", "atmospheric engine", "force", "ctenophores", "trial and rehabilitation of Joan of Arc", "John Hurt", "an Australian public X.25 network operated by Telstra", "Arizona Cardinals", "Von Miller", "Indianapolis Colts", "42%", "1957", "imprisonment", "orogenic wedges", "one", "Catholic", "Edict of Fontainebleau", "Fort Caroline", "Pittard Sullivan", "wealth", "Niagara Falls", "Hugh Downs", "House of Hohenstaufen", "3D printing technology", "Daniel Burke", "internal strife", "Matthew Murray", "400 AD", "the United States", "Satya Nadella", "the difference between a problem and an instance", "Richard E. Grant", "Inner Mongolia", "cortisol and catecholamines", "a third group of pigments found in cyanobacteria", "isopentenyl pyrophosphate synthesis", "1963", "hotel room", "Italy", "Joan Hughes", "workhouse", "Khartoum", "William Henry Harrison", "Playboy rabbit", "Dutch metaphysicians", "Puerto Rico", "Court TV", "King Tut", "The Prairie Wolf", "Inhospitable Sea", "Moshe Dayan", "Joan Van Dinh", "active athletes", "helicopters and boats", "$17,000"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6802522130647131}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 0.4615384615384615, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7103", "mrqa_squad-validation-7357", "mrqa_squad-validation-4452", "mrqa_squad-validation-3247", "mrqa_squad-validation-230", "mrqa_squad-validation-363", "mrqa_squad-validation-10083", "mrqa_squad-validation-3139", "mrqa_squad-validation-1045", "mrqa_squad-validation-5542", "mrqa_squad-validation-1670", "mrqa_squad-validation-6263", "mrqa_squad-validation-8839", "mrqa_squad-validation-7711", "mrqa_searchqa-validation-8170", "mrqa_searchqa-validation-12199", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-6624", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-3588"], "SR": 0.609375, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 4, "before_eval_results": {"predictions": ["whether he stood by their contents", "1850s", "Troika", "complexity classes", "Parliament of the United Kingdom at Westminster", "teaching", "Desperate Housewives, Lost and Grey's Anatomy", "8 November 2010", "the Y. pestis was spread from fleas on rats", "coastal beaches and the game reserves", "1524", "2p \u2212 1", "horizontal", "Crash the Super Bowl", "collenchyma tissue", "around a billion years ago", "Croatia", "Long Beach", "Edinburgh", "Broncos", "Papin", "Cricket", "William Morris", "T. J. Ward", "the Daleks", "San Diego", "1017", "heat and pressure", "1072", "Chevron", "Africa", "New York City", "Marshall Cohen", "Hypersensitivity", "Business Connect", "Henry Young Darracott Scott", "not break any law", "income inequality", "Bainbridge's", "1294", "chao", "Apollo 17", "Ghirardelli", "Ghirardelli", "Stephen Hawking", "Ghirardelli", "Europe", "Ghirardelli", "Ghirardelli", "Ghirardelli", "Ghirardelli", "Moscow", "crystal anniversary", "prairie crocus", "Detroit River", "Seth Taft", "New Testament", "Dublin", "Ghirardelli", "Ghirardelli", "Albert Einstein", "Doctor Who", "1990", "Zimbabwe"], "metric_results": {"EM": 0.640625, "QA-F1": 0.670217803030303}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5878", "mrqa_squad-validation-5029", "mrqa_squad-validation-2765", "mrqa_squad-validation-821", "mrqa_squad-validation-5344", "mrqa_squad-validation-7615", "mrqa_squad-validation-2679", "mrqa_squad-validation-1061", "mrqa_squad-validation-4147", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-9601", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-4300"], "SR": 0.640625, "CSR": 0.696875, "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 5, "before_eval_results": {"predictions": ["November 2006", "achievement-oriented", "a malfunction in the chameleon circuit", "SAP Center", "March 2011", "above the top of the range", "12th", "1226", "The Late Late Show", "bird", "P", "theNP-complete knapsack problem", "1928", "when the immune system is less active than normal", "disturbances", "Fraud", "1562 to 1598", "Emmerich Rhine Bridge", "ten", "1993", "a primitive intermediate between cyanobacteria and the more evolved chloroplasts", "lung tissue", "US$100,000", "Bakersfield", "Bruno Mars", "patients' prescriptions and patient safety issues", "2009", "the Common Core", "Recognized Student Organizations", "a cubic interpolation formula", "Thomas Edison", "the Swiss Reformation", "phagocytes", "15", "Satyagraha", "two", "microorganisms", "1968", "Aloha \u02bbOe", "1.6 million", "Eric Whitacre", "gourd-bows", "Angus Young", "New York", "the waltz Gunstwerber", "Cherokee River", "Odisha", "January 28, 2016", "138,535", "Adam Rex", "1933", "Sivakumar", "1968", "a Chaplain to the Forces", "astronomer", "Warrington", "1866", "Chattahoochee", "the Provisional Irish Republican Army", "pneumonoultramicroscopicsilicovolcanoconiosis", "Boutros Ghali", "Vertikal-T", "The Wizard of Oz", "he was mad at the U.S. military"], "metric_results": {"EM": 0.59375, "QA-F1": 0.672636217948718}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8531", "mrqa_squad-validation-434", "mrqa_squad-validation-9185", "mrqa_squad-validation-1759", "mrqa_squad-validation-1860", "mrqa_squad-validation-8696", "mrqa_squad-validation-3496", "mrqa_squad-validation-2634", "mrqa_squad-validation-664", "mrqa_squad-validation-3240", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2158", "mrqa_newsqa-validation-1468", "mrqa_searchqa-validation-13492", "mrqa_newsqa-validation-3290"], "SR": 0.59375, "CSR": 0.6796875, "EFR": 1.0, "Overall": 0.83984375}, {"timecode": 6, "before_eval_results": {"predictions": ["UHF", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "Africa", "1976", "inverse", "non-deterministic time", "60", "Warsaw", "5,000 years", "Edgar Atheling", "Writers Guild of America", "UNEP", "Conservative", "11 million", "a water-cooled undergarment", "the Queen", "3 in 1,000,000", "2009 onwards", "Super Bowl Opening Night", "Fresno Street and Thorne Ave", "southern and central parts of France", "2014", "40%", "John F. Kennedy", "innate immune system", "15\u20131", "history of arms", "Industry and manufacturing", "this contact with nature made him stronger, both physically and mentally.", "1543", "\u015ar\u00f3dmie\u015bcie", "Hmong or Laotian", "Stromules", "oxygen will act as a fuel;", "Johnny Herbert", "\"jus sanguinis\"", "Pharrell Williams", "James Dearden", "J\u00f3zsef Pulitzer", "June 17, 2007", "\"The Frost Report\"", "National Basketball Development League", "Danish", "24 January 76", "Kona coast", "Dave Lee Travis", "Northrop P-61 Black widow", "People v. Turner", "\"Veyyil\"", "Helena Sternlicht", "Highlands Course", "Illinois", "Cartoon Cartoon Fridays", "Timo Hildebrand", "2016", "Ginger Rogers", "Corps of Discovery", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "801,200", "giraffe", "navy", "Ecuador", "Abraham Lincoln", "a final contest"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7132575757575758}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10424", "mrqa_squad-validation-1730", "mrqa_squad-validation-1114", "mrqa_squad-validation-8523", "mrqa_squad-validation-4070", "mrqa_squad-validation-7770", "mrqa_squad-validation-3764", "mrqa_squad-validation-1232", "mrqa_squad-validation-3483", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2183", "mrqa_naturalquestions-validation-4953", "mrqa_newsqa-validation-3782", "mrqa_searchqa-validation-5374"], "SR": 0.671875, "CSR": 0.6785714285714286, "EFR": 1.0, "Overall": 0.8392857142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["convection of the mantle", "Continental Edison Company in France", "South", "T. J. Ward", "25 minutes of transmission length", "1903", "1993", "King George III", "occupancy permit", "Hereford", "a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "Arts & Entertainment Television (A&E)", "NASA", "Albert Einstein", "Karl von Miltitz", "a matter of custom or expectation,", "Silk Road", "a war", "13 years and 48 days", "Bolshevik leaders", "University of Aberdeen", "transportation, sewer, hazardous waste and water", "during the plague of Athens in 430 BC", "Pedro Men\u00e9ndez de Avil\u00e9s", "expansion", "a deterministic Turing machine", "linear", "when the oxygen concentration is too high", "1290", "17,786,419,", "smallest state on the Australian mainland", "Montreal Montreal", "7000301604928199000", "was an incident on March 5, 1770, in which British Army soldiers shot and killed people while under attack by a mob", "Pakistan, India, and Bangladesh are among the largest individual contributors with around 8,000 units each", "2000", "Christina Calvano", "Moore", "the human hands and face", "Martin Lawrence", "Steve Hale", "a multilayer", "January 2017 patch", "Idaho", "September 6, 2019", "multinational retail corporation", "Joely Richardson", "Jack Gleeson", "claims adjusters", "when the forward reaction proceeds at the same rate as the reverse reaction", "heart", "writ of certiorari", "A standard form contract", "Mr. Feeny", "September 30", "Kelly Osbourne, Ian'' Dicko '' Dickson, Christina Monk and Eddie Perfect", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "a Californio nobleman and master living in Los Angeles during the era of Spanish rule", "\"Household Words\",", "56", "Hindu scriptures", "St. Augustine", "\"a grass genus)", "gold"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6797531864937388}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.07692307692307691, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.046511627906976744, 0.0, 0.5, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1239", "mrqa_squad-validation-2315", "mrqa_squad-validation-6878", "mrqa_squad-validation-1819", "mrqa_squad-validation-2881", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-7579", "mrqa_searchqa-validation-12472", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3744"], "SR": 0.609375, "CSR": 0.669921875, "EFR": 0.96, "Overall": 0.8149609375}, {"timecode": 8, "before_eval_results": {"predictions": ["ivory", "Dornbirner Ach", "a certain number of teacher's salaries are paid by the State", "non-deterministic time", "five", "December 2014", "an inauspicious typhoon", "four", "Zwickau prophet", "10 July 1856 \u2013 7 January 1943", "1999", "CBS Sports apps on tablets, Windows 10, Xbox One and other digital media players", "3\u20132.7 billion years ago", "New Testament from Greek", "Von Miller", "economists with the Standard & Poor's rating agency", "Germany", "a vestigial red algal derived chloroplast", "two or more teachers", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "type III secretion system", "worker, capitalist/business owner, landlord", "tungsten", "the state", "the oxidant", "the signals could come from Mars, Venus, or other planets", "the American Revolution", "Book of Discipline", "Fat Albert", "1943", "\"Big Fucking German\"", "Chelmsford City", "William \"Magic\" Johnson Jr.", "22,500 acres", "1951", "Abu Dhabi, United Arab Emirates", "Oklahoma Memorial Stadium", "American", "the Firth of Forth Site of Special Scientific Interest", "one live album, one compilation album, nineteen singles and fourteen music videos", "Blue Valley Northwest High School", "The Birds", "Battle of the Rosebud", "Homebrewing", "Pablo Escobar", "Cartoon Network Studios", "26 June 2013", "25 million records", "twice", "Geraldine Sue Page", "Rochdale", "Charles Reed Bishop", "motor", "Marco Fu Ka-chun", "2015", "Dusty Springfield", "her translation of and commentary on Isaac Newton's book \"Principia\"", "BeBe Winans", "Henry VIII", "Sunday", "purple", "Dumont d'Urville Station", "Under normal conditions", "a spiritual conversion"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7217582707902002}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.125, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.75, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.8, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_squad-validation-1156", "mrqa_squad-validation-531", "mrqa_squad-validation-2272", "mrqa_squad-validation-1912", "mrqa_squad-validation-4849", "mrqa_squad-validation-1529", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-391", "mrqa_triviaqa-validation-4490", "mrqa_newsqa-validation-3405", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-5851"], "SR": 0.578125, "CSR": 0.6597222222222222, "EFR": 1.0, "Overall": 0.8298611111111112}, {"timecode": 9, "before_eval_results": {"predictions": ["1999", "zero", "Mao Zedong", "Chebyshev", "1960", "introduction of Beroe", "1000 CE", "Rollo", "Sunspot, New Mexico", "Sonderungsverbot", "an amending treaty", "the environment in which they lived", "a genetic disease", "C. J. Anderson", "Cadeby", "Warraghiggey", "starts accidentally adding oxygen to sugar precursors", "World Meteorological Organization", "Sunni pan-Islamism", "23\u201316", "yes or no, or alternately either 1 or 0", "white", "the Mongols", "English", "Newton", "1940s and 1950s", "arthur is a 17th-century castle near Muir of Ord and Tore on the Black Isle, in Ross and Cromarty, Scotland", "lyricist", "Taoiseach", "Duval County", "Bill Ponsford", "$10\u201320 million", "Manasseh Cutler Hall", "Denmark", "Hindi", "the \"Pour le M\u00e9rite\"", "Giuseppe Fortunino Francesco Verdi", "Edward Trowbridge Collins Sr.", "1946 and 1947", "Christopher McCulloch", "2016\u201317", "Carson City", "Wembley Stadium", "19th", "Cesar Millan", "Bob Dylan", "Michael Lewis Greenwell", "from 20 March to 1 May 2003", "The Life of Charlotte Bront\u00eb", "2015", "Bill Curry", "Jack White", "Kim Yoon-seok and Ha Jung-woo", "superhero roles", "Brent Robert Barry", "18th congressional district", "the BBC", "2008", "aro Starr", "arthur", "the eastern Afghan province of Logar", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures", "arthur", "Jamaica"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7644800101214575}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7368421052631579, 0.4615384615384615, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.923076923076923, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9714285714285714, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9897", "mrqa_squad-validation-8832", "mrqa_squad-validation-8782", "mrqa_squad-validation-1652", "mrqa_squad-validation-502", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-4397", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-2645", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_searchqa-validation-13221"], "SR": 0.640625, "CSR": 0.6578125, "EFR": 1.0, "Overall": 0.82890625}, {"timecode": 10, "UKR": 0.69921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3894", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4633", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5145", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5527", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-690", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9687", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16753", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4300", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6624", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-10007", "mrqa_squad-validation-10010", "mrqa_squad-validation-10031", "mrqa_squad-validation-10083", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10130", "mrqa_squad-validation-10153", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10234", "mrqa_squad-validation-10249", "mrqa_squad-validation-10258", "mrqa_squad-validation-10305", "mrqa_squad-validation-10345", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10478", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1073", "mrqa_squad-validation-1075", "mrqa_squad-validation-1075", "mrqa_squad-validation-1096", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1232", "mrqa_squad-validation-1239", "mrqa_squad-validation-1254", "mrqa_squad-validation-1296", "mrqa_squad-validation-1372", "mrqa_squad-validation-1529", "mrqa_squad-validation-1543", "mrqa_squad-validation-1586", "mrqa_squad-validation-1632", "mrqa_squad-validation-1652", "mrqa_squad-validation-1681", "mrqa_squad-validation-1723", "mrqa_squad-validation-1730", "mrqa_squad-validation-1731", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-1819", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1877", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-1976", "mrqa_squad-validation-1985", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2111", "mrqa_squad-validation-2153", "mrqa_squad-validation-2181", "mrqa_squad-validation-2189", "mrqa_squad-validation-2246", "mrqa_squad-validation-2247", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2320", "mrqa_squad-validation-2359", "mrqa_squad-validation-2413", "mrqa_squad-validation-2442", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2568", "mrqa_squad-validation-2591", "mrqa_squad-validation-260", "mrqa_squad-validation-2617", "mrqa_squad-validation-2628", "mrqa_squad-validation-2634", "mrqa_squad-validation-2644", "mrqa_squad-validation-2679", "mrqa_squad-validation-2721", "mrqa_squad-validation-2723", "mrqa_squad-validation-2765", "mrqa_squad-validation-2808", "mrqa_squad-validation-2812", "mrqa_squad-validation-2881", "mrqa_squad-validation-2941", "mrqa_squad-validation-2949", "mrqa_squad-validation-2975", "mrqa_squad-validation-2977", "mrqa_squad-validation-30", "mrqa_squad-validation-3111", "mrqa_squad-validation-3139", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3248", "mrqa_squad-validation-3269", "mrqa_squad-validation-3296", "mrqa_squad-validation-3377", "mrqa_squad-validation-3429", "mrqa_squad-validation-3483", "mrqa_squad-validation-3496", "mrqa_squad-validation-3534", "mrqa_squad-validation-3552", "mrqa_squad-validation-360", "mrqa_squad-validation-363", "mrqa_squad-validation-368", "mrqa_squad-validation-3705", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3776", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3885", "mrqa_squad-validation-3952", "mrqa_squad-validation-3981", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4070", "mrqa_squad-validation-4095", "mrqa_squad-validation-4107", "mrqa_squad-validation-4121", "mrqa_squad-validation-4121", "mrqa_squad-validation-4147", "mrqa_squad-validation-4191", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4516", "mrqa_squad-validation-457", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4689", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4758", "mrqa_squad-validation-4797", "mrqa_squad-validation-480", "mrqa_squad-validation-4840", "mrqa_squad-validation-4898", "mrqa_squad-validation-4988", "mrqa_squad-validation-4997", "mrqa_squad-validation-502", "mrqa_squad-validation-5029", "mrqa_squad-validation-5061", "mrqa_squad-validation-5096", "mrqa_squad-validation-5108", "mrqa_squad-validation-5200", "mrqa_squad-validation-5222", "mrqa_squad-validation-5270", "mrqa_squad-validation-5272", "mrqa_squad-validation-5287", "mrqa_squad-validation-531", "mrqa_squad-validation-5337", "mrqa_squad-validation-5344", "mrqa_squad-validation-5347", "mrqa_squad-validation-5382", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5549", "mrqa_squad-validation-5551", "mrqa_squad-validation-556", "mrqa_squad-validation-5621", "mrqa_squad-validation-5670", "mrqa_squad-validation-5741", "mrqa_squad-validation-5794", "mrqa_squad-validation-5839", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6000", "mrqa_squad-validation-6027", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6259", "mrqa_squad-validation-6260", "mrqa_squad-validation-6263", "mrqa_squad-validation-6277", "mrqa_squad-validation-6295", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-65", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6538", "mrqa_squad-validation-6548", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6600", "mrqa_squad-validation-6612", "mrqa_squad-validation-6624", "mrqa_squad-validation-664", "mrqa_squad-validation-6656", "mrqa_squad-validation-6657", "mrqa_squad-validation-6666", "mrqa_squad-validation-6695", "mrqa_squad-validation-6749", "mrqa_squad-validation-6858", "mrqa_squad-validation-6861", "mrqa_squad-validation-6878", "mrqa_squad-validation-6880", "mrqa_squad-validation-6888", "mrqa_squad-validation-689", "mrqa_squad-validation-6898", "mrqa_squad-validation-6900", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-6967", "mrqa_squad-validation-7018", "mrqa_squad-validation-7021", "mrqa_squad-validation-7033", "mrqa_squad-validation-7036", "mrqa_squad-validation-7062", "mrqa_squad-validation-7123", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7246", "mrqa_squad-validation-7268", "mrqa_squad-validation-7302", "mrqa_squad-validation-7312", "mrqa_squad-validation-7323", "mrqa_squad-validation-7357", "mrqa_squad-validation-7362", "mrqa_squad-validation-7373", "mrqa_squad-validation-738", "mrqa_squad-validation-7391", "mrqa_squad-validation-741", "mrqa_squad-validation-7450", "mrqa_squad-validation-7458", "mrqa_squad-validation-7466", "mrqa_squad-validation-7470", "mrqa_squad-validation-755", "mrqa_squad-validation-7562", "mrqa_squad-validation-7603", "mrqa_squad-validation-764", "mrqa_squad-validation-767", "mrqa_squad-validation-7686", "mrqa_squad-validation-7711", "mrqa_squad-validation-7736", "mrqa_squad-validation-7744", "mrqa_squad-validation-7765", "mrqa_squad-validation-7770", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7892", "mrqa_squad-validation-7902", "mrqa_squad-validation-7957", "mrqa_squad-validation-7970", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8232", "mrqa_squad-validation-8236", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8572", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8685", "mrqa_squad-validation-8696", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8743", "mrqa_squad-validation-8760", "mrqa_squad-validation-8763", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8794", "mrqa_squad-validation-8797", "mrqa_squad-validation-8832", "mrqa_squad-validation-8837", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-8965", "mrqa_squad-validation-904", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9236", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-9317", "mrqa_squad-validation-9330", "mrqa_squad-validation-938", "mrqa_squad-validation-9401", "mrqa_squad-validation-9426", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-9579", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9628", "mrqa_squad-validation-9744", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9817", "mrqa_squad-validation-9890", "mrqa_squad-validation-9892", "mrqa_squad-validation-9897", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7579"], "OKR": 0.87890625, "KG": 0.34765625, "before_eval_results": {"predictions": ["Thermochemical techniques", "executive producer", "1987", "Arley D. Cathey", "North", "one-eighth", "coastal beaches and the game reserves", "Vicodin", "\u00a34.2bn", "Katharina von Bora", "9 March 1508", "Shoushi Li", "his means of seizing power", "ideal strings", "after the Franco-German War", "German", "San Andreas Fault", "NYPD Blue", "Northern Chinese", "Oireachtas funds", "the Marconi Company", "countries with bigger income inequalities", "John Robert Cocker", "King Kelly", "a U.S. Army major and psychiatrist", "Richard Masur", "1988", "Bergen County", "The Ones Who Walk Away from Omelas", "hiphop", "Lithuanian national team", "historic buildings, arts, and published works", "Disney Parks Christmas Day Parade", "Esp\u00edrito Santo Financial Group", "David S. Goyer", "Wolf Creek", "YouTube", "onset and progression of Alzheimer's disease", "San Francisco 49ers", "Lake Placid, New York", "Iron Man 3", "Suffolk, England", "singer, songwriter, actress, and radio and television presenting", "Chicago", "Martin \"Marty\" McCann", "247,597", "Mandalay Entertainment", "actor", "a specialized version of the two-seat F/A-18F Super Hornet", "Barnoldswick", "Pacific War", "A41", "Heather Langenkamp", "Leona Lewis", "The Ministry of Utmost Happiness", "BAFTA TV Award Best Actor winner in 1956", "Rodney Crowell", "Andy Serkis", "having or seeing nosebleeds or bleeding", "a cat", "Pervez Musharraf", "cancer", "a toast made when two people get married", "a dieposition reaction to produce Na(s) and N2(g)"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6646730006105006}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7887", "mrqa_squad-validation-7974", "mrqa_squad-validation-2976", "mrqa_squad-validation-1480", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3822", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-1133", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-7417", "mrqa_newsqa-validation-850", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-5418"], "SR": 0.578125, "CSR": 0.6505681818181819, "EFR": 1.0, "Overall": 0.7152698863636363}, {"timecode": 11, "before_eval_results": {"predictions": ["April 1887", "semi-arid savanna", "2014", "Levi's Stadium", "misguided", "San Jose Marriott", "1972", "second-largest global producer", "Decision Time", "Victorian Government", "the American Revolutionary War", "pep", "human", "the Treaties establishing the European Union", "to achieve trans-lunar injection", "Amazonia", "Daniel Andrews", "UNESCO's World Heritage list", "Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley", "31\u20130", "NCAA's Division I", "Mark Helfrich", "Wal-Mart Canada Corp.", "\"Louie\" Zamperini", "Che Guevara", "Carol Ann Duffy", "Karl-Anthony Towns Jr.", "1978", "Danish", "Ukrainian", "\"Life in Hell\" (1977\u20132012)", "\"John\" Alexander Florence", "\"brainwash\")", "9Lives", "\"valley of the hazels'", "Art Bell", "\"The Chosen One\"", "Jon Bellion, and Bebe Rexha", "Point Place", "Knowlton School", "Delilah Rene", "Don Bluth", "Columbus", "the Czech Kingdom", "Anne Fletcher", "Sacramento Kings", "South Asian Games", "Tufts College", "Harrods", "Flamingo Las Vegas", "Ben Savage", "\"Crazy in Alabama\"", "City of Newcastle", "Japan", "Canada", "in positions 14 - 15, 146 - 147 and 148 - 149", "private sector to artisan and agricultural production / trade", "silver", "arachnology", "U.S.", "Republican Gov. Jan Brewer", "\"Annie Get Your Gun\"", "New York", "Hebrew"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7074313860159448}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.0, 0.0, 0.0, 0.15384615384615385, 0.8571428571428571, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8447", "mrqa_squad-validation-7288", "mrqa_squad-validation-4015", "mrqa_squad-validation-7885", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4371", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-818", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-3580", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7583"], "SR": 0.578125, "CSR": 0.64453125, "EFR": 1.0, "Overall": 0.7140625}, {"timecode": 12, "before_eval_results": {"predictions": ["late 1545", "several hundred thousand, some 30%", "five", "every two years", "two-man", "The Hoppings", "Mycobacterium tuberculosis", "C. J. Anderson", "Harvey Martin", "stratigraphic", "geographer Ellen Churchill Semple argued that even though human beings originated in the tropics they were only able to become fully human in the temperate zone", "Wellington", "a problem instance", "the vehicle that is at rest or the outside world", "a sin", "for translation initiation in most chloroplasts and prokaryotes", "\"Isel\"", "Edmonton, Canada", "Anthony Stephen Burke", "O'Neill began to retire from Broadway's commercial pressures and increasing critical backlash in the mid-1930s", "Max Kellerman", "was an institution of higher education in the United States designated by a state to receive the benefits of the Morrill Acts of 1862 and 1890", "VfB Stuttgart", "July 22, 1946", "Julia Verdin", "Pendlebury", "(born 19 January 1980)", "Bismarck", "Comedy Film Nerds", "2016 World Indoor Championships", "MG Cars", "January 18, 1977", "North Greenwich Arena", "The Soloist", "Nikita Khrushchev", "Lipshitz", "Prime Minister of Pakistan", "February 18, 1965", "automobiles", "Republican", "J35", "Chad", "NBA All-Star Game and All-NBA Team", "Emilia Fox", "Freeform", "St James's Palace", "Cristiano Ronaldo", "American", "Chief Strategy Officer", "November 15, 1903", "De La Soul", "American", "Archbishop of Canterbury", "Via Vai", "1985", "after releasing Xander from the obligation to be Sweet's `` bride ''", "when the cell is undergoing the metaphase of cell division", "California", "(multi-user dungeon)", "Gulf of Aden", "Iran", "(Halle Berry", "Sindbad", "Abid Ali Neemuchwala"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7499090982546865}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.9411764705882353, 1.0, 1.0, 0.4, 0.19047619047619047, 1.0, 0.6, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-914", "mrqa_squad-validation-9779", "mrqa_squad-validation-10328", "mrqa_squad-validation-8852", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-3242", "mrqa_newsqa-validation-642", "mrqa_searchqa-validation-13537"], "SR": 0.65625, "CSR": 0.6454326923076923, "EFR": 1.0, "Overall": 0.7142427884615385}, {"timecode": 13, "before_eval_results": {"predictions": ["Tower District", "computational", "social and political action", "Duran Duran", "30", "NYPD Blue", "chemical bonds", "John and Benjamin Green", "Lippe", "between AD 0\u20131250", "2 million", "a statement", "40,000", "Citadel Broadcasting", "$45,000", "stream capture", "1,149 feet", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "Robert Remak", "Eddie Murphy", "Audrey II", "Human fertilization", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time", "The terrestrial biosphere", "Peter Andrew Beardsley MBE", "Leo Arnaud", "lumbar cistern", "not being pushed around by big labels, managers, and agents and being told what to do", "1 US dollar", "digitization of social systems", "Yugoslavia", "Middlesex County", "Sweden had been an active supporter of the League of Nations", "2002", "lightning", "they were weaker when it came to training and tertiary education", "instant messenger", "George Strait", "silk, hair / fur", "Anakin", "Prince James", "104 colonists and Discovery", "wisdom, understanding, counsel, fortitude", "Florida", "Toronto", "Manchuria", "Ben Savage", "at a given temperature", "Max", "Magyarorsz\u00e1g z\u00e1szlaja", "the church at Philippi", "2003", "The Sun", "northern China", "Mackinac Bridge", "Barbarella", "Bergen", "Balvenie Castle", "have been accused of running Zimbabwe's economy into the ground while implementing a draconian crackdown aimed at keeping power", "repression and dire economic circumstances", "Leon Trotsky", "oregon", "Elizabeth Gaskell", "oregon"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6487771776834277}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.8205128205128205, 0.5454545454545454, 1.0, 1.0, 0.1904761904761905, 0.14285714285714285, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 0.2222222222222222, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1600", "mrqa_squad-validation-3599", "mrqa_squad-validation-4304", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-6020", "mrqa_hotpotqa-validation-877", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-16103", "mrqa_hotpotqa-validation-3149"], "SR": 0.5625, "CSR": 0.6395089285714286, "EFR": 0.9642857142857143, "Overall": 0.7059151785714286}, {"timecode": 14, "before_eval_results": {"predictions": ["reaffirmed Catholicism as the state religion of France", "reached an all-time high between 2005 and 2010", "Shropshire", "a desired social goal", "cartels", "Anglo-Saxon", "Ancient Egypt", "Battle of Olustee", "patrimonial feudalism", "Daniel arap Moi", "near Millingen aan de Rijn, close to the Dutch-German border with the division of the Rhine into Waal and Nederrijn", "an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing", "1303", "oxygen", "three, later four, classes with the Han Chinese occupying the lowest rank", "Lance Cpl. Maria Lauterbach", "1994", "Empire of the Sun", "54 bodies", "Roger Federer", "sodium dichromate", "the two remaining crew members", "July", "citizenship", "40", "18 years to life in prison", "to alleviate the flooding", "Expedia", "Kabul", "\"We've got a long way to go, but we've made progress.\"", "Communist Party of Nepal", "Bob Dole", "Eden Park", "in her home", "12.3 million people worldwide", "as soon as 2050", "behind the counter", "Osan Air Base", "18", "National Park Service", "3-2", "Bob Bogle", "40 militants and six Pakistan soldiers dead", "1959", "Pakistan's High Commission in India", "his father", "Obama's race", "Steven Chu", "the Obama administration", "Ed McMahon", "a peace sign", "Muslim festival of Eid al-Adha", "Larry Ellison", "Revolutionary Armed Forces of Colombia", "letters about his life", "Jules Shear", "Soviet Union", "Vienna", "2008\u201309 UEFA Champions League", "310", "White River Valley Chamber of Commerce", "Babylon", "2017", "Algernod Lanier Washington"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6288112928737928}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false], "QA-F1": [0.08333333333333333, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.7499999999999999, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3163", "mrqa_squad-validation-1886", "mrqa_squad-validation-1064", "mrqa_squad-validation-7017", "mrqa_squad-validation-5276", "mrqa_squad-validation-9076", "mrqa_squad-validation-1287", "mrqa_squad-validation-8077", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-1020", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-324", "mrqa_naturalquestions-validation-7056", "mrqa_hotpotqa-validation-2080", "mrqa_searchqa-validation-4684", "mrqa_hotpotqa-validation-5370"], "SR": 0.515625, "CSR": 0.63125, "EFR": 1.0, "Overall": 0.71140625}, {"timecode": 15, "before_eval_results": {"predictions": ["Robert R. Gilruth's", "Arthur Woolf", "Ten", "multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "seven", "Ealy", "St. George's United Methodist Church", "1914", "Wales", "more than 70,000", "Nikita Khrushchev", "increasing unemployment", "X is no more difficult than Y, and we say that X reduces to Y", "March 22", "he acted in self defense in punching businessman Marcus McGhee.", "anyone", "cancer", "in Spain and at Harvard Law School.", "sanctions 17 entities, including three government-owned or controlled companies used by Mugabe and his government \"to illegally siphon revenue and foreign exchange from the Zimbabwean people,\" as well as one individual.\"", "Senate Democrats", "15,000", "Kim \"ordered all military units to halt field exercises and training and return to their bases.\"", "Tim Baker", "A 22-year-old college student in Boston, Massachusetts,", "Democratic", "the IV cafe", "North Korea intends to launch a long-range missile in the near future,", "the District of Columbia National Guard,", "forgery and flying without a valid license,", "work is the hardest and least rewarding work we have ever tried to do.", "14", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "school", "Monday and Tuesday", "27-year-old", "almost 100 vessels off Somalia's coast", "American third seed Venus Williams", "allergies to peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy", "more than two years,", "Manchester United", "Nafees Syed", "Robert Barnett,", "military commissions are inherently illegitimate, unconstitutional and incapable of delivering outcomes we can trust.", "Alfredo Astiz,", "American girl,", "Illness", "procedures", "Jaime Andrade", "56", "Michael Jackson's", "Raymond Thomas,", "racially motivated", "Adam Lambert", "in a far-off land before they even knew each other.", "production of peaches as early as 1571, with exports to other states occurring around 1858", "along the coast of northern California", "citric acid", "Denali", "The New Yorker", "death", "the Mason-Dixon Line", "high and dry", "palace on US soil", "New Orleans Saints"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6186888220493854}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 0.0, 0.0, 0.0, 0.0, 0.05128205128205128, 0.0, 0.5, 0.10526315789473684, 0.0, 0.30769230769230765, 1.0, 0.0, 1.0, 0.888888888888889, 0.2857142857142857, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.16666666666666669, 1.0, 1.0, 1.0, 0.6666666666666666, 0.17391304347826086, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-698", "mrqa_squad-validation-1748", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-2770", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-6596", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-3685"], "SR": 0.515625, "CSR": 0.6240234375, "EFR": 1.0, "Overall": 0.7099609375}, {"timecode": 16, "before_eval_results": {"predictions": ["\"exterminate\" all non-Dalek beings", "San Diego", "30\u201375%", "to implement Islamic values in all spheres of life.\"", "James Watt", "comb jellies", "NewcastleGateshead", "1989", "priest", "Denmark, Iceland and Norway", "the need for alliances", "Anderson", "1969", "Debbie Gibson", "collect", "the shooter must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "the negative of the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "the UK version of The X Factor", "April 6, 1917", "The Fellowship of the Ring", "Montgomery", "at birth", "the power to enact laws without the involvement of the Reichstag", "December 1, 2009", "Miami Heat", "Jim Capaldi, Paul Carrack, and Peter Vale", "Alan Shearer", "The 1700 Cascadia earthquake", "the Central and South regions", "Portugal", "the blood to the liver", "in 1936", "two - year terms", "students", "September 19, 2017", "Andy Serkis", "The Abbott and Costello Show", "John Smith", "Idaho", "Ali", "Abraham Gottlob Werner", "Jason Paige", "Jackie Robinson", "Rufus and Chaka Khan", "the body - centered cubic ( BCC ) lattice", "Michelle Stafford", "the music of the Dominican Republic is primarily influenced by West African traditions, with some minor European, and native Taino influences", "3", "Olivia Olson", "erosion", "the number of legitimate birth certificate versions in use exceeded 14,000", "a integral membrane protein", "Conrad Lewis", "Justice Harlan", "a person employed to write or type what another dictates or to copy what has been written by another", "Purple Rain", "Oakland, California", "World Famous Gold & Silver Pawn Shop", "Crandon, Wisconsin,", "a racially-tinged remark made by his former caddy,", "Psycho", "the colorless", "Jimmy Carter", "yellow"], "metric_results": {"EM": 0.546875, "QA-F1": 0.645281498015873}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false], "QA-F1": [0.888888888888889, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.42857142857142855, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7778", "mrqa_squad-validation-9610", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-3385", "mrqa_triviaqa-validation-1394", "mrqa_hotpotqa-validation-5030", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-3751", "mrqa_triviaqa-validation-262"], "SR": 0.546875, "CSR": 0.6194852941176471, "EFR": 1.0, "Overall": 0.7090533088235295}, {"timecode": 17, "before_eval_results": {"predictions": ["diversity", "\"Provisional Registration\"", "Tyne and wear Metro", "the phlogiston", "Creon", "QuickBooks", "1892", "The coordinating lead authors", "Six", "They can be combined with ideal pulleys", "\u00d6gedei Khan", "Princes Park", "47", "Polk", "Mrs. Eastwood & Company", "first train robbery", "Las Vegas", "attorney", "Owsley Stanley", "The visit", "Hong Kong First Division League", "Unbreakable", "He can play as a striker or left striker", "\"The Maze Runner\"", "Agra", "1.6 million passengers", "film actress", "Gaius Julius Caesar Augustus Germanicus", "Jeff Van Gundy", "Joachim Trier", "Tamil", "1972", "2013", "Golden Globe Award for Best Actor", "Ronald Lyle \" Ron\" Goldman", "1", "Cleopatra VII Philopator", "political", "Tomorrowland", "16,116", "footballer", "Bishop's Stortford", "Jessica Ashley Karpov", "late eighteenth century", "The School Boys", "Operation Iceberg", "Texas Longhorns", "Hordaland", "1968", "160", "\"Pierement Waltz\"", "October 22, 2012", "special effects technician and director", "Noah Schnapp", "on about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "Neptune", "Lyon, France", "70,000 or so", "Miami Beach, Florida", "They No Man Has Gone Before", "Estonian", "Copenhagen", "Aristophanes", "The Killer Angels"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6885106646825396}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.875, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2788", "mrqa_squad-validation-10479", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-1308", "mrqa_naturalquestions-validation-7408", "mrqa_newsqa-validation-1720", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-3630", "mrqa_searchqa-validation-3516"], "SR": 0.5625, "CSR": 0.6163194444444444, "EFR": 1.0, "Overall": 0.7084201388888889}, {"timecode": 18, "before_eval_results": {"predictions": ["Lagos and Quiberon Bay", "seven months old", "Sava Kosanovi\u0107", "return to his side", "\"Monte Carlo\")", "huge", "Graz", "5,792", "Lucas\u2013Lehmer", "the Saudi-interpretation", "February 26, 1948", "Hordaland", "the town of El Nacimiento in M\u00fazquiz Municipality", "Stephen James Ireland", "Koch Industries", "Washington", "the Isle of Man", "2010", "High Falls Brewery", "technical director", "Mike Holmgren", "Nathan Bedford Forrest", "Hyuna", "green and yellow", "Isobel", "Urijah Faber", "Barack Obama's", "Guthred", "Peel Holdings", "May 26, 2010", "College Football Scoreboard", "Marko Tapani \" Marco\" Hietala", "In 2017, Pachulia won his first NBA Championship as a member of the Warriors.", "Sarah Hurst", "An invoice, bill or tab", "Anita O'Day", "Clarence Nash", "Minneapolis", "Marco Fu", "Syracuse University", "Durban International Convention Centre", "Ryan Babel", "Bob Dylan", "a wooden roller roller ride located at Lakemont Park in Altoona, Pennsylvania", "Luca Guadagnino", "Jennifer Lynne \" Jennings\" Brown", "11 Grands Prix wins", "National Collegiate Athletic Association", "Bill Cosby", "thirteen", "Argentinian", "Dana Fox", "Sunday, November 2, 2003", "northwest Washington", "Saint Martins", "Canada", "The Mayor of Casterbridge", "on Friday", "Ferrari president Luca di Montezemolo", "Social Democratic", "The first European sugar preserves made use of that seemingly magical substance, honey.", "sexual harassment", "President Obama and Britain's Prince Charles", "Gloria Allred,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5734871031746032}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.14285714285714285, 0.5714285714285715, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6117", "mrqa_squad-validation-9057", "mrqa_squad-validation-9255", "mrqa_squad-validation-8020", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-4757", "mrqa_naturalquestions-validation-6658", "mrqa_triviaqa-validation-4403", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-2163", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-3464", "mrqa_newsqa-validation-2844"], "SR": 0.484375, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.70703125}, {"timecode": 19, "before_eval_results": {"predictions": ["CEPR", "the Commission and Council", "14,000", "scoil phr\u00edobh\u00e1ideach", "90 to 95 percent", "R\u00fcdesheim am Rhein and Koblenz", "56.2%", "time", "Christ's message and teachings", "Bayern Munich", "fifth", "five", "A123 Systems, LLC", "\"the backside.\"", "Bothtec", "the Manhattan Project", "18 December 1975", "3,000", "youngest TV director ever", "Thom Yorke", "About 200", "Marco Fu", "Orfeo ed Euridice", "Fifteenth Season", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn", "Golden Calf", "Houston Rockets", "Summerlin, Clark County,", "Argentinian", "Europe", "Noel Gallagher", "Savannah River Site", "a family member", "Switzerland", "second largest", "Frank Lowy", "Fat Man", "Robert Marvin \"Bobby\" Hull, OC", "Nye County", "Herman's Hermits", "Mani", "Pendlebury, Lancashire", "300 km north west", "1932", "Arrowhead Stadium", "1885", "House of Borromeo", "a system of processing conflicts in which outcomes depend on what participants do, but no single force controls what occurs and its outcomes.", "Sydney", "Michael Redgrave", "KlingStubbins", "Big 12", "Argand", "2017", "RAF", "John McEnroe", "comoide", "the deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "a one-shot victory in the Bob Hope Classic on the final hole", "\"a system of control\"", "Dune", "fire", "\"teenagers in Versailles.\"", "Jupiter"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7074385683760684}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7001", "mrqa_squad-validation-9093", "mrqa_squad-validation-1665", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-4172", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-119", "mrqa_naturalquestions-validation-954", "mrqa_triviaqa-validation-3281", "mrqa_newsqa-validation-1445", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-2686"], "SR": 0.640625, "CSR": 0.6109375, "EFR": 0.9565217391304348, "Overall": 0.698648097826087}, {"timecode": 20, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1582", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2022", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2295", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3693", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5718", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4155", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-402", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-642", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-1723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10115", "mrqa_squad-validation-10249", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10328", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1064", "mrqa_squad-validation-1064", "mrqa_squad-validation-1114", "mrqa_squad-validation-1162", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1255", "mrqa_squad-validation-1296", "mrqa_squad-validation-1366", "mrqa_squad-validation-1480", "mrqa_squad-validation-1529", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1597", "mrqa_squad-validation-1723", "mrqa_squad-validation-1748", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-2040", "mrqa_squad-validation-2059", "mrqa_squad-validation-2111", "mrqa_squad-validation-2144", "mrqa_squad-validation-2153", "mrqa_squad-validation-2315", "mrqa_squad-validation-234", "mrqa_squad-validation-2475", "mrqa_squad-validation-2536", "mrqa_squad-validation-2568", "mrqa_squad-validation-2628", "mrqa_squad-validation-2701", "mrqa_squad-validation-2786", "mrqa_squad-validation-287", "mrqa_squad-validation-2898", "mrqa_squad-validation-2976", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3198", "mrqa_squad-validation-3240", "mrqa_squad-validation-3483", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4015", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4259", "mrqa_squad-validation-4263", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-4545", "mrqa_squad-validation-457", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4646", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5200", "mrqa_squad-validation-5276", "mrqa_squad-validation-531", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5542", "mrqa_squad-validation-556", "mrqa_squad-validation-5611", "mrqa_squad-validation-5655", "mrqa_squad-validation-5678", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6263", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-6500", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6523", "mrqa_squad-validation-6538", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6657", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-689", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-7001", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7062", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7152", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7302", "mrqa_squad-validation-7302", "mrqa_squad-validation-7323", "mrqa_squad-validation-7391", "mrqa_squad-validation-7403", "mrqa_squad-validation-7450", "mrqa_squad-validation-7603", "mrqa_squad-validation-7692", "mrqa_squad-validation-7732", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8077", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8219", "mrqa_squad-validation-8225", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8447", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-8612", "mrqa_squad-validation-864", "mrqa_squad-validation-8668", "mrqa_squad-validation-8685", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-876", "mrqa_squad-validation-8836", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9061", "mrqa_squad-validation-9076", "mrqa_squad-validation-9101", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9240", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9652", "mrqa_squad-validation-9716", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9956", "mrqa_squad-validation-9991", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4403", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7642"], "OKR": 0.873046875, "KG": 0.409375, "before_eval_results": {"predictions": ["Bento de Moura Portugal", "Mongolia", "polynomial-time reduction", "All India Muslim League", "CBS Sports.com", "1967", "an antigen from a pathogen", "Encoded Archival description (EAD)", "Trey Parker and Matt Stone", "International Federation of Competitive eating", "Democratic Unionist Party", "local South Australian and Australian produced content", "\"Naked Killer\" (1992)", "7 June 1926 to 17 December 1926", "Games of the Olympiad", "2000", "John Lee Hancock", "Hordaland", "Agent 99", "Bonnie Franklin", "Edmonton, Alberta", "Skyscraper", "Love Actually", "a creek", "Orange County, California", "URO VAMTAC", "The bald eagle", "32", "fifty-word", "Philadelphia Naval Shipyard", "The Books", "Rochdale", "2013\u201314 Premier League", "Clara Petacci", "Jamie Fraser", "Lionel Brockman Richie Jr.", "The Two Noble Kinsmen", "Eucritta melanolimnetes", "Jenji Kohan", "Johnson & Johnson", "Internet Let's Play celebrities", "Germanic", "November 5, 2002", "Tamara Ecclestone Rutland", "Thomas Joseph \"T. J.\" Lavin", "Stalybridge Celtic", "Adelaide Lightning", "Kohlberg K Travis Roberts", "\"My Love from the Star\"", "Tottenham", "Sam Bettley", "Don DeLillo", "Nia Kay", "The Fixx", "season seven", "1876", "dynamite", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "\"talk and die\"", "The Humayun's Tomb", "teeth", "Profit maximization happens when marginal cost is equal to marginal revenue", "electron shells", "1901"], "metric_results": {"EM": 0.5, "QA-F1": 0.6183493589743589}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.25, 0.8, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1764", "mrqa_squad-validation-525", "mrqa_hotpotqa-validation-698", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-4819", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-7737", "mrqa_triviaqa-validation-7266", "mrqa_newsqa-validation-2263", "mrqa_searchqa-validation-9565", "mrqa_naturalquestions-validation-3295"], "SR": 0.5, "CSR": 0.6056547619047619, "EFR": 1.0, "Overall": 0.7237090773809524}, {"timecode": 21, "before_eval_results": {"predictions": ["a Serbian Orthodox priest", "December 12", "the Hostmen", "Apollo 8", "Antigone", "euglenophyte", "Sugarfoot", "\"Crossed: Family Values\"", "Forbes", "Mitsubishi Motors", "Tabasco", "1 January 1788", "Lowestoft", "Christopher Nolan", "Taylor Swift", "Al Horford", "Johan Leysen", "1854", "Swiss", "Pittsburgh Steelers", "professional footballer", "Logar", "Lauren Alaina", "Ian Fleming", "Mary Bonauto, Susan Murray, and Beth Robinson", "27 November 1956", "a split 7", "Hindi", "United States Auto Club", "The Clash of Triton", "Albany High School", "BAFTA TV Award Best Actor", "\"The Bob Edwards Show\"", "the heaviest album of all", "John Vereker, 6th Viscount Gort", "15,024", "Prime Minister of Denmark 1852\u20131853", "all-time", "the Cylon Number Six", "2007", "3,000", "Chinese Coffee", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Mineola", "KWPW", "John Richard Schlesinger, CBE", "U.S. Representative", "Blue", "Esperanza Emily Spalding", "the Kree", "Big & Rich", "1916", "American", "18", "one - mile - wide ( 1.6 km )", "David Jason", "olea europaea", "Turkey", "Michael Jackson may soon return to the stage, at least for a \"special announcement.\"", "Entourage", "the Tet Offensive", "Erica Rivera", "Spanish missionaries", "Madison"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6997328192640693}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.888888888888889, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8651", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-2367", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-1961", "mrqa_naturalquestions-validation-3108", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-3348"], "SR": 0.5625, "CSR": 0.6036931818181819, "EFR": 0.9285714285714286, "Overall": 0.7090310470779221}, {"timecode": 22, "before_eval_results": {"predictions": ["during the later decades of the 17th century", "WLQP-LP", "Sunni extremist groups such as Al-Qaeda and the Taliban", "mid-Eocene", "the Soviet Union", "enthusiasm", "the 34th President of the United States", "receive the benefits of the Morrill Acts of 1862 and 1890", "Prussian", "Violet", "German", "Shameless", "Indianola", "the gods themselves", "Nassau County", "What Are Little Boys Made Of", "Andries Jonker", "President John F. Kennedy", "Mollie Elizabeth King", "1959", "129,007", "San Francisco 49ers", "Big Machine Records", "the Parthian Empire", "3 million people", "Matt Groening", "June 10, 1982", "Philip K. Dick", "John Anthony \"Jack\" White", "Samuel Burl \"Sam\" Kinison", "Boston", "Lisa", "perjury and obstruction of justice", "Galleria Vittorio Emanuele II", "Paul Avery", "31 October 1783", "Puli Alam", "the Peninsular War in Spain", "1838", "Margaret Thatcher", "Ashland is home to Scribner-Fellows State Forest", "Manchester Victoria station in air rights space", "the east of Ireland", "Estadio de L\u00f3pez Cort\u00e1zar", "March 21, 2004", "Jesus", "Agent Carter", "Plies", "Tim \"Ripper\" Owens", "Walt Disney Productions", "chalk quarry", "Rhode Island", "electron donors", "Johnson", "Sarafina", "Peter Townsend.", "River Thames", "the Sunday Post", "the lower house of parliament", "death of cardiac arrest on June 25.", "Stephen Johns", "dugout canoe", "$400", "the place to be for U.S. candidates"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7500278520499108}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.9411764705882353, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-968", "mrqa_squad-validation-3754", "mrqa_squad-validation-9647", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-3736", "mrqa_naturalquestions-validation-5288", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-2945", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-13831", "mrqa_searchqa-validation-6928"], "SR": 0.640625, "CSR": 0.6052989130434783, "EFR": 1.0, "Overall": 0.7236379076086956}, {"timecode": 23, "before_eval_results": {"predictions": ["MBH99 reconstructions", "the Neckar", "rotors", "Pittsburgh Steelers", "14,000", "Scandinavia and northern Europe", "1911", "ESPN's \"SportsCenter\"", "Prince Antoni Radziwi\u0142\u0142", "northern Italy's Lombardy region", "Caesars Palace Grand Prix", "Ford Field in Detroit, Michigan", "Wilton Mall at Saratoga", "the Sun", "Point of Entry", "Wilmette, Illinois", "Malayalam", "Pendlebury, Lancashire", "Leona Lewis", "Laura Dern", "Melbourne", "democracy and personal freedom", "Cool Runnings", "Indian", "Labour", "Thor", "Copa Airlines", "Chiltern Hills", "Rudebox", "Seattle", "Massachusetts", "Edward James Olmos", "ghaedheil", "Slaughterhouse-Five", "Nashville", "simple language", "Telugu and Tamil", "Peshwa", "Charles VI", "Oracle Corporation", "1999", "William Shakespeare", "January 23, 1898", "1953", "Bergen", "Scribner", "Alleyne v. United States", "Oregon State Beavers", "Jack Elam", "1907", "The Design Inference", "R&B vocal group", "pilgrimages to Jerusalem", "art pottery", "a song about restoring someone's faith in love and family relationships", "California Chrome", "Moby Dick", "Christine Keeler", "137", "a small-minded town.", "Sunday", "Jackie Robinson", "the DEW Line", "a narrow fellow in the grass"], "metric_results": {"EM": 0.625, "QA-F1": 0.7169199039264829}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false], "QA-F1": [0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.3333333333333333, 1.0, 1.0, 0.5, 0.8571428571428571, 0.5, 0.0, 0.7368421052631577, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8702", "mrqa_squad-validation-3469", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-4467", "mrqa_hotpotqa-validation-994", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-354", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2866", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-3186", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1059", "mrqa_searchqa-validation-10472"], "SR": 0.625, "CSR": 0.6061197916666667, "EFR": 0.9583333333333334, "Overall": 0.71546875}, {"timecode": 24, "before_eval_results": {"predictions": ["Los Angeles", "variously combustion chamber", "13th-century", "spinat", "broken arm", "Bury F.C.", "Las Vegas", "Suzuki YZF-R6", "Ahold N.V.", "East Lothian", "Gettysburg Address", "Engineering", "Robert \"Bobby\" Germaine, Sr.", "American 3D computer-animated comedy", "Asia-Pacific War", "Amy Poehler", "professional footballer", "British Labour Party", "USC Marshall School of Business", "Theme Hospital", "1937", "The Wolf of Wall Street", "Maxwell Smart", "The Walking Dead", "2008", "Yasir Hussain", "\"Feels Like Love\"", "Ronald Joseph Ryan", "Elena Verdugo", "soccer", "Peel Holdings", "Chechen Republic", "alcoholic drinks for consumption on the premises", "Zaire", "Debbie Harry", "Michael Burger", "novelist and poet", "1986", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Hilary Duff", "100 million", "an Albanian political party in Montenegro", "2015 Masters Tournament", "John Schlesinger", "Venice", "Rockstar San Diego", "A.S. Roma", "genderqueer", "Gothic Revival", "Melbourne's City Centre", "South West Peninsula League", "Bury St Edmunds", "1608", "metamorphic rock", "Rugrats in Paris", "auk", "zeny juice", "Rose-Marie", "Sen. Barack Obama", "A Colorado prosecutor", "World number two Roger Federer", "pink", "calcium", "Anne Frances Reagan (Robbins)"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6970238095238095}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.8, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.8, 0.22222222222222224, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-3202", "mrqa_squad-validation-10449", "mrqa_squad-validation-306", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1741", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-3934", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-9626", "mrqa_triviaqa-validation-6356", "mrqa_newsqa-validation-1361", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-14782"], "SR": 0.5625, "CSR": 0.604375, "EFR": 1.0, "Overall": 0.723453125}, {"timecode": 25, "before_eval_results": {"predictions": ["August 15, 1971", "Geordie", "Peter Howell", "many elements", "formalize a unified front in trade and negotiations with various Indians", "French", "New York City", "2017", "Logan International Airport", "GZA", "Dziga Vertov", "no. 3", "John John Florence", "Two Is Better Than One", "July 16, 1971", "Microsoft Office file formats", "Baldwin in Nassau County, New York, United States", "Sir Elton Hercules John", "Firestorm", "the Ruul", "March 14, 2000", "David Wells", "Northern Lights", "the Chengdu Aircraft Corporation (CAC) of China", "Michael Cremo", "Minnesota", "Oklahoma", "I Should Have Known Better", "Smithfield, Rhode Island, U.S.", "Julie Taymor", "Friday 11 March 2011", "Columbia Records", "1943", "Maria Brink", "the traditional song \"The Braes o' Bowhether\"", "Cody Miller", "Darkroom", "\"Tainted Love\"", "Christopher Nolan", "\"The Blue Album\"", "1992", "2016 United States elections", "bushwhackers", "Princes Park in Melbourne", "The Late Late Show", "2012", "1978", "Donald Carl \"Don\" Swayze", "John Morgan", "June", "an organ", "the People's Republic of China", "49 cents", "Certificate of Release or Discharge from Active Duty", "Jocelyn Flores", "Hyperbole", "Egypt", "a tiger", "1220", "June 2002", "1991-1993", "Thomas Nast", "ice hockey", "Yo soy Betty, la fea"], "metric_results": {"EM": 0.625, "QA-F1": 0.7356837606837607}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.2222222222222222, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.4, 1.0, 0.46153846153846156, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-3386", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-1718", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-2092", "mrqa_triviaqa-validation-3925", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1230", "mrqa_searchqa-validation-172"], "SR": 0.625, "CSR": 0.6051682692307692, "EFR": 0.9583333333333334, "Overall": 0.7152784455128205}, {"timecode": 26, "before_eval_results": {"predictions": ["Italian physicist", "illiberal Islamic regimes", "The Judicial Council", "CBS", "berceuse", "Blue Jean", "Ibex", "Hebridean", "prostate", "Vitus Bering", "fuel", "Burundi", "larva", "nails", "Der Zauberberg", "zhaleika", "Canada", "Komodo Dragon", "a laparoscope", "Billy Bob Thornton", "Ice Cream Salesman", "won't get you a guppy", "Last Summer", "Infrared", "a tiger", "Isis", "Eliza", "tendonitis", "En banc", "Franklin D. Roosevelt", "six ounces", "Violeta Barrios de Chamorro", "Take My Breath Away", "Rafael Nadal", "Canberra", "Ich bin ein Berliner", "\"Caesar's wife must be above suspicion.\"", "Good fiction", "Neverbeen Kissed", "Antichrist", "William Augustus, Duke of Cumberland", "Harry Belafonte", "Nanjing", "asparagus beetles", "Auf Wiedersehen", "blubber", "catalysts", "the L-M version of the. Stanford-Binet", "Germany", "Deep Purple", "Jesus, Moses, the Buddha, and Mohammed", "Ernie Klump", "to solve its problem of lack of food self - sufficiency", "Speaker of the House of Representatives", "Gorlice -- Tarn\u00f3w Offensive of the Central Powers against the Russian army", "20 numbered", "Time Bandits", "sedpies", "Kerry Butler", "35", "three centuries", "Consumer Product Safety Commission", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "United Nations"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5361505681818182}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.16666666666666666, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15646", "mrqa_searchqa-validation-10928", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-2775", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-11676", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-6143", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-10252", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-7196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-2819", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1748"], "SR": 0.421875, "CSR": 0.5983796296296297, "EFR": 1.0, "Overall": 0.7222540509259259}, {"timecode": 27, "before_eval_results": {"predictions": ["May 18, 1756", "canceled", "modern fashion", "more than half of the global wealth", "Lismore", "STS-51-L", "Paradise, Nevada", "English", "Newcastle upon Tyne, England", "Colonel", "Cody Miller", "Kentucky", "Hertz Corporation", "Wiz Khalifa", "Disney California Adventure", "Maria Brink", "The Sound of Music", "G\u00f6tene in Sweden", "Argentine", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present", "monthly", "6teen", "South America", "Princes Park", "Glenn Close", "the Knight Company", "My Gorgeous Life", "Ashanti Region", "the Dutch Empire", "Culiac\u00e1n, Sinaloa", "Northampton, England", "Black Panthers", "Fred Willard", "a beer", "Dara Grace Torres", "nine", "1909", "the E22", "3,384,569", "an anvil firing", "House of Hohenstaufen", "James G. Kiernan", "Johnnie Ray", "Forrest Gump", "2009", "1919", "a skerry", "1620 to 1691", "the exiled House of Stuart", "Mickey Gilley's Club", "Blue Origin", "2015 Orange Bowl", "2017", "Eleanor Roosevelt", "Utah, Arizona, Wyoming, and Oroville, California", "a company that does good things for the world", "the moon", "English rock group the Kinks", "punching businessman Marcus McGhee.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "1975", "spermaceti", "libraries", "NASA"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6848370927318296}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.28571428571428575, 0.4, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7559", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-1233", "mrqa_hotpotqa-validation-296", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4650", "mrqa_hotpotqa-validation-1965", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-4645", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-2829", "mrqa_triviaqa-validation-1527", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2586", "mrqa_searchqa-validation-3279"], "SR": 0.578125, "CSR": 0.59765625, "EFR": 1.0, "Overall": 0.7221093749999999}, {"timecode": 28, "before_eval_results": {"predictions": ["British patrons", "Laverne & Shirley", "South", "Roger Bacon", "bacteria", "apogee", "Pitney Bowes", "apogee", "\"spare the rod\"", "apogee", "Qubec", "Edith Piaf", "the Krntnertor Theater", "Sappho", "apogee", "Colorado River", "Hershey", "Timothy Leary", "apogee", "thought Police", "The Street Lawyer", "a nose", "apogee", "\"Bridges of Madison County", "a 1.5 km", "calcium", "David Beckham", "Wisconsin", "Raphael", "To Build a Fire", "auk", "Around 2500 B.C.", "a nave", "centigrade", "silver", "BBC", "the jackass penguin", "Jack", "Blackwater USA", "apogee", "Nicky Hilton", "December", "Arsinoe", "Hadrosaurus", "\"E-T\"", "a Contra", "a root canal", "asthma", "a duck", "a trumpet", "Narcissus", "Marion", "solids", "its population", "Sarah Silverman", "Anwar Sadat", "armada", "Barings Bank", "Esp\u00edrito Santo Financial Group", "Earvin \"Magic\" Johnson Jr.", "Elliot", "a free laundry service", "drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "city of Quebradillas"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4960700757575757}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.18181818181818182, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-11819", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-13291", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-8603", "mrqa_searchqa-validation-4398", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-6355", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-11293", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-841", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-7159", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-4972", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3961"], "SR": 0.4375, "CSR": 0.5921336206896552, "EFR": 0.9722222222222222, "Overall": 0.7154492935823755}, {"timecode": 29, "before_eval_results": {"predictions": ["NBA", "11 million", "GTE", "secondary school", "John Lee Hancock", "2007", "Westfield Tea Tree Plaza", "Wells Fargo Center in Philadelphia", "237", "Gal Gadot", "1860", "Eddie Izzard", "The Beatles' 1966 US tour", "Miracle", "Timothy McVeigh", "studied Arabic grammar", "Humberside Airport", "8/7c", "2016", "Kind Hearts and Coronets", "Centennial Olympic Stadium", "Leatherheads", "Polka", "Sinngedichte", "James G. Kiernan", "Australia", "heavy metal band", "Anah\u00ed", "PPA", "Tampa Bay Lightning", "tabasco peppers", "Patricia Arquette", "\"Secrets and Lies\"", "Richard John Bingham", "coca wine", "Crystal Dynamics", "Geraldine Page", "pornographicstar", "Europe", "39 nations", "three", "Sam the Sham", "Frank Thomas' Big Hurt", "Genesee Brewing Company", "Las Vegas", "PPG Paints Arena", "George I of Great Britain", "Allison J71", "\"Super Hit\"", "Romance", "Bohemia, New York", "Macomb County", "birth", "biochemistry", "provides the public with financial information about a nonprofit organization", "Classical Archives", "Stockholm syndrome", "magnetism", "Morgan Tsvangirai", "mpire of the Sun", "Amanda Knox's aunt", "Scotland", "Dallas", "gulls"], "metric_results": {"EM": 0.5, "QA-F1": 0.5976438492063492}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.3333333333333333, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5826", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-9088", "mrqa_triviaqa-validation-5397", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3278", "mrqa_searchqa-validation-15695", "mrqa_searchqa-validation-5455", "mrqa_searchqa-validation-9860"], "SR": 0.5, "CSR": 0.5890625, "EFR": 1.0, "Overall": 0.720390625}, {"timecode": 30, "UKR": 0.708984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1180", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1342", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1488", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-172", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2199", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2825", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-3071", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3191", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4285", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4467", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5561", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5713", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-704", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-98", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1199", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11434", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-13831", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-2686", "mrqa_searchqa-validation-3516", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6045", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-8245", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9601", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10223", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10424", "mrqa_squad-validation-10449", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1064", "mrqa_squad-validation-1096", "mrqa_squad-validation-1114", "mrqa_squad-validation-1177", "mrqa_squad-validation-1232", "mrqa_squad-validation-1255", "mrqa_squad-validation-1258", "mrqa_squad-validation-1296", "mrqa_squad-validation-1480", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1723", "mrqa_squad-validation-1940", "mrqa_squad-validation-1976", "mrqa_squad-validation-2153", "mrqa_squad-validation-2272", "mrqa_squad-validation-2301", "mrqa_squad-validation-234", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2591", "mrqa_squad-validation-287", "mrqa_squad-validation-2976", "mrqa_squad-validation-306", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3296", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3716", "mrqa_squad-validation-3767", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4026", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4259", "mrqa_squad-validation-4286", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4545", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4810", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5276", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5532", "mrqa_squad-validation-556", "mrqa_squad-validation-5578", "mrqa_squad-validation-5611", "mrqa_squad-validation-5779", "mrqa_squad-validation-5839", "mrqa_squad-validation-5881", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6443", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6888", "mrqa_squad-validation-6967", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7125", "mrqa_squad-validation-7152", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7323", "mrqa_squad-validation-7403", "mrqa_squad-validation-7458", "mrqa_squad-validation-7603", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-8219", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-864", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8839", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9061", "mrqa_squad-validation-9191", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9521", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9647", "mrqa_squad-validation-9652", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2654", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-830"], "OKR": 0.86328125, "KG": 0.45625, "before_eval_results": {"predictions": ["the First Minister", "quadratic time", "1879", "Xiu Li Dai and Yongge Dai", "St. Theodosius Russian Orthodox Cathedral", "The Mecca '' is located in northwest Washington", "1924", "England", "Bud Light", "the status line", "Benjamin Franklin", "late - night", "into the intermembrane space", "Chinese", "Philadelphia", "The United States Secretary of State", "electrical activity produced by skeletal muscles", "thick skin", "an Islamic shrine", "Sylvester Stallone", "Anakin Skywalker", "September 27, 2017", "convert single - stranded genomic RNA into double - stranded cDNA", "the economy", "Victory gardens", "Paul Hogan", "961", "northern China", "gathering money from the public, which circumvents traditional avenues of investment", "a beach in Malibu, California", "Humpty Alexander Dumpty", "homicidal thoughts of a troubled youth", "a part of the continent of North America, Greenland has been politically and culturally associated with Europe ( specifically Norway and Denmark, the colonial powers, as well as the nearby island of Iceland )", "Sun Tzu", "18th century in the United Kingdom", "Setsuko Thurlow", "the temporal lobes of the brain and the pituitary gland", "a bridge over the Merderet in the fictional town of Ramelle", "DNA and other molecules that mediate the function of the genome", "mining", "Keith Thodeaux", "six - hoop game", "Atlantic", "butane", "Julia Roberts", "12 November 2010", "Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom, the United States, and the European Union", "Aaron Harrison", "Panning", "The Young and the Restless'look influenced the taping styles of other soap operas", "Johnny Depp", "James Chadwick", "the Swirral Edge ridge", "nytoplankton", "Worcester Cathedral", "Germany", "Rachel, Nevada", "Atlanta", "more than 30 Latin American and Caribbean nations", "two women", "police dogs", "a astronomical viewing facility", "Jean Acker Rudolph Valentino", "Hannibal of Carthage"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5472209557807384}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4864864864864865, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8695652173913044, 0.6666666666666666, 1.0, 1.0, 0.34285714285714286, 1.0, 0.4166666666666667, 0.0, 0.4444444444444445, 0.9333333333333333, 0.0, 1.0, 0.5, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.10000000000000002, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.25, 1.0, 0.6666666666666666, 0.5454545454545454, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9540", "mrqa_squad-validation-1818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-6465", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-782", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2873", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-414", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-4098"], "SR": 0.34375, "CSR": 0.5811491935483871, "EFR": 1.0, "Overall": 0.7219329637096774}, {"timecode": 31, "before_eval_results": {"predictions": ["weight in burning was hidden by the buoyancy of the gaseous combustion products", "the body forms of their parents", "September 19 - 22, 2017", "social commentary", "John Roberts", "in contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "between the Eastern Ghats and the Bay of Bengal", "a bow bridge with 16 arches shielded by ice guards", "12 to 36 months old", "Bobb McKittrick", "Chelsea", "Darlene Cates", "fascia surrounding skeletal muscle", "Jerry Leiber", "a Norwegian town", "Lisa Stelly", "Fools and Horses", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms and is thus placed in a `` take", "Robin", "Jack Barry", "Missouri River", "Randy", "August 18, 1945", "19 June 2018", "Daniel A. Dailey", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "international educational foundation headquartered in Geneva, Switzerland", "October 28, 2007", "Jaydev Shah", "domestication of the wild mouflon in ancient Mesopotamia", "action comedy", "in a thousand years", "a state or other organizational body that controls the factors of production", "90 \u00b0 N 0 \u00b0 W", "in Ephesus in AD 95 -- 110", "1984", "sport utility vehicles", "Americans", "1916", "30 years after Return of the Wars franchise", "Jack Lord", "founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O' Neal and Dougy Williams", "Rashidun Caliphs", "off the rez", "Woody Paige", "Ren\u00e9 Descartes", "2007 via Valve's Steam content distribution platform", "Asuka", "A Turtle's Tale : Sammy's Adventures and the TV show Suburgatory", "three", "Steve Biko", "Rudolph", "chess", "Anne Fletcher", "October 21, 2016", "Arizona Health Care Cost Containment System", "21", "Robert Barnett, a prominent Washington attorney", "Oaxaca, Mexico", "Algeria", "the egg in front of the opening", "ppa"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5965606878816605}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false], "QA-F1": [0.09090909090909091, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0625, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.5, 1.0, 0.888888888888889, 0.5714285714285715, 1.0, 1.0, 0.3333333333333333, 0.9333333333333333, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.9859154929577464, 0.22222222222222224, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.7692307692307692, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3494", "mrqa_squad-validation-4566", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7412", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-2748", "mrqa_hotpotqa-validation-1803", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-3785", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-4046"], "SR": 0.4375, "CSR": 0.57666015625, "EFR": 0.8888888888888888, "Overall": 0.6988129340277778}, {"timecode": 32, "before_eval_results": {"predictions": ["September 1565", "the Anglo-Saxon populations who migrated to and conquered much of England after the end of Roman Imperial rule", "Pin the Tail on the Donkey", "the martini", "Wiener Sangerknaben", "cinnamon", "Big Bang", "Halloween", "R.E.M.", "Brian Piccolo", "French Fifth Republic", "Azerbaijan", "Abraham Lincoln", "the Yangtze River", "mande", "Angelina Jolie", "Sharon Epatha Merkerson", "air pressure", "Harold Macmillan", "bony frill", "shark", "the Deaf President Now protest", "school", "\"I've already said too much\"", "the orangutan", "anaphylactic shock", "camels and travel from place to place, subsisting on milk, meat and... started working its way into Bedouin society", "gangrene", "ex", "Bonnie and Clyde", "John Harvard", "Romance languages", "\"David Cassidy: Man Undercover,\"", "Dorothy Gale", "Guatemala", "\"JK\"", "Barack Obama", "Jos Joaqun de Olmedo", "Albert Einstein", "mandelope & Grand Canyons", "Fort Lee, New Jersey", "Louisa May Alcott", "Tulip", "\"I love you\"", "Providence", "Tasmanian devil John Quincy", "a vine that is now over 400 years old", "South Africa", "Swan Lake", "dry ice", "Kim Dae Jung", "tooth", "Gibraltar", "1999", "9 February 2018", "Argentina", "Sarah Sawyer", "Aviva plc", "Russian Empire", "1967", "44,300", "228", "Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "\"We'll see fewer often women, have fewer kids and more often have college educations.\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.6138888888888889}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.1, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_squad-validation-5276", "mrqa_searchqa-validation-10043", "mrqa_searchqa-validation-5405", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-5348", "mrqa_searchqa-validation-12676", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-15681", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-4645", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-2559", "mrqa_searchqa-validation-10513", "mrqa_searchqa-validation-391", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-554", "mrqa_triviaqa-validation-4432", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-2395"], "SR": 0.515625, "CSR": 0.5748106060606061, "EFR": 1.0, "Overall": 0.7206652462121211}, {"timecode": 33, "before_eval_results": {"predictions": ["11", "R\u00fcdesheim am Rhein", "Diana", "Tennessee Williams", "Ring Magazine", "a pancake", "John Henry", "Zombies", "Colombia", "belle", "Friday Night Lights", "belle", "the Emperor", "a port-wine stain", "a Skyscraper", "Pinta", "Czechoslovakia", "Ferris B Mueller", "Mike Judge", "Unforgiven", "Court TV", "the Galaxy", "Germany", "Gunsmoke", "astronomer", "Candy", "AT&T", "asthma", "Microsoft", "blue", "Puerto Rico", "24 hours", "a flying saucer", "Shakespeare", "a liter", "Edward", "The Silence of the Lambs", "Prymaat", "stuffing", "a fraction", "carbonite", "Spain", "the phi phenomenon", "an obelisk", "Sam Kinison", "Katharine Hepburn", "Harry S. Truman", "Kublai Khan", "the South Ossetia", "(DC)", "a bow", "Newfoundland", "538", "a destructive ex-lover", "Lee Baldwin", "googol", "Laos", "Wigan", "1995", "Champion Jockey", "Donald J. Trump", "Obama", "200", "around 8 p.m. local time Thursday"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6510416666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7868", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-6554", "mrqa_searchqa-validation-3111", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-10095", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-9672", "mrqa_searchqa-validation-2404", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-227", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-5234", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3445", "mrqa_newsqa-validation-822"], "SR": 0.609375, "CSR": 0.5758272058823529, "EFR": 1.0, "Overall": 0.7208685661764705}, {"timecode": 34, "before_eval_results": {"predictions": ["Hisao Yamada", "Percy Shelley", "3D computer-animated comedy", "aluminum foil", "Montreal, Quebec, Canada", "Lego", "Daniil Borisovich Shafran", "Doc Hollywood", "Michael Cremo", "the Virgin label", "John Christopher Lujack Jr.", "26 June 2013", "Freddie Jackson", "Michael Swango", "Roman Polanski", "322,520", "1754", "Cate Blanchett", "Westfield Marion", "Montana State University", "1961", "Eisenhower Executive Office Building", "137th", "Mohsin Fani", "High Falls Brewery", "2016", "1998", "Sean", "2015", "Mel Blanc", "Corendon Airlines", "Tamil", "number five", "Champion Jockey", "University of Columbia", "Jennifer Aniston", "Larry Eustachy", "Anne Perry", "March 17, 2015", "Julie Taymor", "Mika H\u00e4kkinen", "10", "a bass", "Ringo Starr", "January 1788", "Lord Chancellor of England", "MGM Resorts International", "Cleveland, Ohio", "The song also features rap parts from Darryl, RB Djan and Ryan Babel", "Mark Anthony \"Baz\" Luhrmann", "Syracuse University", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "1969", "Georgia Nicholson", "the referee", "Botticelli", "American", "Keats", "Dr. Maria Siemionow", "it has not", "Arsene Wenger", "atoms", "Bellerophon", "Monica Lewinsky"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7578125}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5530", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-421", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-2989", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-2977", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-7800"], "SR": 0.671875, "CSR": 0.5785714285714285, "EFR": 1.0, "Overall": 0.7214174107142857}, {"timecode": 35, "before_eval_results": {"predictions": ["June 4, 2014", "highly diversified", "Walter Pauk", "2018", "a noble gas", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "Ross Elliott", "Siddharth Arora / Vibhav Roy", "the Charlotte Hornets", "one person", "Orographic lift", "August 10, 1945", "6 March 1983", "drawing letters in the air", "Around 1200", "April 21, 2015", "1854", "at the Mount Mannen in Norway", "1937", "Kennedy Space Center ( KSC ) in Florida", "Rocky Dzidzornu", "35 to 40 hours per week", "Abbot Suger", "since the early 20th century", "Authority", "warning signs", "low coercivity", "near the mouth of the Pinarus River and the town of Issus", "1992", "ulcerative colitis", "April 20, 1983", "Turducken", "the brain and spinal cord", "fled to exile in the Netherlands", "Massachusetts", "Hans Zimmer, Steve Mazzaro & Missi Hale", "c. 1000 AD", "Times Square in New York City west to Lincoln Park in San Francisco", "Jerry Leiber and Mike Stoller", "111", "49 cents", "December 1, 1969", "Central Germany", "1978", "Javier Fern\u00e1ndez", "by observing the magnetic stripe `` anomalies '' on the ocean floor", "speech", "the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council ( UNSC )", "peninsular mainland", "Santo Domingo", "after Derek left New York", "born November 28, 1973", "Battle of the Somme", "Frederick William III", "Majorca", "National Football League", "Arlo Looking Cloud", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "\"Bhutto,\"", "spend $60 billion on America's infrastructure.", "Wigan Athletic", "Ireland", "Asteroid impact avoidance", "Colorado"], "metric_results": {"EM": 0.453125, "QA-F1": 0.614725716534927}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.888888888888889, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.631578947368421, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, 0.13333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-1985", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-6276", "mrqa_hotpotqa-validation-3613", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1970", "mrqa_searchqa-validation-13764"], "SR": 0.453125, "CSR": 0.5750868055555556, "EFR": 0.9714285714285714, "Overall": 0.7150062003968254}, {"timecode": 36, "before_eval_results": {"predictions": ["a crust and lithosphere", "Kanun\u00ee Sultan S\u00fcleyman", "Skatoony", "number 1", "Satchmo, Satch or Pops", "San Antonio", "Polish", "Danish", "Milwaukee Bucks", "1994", "Fox musical comedy-drama series \"Glee\"", "1965", "100 million", "Oneida Limited", "in Wilmington, North Carolina, United States", "Pieter van Musschenbroek", "Southbank", "London", "Australian", "Rochdale, North West England", "Bardot", "Mario Lemieux", "To SquarePants or Not to SquarePant", "The Sun", "Sydney, New South Wales, Australia", "Ferdinand Magellan", "King of France", "1694", "Liam Cunningham", "Nan Britton", "Minette Walters", "leopard", "the Moselle", "Anne and Georges", "the Bank of China Tower", "American playwright and Nobel laureate in Literature", "Cheshire County", "Bob Gibson", "1770", "1974", "James J. Hill's Great Northern Railway transcontinental railway line", "Woody Woodpecker", "2 congressional districts", "Edward Albert Heimberger", "IFFHS World's Best Goalkeeper", "three", "1989 until 1994", "Pittsburgh Steelers", "9 venues", "1993 to 2001", "Double Crossed", "economic recession", "needle - like teeth", "Bacon", "human rights lawyer", "mathematically obsolete", "1812", "energy propels the boat that travels between 5 and 10 knots an hour", "the Casalesi Camorra clan", "Kingdom City project will be taller than Burj Dubai tower", "Queen Wilhelmina", "greece", "Santa Fe", "1992"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7123883928571428}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.25, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4952", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-2495", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3417", "mrqa_hotpotqa-validation-5213", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-2319", "mrqa_naturalquestions-validation-5119", "mrqa_triviaqa-validation-994", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-743", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-6519"], "SR": 0.609375, "CSR": 0.5760135135135135, "EFR": 0.96, "Overall": 0.7129058277027027}, {"timecode": 37, "before_eval_results": {"predictions": ["lasting damage", "Favre", "Geraldine A. Ferraro", "Wool Sack dress", "LITERATURE", "Oliver Twist", "Hans Christian Andersen", "blue", "temperature", "Cameroon Pidgin", "x 52.55", "Destiny's Child", "antipope", "California", "Ocean's Thirteen", "einherjar", "Little Women", "Ich bin ein Berliner", "xineo", "World War II", "difference", "Gogol", "Malcolm X", "Rocky Mountain columbine", "phonetics", "Michigan", "Sigmund Freud", "red meat", "T. S. Eliot", "Dumpling", "Trinity Church Cemetery", "New Zealand", "rum", "Theology of God", "Classical Net", "Stephen Decatur", "Castor & Pollux", "Paraguay", "R2-D2", "6 to 8 glasses", "tense", "Vassar College", "forensic medicine", "National Air and Space Museum", "Vespa", "Warren G. Harding", "Emma Peel", "Tennessee", "Richard I", "Will Rogers", "Bee Gees", "Honor\u00e9 Mirabeau", "in 2018", "Hanna Alstr\u00f6m", "NU.P.E.", "website", "Scotland", "1979", "1887", "Bonkyll Castle", "more and more suspicious of the way their business books were being handled.", "Top Gun", "Dame Elizabeth,", "1875"], "metric_results": {"EM": 0.5, "QA-F1": 0.6185763888888889}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-16215", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-7080", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-8720", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9251", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-13049", "mrqa_searchqa-validation-8295", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-10821", "mrqa_searchqa-validation-2859", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-8890", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-7359", "mrqa_triviaqa-validation-6174", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-2552", "mrqa_hotpotqa-validation-4203"], "SR": 0.5, "CSR": 0.5740131578947368, "EFR": 1.0, "Overall": 0.7205057565789473}, {"timecode": 38, "before_eval_results": {"predictions": ["the Butcher Market", "the Grito de Dolores", "grommets", "Soundgarden", "pew", "Russia", "the Penguin", "Digitalis glycosides", "Canada", "the stave", "pole vault", "California", "Jordan", "an uptset printing", "the Battle of Waterloo", "Ukraine", "goombah", "Paris", "#49 David Geffen", "Hallmark Cards", "Joan of Arc", "John Tyler", "simile", "La-Z-Boy", "back drafts", "a phylum", "Narnia", "East Germany", "Ginger Rogers", "Judges 5", "Dracula", "Marlee Matlin", "frogs", "Iraq", "debts", "Lady Jane Grey", "yellow fever", "Westin", "Guatemala", "Harold Edward \"Red\" Grange", "Peter", "indirect printing", "couscous", "1917", "Colonel (Tom) Parker", "lilac", "American Pie", "a diamond", "a Bowhead whale", "Ohio State", "Sweet Home Alabama", "John Prine", "Ohio", "1960", "The Nutcracker", "Dodoma", "the Red Lion", "martial arts action films", "Black Swan", "the Sun", "girls", "Vernon Forrest", "President Barack Obama", "Lewiston"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6375}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-6090", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-14171", "mrqa_searchqa-validation-12204", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-3640", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-5033", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-6757", "mrqa_searchqa-validation-3197", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-1976", "mrqa_triviaqa-validation-6420", "mrqa_hotpotqa-validation-1192", "mrqa_newsqa-validation-1371", "mrqa_triviaqa-validation-700"], "SR": 0.53125, "CSR": 0.5729166666666667, "EFR": 1.0, "Overall": 0.7202864583333334}, {"timecode": 39, "before_eval_results": {"predictions": ["substantially increased the asking price", "body at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "\"To My Mother\"", "Lucky Dube", "Festival Foods", "fallen comrades lost in the heat of battle.", "Juri Kibuishi", "1918-1919", "participate in Iraq's government.", "(St.) Mohamed Anwar al-Sadat,", "Honduras", "Los Angeles", "FBI", "201-262-2800", "expressed concern that taking the product off the market would result in hardship for terminally ill patients and their caregivers", "were directly involved in an Internet broadband deal with a Chinese firm.", "Iraq", "40 militants and six Pakistan soldiers", "1973", "Argentina", "Washington", "\"It hurts my heart to see him in pain, but it enlightened at the same time to know my son is strong enough to make it through on a daily basis.\"", "Laura Ling and Euna Lee", "January 24, 2006", "\"What's happening in our schools?\"", "Haleigh,", "Casalesi Camorra", "nine", "\"They just were all good little soldiers and pulled right over,\"", "two", "two", "anxious.", "Kurt Cobain", "genocide, crimes against humanity, and war crimes.", "Salt Lake City, Utah", "barter -- trading goods and services without exchanging money", "\"Dance Your Ass Off.\"", "December", "Larry Zeiger", "Japan", "upper respiratory infection", "he reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees.", "a building falls down", "African National Congress", "abuse", "The man ran out of bullets and blew himself up.", "two", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip", "14", "11,", "Royal Navy servicemen", "Clarence Darrow", "fructose", "to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "1768", "chariots", "George Orwell", "\"Free R\"", "1955", "\"I'm Shipping Up to Boston\"", "a lock", "James Abbott McNeill Whistler", "Liberty Bond", "email"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4931746935447594}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.16666666666666666, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9473684210526316, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.75, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.07692307692307693, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08333333333333333, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2946", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-1259", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4211", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-1585", "mrqa_searchqa-validation-16932", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-11411", "mrqa_triviaqa-validation-90"], "SR": 0.40625, "CSR": 0.56875, "EFR": 0.9736842105263158, "Overall": 0.7141899671052631}, {"timecode": 40, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1348", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2749", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3171", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-543", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6680", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-2423", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-8890", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-9163", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9967", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-138", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3341", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3787", "mrqa_squad-validation-3813", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4107", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5222", "mrqa_squad-validation-5237", "mrqa_squad-validation-525", "mrqa_squad-validation-5269", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-531", "mrqa_squad-validation-5319", "mrqa_squad-validation-5483", "mrqa_squad-validation-5501", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5741", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-689", "mrqa_squad-validation-7001", "mrqa_squad-validation-7002", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-741", "mrqa_squad-validation-7458", "mrqa_squad-validation-7470", "mrqa_squad-validation-7525", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7736", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9744", "mrqa_squad-validation-9845", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9969", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830"], "OKR": 0.853515625, "KG": 0.49296875, "before_eval_results": {"predictions": ["pressure the lazy, inspire the bored, deflate the cocky", "Gov. Jan Brewer.", "Casey Anthony,", "Tehran,", "Britain's", "\"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "they did not receive a fair trial.", "Gordon Brown", "regulators in the agency's Colorado office", "Oprah: A Biography", "Vivek Wadhwa", "\"To all of our valiant men and women, know that the American people believe in you, support you and are 100 percent behind you, and we thank God every day that you have our back.\"", "former Mobile County Circuit Judge Herman Thomas", "eight-day", "a long-range missile on its launch pad,", "he checked himself into a Los Angeles mental institution in an effort to get away from the temptation of drugs.", "committed to equality,", "Harry Nicolaides,", "in central Cairo,", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "opium poppies", "animal products.", "the lead plaintiff in perhaps the most controversial case involving Judge Sonia Sotomayor,", "end of a biology department faculty meeting at the University of Alabama in Huntsville,", "1957,", "secretary of defense", "The Real Housewives of Atlanta", "dancing", "September,", "the stars of TLC's \"The Little Couple,\"", "Waterloo Bridge", "two", "ties", "a long-range missile test.", "Nicole", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Orlando police", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the execution.", "14", "the local political representative", "vitamin injections that promise to improve health and beauty.", "delivers a big speech", "Ennis, County Clare", "$17,000", "Prime Minister Fredrik Reinfeldt", "on Anjuna beach in Goa", "the patient,", "South African", "of the three head wounds are consistent with self-inflicted wounds, and not consistent with long-range rifle fire.", "a rally at the State House next week", "J. Presper Eckert", "Latitude", "excessive growth", "Saint Cecilia", "One Thousand and One", "George lV", "2011", "Violet", "Kinnairdy Castle", "the hippopotamus", "Boll weevil", "Alexander Solzhenitsyn", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6266326421277366}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.8125000000000001, 1.0, 1.0, 0.0, 1.0, 0.0, 0.13953488372093023, 0.6, 1.0, 0.2857142857142857, 0.6399999999999999, 0.25, 1.0, 0.8, 0.9333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.3333333333333333, 1.0, 0.9743589743589743, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.5, 0.25, 0.8, 0.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1919", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-4059", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-705", "mrqa_searchqa-validation-16464"], "SR": 0.46875, "CSR": 0.5663109756097561, "EFR": 0.9705882352941176, "Overall": 0.7231610921807748}, {"timecode": 41, "before_eval_results": {"predictions": ["Jim Nantz and Phil Simms", "Texas", "Detroit", "birds", "george iv payez", "Taps", "The Magic of David Copperfield XVI", "Dr. Strangelove", "a cat", "Atlanta", "on a brief comment made by Carnap in the... metaphysics are without meaning because they are not deducible", "a baseball movie starring Kevin Costner", "Coors Field", "Boise State", "Doc Holliday", "Chicken Run", "Amphitryon", "The Rama", "hydrogen", "Svengali", "Magda \"Magda\" Gabor", "Mammoth Cave", "a sousaphone", "the 2/27/10 M8.8 Chile earthquake", "Poseidon", "Queen Elizabeth II", "The 39 Steps", "Cynic", "Judges", "Oreo", "St. Lawrence", "the seashore", "Indiana Jones", "America", "Bill Clinton", "Cloverfield", "Paraguay", "Rassendyl", "the Gulf of Tonkin", "a state of resting after exertion or strain", "jealous", "on", "a zipper", "Tuesday", "Olivia Newton-John", "Robert Cohn", "oil", "South Africa", "De Hooch", "Arnold J. Toynbee", "Heathers", "2004", "Rachel Sarah Bilson", "Pradyumna", "Cheshire", "George H. W. Bush", "blood", "weighed against the feather of truth", "Indooroopilly Shopping Centre", "runner-up", "opium", "Adam Yahiye Gadahn,", "on an island stronghold of the Islamic militant group Abu Sayyaf,", "Johnny Weissmuller"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6185267857142858}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [0.20000000000000004, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-597", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-2509", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-2758", "mrqa_searchqa-validation-3701", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-12223", "mrqa_searchqa-validation-2756", "mrqa_naturalquestions-validation-3124", "mrqa_triviaqa-validation-6163", "mrqa_triviaqa-validation-5356", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-294", "mrqa_newsqa-validation-3404", "mrqa_triviaqa-validation-2080"], "SR": 0.53125, "CSR": 0.5654761904761905, "EFR": 0.9666666666666667, "Overall": 0.7222098214285715}, {"timecode": 42, "before_eval_results": {"predictions": ["Divine Right of Kings", "Mussolini", "Cher", "tennis", "Tarsus", "Charles Chaplin", "Dancing On Ice", "France", "Vietnam", "foil", "Agatha Christie", "Cold Blood", "Social Media and Social Change", "Jackie Joyner", "a whale", "Nelson Mandela", "the Taurid", "Cuba Libre", "Thomas Jefferson", "Tanzania", "David Hare", "Mexico City", "Pennsylvania", "Borneo", "mckinley", "Walla Walla", "Netflix", "Roger Bannister", "Le Corbusier", "(Scott) Peterson", "an enigma", "Franco", "Bolivia", "rugby", "Ireland", "Vanna White", "Catherine the Great", "yellow", "william misfires", "ROE", "Elizabeth Cady Stanton", "the Wetterstein Mountains", "Francis Ford Coppola", "wives and concubines", "meander", "The Wind in the Willows", "Jack Dempsey", "hex", "The Two Gentlemen of Verona", "the chimpanzee", "the Red Cross (ICRC)", "pigs", "August 2012", "Jack Nicklaus", "the Central African Republic", "six", "Yalta", "Emad Hashim", "The Suite Life on Deck", "The Daily Stormer", "Bayern Munich", "United States", "New York's Madison Square Garden.", "mckinair"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6197916666666666}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-16519", "mrqa_searchqa-validation-11329", "mrqa_searchqa-validation-11452", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-2589", "mrqa_searchqa-validation-16809", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-10935", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-12644", "mrqa_searchqa-validation-12772", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-7323", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-11809", "mrqa_naturalquestions-validation-7702", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2135", "mrqa_hotpotqa-validation-5307", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-2846", "mrqa_triviaqa-validation-6337"], "SR": 0.53125, "CSR": 0.5646802325581395, "EFR": 1.0, "Overall": 0.728717296511628}, {"timecode": 43, "before_eval_results": {"predictions": ["Orthogonal components", "(Henry) VIII", "Tomorrow Never Dies", "Monaco", "ring the bell on the last lap in Melbourne", "(O my son) Absalom", "Columbus", "Brett Favre", "South African", "Brian Deane", "Marie Antoinette", "Argentina", "William Conrad", "1875", "Lloyd Webber", "Iran", "Fairey Swordfish", "the Isle of Arran", "London", "Playboy", "to shave", "Matalan", "Chesney Wold", "boise", "a griffin", "red", "the Pussycats", "Chalin", "Judy Cassab", "gold rings", "Karl Marx and Friedrich Engels", "Utrecht", "(Chancellor) of the Exchequer", "(Strangeways)", "Carousel", "36", "Richard Wagner", "the brain", "\"Garp\"", "(Frederick) William Herschel", "Belgium", "October 31st", "a beetle", "(Deacon) Blues", "Pompey", "(Denali)", "auction houses", "haddock", "L. P. Hartley", "Italy", "a snake", "Andy Serkis", "2001", "the human hands", "Distinguished Service Cross", "Shenae Grimes", "five-time", "prostate cancer,", "at the U.S. Holocaust Memorial Museum,", "1927", "Dale", "a game show", "London", "Uru-Salim"], "metric_results": {"EM": 0.5, "QA-F1": 0.6006944444444444}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false], "QA-F1": [0.5, 0.5, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10394", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-6459", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-4052", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3137", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-2943", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-9414"], "SR": 0.5, "CSR": 0.5632102272727273, "EFR": 1.0, "Overall": 0.7284232954545455}, {"timecode": 44, "before_eval_results": {"predictions": ["co-chair", "a cappella group", "iceland", "Evita", "Victoria", "Sikhism", "william vii", "Sinclair Lewis", "Argentina", "giblet", "Guatemala", "olive", "Munich", "violin", "a sash", "Paul Nash", "a mammal", "first among equals", "a robin", "Indira Priyadarshini Gandhi", "Colombia", "jean Baptiste Say,", "Uranus", "Prince Igor", "November", "lingerie", "a watt", "The Wicker Man", "henry vii", "Gorky", "South Africa", "hovercraft", "john McEnroe", "white", "john Mellencamp", "Tina Turner", "jar", "brash", "bees", "harold wilson", "The Spice Girls", "oregon", "dan ladd", "a detergent", "Wolfgang Amadeus Mozart", "bubba", "Utah", "Richard Lester", "December", "peregrines", "steel", "7 June 2005", "Billboard", "Britain", "Marcus T. Reynolds", "35,000 members", "Tel Aviv University", "response to a civil disturbance call,", "the foyer of the BBC building in central London", "\"Hillbilly Handfishin'\"", "diogenes", "Roosevelt, Churchill", "a flanker", "an integral membrane protein that builds up a proton gradient across a biological membrane"], "metric_results": {"EM": 0.625, "QA-F1": 0.6976562500000001}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true], "QA-F1": [0.2857142857142857, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.7499999999999999, 0.7142857142857143, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8581", "mrqa_triviaqa-validation-5869", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-7543", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-5476", "mrqa_hotpotqa-validation-773", "mrqa_hotpotqa-validation-3313", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-220", "mrqa_searchqa-validation-13219", "mrqa_searchqa-validation-9558"], "SR": 0.625, "CSR": 0.5645833333333333, "EFR": 0.9583333333333334, "Overall": 0.7203645833333334}, {"timecode": 45, "before_eval_results": {"predictions": ["Apollo spacecraft", "1853", "Daniel A. Dailey", "adrenal medulla", "anakin Skywalker", "Plank", "Ann Gillespie", "near Chesapeake Bay", "man ''", "March 26, 1973", "drizzle, rain, sleet", "Tommy Shaw", "1757", "Charles Perrault", "John Daly", "1998", "Elizabeth Dean Lail", "March 31, 2017", "Jonathan Breck", "Aristotle", "2007", "plate tectonics", "eight", "more than a million", "last book accepted into the Christian biblical canon", "1995", "Rock Island, Illinois", "1926", "2016", "the south", "on the pelvic floor", "The Osmonds", "the British group Ace", "from the Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s, meaning `` herdsman '' or `` plowman ''", "Teddy Randazzo", "heart", "Christopher Jones", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "current day Poole Harbour towards mid-Channel", "Neal Dahlen", "The balance sheet", "London, United Kingdom", "Hellenism", "232", "January 2, 1971", "Andy Cole", "\u20b9 39.50 lakh", "white blood cell", "a crust of potatoes", "the team", "4.5 pounds or 2.04 kg", "Amy Johnson", "12", "buk'wus", "Westgate Las Vegas Resort & Casino", "Gloria Trevi", "postal delivery", "he dropped his children off at a relative's house,", "the leader of a drug cartel", "\"Slumdog Millionaire\"", "Clifford Odets", "Margaret Mitchell House", "Microsoft", "travis"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6716156347840254}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.06451612903225806, 0.8, 1.0, 0.5, 0.6956521739130436, 0.0, 1.0, 0.5, 0.9, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-7509", "mrqa_triviaqa-validation-2110", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-3832", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-13643", "mrqa_searchqa-validation-947"], "SR": 0.546875, "CSR": 0.5641983695652174, "EFR": 0.896551724137931, "Overall": 0.7079312687406297}, {"timecode": 46, "before_eval_results": {"predictions": ["Gamal Abdul Nasser", "2016", "committed and effective Sultans", "Germany", "contributed military and civilian police personnel to peace operations", "Steve Hale", "King Saud University", "Parashara", "John Dalton", "Vienna", "pepsinogen", "Carol Worthington", "ancient Rome", "1942", "April 29, 2009", "19 June 2018", "Todd Bridges", "Ben Savage", "the senior-most judge of the supreme court", "multinational retail corporation", "nachos", "Andrea Brooks", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "Max Martin", "July 2010", "232", "forested parts of the world", "3.5 mya", "1996", "Each side", "mitosis", "an orangutan", "Texhoma", "solemniser", "Nat Finkelstein", "The Royalettes", "Latin centum", "Vienna", "Andrew Johnson", "microfilament", "four distinct levels", "rapidly", "Cleveland Indians", "travis", "Kid Creole", "Stephen Graham -- Detective Superintendent Dave Kelly", "Anna Maria Demara", "plane crash", "Tatsumi", "Ernest Hemingway", "14 November 2001", "Venus", "Laos", "nastase", "Bennett Cerf", "Scotty Grainger Jr.", "uncle", "Steve Jobs", "motion for a preliminary injunction against a Mississippi school district and high school in federal court", "released", "the poverty line", "uterus", "panting", "(John) Y. Brown Jr."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5920305735930735}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.6666666666666666, 0.5, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.88, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-7027", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-3771", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-5936", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-959", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-1475", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-14439", "mrqa_searchqa-validation-15864"], "SR": 0.484375, "CSR": 0.5625, "EFR": 1.0, "Overall": 0.72828125}, {"timecode": 47, "before_eval_results": {"predictions": ["2014", "Ren\u00e9 Descartes", "bearers", "Dante Pastula", "Havana Harbor", "sedimentary", "April 10, 2018", "Tracy McConnell", "the North Atlantic Ocean", "self - closing flood barrier", "111", "2 %", "derived from the French Jeanette", "the head of the Imperial Family and the traditional head of state of Japan", "November 17, 2017", "an instant messaging client that was first developed and popularized by the Israeli company Mirabilis", "appellate court", "her abusive husband", "84", "William Chatterton Dix", "`` Killer Within ''", "Broken Hill and Sydney", "appendicular skeleton", "a database maintained by the United States federal government, listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "a couple broken apart by the Iraq War", "Gutenberg", "National Industrial Recovery Act ( NIRA )", "She's Out of My League in Pittsburgh", "the ulnar nerve", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "31 October 1972", "twelve Wimpy Kid books", "Matt Flinders", "The person who has existence in two parallel worlds", "IETF protocols", "Ra\u00fal Eduardo Esparza", "endoproteases", "4 September 1936", "drivers who meet more exclusive criteria", "H.L. Hunley", "Orangeville, Ontario, Canada", "the Bactrian camels", "the Gentiles", "James Hutton", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "decades after its initial release", "2005", "March 18, 2005", "Fats Waller", "densely packed in the fovea centralis", "the Russian army", "the ship", "Rudyard Kipling", "THE PENGUIN", "Otto von Bismarck", "Ukrainian", "237 square miles", "Apple employees", "she offered her \"sincere apologies for any offense.\"", "in July", "Margaret Mitchell", "crucifixion", "Lisa Lisa & Cult Jam", "right-hand batsman"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6487385852723023}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8108108108108109, 0.0, 0.6666666666666666, 0.888888888888889, 0.14285714285714285, 1.0, 0.13333333333333333, 1.0, 0.38095238095238093, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.4, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-10321", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-7358", "mrqa_triviaqa-validation-3828", "mrqa_hotpotqa-validation-1056", "mrqa_hotpotqa-validation-5541", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-272", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-16263", "mrqa_hotpotqa-validation-181"], "SR": 0.484375, "CSR": 0.5608723958333333, "EFR": 0.9696969696969697, "Overall": 0.7218951231060606}, {"timecode": 48, "before_eval_results": {"predictions": ["Magdalen College", "Rana Daggubati", "13 people", "S Pictures' \"Veyyil\"", "1 draw", "Kristy Lee Cook", "The LA Galaxy", "Volvo 850", "February 14, 1859", "'Tis the Fifteenth Season", "Biola University", "BAFTA Award for Best Production Design", "2012", "October 13, 1980", "tomato", "Gracie Mansion", "Arsenal F.C.", "Operation Neptune", "Steve Prohm", "a super-regional shopping mall", "Lady Charlotte Elliot", "28th season", "seven", "28 June 1945", "University of California", "Glendale, Arizona", "Indian origin", "Donna Paige Helmintoller", "Graham Hill", "Masahiko Takehita", "Hillary Clinton", "1896", "Edward Michael \" Mike\"/\"Spanky\" Fincke", "the D\u00e2mbovi\u021ba River", "Philip Mark Quast", "Pierce County", "PPG Paints Arena", "May 10, 1976", "Devon Bostick", "Operation Julin", "BAFTA TV Award Best Actor", "First Balkan War", "1618", "Marty Ingels", "Carl David Tolm\u00e9 Runge", "1941", "June 2, 2008", "Charice", "Sleepy Brown", "Waimea Bay", "Ustad Vilayat Khan", "at the bottom of the brain immediately below the hypothalamus", "his last starring role was as Boston police detective Barry Frost", "restoring someone's faith in love and family relationships", "\"Eddie\"", "gold anniversary", "auk family", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance", "his newly rented Bailey, Colorado, home", "The Sopranos", "Winslow Homer", "Elizabeth II", "the Crimean War", "Alexey Pajitnov"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6709033613445379}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.823529411764706, 1.0, 0.0, 0.0, 0.1142857142857143, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-4551", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-4394", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-438", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5535", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1997", "mrqa_searchqa-validation-14284"], "SR": 0.53125, "CSR": 0.5602678571428572, "EFR": 0.9666666666666667, "Overall": 0.7211681547619049}, {"timecode": 49, "before_eval_results": {"predictions": ["The Yardbirds", "Dubai", "Silverstone", "Ted", "Triumph and Disaster", "1720", "Henry V", "Darren Aronofsky", "beetles", "cuticle", "100 years", "Rudyard Kipling", "dragonflies", "Cole Porter", "liriope", "\"For God's sake bring me a large scotch. What a bloody awful country\"", "Big Brother", "My Town Tokyo", "Beaujolais Nouveau", "Christchurch", "Paul Dukas", "Tom Watson", "9", "the inner ear", "Tokyo", "low-E", "the keeper of the Longstone (Fame Islands) lighthouse.", "God bless America, My home sweet home.", "Dangerous Minds", "death", "Apollo", "Ken Platt", "South Korea", "Boxing Day", "St Pancras", "fish", "wainscot", "toxins", "Scarborough", "Alan Turing", "Newton", "Calcium carbonate", "Bombay", "Anna Eleanor Roosevelt", "Roy Rogers", "naxos", "croquet", "Hitachi", "ruby", "Belgrade", "the gizzard", "the St. Louis Cardinals", "counter clockwise direction", "~ 3.5 million years old", "Tampa Bay Lightning", "Los Angeles Dance Theater", "Battle of Dresden", "Dr. Rajiv Shah", "$150 billion over 10 years in clean energy.", "root out terrorists within its borders.", "Latvia", "alligator", "Jerry Rice", "Johnny Weissmuller"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6552083333333334}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-2114", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7216", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6646", "mrqa_triviaqa-validation-7667", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-4647", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-675", "mrqa_searchqa-validation-2086"], "SR": 0.578125, "CSR": 0.5606249999999999, "EFR": 1.0, "Overall": 0.72790625}, {"timecode": 50, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2584", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5405", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3559", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14232", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7260", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7002", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96"], "OKR": 0.841796875, "KG": 0.48671875, "before_eval_results": {"predictions": ["Medusa", "Hawaii", "Easy Rider", "scrabble", "Percy Bysshe Shelley", "Billy Joel", "pardon", "New York City", "\"Apocalypse Now\"", "Roman Polanski", "Dogberry", "Battle Creek", "the Red Sea", "Mary Todd Lincoln", "Mary Poppins", "yokohima", "phonetics", "The Naked Brothers Band", "Julianne Moore", "saddle bags", "Holly Golightly", "quilt", "anemoi", "butter", "the Tagus", "the CIO", "acting", "nautilus", "bantu", "Denmark", "student loan", "\"The Passion of the Christ\"", "the flute", "Seattle", "Michael Jordan.", "John Quincy Adams", "Legion of Honour", "Louis XIII", "Korea", "Wintering at Valley Forge", "chancellor of West Germany", "Knickerbocker", "Crimean Tatar", "Almond Joy", "the White House", "a seashell", "Julius Caesar", "one dollar and eighty seven cents", "Dean Acheson", "Steelers", "jury trials", "Neil Patrick Harris", "Kyla Pratt", "Jonathan Goldstein", "9 imperial gallons", "(Ryan) Turner", "Spain", "Sean Yseult", "1754", "Robert Harper", "outside the Iranian consulate in Peshawar", "in his native Philippines", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "September 21, 2014"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6227272727272728}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.12121212121212123, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6167", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-11463", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-4385", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-1251", "mrqa_searchqa-validation-9809", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-15599", "mrqa_searchqa-validation-13662", "mrqa_searchqa-validation-9986", "mrqa_naturalquestions-validation-5464", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-674", "mrqa_hotpotqa-validation-3612", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-551", "mrqa_hotpotqa-validation-4911"], "SR": 0.515625, "CSR": 0.5597426470588236, "EFR": 1.0, "Overall": 0.7171047794117646}, {"timecode": 51, "before_eval_results": {"predictions": ["Reddi-wip", "a dachshund", "Saturn", "Women in the Work Force", "Risk", "a Bar Mitzvah", "cauliflower ear", "Clark Gable", "May 12, 1907", "Metacomet", "surrender", "Tarsus", "the Niagara River", "Hannibal Lecter", "\"A Drop of Water.\"", "the Arc de Triomphe", "George Frideric Handel", "the Cologne", "Indonesia", "Florence Henderson", "Linus Pauling", "gold", "the English Channel", "a whelp", "gas", "Ohio", "Million Dollar Baby", "rum", "an organ", "papua", "Macy\\'s", "Bush", "the Arctic Ocean", "enamel", "Port-au-Prince", "the Barbary Coast", "humility", "Michael Phelps", "rice", "gas masks", "\"to look like\"", "\"Juno\"", "the breast", "a jet of water", "Louis XIV of France", "a suspension bridge", "petticoats", "trudge", "JetBlue", "Ryan Seacrest", "socket", "Lake Michigan", "Spanish colonies", "as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "piscinae", "the house sparrows", "Islamabad", "Indianapolis Motor Speedway", "$26 billion", "Deftones", "Adam Lambert and Kris Allen", "27-year-old", "more than two years,", "Michael Madhusudan Dutta"], "metric_results": {"EM": 0.5, "QA-F1": 0.5679976851851851}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5185185185185185, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-14737", "mrqa_searchqa-validation-13742", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-13086", "mrqa_searchqa-validation-10764", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10726", "mrqa_searchqa-validation-3994", "mrqa_searchqa-validation-6187", "mrqa_searchqa-validation-10574", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-10180", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-8155", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-2705", "mrqa_newsqa-validation-3066"], "SR": 0.5, "CSR": 0.55859375, "EFR": 1.0, "Overall": 0.716875}, {"timecode": 52, "before_eval_results": {"predictions": ["Alfred Preis", "Nip/Tuck", "Adelaide Lightning", "Homeland", "1983", "3.9 mi", "Hilux", "Roc Me Out", "Snowball II", "Anna Clyne", "Flushed Away", "Elbow", "Mickey's Christmas Carol", "Ellie Kemper", "Aamir Khan", "Eugene Levy", "25 million", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu", "Best Actress", "Seb Rochford", "Samantha Spiro", "Martin O'Leary", "Los Angeles Galaxy", "Aamina Sheikh", "Total Nonstop Action Wrestling", "Don Hahn", "half of the Nobel Prize in Physics", "chamber music", "the east of Ireland", "Blue Grass Airport", "Tim Whelan", "the Cleveland Celtics", "Ashley Jensen", "Franc Roddam", "Ben R. Guttery", "Pieter van Musschenbroek", "ABC", "Rosie O'Donnell", "\"media for the 65.8 million,\"", "1902", "the USS \"Enterprise\" Kirk", "Las Vegas", "Todd Emmanuel Fisher", "supply chain management", "John M. Dowd", "August 9, 2017", "MGM Grand Detroit", "James Fell", "Clara Petacci", "from 1986 to 2013", "Bill Ponsford", "one of Jesus'disciples", "Edo Oparei", "the Gaget, Gauthier & Co.", "(Thomas) Jefferson", "Robert Boyle", "Vienna", "Harrison Ford", "the quarter lot in a suburb", "27-year-old's", "the Squirrel", "a knife", "water vapor", "Kitty Kelley"], "metric_results": {"EM": 0.5, "QA-F1": 0.5817843614718614}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-1584", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-1306", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2871", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-3286", "mrqa_hotpotqa-validation-2153", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-3391", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-2082", "mrqa_newsqa-validation-2415", "mrqa_searchqa-validation-11381", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-4118"], "SR": 0.5, "CSR": 0.5574882075471699, "EFR": 1.0, "Overall": 0.716653891509434}, {"timecode": 53, "before_eval_results": {"predictions": ["London", "April 2, 2018", "Welch, West Virginia", "1", "2010", "Mark Jackson", "Indonesia", "December 24, 1836", "2 September 1990", "BC Jean", "Toronto", "off the southernmost tip of the South American mainland", "Roger Dean Stadium", "Jesse Wesley Williams", "19 June 2018", "Prince William", "Sanchez Navarro", "Jason Mantzoukas", "pigs", "in Pittsburgh", "2018", "to the left of the dinner plate", "headdresses", "in a Norwegian town", "1960", "1840s", "heavy tank", "semi-automatic", "Humpty Dumpty and Kitty Softpaws", "displacement", "chlorofluorocarbons", "200 to 500 mg", "John Smith", "November 5, 2017", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys", "Qutab Ud - Din - Aibak", "1868 war veterans, such as Polish internationalist General Carlos Roloff and Seraf\u00edn S\u00e1nchez in Las Villas", "muscle fibers in a larger muscle or muscle bundle", "Robin", "March 26, 1973", "New England Patriots", "New York City", "S", "31 - member", "Ajay Tyagi", "Azpeitia, Spain", "in a brownstone in Brooklyn Heights, New York", "book and architecture", "19 June 2018", "Efren Manalang Reyes", "in a surfing trip around the world", "Barcelona", "iron", "Dodi Fayed", "January 4, 1976", "1921", "Edmund Allenby", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "\"bleaching\" in which algae living in the coral die and leave behind whitened skeletons.", "22", "the altitude", "\"Boots\"", "Foo Fighters", "Jennifer Grey"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6035188667541609}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.2222222222222222, 0.923076923076923, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.2857142857142857, 0.5714285714285715, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7058823529411764, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8873", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-3801", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-3837", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-6364", "mrqa_hotpotqa-validation-5720", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1638", "mrqa_searchqa-validation-6725"], "SR": 0.484375, "CSR": 0.5561342592592593, "EFR": 1.0, "Overall": 0.7163831018518519}, {"timecode": 54, "before_eval_results": {"predictions": ["the martini", "a final contest", "New York City", "Vclav Havel", "Parmesan cheese", "Tony Gwynn", "henri paulan", "North Carolina", "collagen", "Just say no", "Typewriter", "Diane Arbus", "Cincinnati", "Cleopatra, Queen of Denial", "Suez Canal", "Planet of the Apes", "Jeapardy", "Burt Reynolds", "north, east, and west", "the word encourage", "a projecting beam", "William Shakespeare", "phobias", "San Jose", "piano", "the Byzantine Empire", "Dunkirk", "Black", "Idolatry", "a pearl", "gelato", "Jesus", "viruses", "George Balanchine", "Alfred Stieglitz", "Bryan Adams", "Africa", "Marcus Junius Brutus", "Applebee\\'s", "Mercator", "Robin Hood", "Stegosaurus", "(Boris) Godunov", "Daniel Boone", "William Tecumseh Sherman", "Prison Break", "a hippopotamus", "Black Beauty", "\"Candid Microphone\"", "Sinclair Lewis", "Leo", "85 %", "Jamestown", "Uruguay", "cat", "Lord Fancourt Babberley", "Boulder Dam", "Personal History", "Hellenism", "Barnoldswick", "bodies and heads", "the U.S. Holocaust Memorial Museum", "BET", "Havana Harbor"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6479166666666667}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-12810", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-1885", "mrqa_searchqa-validation-601", "mrqa_searchqa-validation-13725", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-14985", "mrqa_searchqa-validation-1876", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-308", "mrqa_searchqa-validation-7376", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-1763", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-14717", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1749", "mrqa_newsqa-validation-23"], "SR": 0.609375, "CSR": 0.5571022727272728, "EFR": 1.0, "Overall": 0.7165767045454545}, {"timecode": 55, "before_eval_results": {"predictions": ["a zebra", "Sarah McLachlan", "a cake", "Japan", "C Daryl Whittier Chessman", "grade point average", "grapefruit", "Detroit", "Tiger Woods", "John Paul II", "Blackbeard", "goose", "Jane Goodall", "the Tower of London", "Ethiopian", "1", "Stephen Crane", "Luxor", "gung ho", "nickel", "Clinton", "Wyoming", "septum", "Nantucket", "M.A. and Ph.D., College of William", "Elvis Presley", "Gianlorenzo Bernini", "mosquitoes", "Frank Sinatra", "a British rock band", "photons", "National Archives Building", "low blood pressure", "Mousehunt", "Israel", "honey", "Rugby Football Union", "the Shrew", "a palace", "decaffeination", "Knott\\'s Berry Farm", "Phaedra", "Carl Linnaeus", "Australia", "Cecil B. DeMille", "Ventricular Tachycardia", "Barbary pirates", "a bagels", "a sitcom", "Matilda", "holly halleck", "Robert Frost", "Master Christopher Jones", "T.J. Miller", "7", "Christian", "Jerry Mouse", "AVN Adult Entertainment Expo", "England and Ireland", "The Beatles", "identity documents", "Her treatment met the legal definition of torture. and that's why I did not refer the case\" for prosecution.", "the 11th anniversary of the September 11, 2001, terror attacks.", "Philip Billard Municipal Airport"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6627938034188035}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.20000000000000004, 0.3076923076923077, 0.888888888888889]}}, "before_error_ids": ["mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-15527", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-12914", "mrqa_searchqa-validation-11601", "mrqa_searchqa-validation-14248", "mrqa_searchqa-validation-7849", "mrqa_searchqa-validation-12284", "mrqa_naturalquestions-validation-10546", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-7361", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2215", "mrqa_hotpotqa-validation-2840"], "SR": 0.5625, "CSR": 0.5571986607142857, "EFR": 0.9285714285714286, "Overall": 0.7023102678571429}, {"timecode": 56, "before_eval_results": {"predictions": ["Jean Lafitte", "Isomachus of Croton", "argyle", "the Pacific Ocean", "Easter", "forgive", "Dalai Lama", "the heptathlon", "a \"rain-catcher\"", "Thomas L. Friedman", "tea", "arterial blood vessels", "Nicholas II", "Amerigo Vespucci", "Patrick Henry", "eau de Toilette", "punk rock band", "Ho Chi Minh", "the pituitary gland", "Ben Johnson", "Kentucky", "the Roman Empire", "Nine to Five", "Cambodia", "lunch", "Velvet Revolver", "Sears", "flavonoids", "cherries", "Florence", "Ma Barker", "Joe DiMaggio", "Tie", "Naples", "\" Nick and Norah\\'s Infinite Playlist", "the Baruch Plan", "stars", "wine", "silk", "\"The Safety Dance\"", "the Cymric cat", "the Balconies of Lima", "a GPS", "North Carolina", "M&M'S Pe peanut Chocolate Candies", "a cake knife", "Tchaikovsky", "the Tuileries", "the Panama Canal", "Mathias Rust", "General McClellan", "Andy Serkis", "parthenogenesis", "Pangaea or Pangea ( / p\u00e6n\u02c8d\u0292i\u02d0\u0259 / ) was a supercontinent that existed during the late Paleozoic and early Mesozoic eras", "France", "mule", "60", "Silvia Navarro", "26 November", "John Morgan", "\"It's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "Mary Phagan, a white child laborer,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "the ship armed with AK-47s -- took Phillips with them aboard the ship's 28-foot lifeboat,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6268738055422838}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913043, 0.5714285714285715, 0.4615384615384615, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-6160", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-10639", "mrqa_searchqa-validation-9458", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-11186", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-795", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-11589", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-13439", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-15973", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-3262", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-1022"], "SR": 0.5625, "CSR": 0.5572916666666667, "EFR": 1.0, "Overall": 0.7166145833333334}, {"timecode": 57, "before_eval_results": {"predictions": ["a bystander", "French toast", "Mexico", "plug in", "William Faulkner", "Patty Duke", "Franklin Pierce", "Hindu", "Barbara Busey-Maurio", "Intel", "Hank Williams Jr.", "George C. Wallace", "the state\\'s largest city", "a four-line of scrimmage", "West Virginia", "Edward Hopper", "asteroids", "Mark Twain", "the Large Orbiting Telescope", "Pop-Tarts", "Robert Johnson", "John Hanks", "Adam Smith", "Tootsie", "water", "albino", "gdansk", "junk", "chinchillas", "Tennessee", "No Child Left Behind", "William S. Hart", "Gapers Block", "Francisco Franco", "Tennessee Williams", "surround", "Douglas Fairbanks", "West Point", "Revolver", "Steely Dan", "word", "Norway", "chicken Kiev", "George Clooney", "a diamond", "the Baltimore Orioles", "postcards", "Kentucky", "skateboarding", "Gaul", "blasters", "funding for operations, personnel, equipment, and activities", "Havana", "Melbourne", "Atticus Finch", "an elephant", "Planck", "Ella Jane Fitzgerald", "Jay Pritchett", "\"First Blood\"", "\"Dancing With the Stars.\"", "about 75 miles east of Yakima", "Monday.", "Upstairs"], "metric_results": {"EM": 0.625, "QA-F1": 0.6933894230769231}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-2264", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-6362", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6688", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-13808", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-8729", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-8340", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-381", "mrqa_newsqa-validation-2446", "mrqa_triviaqa-validation-7365"], "SR": 0.625, "CSR": 0.5584590517241379, "EFR": 1.0, "Overall": 0.7168480603448276}, {"timecode": 58, "before_eval_results": {"predictions": ["Gov. James Grant", "Sri Lanka", "Friendship 7", "Hinduism", "Billie Holiday", "wedlock", "trans fat and trans-unsaturated fatty acids", "Bridge to Terabithia", "Edward III", "Hello, Dolly!", "the Mesozoic Era", "Gettysburg", "Martin Lawrence", "plantains", "Hera", "Fosse", "stem cells", "a blade", "the Bodleian Library", "pupils", "a front", "James Franco", "salmon", "The Crow", "goat cheese", "James Watt", "1945", "the birthstone", "Ichabod Crane", "An Old Man, a Young Man", "Heather Locklear", "noun", "Holden Caulfield", "peanut sauce", "Saudi Arabia", "lamb", "LaDainian Tomlinson", "wheat", "Duke", "a photoelectric cell", "Cape Town", "sperm", "Austin Powers", "crackers", "moissanite", "vice presidential running mate", "Joseph Stalin", "La Guardia", "Chastity", "Turandot", "Texas Rangers", "Jean - Joseph Benjamin - Constant", "ice giants", "Amenhotep IV", "Zimbabwe", "colony", "phobia", "Haiti", "Argand lamp", "1891", "Secretary of State Hillary Clinton", "three-1", "an upper respiratory infection.", "Britain"], "metric_results": {"EM": 0.578125, "QA-F1": 0.62890625}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-7364", "mrqa_searchqa-validation-11844", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-16918", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-16956", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5137", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-14926", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-11795", "mrqa_searchqa-validation-13956", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2685", "mrqa_hotpotqa-validation-1482", "mrqa_newsqa-validation-2402", "mrqa_newsqa-validation-2472", "mrqa_naturalquestions-validation-3214"], "SR": 0.578125, "CSR": 0.558792372881356, "EFR": 1.0, "Overall": 0.7169147245762713}, {"timecode": 59, "before_eval_results": {"predictions": ["the Trade Mark Registration Act 1875", "Mary Magdalene", "The Pillow Book", "General Paulus", "the Grail", "butcher", "The Double", "Dr. Samuel Johnson", "Jessica Simpson", "Humble Pie", "gallbladder", "radical", "Amram and Jochebed", "Leo Tolstoy", "Birmingham", "an impossible object", "A Fistful of Dollars", "Australian shearers' strike", "Theodore Roosevelt", "raven", "John of Gaunt", "typhoid fever", "tin", "Microsoft", "Saint Laurent", "the Big Bang", "Willie Nelson", "horseracing", "Stars on 45 Medley", "Lundy", "Guinea", "Kerri Strug", "Belgium", "Del", "T. M. Lewin", "non-Orthodox synagogues", "Stitch", "Herbert Asquith", "Nirvana and Kiss", "mr humphries", "Paul Gauguin", "wildebeest", "the Low Countries", "50", "Charlie Harper", "nirvana", "Tarzan", "purple", "Bob Ferris", "aardvark", "Charles Darwin", "5.7 million", "Oklahoma", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "8,515", "villanelle", "Awake", "2008", "China, Taiwan, Hong Kong and Mongolia", "Patrick McGoohan", "prairie wolf", "heating", "A Tale of Two Cities", "Charlie Wilson"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6334077380952381}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4273", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-1078", "mrqa_triviaqa-validation-3489", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-6759", "mrqa_triviaqa-validation-7013", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-229", "mrqa_triviaqa-validation-6132", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-577", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1182", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-2061", "mrqa_searchqa-validation-13238"], "SR": 0.546875, "CSR": 0.55859375, "EFR": 0.9310344827586207, "Overall": 0.7030818965517242}, {"timecode": 60, "UKR": 0.66796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1121", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-122", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4518", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5828", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-1071", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10905", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-12032", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15373", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16915", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3329", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4232", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6187", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8373", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9809", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4887", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9613", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1078", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-5183", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-998"], "OKR": 0.810546875, "KG": 0.4421875, "before_eval_results": {"predictions": ["15", "G\u00f6tene in Sweden", "William Walton", "Rensselaer County", "more than 20", "\"Beauty and the Beast\"", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies.", "penny bun", "Overtime", "north", "Hockey Club Davos", "2014 New Year Honours for services to the arts and to charity.", "Domingo \"Sam\" Samudio", "Sunday", "Taylor Swift", "James Gay-Rees, George Pank, and Paul Bell", "to steal the plans for the Death Star, the Galactic Empire's super Weapon.", "graffiti", "ESPN", "Bangor International Airport", "October 29, 1985", "Point of Entry", "Mickey's Christmas Carol", "the Harpe brothers", "the 1940s and 1950s", "Port Clinton", "deadpan sketch group", "Bharat Ratna", "Ronald Ryan", "made into a TV series", "the 2011 Pulitzer Prize in General Nonfiction", "john le Page and unenrolled attorney Eliot Cutler", "IT products and services", "American", "As of the 2010 census", "Critics' Choice Television Award for Best Supporting Actress in a Comedy Series", "Jeff Meldrum", "Picric acid", "23 March 1991", "1979", "Hannaford", "in 1968", "under the Caesar Julian and later a consul", "Rigoletto", "Bill Clinton", "\"Twister\"", "94", "horror film", "&quot;Eh-oh!&quot", "law", "28,776", "a Canaanite god associated with child sacrifice", "on the microscope's stage by slide clips, slide clamps or a cross-table", "commemorating fealty and filial piety", "(Cain) and Eve's eldest son, who was also the world's first murderer.", "video", "thomas", "one group -- People Against Switching Sides (PASS) --", "a pair of U.N. agencies", "jund Ansar Allah", "the Thames", "Pamela Anderson", "Henry Ford", "methane"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6231274801587301}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666, 0.4, 0.16666666666666669, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.4, 0.8, 0.28571428571428575, 1.0, 0.16666666666666669, 1.0, 0.0, 0.8333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-804", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5589", "mrqa_hotpotqa-validation-2263", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-978", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-1151", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-1570", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-3314", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-2732"], "SR": 0.46875, "CSR": 0.5571209016393442, "EFR": 0.9705882352941176, "Overall": 0.6896824523866923}, {"timecode": 61, "before_eval_results": {"predictions": ["Home Fires", "Law Adam", "Daniel Wroughton Craig", "Magnus Carlsen", "a facelifted 850 saloon", "400 MW", "1991", "Dra\u017een Petrovi\u0107", "Lord's Resistance Army", "Andrew Joseph \" Andy\" Cohen", "Manhattan", "IFFHS World's Best Goalkeeper", "David May", "New Jersey", "Sir Derek George Jacobi", "Waimea Bay", "Waylon Jennings, Kris Kristofferson", "Mikoyan MiG-29", "Nickelodeon on Sunset", "Terry the Tomboy", "a Anglo-Saxon saint", "Give Up", "Matthew Ward Winer", "University of Kentucky", "WB Television Network", "Ice Princess", "2004", "democracy and personal freedom", "Australian", "Norse mythology", "Konstant\u012bns Raudive", "Melville", "5,922", "White Horse", "Black Abbots", "\"Real\"", "Kentucky, Virginia, and Tennessee", "2011", "five", "Veronica Hamel", "literary magazine", "French, English and Spanish", "Edward James Olmos", "John R. Leonetti", "\"Alceste\"", "Perth", "Cersei Lannister", "Baltimore", "stopwatch feature", "Tayeb Salih", "\"The Simpsons\"' thirteenth season", "Ferm\u00edn Francisco de Lasu\u00e9n", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W \ufeff / \ufefb\ufffd 22.000 \u00b0 W", "Yuzuru Hanyu", "Jose Antonio Reyes", "Jordan", "Don Black", "sniff out cell phones.", "18", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "Prison Break", "Tebbs", "Sicily", "Yukon"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7190468385780886}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.8, 0.5, 0.4, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-175", "mrqa_hotpotqa-validation-2135", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3542", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-2187", "mrqa_naturalquestions-validation-5451", "mrqa_triviaqa-validation-1648", "mrqa_newsqa-validation-4032", "mrqa_searchqa-validation-16178"], "SR": 0.609375, "CSR": 0.5579637096774194, "EFR": 0.96, "Overall": 0.6877333669354839}, {"timecode": 62, "before_eval_results": {"predictions": ["an engagement", "Pope Benedict XVI refused", "public-television show", "company Polo", "punish participants", "Venezuela", "United Nations", "EU naval force", "a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "India", "snowstorm", "A member of the group dubbed the \"Jena 6\"", "the Harris Fire", "the legal right to freedom from tyranny", "Dead Weather's \"Horehound\"", "Cash for Clunkers", "San Diego", "combat veterans", "Robert Park", "Mexican military", "dinosaur", "David", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "Thursday", "his wife's name", "$17,000", "helping people out, putting a smile on local nationals' faces, little kids that need our help.", "Bob Johnson", "Matthew Fisher", "26", "angel", "$1,500", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "American Civil Liberties Union", "safety issues", "1994", "High Court Judge Justice Davis", "to provide security as needed.", "$83,27014", "$250,000", "kase Ng", "Islamabad", "hundreds of people", "North Korean newspaper Rodong Sinmun", "Osama", "criminal investigation into the statements and reports given by the woman.", "\"exceptional circumstances surround these memos and require their release.\"", "the capital city of Harare", "World Boxing Council welterweight champion", "sexual assault with a minor", "the District of Columbia National Guard,", "March 1930", "1961", "Rajendra Prasad", "Manchester", "Secretary of State William H. Seward", "Cascade Range", "Ice Princess", "Etihad Aldar Spyker F1 Team", "compact car", "delete", "Kansas", "John James Audubon", "Kim Basinger"], "metric_results": {"EM": 0.375, "QA-F1": 0.4720634493021347}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 0.33333333333333337, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.0, 0.08, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.0, 0.06896551724137932, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.4, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-1289", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6501", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-3993", "mrqa_searchqa-validation-5326"], "SR": 0.375, "CSR": 0.5550595238095238, "EFR": 0.975, "Overall": 0.6901525297619048}, {"timecode": 63, "before_eval_results": {"predictions": ["Israel", "billboards with an image of the burning World Trade Center", "Saturn", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.", "Kgalema Motlanthe,", "Ken Choi,", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "travel in cars with tinted windows", "up to $50,000 for her,", "gun charges", "January 24, 2006.", "usion teams", "Philippines", "used", "July", "her home", "natural gas", "at the House of Blues in Hollywood.", "jazz", "almost 30 tunnels, including the 6.2-mile Moffat Tunnel,", "put a gun to Valencia's head.", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "KBR", "Ralph Lauren", "Dubai", "Al-Shabaab", "\"Wicked.\"", "269,000", "eight", "Dube attempted to escape but died almost instantly from his wounds.", "North Korea", "February 2008", "Wally", "Alina Cho", "Venus Williams", "80 percent of the woman's face", "1983", "he gave the victims \"assurances of the church's action\" after the April 18 meeting.", "against people who independent of their race, religion, ethnicity, social condition etc. accepted money and put themselves at the service of the army in an area that is the object of military operations.", "three-time road race world champion,", "\"His treatment met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "Yemen,", "11", "Matthew Fisher", "Afghanistan's restive provinces", "Rob Lehr,", "insect stings,", "Tennessee", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's Vanni region.", "cancer-causing toxic chemical.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "Sleeping with the Past", "in the ark of the covenant", "the pachytene stage of prophase I of meiosis", "the Great Chicago Fire", "4", "Edward III", "fourth", "Academy Award for Best Art Direction", "1974", "Alexander Pushkin", "The New York Botanical Garden", "Percheron", "November"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4776729544315819}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.9411764705882353, 1.0, 0.10526315789473685, 0.4444444444444445, 0.8, 0.2608695652173913, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.25, 1.0, 0.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.11764705882352942, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.2666666666666667, 1.0, 0.9166666666666666, 1.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-3049", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-7035", "mrqa_triviaqa-validation-1159", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-3383", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1794"], "SR": 0.34375, "CSR": 0.5517578125, "EFR": 1.0, "Overall": 0.6944921875000001}, {"timecode": 64, "before_eval_results": {"predictions": ["\"We Found Love\"", "Elisabeth,", "\"We have received three people from the blast at Rabbani's house. Among the injured are Masoom Stanikzai, one bodyguard and an assistant\" to Rabbani.", "40", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "might give buyers the peace of mind knowing they will get a replacement vehicle.", "19 American tourists and two Egyptians -- the bus driver and a tour guide -- were injured.", "Paul McCartney and Ringo Starr", "great jazz", "homicide.", "Sodra nongovernmental organization,", "The few things we can confirm -- yes the four story Caribbean Market building is completely demolished,\"", "full-length computer-generated animated film", "first Oscar Award-winning Hollywood actor ever to star in a Bollywood movie,", "the 12th on the Blue Monster course at Doral", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "signed a power-sharing deal with the opposition party's breakaway faction,", "collaborating with the Colombian government,", "the Russian air force,", "Rod Blagojevich,", "Fiona MacKeown", "about 50", "the legitimacy of that race.", "President Obama", "Yoko Ono Lennon and Olivia Harrison,", "aron Bialek", "1998.", "45 minutes, five days a week.", "Israel and the United States", "Monday.", "Frank Ricci,", "Sixteen", "Most of the 103 children that a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans,", "The EU naval force", "Al-Shabaab", "Daytime Emmy Lifetime Achievement Award", "the country's Tamil insurgents are on the verge of total defeat,", "the foyer of the BBC building in Glasgow, Scotland", "The UNHCR", "The EU naval force", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple, with the latest resulting in the arrest of Mesac Damas in January,", "a botched robbery that left an off-duty New York police officer dead.", "two", "6-2 6-1", "to the U.S. Consulate in Rio de Janeiro,", "John Demjanjuk", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "secure more funds from the region.", "Redwood Original", "Emily Blunt", "pulmonary heart disease ( cor pulmonale )", "a board that has lines and pads that connect various points together", "Richard Wagner", "H. H. Asquith", "Centers for Medicare and Medicaid Services", "J. Edgar Hoover", "Macomb County", "Custer", "Louis XIV", "a waterbed", "Risk"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5397700193118493}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.14285714285714288, 0.33333333333333337, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.10526315789473684, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5454545454545454, 0.5, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.06451612903225806, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352942, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 0.3, 0.13333333333333333, 0.7692307692307693, 0.23529411764705882, 0.0, 0.7499999999999999, 0.15384615384615385, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-372", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-4188", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-574", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-5934", "mrqa_hotpotqa-validation-2837"], "SR": 0.421875, "CSR": 0.5497596153846154, "EFR": 1.0, "Overall": 0.6940925480769231}, {"timecode": 65, "before_eval_results": {"predictions": ["North West England", "Carol Ann Duffy", "liquidambar styraciflua", "George Orwell", "Battleship", "Hurricane Faith", "the Slavic women accompanying their husbands in the First Balkan War.", "Teutonic Knights", "9", "James Harrison", "Germany", "Jonathan Katz", "Ford Island", "2011", "NCAA Division I", "Tim Allen", "Latium in central Italy", "Paul Avery", "Berea College", "Christopher Lloyd Smalling", "January 4, 1976", "a zero-g-roll", "1971", "Clovis I", "Tahir \"Tie\" Domi", "in 2007", "poet, and writer", "Quasimodo,", "Savin Yeatman-Eiffel of Sav!", "Pieter van Musschenbroek", "20 May 1973", "American actor", "Attorney General and as Lord Chancellor of England", "Plato", "Fife, Scotland", "Henry Mills", "ribosomal RNA", "Ronald Ryan", "A Hard Day's Night", "Humberside", "Dumfries and Galloway,", "Rudolph the Red-Nosed Reindeer", "from 1989 until 1994", "Cecily Legler Strong", "Polish-Jewish", "Philip Aaberg", "in 2005", "Levon Helm", "Chengdu Aircraft Corporation (CAC)", "White Knights of the Ku Klux Klan", "Reunited Worlds", "Lou Rawls", "the first year", "Michael English", "Parkinson's disease", "I Will survive", "William Butler Yeats", "the Dominican Republic", "11", "why you broke up", "Carmen", "New York City", "Massachusetts", "in July"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6873511904761904}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5875", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-2872", "mrqa_hotpotqa-validation-5662", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-4711", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-4573", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1180", "mrqa_searchqa-validation-2844", "mrqa_newsqa-validation-271"], "SR": 0.546875, "CSR": 0.5497159090909092, "EFR": 0.896551724137931, "Overall": 0.6733941516457681}, {"timecode": 66, "before_eval_results": {"predictions": ["Psalms", "alligator", "cancer, diabetes and endocrinology", "quoit", "Ramona", "Tobacco Road", "M*A*S*H", "\"Opportunity seldom knocks twice\"", "Smokey Robinson", "a snake", "Gladiator", "primaries", "trachea", "Cairo", "The Cotton Club", "a sandstorm", "George Noel Gordon", "neutrino", "Jodie Foster", "George Eliot", "clouds", "\"The Maracot Deep\"", "San Juan Capistrano", "Auschwitz Birkenau", "China", "Uganda", "salad dressings", "Edward", "pomegranate", "Bali", "Paris", "2004 Athens Games", "Elizabeth", "kings", "blacklist", "take a small boat", "a jumper", "Jean", "Yogi Berra", "China", "Hawaii", "birds", "peripheral vision", "Turkish", "the Delacorte", "head", "Rebecca Romijn", "Buttercream Frosting", "potential energy", "the Byzantine Empire", "Reno", "16 seasons", "photoelectric", "Moscow, Russia", "Joan Crawford", "Bassenthwaite Lake", "the moon passes directly between the sun and Earth", "Vanilla Air Inc.", "Jack Ridley", "diplomat", "children's books", "Six", "attempted burglary", "missile"], "metric_results": {"EM": 0.5, "QA-F1": 0.5518229166666666}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-995", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-13562", "mrqa_searchqa-validation-8242", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-14972", "mrqa_searchqa-validation-5294", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-10375", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-16008", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15880", "mrqa_searchqa-validation-5807", "mrqa_searchqa-validation-488", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-1198", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-3230", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-15274", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-14831", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-2870", "mrqa_triviaqa-validation-3020", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-5467"], "SR": 0.5, "CSR": 0.5489738805970149, "EFR": 1.0, "Overall": 0.693935401119403}, {"timecode": 67, "before_eval_results": {"predictions": ["Steven Spielberg", "Louisiana", "Wilbur Wright", "Charles Lindbergh", "The Madness", "Stephen Sondheim", "the Genesis Flood", "a calculating machine", "Bill Wyman", "U.S. Public Health Service", "T.S. Eliot", "lead", "the American Girl", "French", "gravitational", "Fisherman\\'s Wharf", "Santa Fe", "Ted Koppel", "Sex Pistols", "chess", "Michael Jordan", "fairground", "doughboy", "Brge Rosenbaum", "Liston", "turkeys", "Secretariat", "soupman", "a lance head", "citric", "Homer", "an oar", "a woman scorned", "John Paul II", "Will Rogers", "Hairspray", "Orlando", "15", "the U.S. Navy", "River Phoenix", "the Sydney, Australia Harbor", "mutton", "easel", "Napoleon", "the flag of Mongolia", "Peter the Great", "barn-raising", "corporal punishment", "Missouri", "Sweeney Todd", "Paris", "1956", "Ritchie Cordell", "\" Two Days Before the Day After Tomorrow ''", "jujitsu", "Salvador Dali", "Robert De Niro", "1993", "October 17, 2017", "from 1986 to 2013", "Afghanistan", "Mammoths", "co-writing credits", "Gary Grimes"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5807291666666666}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-15468", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-11812", "mrqa_searchqa-validation-7771", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-626", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-288", "mrqa_searchqa-validation-7053", "mrqa_searchqa-validation-1250", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-15211", "mrqa_searchqa-validation-4698", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-11023", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-1882", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-512", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2152"], "SR": 0.546875, "CSR": 0.5489430147058824, "EFR": 0.9655172413793104, "Overall": 0.6870326762170386}, {"timecode": 68, "before_eval_results": {"predictions": ["\"Fisherman's ring\"", "Omaha", "Antwerp", "the Matterhorn", "Loch Lomond", "Alaska", "Frasier", "a temporary need", "Denmark", "\"ball in tube\" or electromechanical crash sensor", "antonyms", "Pygmalion", "cholera", "Edward Estlin", "Wilhelm Conrad Roentgen", "Kathleen Kennedy Townsend", "Yes", "the Green Hornet", "\"People, people who need\" Peabodys", "geolu", "amniotic fluid", "\"300\"", "Diner", "Cleopatra", "pep", "St. Petersburg, Russia", "Japan", "the Jordan", "Derek Jeter", "Hans Christian Andersen", "a value of type", "defense", "\"The Tyger\"", "Percy Shelley", "pearls", "carbon dioxide", "earthquakes", "Earvin \"Magic\" Johnson Jr.", "Citizen Kane", "\"zero-g\"", "Mathew Brady", "Clinton", "opponent", "Tasmania", "Wyoming", "Bilbo Baggins", "the quick brown fox", "Denmark", "wheat", "\"Free Bird\"", "a frigate", "a protocol", "presidential representative democratic republic", "the Pir Panjal Range in Jammu and Kashmir", "Barcelona", "China", "Leander", "the Lewis and Clark Expedition", "Morocco", "1910", "club managers,", "flannel or wool", "several weeks", "2011"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6623511904761905}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-16760", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-7831", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-16947", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-1342", "mrqa_searchqa-validation-2169", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-10953", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-15704", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-3156", "mrqa_hotpotqa-validation-3841", "mrqa_newsqa-validation-3500"], "SR": 0.59375, "CSR": 0.5495923913043479, "EFR": 1.0, "Overall": 0.6940591032608696}, {"timecode": 69, "before_eval_results": {"predictions": ["Kentucky Fried Chicken", "a dickey", "Follies", "Ford", "Andrew Jackson", "Agamemnon", "spurs", "Robert Bartlett", "\"tears\"", "cantons", "crawfish", "a shopping center", "percussus", "Nixon", "Diana", "chert", "Constellations", "Indiana Jones and the Kingdom of the Crystal Skull", "Fox", "20", "Mendel", "Maria Callas", "Hulk Hogan", "Rose DeWitt Bukater", "a horse", "Hard Day", "Making the Band 3", "Garland", "Autumn in New York", "telephone operator", "Franklin D. Roosevelt", "Tranio", "\"tears of\"", "La Salle", "lattice", "a penny", "succotash", "the retina", "a prayer", "northern Idaho", "the Sopranos", "Hark", "Huguenots", "the Brooklyn Dodgers", "king", "blue", "mascara", "Rooster", "ponderosa", "Homestead", "Lawrence Wien", "Russell Westbrook", "Mexico", "Andrew Michael Harrison", "Crete", "Miles Morales", "Peter Nichols", "the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "1858", "Kentwood, Louisiana", "Newcastle", "five years", "\"Sad, this is not an anomaly in Naples and in that neighborhood.\"", "a mermaid"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5660879629629629}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.6666666666666666, 0.2962962962962963, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-7946", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-5203", "mrqa_searchqa-validation-843", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-10815", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-12397", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-4606", "mrqa_searchqa-validation-705", "mrqa_searchqa-validation-16153", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-5659", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-12330", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-5206", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-2727"], "SR": 0.453125, "CSR": 0.5482142857142858, "EFR": 1.0, "Overall": 0.6937834821428572}, {"timecode": 70, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-947", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16953", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4642", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7593", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9643", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10254", "mrqa_squad-validation-10328", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1075", "mrqa_squad-validation-1183", "mrqa_squad-validation-1319", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-2975", "mrqa_squad-validation-3139", "mrqa_squad-validation-3168", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4147", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5532", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5826", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6624", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-821", "mrqa_squad-validation-8253", "mrqa_squad-validation-8410", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5488", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6565", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.849609375, "KG": 0.51171875, "before_eval_results": {"predictions": ["Polk", "Stanch", "delta", "barroco", "St. Petersburg", "China", "Prohibition", "Onomastic Sobriquets", "The Godfather", "Maria Sharapova", "McDonald\\'s", "Sonny Corleone", "11", "The Stars and Stripes Forever", "Jackie Moon", "Pulp Fiction", "expunge", "the Rhine", "a rocket launcher", "dilithium", "Schwarzenegger", "Epstein-Barr virus", "hydrogen", "a cadence", "U.S. Naval Academy", "Iowa", "indirect discourse", "a circle", "Pussycat Dolls", "Shakespeare", "a lump", "Vin Diesel", "(Charles) Le Brun", "Heath", "Odysseus", "(Michael) Phelps", "Annapolis", "the Maccabees", "Rolls Royce", "a doses", "the Caucasus Mountains", "Lafayette", "the gopher", "Paul", "Coca-Cola", "Warren Burger", "apogee", "a moon", "a mirror", "david archuleta", "Union Carbide", "62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Ireland", "1949", "\u2018Marry in \u2018when\u2019 and you\u2019ll go.", "Stockholm syndrome", "Ilkley", "Geographical Indication", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "The play All's Well That Ends Well", "military veterans", "3.42 points", "1000 square meters", "Larry Ellison"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6370132522706051}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true], "QA-F1": [0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9803921568627451, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 0.888888888888889, 0.9090909090909091, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16561", "mrqa_searchqa-validation-8833", "mrqa_searchqa-validation-7803", "mrqa_searchqa-validation-5802", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-9498", "mrqa_searchqa-validation-13731", "mrqa_searchqa-validation-15266", "mrqa_searchqa-validation-958", "mrqa_searchqa-validation-10678", "mrqa_searchqa-validation-11878", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-12155", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6489", "mrqa_triviaqa-validation-1074", "mrqa_hotpotqa-validation-2854", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1701"], "SR": 0.515625, "CSR": 0.5477552816901409, "EFR": 1.0, "Overall": 0.7275198063380282}, {"timecode": 71, "before_eval_results": {"predictions": ["Boletus edulis", "119", "560", "Nine Inch Nails", "Klasky Csupo", "influenced by the music genres of electronic rock, electropop and R&B", "the \"Home of the Submarine Force\"", "McG", "River Shiel", "\"Vera Cruz\"", "Lommel differential equation", "Harry Booth", "Southland", "1.23 million", "Oxford, UK", "281", "Northern Ireland", "1916 Easter Rising", "Park Hyung-Sik", "Conservative Party", "quantum mechanics", "\"battlefield of Europe\"", "Theo James Walcott", "April 8, 1943", "\"Traceless\" or \"Without a Trace\"", "their unusual behavior", "10", "Victoria Peak", "\"Back to December\"", "Volvo 850", "Hindi", "Statutory List of Buildings of Special Architectural or Historic Interest", "High Falls Brewery", "one child, Lisa Brennan-Jobs", "Fort Frederick", "Autopia", "Hindi", "Art Deco-style", "Jon Walker", "\"Beauty\"", "Mani", "Green Chair", "Walker Smith Jr.", "the mouth of the Waimea River", "Juan Manuel Mata Garc\u00eda", "Umina Beach", "\"Mickey Mouse Cup\"", "Kinnairdy Castle", "the rarer Persian fallow deer as a subspecies (\"D. d. mesopotamica\")", "Stephen James Ireland", "Lola Dee", "1924", "the cell nucleus", "inner core", "Helen of Sparta", "Vince Cable", "a number", "\"A good vegan cupcake has the power to transform everything for the better,\"", "Sporting Lisbon", "Illness", "a computer programming competition", "bowling", "Gin Rummy", "enamel"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6653769841269841}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5589", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-1397", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-5371", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-237", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-1716", "mrqa_naturalquestions-validation-2502", "mrqa_triviaqa-validation-2838", "mrqa_triviaqa-validation-2441", "mrqa_searchqa-validation-12240", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267"], "SR": 0.578125, "CSR": 0.5481770833333333, "EFR": 1.0, "Overall": 0.7276041666666666}, {"timecode": 72, "before_eval_results": {"predictions": ["U2 360\u00b0 Tour", "Brent Wilson was fired", "Peter Kay's Car Share", "Michael Crawford", "Brazilian Jiu-Jitsu", "2012 Olympic bronze medalist", "six-color printing process", "Colonel", "the first month of World War I", "Germany", "River Clyde", "George Washington Bridge", "Argand lamp", "Lowestoft, Suffolk", "The Ethics of Ambiguity", "Bishop's Stortford", "The Rural Electrification Act of 1936", "Vitor Belfort", "Carlos Coy", "Hawaiian", "Cuban descent", "35,000 members", "24 NCAA sports", "the Bahamian island of Great Exuma over two weekends in April and May 2017.", "scholarly analysis and research-based study of music", "Joseph Bonaduce", "Premier Division", "Kelly Bundy", "Canada's first train robbery,", "Carson City", "Ben R. Guttery", "arts manager", "Toronto", "New Zealand", "August 14, 1848,", "Peel Holdings", "24 December 1692", "Indian epic historical drama film", "New jack swing", "672 km2", "140 million", "Lamar Hunt", "Vienna", "Alemannic", "1718", "SpongeBob SquarePants 4-D", "Seti I", "Raabta", "Joseph E. Grosberg", "January 15, 1975", "2015", "medieval realism", "energy loss", "Gautamiputra Satakarni", "Louisa of Saxe-Coburg,", "Audi A4", "Valletta", "drugs are funding the insurgency,", "Iran", "drug cartels", "New York City", "Turandot", "Champagne", "seabirds"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6221354166666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.2666666666666667, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.33333333333333337, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-4558", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4214", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-3483", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4489", "mrqa_hotpotqa-validation-1006", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-6234", "mrqa_triviaqa-validation-6188", "mrqa_triviaqa-validation-4960", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-1604", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3199"], "SR": 0.484375, "CSR": 0.5473030821917808, "EFR": 1.0, "Overall": 0.7274293664383562}, {"timecode": 73, "before_eval_results": {"predictions": ["Adonijah", "Poland", "Hillary Clinton", "Hannibal", "Tartarus", "Birmingham", "syndicate", "Hansel and Gretel", "J.M.W. Turner", "Heisenberg", "astronaut", "glockenspiel", "Laura and Kenneth Hockney", "Kyoto Protocol", "alexandie", "Croatian", "Kansas City", "South Carolina", "Survivor Series", "taxis", "peppers", "piscinae", "Edward III", "Bruce Wayne", "lighting", "Tesco", "Cologne", "Reducing Climate Impact", "Midtown", "Nikola Tesla", "smartphones and similar devices to establish radio communication with each other by touching them together or bringing them into close proximity,", "North Carolina", "Grimbsy", "Sandstone Trail", "Robert Guerrero", "Virginia Plain", "Columbia", "Scotland", "Freema Agyeman", "Spanish", "a toilet", "helped by the gods Hermes and Athena", "1911", "alexandrovsk", "world heavyweight champion", "Dezeen", "Madness", "Mao Zedong", "Sweet Home Alabama", "Greece", "St Helens", "a valuable way to feed the poor, and would relieve some pressure of the land redistribution process", "required many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the", "biscuit", "High Knob", "Sacramento Kings", "200", "African National Congress Deputy President Kgalema Motlanthe", "Michelle Obama", "Quetta, the capital of Balochistan province,", "campanile", "Bret Maverick", "Ronald McDonald House Charities", "#364"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5206168831168831}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0909090909090909, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9714285714285714, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4166", "mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-5341", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2517", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-5672", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-6862", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-4776", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-5719", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-5468", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-4414", "mrqa_hotpotqa-validation-5878", "mrqa_newsqa-validation-3358", "mrqa_searchqa-validation-6760", "mrqa_searchqa-validation-1361"], "SR": 0.4375, "CSR": 0.5458192567567568, "EFR": 0.9444444444444444, "Overall": 0.7160214902402402}, {"timecode": 74, "before_eval_results": {"predictions": ["the iPod Classic", "1-0 victory", "Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "people", "Simon Cowell", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "1983", "free milk.", "syria and aid workers", "love the outdoors, particularly if they have a garden to eat from,", "the crew of the Bainbridge,", "10,000", "his business dealings for possible securities violations", "former Procol Harum bandmate Gary Brooker", "California, Texas and Florida,", "morphine sulfate oral solution 20 mg/ml.", "\"it should stay that way.\"", "Iran", "and censorship remain rife across the Middle East and North Africa,", "Rawalpindi", "death", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "peanuts", "United States", "Samoa", "video network", "Six", "the Irish capital.", "former Spice Girl Geri Halliwell and singer Robbie Williams", "At least 13", "Whitney Houston", "Ferraris", "free fixes for the consumer.", "nine-wicket win", "10 below", "Madhav Kumar Nepal", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "so people are ready and looking for those kinds of images", "fifth", "the 1979 Islamic Revolution.", "and British Foreign Office", "JBS Swift Beef Company", "Hurricane Gustav", "President Obama", "murder", "Manny Pacquiao", "The American Civil Liberties Union", "1,073 immigration detainees", "flying without a valid license,", "Alfredo Astiz", "Los Angeles", "Super Bowl VII", "3", "Steve Ford", "whetstones", "Edinburgh", "sharks can be stared down", "Norbertine", "Anheuser-Busch", "Beauty and the Beast", "the Chiefs", "Cleopatra", "Philip", "Sparafucile"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5896824223570547}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.6, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7272727272727273, 0.7272727272727273, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2101", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1373", "mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-369", "mrqa_searchqa-validation-14806"], "SR": 0.484375, "CSR": 0.5449999999999999, "EFR": 1.0, "Overall": 0.7269687499999999}, {"timecode": 75, "before_eval_results": {"predictions": ["1-1 draw", "Ten South African ministers and the deputy president", "Sharon Bialek", "being back here,", "voluntary manslaughter", "The Kirchners", "Sharon Bialek", "Roy Foster", "March 24,", "African-Americans", "Bill Haas", "David Beckham", "Rwanda", "Caster Semenya", "nuclear weapon", "$627,", "Turkish President Abdullah Gul.", "People Against Switching Sides (PASS)", "sedative", "26", "Bob Dole,", "U.S. 93 in White Hills, Arizona,", "the assassination of President Mohamed Anwar al-Sadat", "Australia", "re-examine other regions where similar blowback might take place.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Veracruz, Mexico,", "Sharon Bialek", "About 100,000 workers", "were separated", "San Simeon, California,", "ABCs", "Splash D'Souza,", "2001", "Toy Story", "a news blackout was imposed on the foreign media.", "9 percent", "Jezebel.com's Crap E-mail From A Dude", "Kenner, Louisiana", "Saturday", "Lashkar-e-Tayyiba (LeT)", "2005", "Hong Kong's Victoria Harbor", "from the Bronx.", "mentioned anyone wanting to harm them.", "71 percent of Americans consider China an economic threat to the United States,", "Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "the Bronx", "Idriss Deby hopes the journalists and the flight crew will be freed,", "April 6, 1994", "murder in the beating death of", "February 7, 2018", "1439", "Blue laws", "Buddha", "Prince Bumpo", "nitrogen dioxide", "Araminta Ross", "ABC1 and ABC2", "G\u00f6tene in Sweden", "the kitchen sink", "whey", "Benito Mussolini", "roosevelt"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5555759803921568}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8235294117647058, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4014", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-406", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-177", "mrqa_searchqa-validation-7856"], "SR": 0.484375, "CSR": 0.544202302631579, "EFR": 0.9696969696969697, "Overall": 0.7207486044657097}, {"timecode": 76, "before_eval_results": {"predictions": ["cricket", "Alan Greenspan", "Bolivia", "Matalan", "Ub Iwerks", "Macbeth Soliloquy", "German Chancellor Angela Merkel", "Monopoly", "transsexual", "black", "the skull, jaw, shoulder, rib cage, and pelvis", "doubles", "Paul Gauguin", "Ben Jonson", "a parallelogram", "Willy Lott's", "19-9", "Dubai", "london", "14", "lice", "palladium", "NASA's Hubble Space Telescope", "James Van Allen", "Rawalpindi", "Mexico", "Philip Glenister", "Miss Prism", "Charles Greville", "Beethoven", "Lakeland", "1957", "Margaret Thatcher", "Mauricio Pochettino", "USS Missouri", "Sensurround", "Venus", "Olympic Games", "blue", "Rihanna", "Tripoli.", "euthanasia", "Eva Duarte", "Doctor Who", "pink", "Emile de Becque", "Illinois", "The Magnificent Seven", "Uranus", "typhoid fever", "Ross MacManus", "September 9, 2012", "15 Bonanza Creek Lane", "seven", "Ricky Marco i Vives", "American pharmaceutical company", "Queens, New York", "Manchester United's", "Damon Bankston", "powerful Flemish tapestries in an east-facing sitting room called the Morning Room.", "The Big Sleep", "Dairy Queen", "Paul the Apostle", "Confederate victory"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6409970238095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-2144", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-4855", "mrqa_triviaqa-validation-1504", "mrqa_triviaqa-validation-2828", "mrqa_triviaqa-validation-4525", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-6179", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-2333", "mrqa_naturalquestions-validation-4746", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2631", "mrqa_searchqa-validation-15341", "mrqa_naturalquestions-validation-767"], "SR": 0.546875, "CSR": 0.544237012987013, "EFR": 1.0, "Overall": 0.7268161525974026}, {"timecode": 77, "before_eval_results": {"predictions": ["nearly $162 billion in war funding", "poor.", "183 people, including 137 children,", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost, and what we need to achieve to secure our interests.\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Transport Workers Union leaders", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "President Bush", "November 26,", "Wednesday.", "change course", "felony drug charges", "sitting in Renaissance-era clothes and holding a book.", "off the coast of Dubai", "his past and his future", "Lance Cpl. Maria Lauterbach and her fetus", "Hussein's Revolutionary Command Council.", "McDonald's' plans", "$31,000", "Black History Month", "tenement in the Mumbai suburb of Chembur,", "Monday.", "weren't taking it well.", "The island's dining scene", "22", "the foyer of the BBC building in Glasgow, Scotland", "Jenny Sanford,", "The president,", "Graham's wife", "hank Moody", "two", "ambassadors", "fritter his cash away on fast cars, drink and celebrity parties.", "Everton", "criminals", "The Ministry of Defense", "civilians,", "21-year-old", "Ignazio La Russa", "Passers-by", "Pixar's", "NATO fighters", "New York City Mayor Michael Bloomberg", "A Colorado prosecutor", "16 Indiana National Guard soldiers", "1940's", "The Maraachlis' daughter, Zeina,", "a member of the self-styled revolutionary Symbionese Liberation Army", "South Africa", "\"I am sick of life -- what can I say to you?\"", "tanker that sailed under a Saudi flag,", "very important in meat technology", "Walter Pauk", "Louis Prima", "Alanis Morissette", "mercury", "Baroness Thatcher", "February 16, 1944", "Fort Saint Anthony", "Wilmette, Illinois", "quotient", "Vermont", "the International Date Line", "Jay Van Andel"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6113688394938395}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true], "QA-F1": [0.1818181818181818, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.9333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.3076923076923077, 0.2857142857142857, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-162", "mrqa_newsqa-validation-780", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-9", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2556", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-6088", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-1874", "mrqa_searchqa-validation-11971"], "SR": 0.53125, "CSR": 0.5440705128205128, "EFR": 0.9666666666666667, "Overall": 0.7201161858974359}, {"timecode": 78, "before_eval_results": {"predictions": ["the test results by the medical examiner's office,", "the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic.", "\"He is a very special member of our family. We miss having his love and compassion in our home,\"", "Isabella", "5 1/2-year-old son, Ryder Russell, together.", "finance", "gun charges,", "forgery and flying without a valid license,", "in the Willamette Valley to the Pacific coast.", "There's no chance of it being open on time.", "the peace with Israel", "two", "70,000", "rural California,", "Italian leader Silvio Berlusconi.", "650", "New Braunfels, Texas", "i report form", "Virgin America", "Italian government", "\"Sen. Piedad Cordoba is the most likely recipient among three leading contenders,", "helping on the sandbags lines", "four decades", "Damon Bankston", "\"Jenny Sanford,", "papillomavirus", "helicopters and unmanned aerial vehicles", "two tickets to Italy", "Dr. Maria Siemionow,", "Hillary Clinton", "Barack Obama:", "NATO", "assassination of", "the eradication of the Zetas cartel", "June 6, 1944,", "Orbiting Carbon Observatory,", "the nomination of Sonia Sotomayor", "the Airbus A330-200", "divorced Goldman and married a Brazilian lawyer.", "poems telling of the pain and suffering of children just like her", "be silent.", "Sri Lanka", "Adidas,", "2,700-acre sanctuary in rural Tennessee.", "a right-wing paramilitary group known as United Self- defense Forces of Colombia,", "can indeed help people with irritable bowel syndrome,", "martial arts,", "a student who admitted to hanging a noose in a campus library,", "543 elected members, of which 58 are women.", "Obama", "the Louvre", "New Zealand", "Elvis Presley", "\" Comanche '' is from the Ute name for them, k\u0268mantsi ( enemy )", "Neville Chamberlain", "Count Basie Orchestra", "Billy Cox", "wooden", "orisha", "boxer", "chrysanthemums", "John Wesley", "Colonel Harland Sanders", "depicting multiple alternative realities rather than a novel"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5563304411470172}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false], "QA-F1": [0.5, 0.18181818181818182, 0.1904761904761905, 1.0, 0.0, 0.5, 1.0, 0.2857142857142857, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.3076923076923077, 0.75, 1.0, 0.0, 0.0, 0.5, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6956521739130436, 1.0, 0.5, 0.2222222222222222, 0.4, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3030", "mrqa_newsqa-validation-3240", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2609", "mrqa_naturalquestions-validation-8095", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-5101", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-13641", "mrqa_naturalquestions-validation-2729"], "SR": 0.40625, "CSR": 0.5423259493670887, "EFR": 1.0, "Overall": 0.7264339398734178}, {"timecode": 79, "before_eval_results": {"predictions": ["What's Going On", "1990", "Bill Irwin", "Hon July Moyo", "glycine and arginine", "38 - 7", "12951 / 52 Mumbai Rajdhani Express", "the Rolling Stones", "My Summer Story", "In 2010", "2018", "1975", "Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "David Tennant", "the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "the Canadian Rockies continental divide east to central Saskatchewan,", "Exodus 20 : 1 -- 17", "the President", "John Musker", "Andy Cole", "each team", "in case of `` a national emergency created by attack upon the United States, its territories or possessions, or its armed forces", "Sauron", "February 1775", "Supplemental oxygen", "the level of the third lumbar vertebra, or L3, at birth", "six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "103", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Reba McEntire and Linda Davis", "New Mexico", "in the 2014 live - action film Guardians of the Galaxy", "In 1889", "SI joint ( SIJ )", "was quartermaster under the notorious Captain Flint", "Arkansas", "Mickey Rourke", "a circular movement of an object around a center ( or point ) of rotation", "Rick Marshall", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Hamburg steak is a similar product but differs in ingredients", "Sundays at 19 : 00 on BBC Two", "Charles R Ranch, County Road 24, Las Vegas, New Mexico, USA", "a major fall in stock prices", "in April 1979", "Reverend J. Long", "2005", "save, rescue, savior", "as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Brad Johnson", "Nicole Gale Anderson", "25", "at Edgehill", "James Taylor", "as the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union.", "goalkeeper", "Emancipation Proclamation", "4.6 million", "executive director of the Americas Division of Human Rights Watch,", "Pope Benedict XVI", "Afghanistan", "the Erie Canal", "the lion", "the Black Sea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6865753961395582}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6451612903225806, 1.0, 0.7692307692307692, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22857142857142856, 1.0, 0.6666666666666666, 0.0, 0.16666666666666666, 0.9473684210526316, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 0.4, 0.2222222222222222, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.12500000000000003, 0.8, 1.0, 1.0, 0.5, 0.6956521739130435, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-1979", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-9409", "mrqa_triviaqa-validation-752", "mrqa_triviaqa-validation-5693", "mrqa_hotpotqa-validation-4599", "mrqa_searchqa-validation-10525"], "SR": 0.515625, "CSR": 0.5419921875, "EFR": 0.9354838709677419, "Overall": 0.7134639616935484}, {"timecode": 80, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3139", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1535", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.830078125, "KG": 0.48828125, "before_eval_results": {"predictions": ["prevent any contaminants in the sink from flowing into the potable water system by siphonage", "all - female population", "late - September", "Moscazzano", "Prem Lata Agarwal", "Cal", "2009", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "Joe Pizzulo and Leeza Miller", "the British Indian Association", "the eighth series of the UK version of The X Factor", "Jenny Slate", "Gunpei Yokoi", "January 15, 2010", "One Son ''", "leaves of the plant species Stevia rebaudiana", "Julie Deborah Kavner", "Vincent Price", "infection", "Jewel Akens", "the surname Watson ( `` Wat's son '' )", "April 7, 2016", "Sauron", "12", "Frank Langella", "Number 4, Privet Drive, Little Whinging in Surrey, England", "shared", "Southport, North Carolina", "John C. Reilly", "Wednesday, September 21, 2016", "Washington metropolitan area", "the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho ( including the communities of Parma, Wilder, Greenleaf, and Notus )", "1885", "Kevin Kline", "U.S. State Department", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "Monk's Caf\u00e9", "Pittsburgh", "Spanish missionaries", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "butch", "Norway", "435", "1997", "Brooks & Dunn", "12 November 2010", "homicidal thoughts of a troubled youth", "W. Edwards Deming", "convergent plate boundary", "Sylvester Stallone", "1967", "Harrods, London", "Fontane di Roma", "Steve Coogan", "Boston University", "Bay of Fundy", "Lincoln Riley", "The Rosie Show", "Shemsu Sirgaga", "2,000 euros", "dishwasher", "letter", "(Albert) Einstein", "wheezing"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5934319796466974}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.1, 0.4, 1.0, 0.0, 0.5, 0.4, 0.25, 0.6666666666666666, 0.967741935483871, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-937", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2842", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-5589", "mrqa_hotpotqa-validation-4160", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2730", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-9418"], "SR": 0.46875, "CSR": 0.541087962962963, "EFR": 0.9705882352941176, "Overall": 0.7093664896514161}, {"timecode": 81, "before_eval_results": {"predictions": ["Jason Momoa", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "1623", "March 14, 1942", "Lafayette", "April 3, 1973", "5 liters", "Kate Walsh", "13 to 22 June 2012", "2.5 %", "232", "mid November or early December", "Guwahati", "Nala", "1979 / 80", "2017", "Augustus", "Andy", "electron shells", "snow", "compasses", "production -- possibility frontier ( PPF ) or production possibility curve ( PPC )", "1987", "in the eye", "Eagle Ridge Outdoor pool in Coquitlam, BC", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "Charlton Heston", "the fourth ventricle", "October 27, 2017", "John Cooper Clarke", "15 May 2004", "Cheryl Campbell", "Spanish", "0.072 mm", "March 2, 2016", "Jeff Bezos", "Erica Rivera", "Most days are sunny throughout the year", "January 2, 1971", "Tracy McConnell", "gastrocnemius", "Parker's pregnancy at the time of filming", "3", "the Turco - Mongol Timurid dynasty of Central Asia", "1830", "Frank Oz", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Celtic", "Guant\u00e1namo Bay in Cuba", "Morgan Freeman", "two installments", "in the muscle tissue of vertebrates", "toe-line", "synagogue", "Katarina Witt", "Bhaktivedanta Manor", "Lowe's Companies, Inc.", "Honduran", "Kenyan forces who have entered Somalia,", "the mammoth's skull,", "he was also the hard-nosed man of violence", "Quebec", "churrasco", "the evaporator"], "metric_results": {"EM": 0.484375, "QA-F1": 0.586873759920635}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.375, 0.0, 0.5, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.8000000000000002, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666667, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5185", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-3302", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-1512", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-12256", "mrqa_searchqa-validation-12239"], "SR": 0.484375, "CSR": 0.5403963414634146, "EFR": 0.9696969696969697, "Overall": 0.7090499122320769}, {"timecode": 82, "before_eval_results": {"predictions": ["whittling", "Hans", "purple", "Charles Lindbergh", "fructose corn syrup", "T.S. Eliot", "Superman", "Nokomis", "Yale", "tides", "The Nutcracker", "Over the hifls", "circumnavigate", "Kennebunkport", "tarzan", "Theodore Roosevelt", "the Manx", "rum", "Baroque", "pterodactyl", "licorice", "Count Dracula", "Jon Heder", "Sweden", "War and Peace", "Hannah Montana", "Van Allen", "Mitch McConnell", "bravery or valor", "the gallbladder", "the Invisible Man", "the Himalaya", "Chile", "the Democratic Socialist Republic of Sri Lanka", "St. Valentine's Day Massacre", "Saturday Night Live", "Sayonara", "Oakland", "The Taming of the Shrew", "Hypertext Transfer Protocol", "Andrew Johnson", "the ACL", "NASA", "Gavin MacLeod", "a snake", "The Count of Monte Cristo", "a country farm", "Peter Shaffer", "Wyandotte County", "Denton True Young", "Stephen Sondheim", "December 27, 2015", "digital transmission modes", "Andy Cole", "yellow", "Christiaan Huygens", "Ann Dunham", "Afro-American religions", "881 Seventh Avenue", "Awake", "sports cars", "The show which looks at how children as young as eight would cope without their parents for two weeks.", "Asian qualifying Group 2", "Washington"], "metric_results": {"EM": 0.515625, "QA-F1": 0.616875}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.5, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.08, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5014", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-3006", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-13546", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3227", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-7984", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-14521", "mrqa_searchqa-validation-6416", "mrqa_searchqa-validation-8439", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-2222", "mrqa_triviaqa-validation-909", "mrqa_triviaqa-validation-1023", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-5005", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3226"], "SR": 0.515625, "CSR": 0.5400978915662651, "EFR": 1.0, "Overall": 0.7150508283132531}, {"timecode": 83, "before_eval_results": {"predictions": ["the Coca-Cola Company", "Oklahoma", "to get air time through any means possible", "Pippin", "Georgia", "Chesapeake Bay", "the dugout", "cement", "heat", "(John Cusack) Mill", "(James Fenimore Cooper) Cooper", "Platoon", "(Oscar) Christie", "potato chips", "the Bay of Bengal", "the Clark bar", "o", "stanley", "John Ashcroft", "Phil of the Future", "Newman", "the Death Valley", "rings", "\"Go pick up your toy\"", "Hayslope", "the Eagles", "(L.) Frank Baum", "the Big Bopper", "jaded", "'S Sgt. Pepper's Lonely Hearts Club Band'", "palindrome", "the trapezoid", "Scrubs", "Henrik Ibsen", "Elizabeth I", "Canticle", "Friedrich Nietzsche", "\"Can we all get along?\"", "Halloween season", "to set", "Tanzania", "Coca-Cola", "(Deepak) Chopra", "Rudolf Hess", "Siberia", "Rings Twice", "Rocky", "(Edna) Ferber", "the Etch A Sketch", "safari", "the Arkansas Diamond Mine", "During his epic battle with Frieza", "1997", "the revenue from the opium trade was a key source of government funds", "Joseph Priestley", "Granada", "stanley", "1940s and 1950s", "Best Musical", "York County", "six", "Michelle Rounds", "Derek Mears", "Adidas"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6831730769230768}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2363", "mrqa_searchqa-validation-12304", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-16183", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-8962", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-14844", "mrqa_searchqa-validation-15025", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-16469", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-15171", "mrqa_searchqa-validation-1066", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-11096", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-2381", "mrqa_hotpotqa-validation-5309"], "SR": 0.59375, "CSR": 0.5407366071428572, "EFR": 1.0, "Overall": 0.7151785714285714}, {"timecode": 84, "before_eval_results": {"predictions": ["Portugal", "Norwegian", "daisy", "(John) Galsworthy", "Belfast", "Wymark Jacobs", "the Kurguelen Island Group", "East of Eden", "John Buchan\u2019s grandson, Toby", "Doncaster Rovers", "Jane Eyre", "Yoshino", "9", "Supertramp", "glasgow", "Joanne Harris", "abacus", "Eriksson", "bison", "displacement", "Tom Baker", "(John Francome)", "a moon", "uranus", "white spirit", "aglet", "lemurs", "Saskatchewan", "Fabio Capello", "Mickey Mouse", "cricket", "1973", "William Neil Connor", "Baku", "logic", "Spain", "Ferdinand Ferdinand", "Chief Inspector of Prisons", "Moulin Rouge", "golf", "two mice", "(Lettice Knollys)", "Hamelin", "Prague", "George Osborne", "oxygen", "Toyota", "a pest", "HMS Amethyst", "a hairdresser", "Antony", "Jethalalal Gada", "Spektor", "Vancouver, British Columbia", "F\u00fchrer", "2 km", "Lake Buena Vista, Florida", "about 1,300 meters in the Mediterranean Sea.", "Afghanistan,", "Deutschneudorf,", "a chimp", "tin", "The Usual Suspects", "repel bullets"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5942708333333333}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-7371", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-6666", "mrqa_naturalquestions-validation-10367", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2685", "mrqa_newsqa-validation-796", "mrqa_searchqa-validation-1820", "mrqa_naturalquestions-validation-2309"], "SR": 0.515625, "CSR": 0.5404411764705883, "EFR": 0.9354838709677419, "Overall": 0.7022162594876661}, {"timecode": 85, "before_eval_results": {"predictions": ["Grand Harbour", "Eurasia", "Tom Ewell", "American", "over 9,000", "My Beautiful Dark Twisted Fantasy", "secondary school study", "Shut Up", "30.9", "William Shakespeare", "Nic Cester", "the Kingdom of Morocco", "a subalpine to alpine species,", "Prince George's County", "Ariel Ram\u00edrez", "Apple iPod Classic", "four months in jail", "Objectivism", "Vixen", "to steal the plans for the Death Star, the Jedi Empire's super Weapon", "Stephen Crawford Young", "co-founder and lead guitarist of the alternative rock band R.E.M.", "dreikaiserbund", "Baron Cherwell", "Sunflower County", "Alexandre Dimitri Song Billong", "Dutch", "Rabies", "Malayalam cinema", "Sim Theme Park", "Outside", "novelty songs", "Sri Lanka Freedom Party", "Jennifer Joanna Aniston", "the North Atlantic Conference", "a cappella singing group", "Downtown", "Rockstar San Diego", "Strato of Lampsacus", "Gregg Berhalter", "Beno\u00eet Jacquot", "the Manor of the More", "Anita Dobson", "500-room", "Rajmund Roman Thierry Pola\u0144ski", "Space Shuttle \"Discovery\" on STS-51-C.", "\"Orchard County\"", "Saint Michael, Barbados", "Championnat National 3", "Boston, Massachusetts", "\"The King of Chutzpah\"", "one person", "anion", "Gibraltar", "the Seine", "ferdinand", "dal\u00b7ton\u00b7ism", "Virgin America", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Zelaya", "doctrine", "Japan", "Stalin", "Rupert\\'s Land"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7434695512820513}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6153846153846153, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-2506", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-1202", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-1156", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-5450"], "SR": 0.671875, "CSR": 0.541969476744186, "EFR": 1.0, "Overall": 0.7154251453488373}, {"timecode": 86, "before_eval_results": {"predictions": ["California", "the object is placed further away from the mirror / lens than the focal point", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product, whether or not related to the medicinal", "18 - season", "December 24, 1836", "the bank, rather than the purchaser, is responsible for paying the amount", "Carroll O'Connor", "20 years from the filing date", "the French m\u00e9tayage", "the 1940s", "American production duo The Chainsmoker", "Steve Russell", "the President pro tempore", "Donna", "the Dutch", "turlough", "Vancouver", "the Prince - Electors", "9 February 2018", "1933", "2013", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "supervillains who pose catastrophic challenges to the world", "two", "the Charbagh structure", "Elizabeth Dean Lail", "a password recovery tool for Microsoft Windows", "semi-autonomous organisational units within the National Health Service in England", "After Margaret Thatcher became Prime Minister in May 1979", "from 13 to 22 June 2012", "60", "electron donors", "the pouring rain", "monitor lizards", "Abbot Suger", "the Mahalangur Himal sub-range of the Himalayas", "The Royalettes", "the winter solstice", "State Bar of Arizona", "Marie Fredriksson", "Geothermal gradient", "2001", "Hellenismos", "Urge Overkill", "2000", "blue", "the Mishnah", "1078", "April 1979", "around 1872", "Atlanta, Georgia", "Cyrillic", "the US", "\"Little Red Rented Rowboat\"", "France", "musical", "November 10, 2017", "jobs up and down the auto supply chain:", "Buenos Aires", "ancient Egyptian antiquities in the world,", "ten", "Dragnet", "Harry Potter", "Fairfax"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5292151478404103}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.962962962962963, 0.0, 1.0, 0.19999999999999998, 0.0, 0.9090909090909091, 0.0, 1.0, 0.75, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.9387755102040816, 1.0, 0.6666666666666666, 0.0, 1.0, 0.058823529411764705, 1.0, 0.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4166", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-916", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3197", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3536", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2614", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-1911", "mrqa_hotpotqa-validation-82"], "SR": 0.421875, "CSR": 0.5405890804597702, "EFR": 0.918918918918919, "Overall": 0.6989328498757378}, {"timecode": 87, "before_eval_results": {"predictions": ["$10 billion", "Don Draper", "President Obama", "Two pages -- usually high school juniors who serve Congress as messengers --", "Marcell Jansen", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "the U.S.-Mexico border", "California-based Current TV", "riders a 19th-century experience on cruises complete with the carnival-like sounds of the steam-whistle calliope.", "going somewhere very special, far away, because under the Communist regime you didn't travel that much and Prague was \"wow.\"", "peanuts, fish, shellfish, peanuts, tree nuts, wheat and soy", "Piers Morgan Tonight", "Dubai", "Columbia, Illinois,", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "free services.", "Steven Chu", "Mark Obama Ndesandjo", "Hayden", "30,000", "Kris Allen", "Ashura -- the 10th day of the month of Muharram", "$1.4 million,", "250,000", "Theikini rocketed to fame in 1960 with Brian Hyland\\'s hit single, \"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "Juarez drug cartel.", "Muslim festival", "A multinational fleet -- including vessels from the United States, NATO member states, Russia and India", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "his business dealings", "150", "cancer for several years.", "U.S. Chamber of Commerce", "Miss USA Rima Fakih is a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "Lee Myung-Bak", "an open window", "Jiverly Wong,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "bronze medal in the women's figure skating final,", "98", "President Obama", "Stratfor, a global intelligence company,", "Cologne, Germany,", "that former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Tennessee", "five minutes before commandos descended from ropes that dangled from helicopters,", "The president would legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "Mary Procidano,", "The son of Gabon's former president", "Samoa", "Derek Mears", "January to May 2014", "Frederick County", "RMS Titanic", "calcium carbonate", "The Great Leap", "Bangladesh", "Elisha Nelson Manning", "CBS", "\"Dr. Gr\u00e4sler, Badearzt\"", "a seal", "glaucoma", "Carl Sandburg", "Meriwether Lewis"], "metric_results": {"EM": 0.5, "QA-F1": 0.6118790905785392}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.3076923076923077, 1.0, 0.6153846153846153, 0.0, 0.0, 0.0, 0.07407407407407407, 0.7142857142857143, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 0.25, 0.0, 0.0, 0.5925925925925926, 0.5, 1.0, 0.761904761904762, 0.26666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 0.17647058823529413, 1.0, 0.625, 0.07142857142857144, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-931", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-3855", "mrqa_hotpotqa-validation-3563"], "SR": 0.5, "CSR": 0.5401278409090908, "EFR": 0.9375, "Overall": 0.7025568181818181}, {"timecode": 88, "before_eval_results": {"predictions": ["Casey Anthony,", "12", "$22 million", "The federal officers' bodies", "\"We tortured (Mohammed al-) Qahtani,\"", "near a disputed border temple that was the site of clashes last year,", "police to question people if there's reason to suspect they're in the United States illegally.", "\"Oprah: A Biography,\"", "and Jquante Crews,", "Chad", "poems", "social media networks", "Cleve Landsberg,", "Saturday", "Columbian mammoth", "Taliban", "genocide, crimes against humanity, and war crimes.", "in late-October", "in the county jail in Spanishfork,", "304,000", "stolen tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "28", "an independent homeland since 1983.", "Miguel Cotto", "South Africa", "seven", "fuel economy", "returning combat veterans", "scientific reasons.", "dismissed all charges", "United States, NATO member states, Russia and India", "75 percent", "Ameneh Bahrami", "will not support the Stop Online Piracy Act,", "Ma Khin Khin Leh,", "117-111, 115-113 and 116-112.", "The Rosie Show", "The station", "Larry King", "the son of Gabon's former president", "in the Horn of Africa,", "Sydney", "part of the proceeds", "Olympic", "vitamin injections that promise to improve health and beauty.", "Piers Morgan, the prickly judge on \"Britain's Got Talent\"", "Ciudad Juarez,", "Rolling Stone", "Indonesian", "Robert Park", "At least 14", "late 2018 or early 2019", "Graub\u00fcnden, in the eastern Alps region of Switzerland", "2,050 metres ( 6,730 ft )", "bird skin", "battle of Agincourt", "the Esmeralda's Barn night", "Belgian", "Geelong Football Club", "Richard L. Thompson", "a fisheye lens", "a clavichord", "birds and the bees", "(John Knox) Knox"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5614536965148378}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.08, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4347826086956522, 0.7777777777777778, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-4007", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-306", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-3162", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-1586", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-798", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8794", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-277", "mrqa_searchqa-validation-14968", "mrqa_searchqa-validation-2827", "mrqa_searchqa-validation-8747"], "SR": 0.484375, "CSR": 0.539501404494382, "EFR": 1.0, "Overall": 0.7149315308988764}, {"timecode": 89, "before_eval_results": {"predictions": ["J. Crew outfits", "Ignazio La Russa", "collaborating with the Colombian government,", "collaborating with the Colombian government,", "potential revenues from oil and gas", "2001,", "$24.1 million,", "Nechirvan Barzani,", "Idriss Deby", "U.S. Navy", "a hunting party of three men,", "U.S. State Department and British Foreign Office", "the hunt for Nazi Gold", "U.S. President-elect Barack Obama", "Web", "News of the World tabloid.", "Too many glass shards left by beer drinkers in the city center,", "not for sale,", "\"an eye for an eye,\"", "if they can demonstrate they have been satisfactorily treated", "Jenny Sanford,", "Kurdish Workers' Party,", "music, street dancing and revelry", "\"falling space debris,\"", "At Wilhelmina Kids,", "France", "581 points", "Robert Barnett,", "Mexican military", "14", "Venezuela", "41,", "2007.", "Idriss Deby hopes the journalists and the flight crew will be freed,", "Iran could be secretly working on a nuclear weapon is a major development, but not one that should lead the U.S. to consider a military strike against the Tehran regime,", "\"illegitimate.\"", "\"Zionist\" and \"a zealous supporter of the usurper entity, and a prominent member of a number of Zionist hate organizations\"", "2-1", "Haeftling,", "Saturday.", "five minutes before commandos descended", "nose, cheeks, upper jaw and facial tissue from a female cadaver", "cell phones have become perhaps the hottest commodity.", "The cause of the child's death will be listed as homicide by undetermined means,", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "the rest of the year", "July", "fired employees attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Dominic Adiyiah", "Glasgow, Scotland", "Caylee Anthony's", "2006 -- 05", "member", "John Vincent Calipari", "New Zealand", "Jessica Simpson", "fox hunting", "Los Angeles", "S6 Edge+", "Hilux pickup truck", "eggshells", "Manchester", "Gertrude Stein", "Dick Van Dyke"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6144803059675026}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.125, 0.2857142857142857, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.05714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8235294117647058, 0.14634146341463414, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.0, 0.36363636363636365, 0.15384615384615385, 0.5217391304347826, 0.8571428571428571, 0.6666666666666666, 0.375, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-925", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3450", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-1000", "mrqa_naturalquestions-validation-5602", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5380"], "SR": 0.46875, "CSR": 0.5387152777777777, "EFR": 1.0, "Overall": 0.7147743055555555}, {"timecode": 90, "UKR": 0.64453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-342", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.81640625, "KG": 0.4890625, "before_eval_results": {"predictions": ["eels", "france", "Stevie Wonder", "Sir Tom Finney", "Jackson Pollock", "the Black Sea", "Jumping Jack Flash", "Joseph Priestley", "Dumbo", "New Zealand", "Call for the Dead", "Bill Bryson", "Hans Christian Andersen", "Christian Louboutin", "Vespa", "naked", "Laputa", "a Hungarian", "Jumanji", "Flo Rida", "@", "The Princess bride", "Word", "pigs", "Dancing With The Stars", "Australia", "Leicester", "Charles S. Brooks", "Andr\u00e9s Iniesta", "Bath", "in 1924", "a barred spiral", "Duty Free", "Mark Twain", "fruit", "carbon", "Caernarfon", "Sir Herbert Kitchener", "Johnny Mathis", "Sergio Garc\u00eda Fern\u00e1ndez", "Chad", "Arthur", "Yulia Tymochenko", "E. Nesbit", "Charles Greville", "John F. Kennedy", "Sheree Murphy", "Robert Louis Stevenson", "the R34", "Yukon", "\"Eliver Twist\"", "gas, ethane, and propane", "three", "the coffee shop Monk's", "Mark Alan Dacascos", "DreamWorks Animation", "Adelaide", "338", "Pakistan", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "the Marine Band", "Joe Biden", "Daumier", "Austria"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5633378623188405}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.08695652173913043, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-436", "mrqa_triviaqa-validation-2698", "mrqa_triviaqa-validation-5650", "mrqa_triviaqa-validation-1187", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3335", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-2111", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-4011", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-6807", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-339", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-2564", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-724", "mrqa_searchqa-validation-13048", "mrqa_searchqa-validation-8979", "mrqa_hotpotqa-validation-5203"], "SR": 0.515625, "CSR": 0.5384615384615384, "EFR": 0.8709677419354839, "Overall": 0.6718858560794045}, {"timecode": 91, "before_eval_results": {"predictions": ["Dumbo", "Ernest Hemingway", "Switzerland", "Mexican Orange Blossom", "Perry Mason", "Bombay", "James I", "trapezium", "Canada", "car that was nicknamed 'The Tiddler'", "seven", "Vancouver", "niger", "Switzerland", "the Union Gap", "Cologne", "punky Brewster", "contagious", "gin", "piano", "Dick Cheney", "tectonic uplift of land and volcanic eruptions", "Virginia", "dysmenorrhea", "pasta harvest", "witch trials", "sailor", "my Favorite Martian", "plutocracy", "ptolemy", "Austria-Hungary", "Ace of Spades", "the Soviet Union", "China", "eyelids", "Venice", "New Zealand", "1973", "st Pauls", "Westminster Repertory Theatre Company", "d.offical", "the popes", "Jimmy Carter", "Danny Baldwin", "Argentina", "Genesis", "special sauce", "khrushchev", "arsenic", "John Peel", "Sheffield Wednesday", "May 2017", "mitosis", "Morgan Freeman", "Mike Fiers", "coca wine", "politician", "\u00a320 million ($41.1 million)", "calling on NATO to do more to stop the Afghan opium trade", "Trevor Rees,", "(Edvard) GRIEG", "Buddhism", "(Thomas Francis Eagleton)", "water"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6245738636363636}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.2727272727272727, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-6471", "mrqa_triviaqa-validation-3942", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3775", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-996", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-2032", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-6212", "mrqa_hotpotqa-validation-2210", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2183", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-11019"], "SR": 0.59375, "CSR": 0.5390625, "EFR": 0.9615384615384616, "Overall": 0.6901201923076924}, {"timecode": 92, "before_eval_results": {"predictions": ["raping and killing a 14-year-old Iraqi girl.", "Karen Floyd", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "Krishna Rajaram,", "U.S. President-elect Barack Obama", "Courtney Love,", "a delegation of American Muslim and Christian leaders", "a cardio", "California-based Current TV", "gun", "\"E! News\"", "1983", "nude beaches.", "tennis", "the administration's", "19", "Max Foster,", "military trials", "Iran of trying to build nuclear bombs,", "\"A Lion Among Men,\"", "\"This is not something that anybody can reasonably anticipate,\"", "\"surge\" strategy", "to clean up Washington State's decommissioned Hanford nuclear site,", "delivers a big speech", "sylt's dining scene", "Saturday,", "sharia in Somalia is part of the laws for thousands of years,", "Missouri River", "gasoline", "\"I'm extremely gratified at the court's decision. I believe it is legally and factually correct.", "2.5 million", "abducting each other for ransoms or retribution.", "Egypt", "about three minutes after launch", "Barack Obama's", "iTunes Music Store,", "president Robert Mugabe", "bodyguard Trevor Rees,", "12.3 million", "ties", "three", "to get involved in service and volunteerism in their communities.", "75 percent", "South Korean President Lee Myung-bak,", "1,500", "can be volatile and dangerous.", "two counts of murder.", "34 civilians and 16 police officers", "16th", "at a construction site in the heart of Los Angeles.", "Alberto Espinoza Barron,", "2013", "Vincenzo Peruggia", "April 1979", "Pelham One Two Three", "Hercule Poirot", "1998", "Debbie Reynolds", "43rd", "USS Essex", "Daylight Saving Time", "Amelia Earhart", "uranium", "Pakistan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6729056686777275}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9411764705882353, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.10256410256410257, 1.0, 0.22222222222222224, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-45", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1516", "mrqa_triviaqa-validation-3035", "mrqa_hotpotqa-validation-1316", "mrqa_searchqa-validation-3479"], "SR": 0.515625, "CSR": 0.5388104838709677, "EFR": 0.967741935483871, "Overall": 0.6913104838709678}, {"timecode": 93, "before_eval_results": {"predictions": ["on-loan David Beckham", "They are co-chairs of the Genocide Prevention Task Force.", "March 8", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "Kearny, New Jersey.", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "five minutes before commandos descended", "Democratic", "Kim Il Sung", "Roy Foster's", "2-0", "managing his time.", "U.S. Consulate in Rio de Janeiro,", "Majid Movahedi,", "Molotov cocktails, rocks and glass.", "Facebook and Google,", "five Texas A&M University crew mates", "customers are lining up for vitamin injections that promise to improve health and beauty.", "Oxbow,", "British Prime Minister Gordon Brown's", "those missing", "summer", "a vast settlement of people left without loved ones, without homes, without life's belongings.", "Friday,", "\"the most important discovery\"", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "producing rock music with a country influence.", "parents", "\"You people don't make good CEO.\"", "Jared Polis", "the underprivileged.", "March 24,", "North Korea,", "Mexican military", "Bill Haas", "to make space for two ocean wind farms -- taking up 2 percent of the state's waters", "By no circumstances will the government accept what is occurring,\"", "Somalia's piracy problem was fueled by environmental and political events.", "Swat Valley.", "Japanese Foreign Ministry spokesman Hidenobu Sobashima", "Grayback forest-firefighters", "Steven Gerrard", "three", "Olympic medal", "standing next to a story,\"", "the Beatles", "At least 88 people had been hurt,", "the Illinois Reform Commission", "use of torture and indefinite detention", "\"Watchmen\" (No. 1)", "severe flooding", "Pastoral farming", "Kaley Christine Cuoco", "Hermann Ebbinghaus", "Route sixty-six", "Morten Skovsby", "16 September 1992", "Cambridge University", "early 20th-century Europe", "American tour", "Mickey Mouse", "The daiquiri", "Israel", "UNESCO / ILO"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6075666898017441}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.782608695652174, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.21428571428571427, 0.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.25, 0.18181818181818182, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1910", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3157", "mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-3946", "mrqa_triviaqa-validation-1202", "mrqa_hotpotqa-validation-3326", "mrqa_searchqa-validation-6"], "SR": 0.546875, "CSR": 0.5388962765957447, "EFR": 1.0, "Overall": 0.697779255319149}, {"timecode": 94, "before_eval_results": {"predictions": ["The gunmen also took hostage Lunsmann's 14-year-old son, Kevin,", "23-year-old", "Paul McCartney", "Thirty to 40", "software magnate Larry Ellison,", "California, Texas and Florida,", "Alberto Espinoza Barron,", "review their emergency plans and consider additional security measures", "4,000", "Whitney Houston", "Marines", "Lillo Brancato Jr.", "United States, NATO member states, Russia", "publicist had no comment on his plans.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "riders love the trip route, which winds through the Rockies and climbs to 9,000 feet.", "$10 billion", "opium trade", "Casa de Campo International Airport in the Dominican Republic", "Pakistan's", "dancing against a stripper's pole.", "women.", "CNN's Campbell Brown", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Iraqi Prime Minister Nouri al-Maliki", "Alfredo Astiz,", "Supreme Court justice.", "Majid Movahedi,", "dismissal.", "three", "1960.", "ending the war in Iraq, reducing government waste, charging polluters for greenhouse gas emissions and ending the Bush tax cuts for wealthy individuals.", "allow students to engage in learning differently, enjoy a customized approach", "the first", "Larry Zeiger", "Charlotte Gainsbourg and Willem Dafoe", "The public endorsement", "tie salesman", "fight against terror will respect America's values.", "2005", "Two UH-60 Blackhawk helicopters", "CNN's \"Larry King Live.\"", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Ghana", "Christopher Savoie", "246", "London's Waterloo Bridge", "Serie A", "in the last few months,", "Since 1980, the 84-year-old Mugabe has been the country's only ruler.", "a nuclear weapon", "2020 National Football League ( NFL ) season", "John Cooper Clarke", "late 1980s", "Chicago", "kolkata", "Painter", "MGM Grand Garden Special Events Center", "Royal Navy", "England national team", "Bangkok", "roof", "cana", "KXII"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6923216383635433}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true], "QA-F1": [0.6956521739130436, 1.0, 0.5714285714285715, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.5, 1.0, 1.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.4, 0.1212121212121212, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1131", "mrqa_naturalquestions-validation-8685", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-4069"], "SR": 0.546875, "CSR": 0.5389802631578947, "EFR": 1.0, "Overall": 0.697796052631579}, {"timecode": 95, "before_eval_results": {"predictions": ["Dano-Nor Norwegian author Aksel Sandemose", "pubs, bars and restaurants", "Retina display", "Chick tract", "Julia Verdin", "a British naval officer and statesman, an uncle of Prince Philip, Duke of Edinburgh, and second cousin once removed of Elizabeth II.", "defender", "Graffiti", "ARY Digital Network", "Drifting", "Larry Wayne Gatlin", "9 October 19408 December 1980", "Key West", "its riverside location,", "Scottish", "Skipton", "Atlas ICBM", "PPG Paints Arena", "Kelly Bundy", "John Joseph Travolta", "Love Letter", "SBS", "The AVN Adult Entertainment Expo (AEE)", "119 minutes", "50 million", "Intelligent Design", "Sinngedichte", "Hidden America with Jonah Ray", "Irish", "Metro-Goldwyn-Mayer", "RAF Tangmere, West Sussex", "The Summer Olympic Games", "1692", "Art Deco-style skyscraper", "American professional baseball left fielder", "25 November 2015", "Hyuna", "Mathieu Kassovitz", "Kentucky Music Hall of Fame", "Louis King", "Danish", "Indian", "two", "Mississippi", "D\u00e2mbovi\u021ba River", "24800 mi", "saint", "143,007", "Michael Edward \" Mike\" Mills", "Edward Vincent Sullivan (September 28, 1901 \u2013 October 13, 1974) was an American television personality, sports and entertainment reporter,", "his fourth term", "2001", "a single, implicitly structured data item", "Marshall Sahlins", "Roberta Flack", "Sir Walter Scott", "Braille", "Caster Semenya", "Heshmatollah Attarzadeh", "London's 20,000-capacity O2 Arena.", "\"heroes\" is Louise Lehzen,", "Wizard", "the Erie Canal", "Andy Warhol"], "metric_results": {"EM": 0.625, "QA-F1": 0.7534722222222222}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 1.0, 0.4, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.09523809523809525, 0.8, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1777", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2046", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-2013", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-8669", "mrqa_newsqa-validation-3651", "mrqa_searchqa-validation-9300"], "SR": 0.625, "CSR": 0.5398763020833333, "EFR": 1.0, "Overall": 0.6979752604166667}, {"timecode": 96, "before_eval_results": {"predictions": ["Black Abbots", "Audrey Mossom", "Nicolas Winding Refn", "PBS", "Ed Lee", "James Victor Chesnutt", "Wayne Rooney", "South Asia and the Middle East", "Martin Ingerman", "Rice University", "NCAA Division I", "Richard Price", "Reich Chancellery", "Kim So-hyun", "pubs, bars and restaurants", "Brad Pitt", "Milan", "in 1885", "Pac-12 Conference", "Lamar Wyatt", "Homer Hickam, Jr.", "Straits of Gibraltar", "Port Clinton", "Patrick Dempsey", "Seattle", "Taylor Swift", "Appleby-in-Westmorland", "2000", "Mickey's Christmas Carol", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay", "Orson Welles", "in 1902", "Brittany Snow", "Finland's northernmost province, Lapland", "Elvis' Christmas Album", "CD Castell\u00f3n", "Coronation Street", "Zimbabwe", "residential", "William Scott Elam", "1998", "Johnny Cash, Waylon Jennings", "Brad Silberling", "ITV", "1982", "the Marx Brothers film", "Tennessee", "Agent Vinod", "Michael Jordan", "Harrods", "2007 Formula One season", "2004", "obliquely", "left - sided heart failure", "Karl Marx", "Laurence Olivier", "Indian Ocean", "Rodong Sinmun", "two-day, two-city charm offensive", "1,900-acre YFZ ranch,", "Vatican City", "a doored platform", "Eric Knight", "unknown origin"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7459595959595959}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2033", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-1932", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-2807", "mrqa_naturalquestions-validation-7760", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-776", "mrqa_searchqa-validation-69"], "SR": 0.6875, "CSR": 0.5413981958762887, "EFR": 1.0, "Overall": 0.6982796391752577}, {"timecode": 97, "before_eval_results": {"predictions": ["Louisiana", "poker", "Budapest", "the Hoppin' John", "capuchin", "bass", "(El Cid) Campeador", "Vestal Virgins", "contract", "Akihito", "lead", "Israel", "(St.) Matthew", "Nancy Astor", "imperative", "the bald eagle", "high altitude", "Oslo", "a leap year", "Little Miss Muffet", "the Gila monster", "The Hague", "Zyrtec", "Buddhism", "Carson City", "Syria", "Cherry, Cherry", "the Council of Better Business Bureaus", "Linda Tripp", "a station wagons", "Aqua Teen Hunger Force", "(James) Webb", "economics", "United Nations", "diseases", "Rocky Mountain spotted fever", "euros", "Lebanon", "typewriters", "Isadora Duncan", "Jaws 2", "George Armstrong Custer", "nag", "Homer", "Motor Trend", "the U.S. Federal Aviation Administration", "Staten Island", "Naxos", "copper", "Jamaica", "(Ed Flanders)", "Lord Banquo", "average speed 112 km / h", "Paul Lynde", "John Part", "March 10, 1997", "Hubble Space Telescope", "\"Household Words\"", "Rockland County", "Trilochanapala", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Turkey", "The first line of law and order here is, number one, the Haitian police, number two, the U.N. forces,\"", "cuban cigars"], "metric_results": {"EM": 0.671875, "QA-F1": 0.759369081439394}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6875000000000001, 1.0, 0.5454545454545454, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2543", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-12404", "mrqa_searchqa-validation-2730", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-9423", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-2784", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-15833", "mrqa_naturalquestions-validation-6519", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-2278", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-2757"], "SR": 0.671875, "CSR": 0.5427295918367347, "EFR": 1.0, "Overall": 0.698545918367347}, {"timecode": 98, "before_eval_results": {"predictions": ["Scary Movie 2", "a mall", "godliness", "TIME", "the Annunciation of Our Lady", "the Thames", "Alyssa Milano", "drowsiness", "a crocus", "Alaska", "Yellowstone National Park", "Marcel Duchamp", "Little Red Riding Hood", "Vaduz", "the tongue", "the English Channel", "Michelin", "an event", "Simple Simon", "hot chocolate", "vibrations", "a metronome", "gigabytes", "the Phillie Phanatic", "\"What a joy to breathe the balmy air of Grosvenor Square\"", "Pringles", "Blondes", "a Stratocaster", "anchors", "Romeo", "single-lens reflex", "Pamela Anderson", "Trampolining", "\"King of the Hill\"", "Bermuda", "Tiger Woods", "dark places", "Elton John", "the Sphinx", "Toy Story", "a lump of coal", "density", "ice hockey", "Heather Locklear", "the Pong", "wheels", "the Holy Grail", "a crone", "John XXIII", "the hog", "Target", "Left Behind", "71 -- 74 \u00b0 C ( 160 -- 165 \u00b0 F )", "the Devastator", "cotton", "Mary Decker", "Donatello", "1939", "Leafcutter John", "University of Vienna", "Ferraris, a Lamborghini and an Acura NSX", "80 percent", "Michael Jackson", "north"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7214285714285713}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true], "QA-F1": [0.8, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9200", "mrqa_searchqa-validation-8408", "mrqa_searchqa-validation-3541", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-15322", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-13276", "mrqa_searchqa-validation-8717", "mrqa_searchqa-validation-13464", "mrqa_searchqa-validation-136", "mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-15535", "mrqa_searchqa-validation-10022", "mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-416", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-13668", "mrqa_searchqa-validation-85", "mrqa_searchqa-validation-381", "mrqa_triviaqa-validation-6743", "mrqa_hotpotqa-validation-537", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-1683"], "SR": 0.59375, "CSR": 0.5432449494949495, "EFR": 1.0, "Overall": 0.6986489898989899}, {"timecode": 99, "UKR": 0.71875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1673", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12845", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14027", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6260", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8234", "mrqa_searchqa-validation-8339", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_searchqa-validation-999", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6448", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7028", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-959"], "OKR": 0.837890625, "KG": 0.528125, "before_eval_results": {"predictions": ["ordered the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guantanamo Bay, Cuba.", "\"Gandhi,\"", "Brooklyn, New York,", "\"I would focus on how much I paid", "the WBO welterweight title from Miguel Cotto", "Sri Lankan", "\"The situation is pretty much resolved,\"", "Addis Ababa,", "Saturn", "piano", "Bronx.", "two years,", "\"We'll starve to death, that's all,\"", "$17,000", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Haleigh Cummings,", "The sailboat, named Cynthia Woods,", "saying Chaudhary's death was warning to management.", "Kashmir has been in the throes of a violent separatist campaign", "Michoacan Family,\"", "two women killed in a stampede at one of his events in Angola,", "exotic sports", "Bryant Purvis,", "role as a bride in the 2007 movie \"License to Wed\"", "Kurt Cobain,", "Department of Homeland Security Secretary Janet Napolitano", "Matthew Fisher", "E. coli bacteria", "July", "an engineering and construction company with a vast personal fortune.", "Sri Lanka", "a one-shot victory in the Bob Hope Classic", "Dubai", "U.S. Vice President Dick Cheney", "Rwandan", "Tehran,", "France's famous Louvre", "Daytime Emmy Lifetime Achievement Award.", "money or other discreet aid for the effort if it could be made available,", "checkposts and military camps in the Mohmand agency,", "Somalia's piracy problem was fueled by environmental and political events.", "five", "1994", "\"I would say even those who voted for Bush don't support this war,\"", "\"We don't comment on individual demonstrations,\"", "President Obama", "The Ministry of Defense", "Wednesday.", "sons", "Michael Arrington,", "Acura", "1973", "1975", "1546", "Kaiser Chiefs,", "rivers", "Ecuador", "Afghanistan", "University College of North Staffordshire", "Nelson County", "the bull-running festival", "Jean-Michel Basquiat", "the Coast Guard", "Aluminium"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6659977175602175}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false], "QA-F1": [0.48484848484848486, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2895", "mrqa_newsqa-validation-411", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-2968", "mrqa_searchqa-validation-14651", "mrqa_searchqa-validation-13028"], "SR": 0.59375, "CSR": 0.54375, "EFR": 1.0, "Overall": 0.7257031250000001}]}