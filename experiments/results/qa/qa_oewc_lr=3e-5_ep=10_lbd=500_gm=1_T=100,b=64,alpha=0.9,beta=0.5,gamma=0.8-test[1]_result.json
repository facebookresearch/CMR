{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=500_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=500.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=500_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4120, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["April 20", "Paul Whiteman", "2005", "Islamism", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "the center of the curving path", "eight", "28", "global", "Fresno", "Amazonia: Man and Culture in a Counterfeit Paradise", "88%", "broken wing and leg", "Emmy Awards", "silt up the lake", "The TEU specifically excludes certain regions, for example the Faroe Islands, from the jurisdiction of European Union law", "legitimate medical purpose", "week 7", "Kromme Rijn", "Robert Boyle", "five", "Paul Samuelson", "The First Doctor encounters himself in the story The Space Museum", "until the age of 16", "80%", "Five", "threatened \"Old Briton\" with severe consequences", "less than 200,000", "Anheuser-Busch InBev", "Charles River", "Veni redemptor gentium", "northwestern Canada", "intracellular pathogenesis", "1998", "seven months old", "The Service Module was discarded", "July", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "Article 17(3)", "Doctor Who \u2013 The Ultimate Adventure", "Eric Roberts", "electricity could be used to locate submarines", "1910\u20131940", "the owner", "2.666 million", "September 1969", "Apollo Program Director", "dreams", "Charles I", "more than 1,100 tree species", "complete the modules to earn Chartered Teacher Status", "true larvae", "apicomplexan-related diseases", "Rev. Paul T. Stallsworth", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "2009", "Daniel Diermeier", "PNU and ODM camps", "136", "12 to 15 million", "flax", "February 1, 2016", "2001", "lipophilic alkaloid toxins"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8867121848739495}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6614", "mrqa_squad-validation-4170", "mrqa_squad-validation-6426", "mrqa_squad-validation-8037", "mrqa_squad-validation-7774", "mrqa_squad-validation-3885", "mrqa_squad-validation-1492", "mrqa_squad-validation-5788", "mrqa_squad-validation-4343"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["Carolina Panthers", "1530", "Gallifrey", "ca. 2 million", "intractable problems", "effective planning", "pyrenoid and thylakoids", "Canterbury", "1873", "A bridge", "clergyman", "Commission v Italy", "sports tourism", "G", "eating both fish larvae and small crustaceans", "slow to complete division", "Astra's", "T. T. Tsui Gallery", "1905", "theatres", "those who proceed to secondary school or vocational training", "1521", "Search the Collections", "CD8", "3 January 1521", "a bill", "the University of Aberdeen", "2014", "Missy", "US$10 a week", "Super Bowl City", "Dudley Simpson", "esoteric", "temperature and light", "4000 years", "new entrance building", "Levi's Stadium", "tutor", "electricity", "2007", "Los Angeles", "zeta function", "adviser", "over $40 million", "Sunday Service of the Methodists in North America", "Lead fusible plugs", "occupational stress", "Synthetic aperture radar", "European Parliament and the Council of the European Union", "the coronation of Queen Elizabeth II", "O(n2)", "the Main Quadrangles", "35", "quadratic time", "antisemitic", "over-fishing and long-term environmental changes", "the Onon River and the Burkhan Khaldun mountain", "Hassan al-Turabi", "robbery", "Arlen Specter", "it is Oprah's daughters", "Venus Williams", "prisoners", "a globose pome"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7448529411764706}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8691", "mrqa_squad-validation-3958", "mrqa_squad-validation-4648", "mrqa_squad-validation-8864", "mrqa_squad-validation-9454", "mrqa_squad-validation-7818", "mrqa_squad-validation-1272", "mrqa_squad-validation-2122", "mrqa_squad-validation-1530", "mrqa_squad-validation-9977", "mrqa_squad-validation-1818", "mrqa_squad-validation-2525", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-808", "mrqa_triviaqa-validation-5325"], "SR": 0.71875, "CSR": 0.7890625, "EFR": 0.8888888888888888, "Overall": 0.8389756944444444}, {"timecode": 2, "before_eval_results": {"predictions": ["it developed into a major part of the Internet backbone", "The best-known legend", "Egyptians", "quantity surveyor", "the \"missile gap\"", "Tanaghrisson", "1852", "1564", "Concentrated O2", "adenosine triphosphate", "300 men", "11.5 inches (292.1 mm)", "1964", "the infected corpses over the city walls of Kaffa", "CD4 co-receptor", "the Lutheran and Reformed states in Germany and Scandinavia", "Chinggis Khaan International Airport", "modern buildings as well as structures dating from the 15th\u201318th centuries", "the warmest months from May through September, while the driest months are from November through April", "NBC", "Three", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Gymnosperms", "to punish Christians by God", "Word and Image department", "Nafzger", "MPEG-4", "BSkyB", "chromalveolates", "embroidery", "three hundred years", "26", "Duran Duran", "The Mallee and upper Wimmera are Victoria's warmest regions with hot winds blowing from nearby semi-deserts", "long distance services", "the chloroplasts of C4 plants", "the violence that subsequently engulfed the country", "primality", "divergent boundaries", "the Chancel Chapel", "Kurt H. Debus", "the construction of military roads to the area by Braddock and Forbes", "In bays where they occur in very high numbers", "soap opera Dallas", "Ted Heath", "late 14th-century", "high-voltage", "A contract", "Arabic numerals", "pamphlets on Islam", "draftsman", "1993\u201394", "the collider is finally ready for an attempt to circulate a beam of protons the whole way around the 17-mile tunnel", "France", "20", "the U.N. General Assembly", "the Koreans edge into second place in Asian qualifying Group 2 to finish ahead of Saudi Arabia on goal difference and seal their place in the finals", "the results by a chaplain about 1:45 p.m., per jail policy", "56", "school", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League with a 3-2 victory", "AbdulMutallab was in the bathroom for about 15 to 20 minutes, \"pushed the plunger on the bomb and prepared to die,\"", "coca wine", "Dissection"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7436701574569222}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5454545454545454, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0909090909090909, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4847", "mrqa_squad-validation-3088", "mrqa_squad-validation-3480", "mrqa_squad-validation-8905", "mrqa_squad-validation-4772", "mrqa_squad-validation-3270", "mrqa_squad-validation-7211", "mrqa_squad-validation-1909", "mrqa_squad-validation-2565", "mrqa_squad-validation-2906", "mrqa_squad-validation-2899", "mrqa_squad-validation-4322", "mrqa_squad-validation-9872", "mrqa_squad-validation-2291", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1201"], "SR": 0.671875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 3, "before_eval_results": {"predictions": ["\"vanguard of change and Islamic reform\"", "less than a year", "dampening the fire", "187 feet (57 m)", "Zagreus", "Swynnerton Plan", "extra-legal", "Harvey Martin", "December 1895", "lion, leopard, buffalo, rhinoceros, and elephant", "Establishing \"natural borders\"", "drama series", "17 years", "sold", "income inequality", "pastor", "1534", "mesoglea", "20%", "Isaac Newton", "Miocene", "\"citizenship\"", "13", "force model", "Super Bowl City", "three", "Georgia", "Fort Caroline", "Horace Walpole", "Afrikaans", "adaptive immune system", "24", "applications such as on-line betting, financial applications", "United Kingdom", "issues related to the substance of the statement", "2007", "Luther's anti-Jewish works", "short-tempered and even harsher", "First Minister", "two catechisms", "orientalism and tropicality", "John Dobson", "Super Bowl 50", "Beryl", "prima scriptura", "the Mongol Empire", "LOVE Radio", "\u201cLady\u201d or a \u201cWoman\u201d", "Robinsons, Unicorn Bitter", "\"The Mullen \u00ef\u00bf\u00bd\"", "cutis anserina", "Bogota", "Artemis", "Malaxis paludosa", "concealpcion", "pilot", "\"Cup of tea!\"", "Mexico", "9", "Dorset", "The Daily Mirror", "Kat ( Jessica Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Paul Lynde", "adenosine diphosphate"], "metric_results": {"EM": 0.6875, "QA-F1": 0.721474358974359}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-10466", "mrqa_squad-validation-3139", "mrqa_squad-validation-2486", "mrqa_squad-validation-9540", "mrqa_squad-validation-433", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-5104"], "SR": 0.6875, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 4, "before_eval_results": {"predictions": ["four days", "the International Fr\u00e9d\u00e9ric Chopin Piano Competition", "Nuda", "non-tertiary", "21", "The Christmas Invasion", "vertebrates", "Miocene", "Robert Underwood Johnson", "Cuba", "the Horniman Museum", "the ability to pursue valued goals", "University of Erfurt", "curved space", "Johann Sebastian Bach", "1524\u201325", "Spain's loss to Britain of Florida (Spain had ceded this to Britain in exchange for the return of Havana, Cuba)", "by using net wealth (adding up assets and subtracting debts), the Oxfam report, for instance, finds that there are more poor people in the United States and Western Europe than in China", "2009", "British", "2005", "Germany and Austria", "self-starting design", "two tumen (20,000 soldiers)", "international metropolitan region", "Paul Revere", "projects sponsored by the National Science Foundation (NSF) beginning in 1985", "Go-Ahead", "football", "second-largest", "biologist", "upper sixth", "arrested", "Pakistan", "1185", "Central business districts", "Hisao Yamada", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "13th century", "organisms", "co-NP", "between 1.4 and 5.8 \u00b0C above 1990 levels", "cornwall", "stand by Me", "agie", "cornwall", "cornwall", "cornwall", "leopold II", "oak leaf", "cornwall", "cornwall", "cornwall", "cornwall", "cornwall", "The Killers", "cornwall", "Russell Crowe", "cornwall", "cornwall", "Skat", "cornwall", "Christopher Nolan", "gun"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6227678571428572}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.06666666666666667, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-680", "mrqa_squad-validation-6981", "mrqa_squad-validation-1621", "mrqa_squad-validation-10145", "mrqa_squad-validation-7554", "mrqa_squad-validation-1360", "mrqa_squad-validation-6046", "mrqa_squad-validation-3119", "mrqa_squad-validation-4848", "mrqa_squad-validation-5374", "mrqa_squad-validation-6279", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-9913", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-2062", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-13775", "mrqa_triviaqa-validation-1816"], "SR": 0.59375, "CSR": 0.70625, "EFR": 1.0, "Overall": 0.853125}, {"timecode": 5, "before_eval_results": {"predictions": ["the wedding banquet", "respiration", "a deficit", "$216,000", "City of Malindi", "Apollo Applications Program", "trial division", "from the mid-sixties through to the present day", "An attorney", "eight", "The Book of Discipline", "Olivier Messiaen", "1759-60", "The Rankine cycle", "deadly explosives", "first half of the eighteenth century", "5,560", "Tricia Marwick", "the main contractor", "\"ctenes", "third most abundant chemical element", "adjustable spring-loaded valve", "agriculture", "metamorphosed", "David Suzuki", "the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "June 1979", "4.95 mL", "December 12", "second half of the 20th Century", "Ten", "the edge railed rack and pinion Middleton Railway", "John Elway", "1598", "The Eleventh Doctor", "Tugh Temur", "War of Currents", "dampening the fire", "eight years", "the Dalai Lama", "\"Battlefield helicopter crews routinely practice landing in fields and confined spaces away from their airfields", "corruption", "the Dalai Lama", "deaths", "2-0", "Chesley \"Sully\" Sullenberger", "Stanford University", "issued his first military orders", "July", "women", "McChrystal", "the Bronx", "Shanghai", "Fernando Gonzalez", "sportswear", "Linda Klein", "killing up to 280,000 people", "the sins of the members of the church", "humans", "a simple majority vote", "George Best", "June 26, 2018", "Mahler Symphonies", "March 19, 2017"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7894975611772487}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.962962962962963, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0625, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.8, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7831", "mrqa_squad-validation-3559", "mrqa_squad-validation-4172", "mrqa_squad-validation-3555", "mrqa_squad-validation-3179", "mrqa_squad-validation-383", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-539", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-6817"], "SR": 0.703125, "CSR": 0.7057291666666667, "EFR": 0.9473684210526315, "Overall": 0.8265487938596492}, {"timecode": 6, "before_eval_results": {"predictions": ["John Mayow", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "1 July 1851", "1562", "32.9%", "the architect's client and the main contractor", "2009", "Katy\u0144 Museum", "Derek Wolfe", "silicates", "methotrexate or azathioprine", "over 100,000", "3.55 inches", "Bukhara", "Beyonc\u00e9", "safaris", "Huntington Boulevard", "Northern Pride Festival", "eight", "Henry Laurens", "Roone Arledge", "the Florida legislature", "autoimmune", "Diarmaid MacCulloch", "permafrost", "University Athletic Association", "idolatry", "Protestant clergy to marry", "a suite of network protocols", "German", "21 to 11", "eight", "\"Yes, I committed the act of which you accuse me.", "6800", "15", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "November 25, 2002", "the center", "fovea centralis", "Milcom", "Choroyuki Tagawa", "MGM Resorts International", "Bobby Darin", "Gene MacLellan", "Matt Monro", "Andhra Pradesh and Odisha", "2013", "Wisconsin", "Jonathan Breck", "Neil Young", "milling", "hot enough that light in the form of either glowing or a flame is produced", "de jure racial segregation was ruled a violation of the Equal Protection Clause of the Fourteenth Amendment of the United States Constitution", "hot summers and mild winters", "Sir Alex Ferguson", "Bachendri Pal", "April 2011", "1979", "Edouard Manet", "Justin Spitzer", "Dr. Paul Appelbaum", "Choir-laid", "Chronic obstructive pulmonary disease", "Jim Inhofe"], "metric_results": {"EM": 0.625, "QA-F1": 0.6934450965700966}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.5, 1.0, 0.18181818181818185, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1166", "mrqa_squad-validation-291", "mrqa_squad-validation-8400", "mrqa_squad-validation-6223", "mrqa_squad-validation-4673", "mrqa_squad-validation-2416", "mrqa_squad-validation-978", "mrqa_squad-validation-6913", "mrqa_squad-validation-5653", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-4865", "mrqa_newsqa-validation-130", "mrqa_searchqa-validation-10794", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-9370"], "SR": 0.625, "CSR": 0.6941964285714286, "EFR": 0.9166666666666666, "Overall": 0.8054315476190477}, {"timecode": 7, "before_eval_results": {"predictions": ["The Hoppings", "Chicago", "an order from Genghis Khan", "Inherited wealth", "\"There is a world of difference between his belief in salvation and a racial ideology.", "StubHub Center", "WMO Executive Council and UNEP Governing Council", "trial division", "1999", "Gian Lorenzo Bernini", "housing stock", "dendritic cells, keratinocytes and macrophages", "Newcastle Mela", "ambiguity", "1665", "The Day of the Doctor", "punts", "1.6 kilometres", "\"winds up\" the debate", "2005", "100,000", "thermal expansion", "John Sutcliffe", "Owen Daniels", "incitement to terrorism", "Brookhaven", "A construction project", "Roy", "17 February 1546", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people), human inequality can be addressed", "September 30, 1960", "Brian Steele", "status line", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "call premium", "left coronary artery", "nine", "Havana Harbor", "four", "1932", "Walter Mondale", "Mitch Murray", "Marie Fredriksson", "December 1, 2009", "virtual reality simulator", "Noah Schnapp", "Antonio Banderas", "points on a sphere or angles in a circle are measured in units called?", "Kanawha River", "Julie Stichbury", "in consistency and content", "William J. Bell", "Spanish", "September 2017", "Krypton", "1990", "the Soviet Union", "gatekeeper", "Murcia", "India", "Wolfgang Amadeus Mozart", "Armin Meiwes", "Mot\u00f6rhead", "Bill Clinton"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6891690178978569}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65, 1.0, 1.0, 1.0, 0.8372093023255813, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6294", "mrqa_squad-validation-2608", "mrqa_squad-validation-8910", "mrqa_squad-validation-5189", "mrqa_squad-validation-6402", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-2674", "mrqa_searchqa-validation-5845"], "SR": 0.640625, "CSR": 0.6875, "EFR": 1.0, "Overall": 0.84375}, {"timecode": 8, "before_eval_results": {"predictions": ["Prague", "extremely high humidity", "Manning", "Sports Night", "2nd century BCE", "taxation", "destroy the antichrist", "collective bargaining, political influence, or corruption", "the Ilkhanate", "a second Gleichschaltung", "1864", "NL and NC", "over fifty", "Albert Einstein", "discarded", "state or government schools", "\"ash tree\" in Spanish, and an ash leaf is featured on the city's flag.", "Commission v France", "purported video, remaining in black and white, contains conservative digital enhancements and did not include sound quality improvements", "1689", "adaptive immune system", "case law by the Court of Justice", "Von Miller", "an archetypal \"mad scientist\"", "Dendritic cells", "Allston Science Complex", "writing a five volume book in his native Greek \u03a0\u03b5\u03c1\u03af \u03cd\u03bb\u03b7\u03c2 \u03b9\u03b1\u03c4\u03c1\u03b9\u03ba\u03ae\u03c2 in the 1st century AD.", "wars", "in inorganic forms, such as calcium carbonate", "to solve South Africa's'' ethnic problems", "Hirschman", "a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Sergeant Himmelstoss", "four", "bohrium", "a major fall in stock prices", "Buddhist missionaries", "March 31 to April 8, 2018", "The primary and secondary recipients may only see their own email address in Bcc", "start fires, hunt, and bury their dead", "Ray Charles", "the story's themes of moral dilemma and choosing between the easy and the right decision", "omitted and an additional panel stating the type of hazard ahead", "Valene Kane", "continues the pre-existing appropriations at the same levels as the previous fiscal year", "Arnold Schoenberg", "Tiffany Adams Coyne", "Kent Robbins", "Elizabeth Banks as Gail Abernathy - McKadden - Feinberger", "Southampton ( 1902, then in the Southern League )", "July 23, 2016", "Auburn Tigers football team", "Kate '' Mulgrew", "1943", "Rumplestiltskin", "Kryptonite", "a long - term infection that can be very difficult to eradicate", "Santiago", "Japan", "Lance Cpl. Maria Lauterbach", "the Demon Barber of Fleet Street", "the actinide berkelium", "(Heifetz, Perlman, etc.?)", "an isosceles triangle has two sides of equal length."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5857408717701413}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.25, 1.0, 0.8, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 0.0, 1.0, 0.8823529411764706, 0.0, 1.0, 1.0, 0.12500000000000003, 0.0, 0.5, 0.0, 0.6, 1.0, 0.0851063829787234, 0.0, 0.0, 0.6896551724137931, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 1.0, 0.5263157894736842, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4634", "mrqa_squad-validation-4029", "mrqa_squad-validation-3113", "mrqa_squad-validation-6678", "mrqa_squad-validation-3946", "mrqa_squad-validation-1248", "mrqa_squad-validation-6310", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-2351", "mrqa_newsqa-validation-2518", "mrqa_searchqa-validation-10924", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-2789"], "SR": 0.46875, "CSR": 0.6631944444444444, "EFR": 0.9411764705882353, "Overall": 0.8021854575163399}, {"timecode": 9, "before_eval_results": {"predictions": ["gold", "the corporate alternating current/direct current \"War of Currents\"", "12", "Antoine Lavoisier", "the United Kingdom, Australia, Canada and the United States", "well before Braddock's departure for North America.", "Philip Webb and William Morris", "forming a 'A National Gallery of British Art'", "the Middle East", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party, while Scotland itself elected relatively few Conservative MPs.", "exceeds any given number", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)", "the Channel Islands", "an additional warming of the Earth's surface", "they were nomads", "private southern Chinese manufacturers and merchants", "Owen Jones", "the Eleventh Doctor", "the BBC National Orchestra of Wales", "bilaterians", "the traditional Chinese autocratic-bureaucratic system", "August 1992", "NBC", "the region was superior to that of the British, since Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.", "Buckland Valley near Bright", "November 17, 2017", "Daren Maxwell Kagasoff", "the Royal Air Force ( RAF )", "the Sunni Muslim family", "3,000 metres ( 9,800 ft )", "to form a higher alkane", "on about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "James Watson and Francis Crick", "September 29, 2017", "11 p.m.", "the Old English wylisc ( pronounced `` wullish '' ) meaning `` foreigner '' or `` Welshman ''", "200,564", "Lake Baikal", "orbit", "7000301604928199000", "wavelengths on the opposite end of the spectrum can be as long as the universe", "Spanish explorers", "the roofs of the choir side - aisles", "the Indian Civil Service", "different parts of the globe", "statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "florida", "5,534", "Louis XVIII", "the level of the third lumbar vertebra, or L3, at birth", "T'Pau", "the actors", "Julia Ormond", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "George Strait", "Karina Smirnoff", "the fallopian tube", "insects and their relationship to humans, other organisms, and the environment", "195029", "Dana Andrews", "Hawaii", "Hakeemullah Mehsud", "Bob Dylan", "the tiger"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6049626484116374}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7647058823529412, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.92, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.5, 1.0, 0.0, 0.05882352941176471, 1.0, 0.14285714285714285, 0.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.4, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.8, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1180", "mrqa_squad-validation-3585", "mrqa_squad-validation-9334", "mrqa_squad-validation-7771", "mrqa_squad-validation-8410", "mrqa_squad-validation-5733", "mrqa_squad-validation-10232", "mrqa_squad-validation-2843", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10283", "mrqa_triviaqa-validation-3868", "mrqa_hotpotqa-validation-940", "mrqa_hotpotqa-validation-1398", "mrqa_newsqa-validation-3354", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-16618"], "SR": 0.484375, "CSR": 0.6453125, "EFR": 1.0, "Overall": 0.82265625}, {"timecode": 10, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-940", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8859", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-996", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-299", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-13775", "mrqa_searchqa-validation-16618", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10035", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10145", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10301", "mrqa_squad-validation-10359", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-10415", "mrqa_squad-validation-10466", "mrqa_squad-validation-1082", "mrqa_squad-validation-1109", "mrqa_squad-validation-1131", "mrqa_squad-validation-1151", "mrqa_squad-validation-1166", "mrqa_squad-validation-1180", "mrqa_squad-validation-1180", "mrqa_squad-validation-1195", "mrqa_squad-validation-121", "mrqa_squad-validation-1217", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1395", "mrqa_squad-validation-1468", "mrqa_squad-validation-1488", "mrqa_squad-validation-1492", "mrqa_squad-validation-1621", "mrqa_squad-validation-1630", "mrqa_squad-validation-1645", "mrqa_squad-validation-170", "mrqa_squad-validation-1719", "mrqa_squad-validation-1732", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1962", "mrqa_squad-validation-1971", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2082", "mrqa_squad-validation-2122", "mrqa_squad-validation-2159", "mrqa_squad-validation-217", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2361", "mrqa_squad-validation-2412", "mrqa_squad-validation-2416", "mrqa_squad-validation-2418", "mrqa_squad-validation-2457", "mrqa_squad-validation-2486", "mrqa_squad-validation-2548", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2608", "mrqa_squad-validation-2633", "mrqa_squad-validation-2667", "mrqa_squad-validation-2678", "mrqa_squad-validation-2843", "mrqa_squad-validation-2861", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2899", "mrqa_squad-validation-291", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-2985", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3088", "mrqa_squad-validation-3104", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3119", "mrqa_squad-validation-3162", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3209", "mrqa_squad-validation-3215", "mrqa_squad-validation-3241", "mrqa_squad-validation-3307", "mrqa_squad-validation-3355", "mrqa_squad-validation-3362", "mrqa_squad-validation-3407", "mrqa_squad-validation-3411", "mrqa_squad-validation-3413", "mrqa_squad-validation-3451", "mrqa_squad-validation-348", "mrqa_squad-validation-349", "mrqa_squad-validation-3522", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3559", "mrqa_squad-validation-3569", "mrqa_squad-validation-3585", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3822", "mrqa_squad-validation-383", "mrqa_squad-validation-3830", "mrqa_squad-validation-3841", "mrqa_squad-validation-3855", "mrqa_squad-validation-3882", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-3946", "mrqa_squad-validation-4008", "mrqa_squad-validation-4028", "mrqa_squad-validation-4046", "mrqa_squad-validation-4121", "mrqa_squad-validation-4172", "mrqa_squad-validation-423", "mrqa_squad-validation-4296", "mrqa_squad-validation-433", "mrqa_squad-validation-4331", "mrqa_squad-validation-4376", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4418", "mrqa_squad-validation-4430", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-4463", "mrqa_squad-validation-4495", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4648", "mrqa_squad-validation-4673", "mrqa_squad-validation-4704", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-477", "mrqa_squad-validation-4772", "mrqa_squad-validation-4803", "mrqa_squad-validation-4807", "mrqa_squad-validation-4841", "mrqa_squad-validation-4848", "mrqa_squad-validation-4936", "mrqa_squad-validation-4983", "mrqa_squad-validation-5023", "mrqa_squad-validation-5063", "mrqa_squad-validation-5083", "mrqa_squad-validation-513", "mrqa_squad-validation-513", "mrqa_squad-validation-5136", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5247", "mrqa_squad-validation-5250", "mrqa_squad-validation-5254", "mrqa_squad-validation-5265", "mrqa_squad-validation-5295", "mrqa_squad-validation-5307", "mrqa_squad-validation-5318", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5418", "mrqa_squad-validation-5445", "mrqa_squad-validation-5485", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5563", "mrqa_squad-validation-5564", "mrqa_squad-validation-5566", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5608", "mrqa_squad-validation-5653", "mrqa_squad-validation-5664", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5766", "mrqa_squad-validation-5782", "mrqa_squad-validation-5788", "mrqa_squad-validation-5821", "mrqa_squad-validation-5843", "mrqa_squad-validation-5852", "mrqa_squad-validation-5951", "mrqa_squad-validation-6046", "mrqa_squad-validation-6049", "mrqa_squad-validation-6067", "mrqa_squad-validation-6073", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6294", "mrqa_squad-validation-6310", "mrqa_squad-validation-638", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-653", "mrqa_squad-validation-6531", "mrqa_squad-validation-6535", "mrqa_squad-validation-6548", "mrqa_squad-validation-6567", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-6678", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6840", "mrqa_squad-validation-6841", "mrqa_squad-validation-6868", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6917", "mrqa_squad-validation-6959", "mrqa_squad-validation-6962", "mrqa_squad-validation-6981", "mrqa_squad-validation-703", "mrqa_squad-validation-7030", "mrqa_squad-validation-7142", "mrqa_squad-validation-7175", "mrqa_squad-validation-7211", "mrqa_squad-validation-7252", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-743", "mrqa_squad-validation-7435", "mrqa_squad-validation-7456", "mrqa_squad-validation-7554", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7632", "mrqa_squad-validation-7633", "mrqa_squad-validation-7678", "mrqa_squad-validation-7699", "mrqa_squad-validation-7704", "mrqa_squad-validation-7709", "mrqa_squad-validation-7716", "mrqa_squad-validation-7747", "mrqa_squad-validation-7766", "mrqa_squad-validation-7771", "mrqa_squad-validation-7774", "mrqa_squad-validation-7775", "mrqa_squad-validation-7800", "mrqa_squad-validation-7818", "mrqa_squad-validation-7831", "mrqa_squad-validation-7846", "mrqa_squad-validation-7863", "mrqa_squad-validation-7888", "mrqa_squad-validation-7899", "mrqa_squad-validation-795", "mrqa_squad-validation-7965", "mrqa_squad-validation-797", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8071", "mrqa_squad-validation-8163", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8365", "mrqa_squad-validation-8372", "mrqa_squad-validation-8410", "mrqa_squad-validation-8414", "mrqa_squad-validation-8441", "mrqa_squad-validation-8486", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8589", "mrqa_squad-validation-859", "mrqa_squad-validation-8598", "mrqa_squad-validation-8600", "mrqa_squad-validation-8666", "mrqa_squad-validation-8670", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8907", "mrqa_squad-validation-9020", "mrqa_squad-validation-9030", "mrqa_squad-validation-9050", "mrqa_squad-validation-9116", "mrqa_squad-validation-9121", "mrqa_squad-validation-9151", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9379", "mrqa_squad-validation-9401", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9454", "mrqa_squad-validation-9465", "mrqa_squad-validation-9484", "mrqa_squad-validation-9540", "mrqa_squad-validation-9590", "mrqa_squad-validation-9608", "mrqa_squad-validation-9689", "mrqa_squad-validation-9738", "mrqa_squad-validation-976", "mrqa_squad-validation-9760", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9954", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2579", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6711"], "OKR": 0.9140625, "KG": 0.4078125, "before_eval_results": {"predictions": ["2009", "Huntington Boulevard", "from the mid-sixties through to the present day", "William Hartnell and Patrick Troughton", "Distributed Adaptive Message Block Switching", "1331", "Confucianism", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "pseudo- Sciencesences", "a military coup d'\u00e9tat", "the A1", "proteolysis", "a form of starch", "Eisleben", "break off the cathode, pass out of the tube, and physically strike him", "late night talk shows", "13\u20137", "around half", "the comprehensive institutions of the Great Yuan", "a D loop mechanism of replication", "Chuck Howley", "with observations", "Annette", "Tim McGraw and Kenny Chesney", "Beorn", "a house edge of between 0.5 % and 1 %, placing blackjack among the cheapest casino table games", "The sperm plasma", "their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "a sociological perspective", "6 January 793", "a historical street in downtown Cebu City that is often called the oldest and the shortest national road in the Philippines", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Jane Fonda", "Robert Irsay", "Edgar Lungu", "UNESCO / ILO", "five points", "Achal Kumar Jyoti", "the President", "Homer Banks", "The management team", "British R&B girl group Eternal", "Montreal Canadiens", "When the others arrive", "six", "Yuzuru Hanyu", "a receptor or enzyme is distinct from the active site", "the geologist James Hutton", "a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "31 October 1972", "December 2, 1942", "September 8, 2017", "the Hollywood Masonic Temple", "Courteney Cox", "the Gaget, Gauthier & Co. workshop", "Renhe Sports Management Ltd", "Tangled", "smartphones", "Kenneth Hood", "a heavy metal band", "Scotland", "a school test score of 98", "the Missouri Compromise of 1850", "Peyton Place"], "metric_results": {"EM": 0.46875, "QA-F1": 0.585250732464944}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.782608695652174, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.4444444444444445, 0.0, 0.7586206896551725, 0.6666666666666666, 0.0, 0.10526315789473684, 0.375, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.09523809523809523, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7832", "mrqa_squad-validation-8160", "mrqa_squad-validation-4849", "mrqa_squad-validation-9831", "mrqa_squad-validation-5357", "mrqa_squad-validation-1388", "mrqa_squad-validation-434", "mrqa_squad-validation-625", "mrqa_squad-validation-8747", "mrqa_squad-validation-8530", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-2208", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3223", "mrqa_searchqa-validation-15757"], "SR": 0.46875, "CSR": 0.6292613636363636, "EFR": 0.9411764705882353, "Overall": 0.7237750668449198}, {"timecode": 11, "before_eval_results": {"predictions": ["The availability of the Bible in vernacular languages", "detention", "Western Xia", "a majority of all MEPs (not just those present) to block or suggest changes", "carbon related emissions", "co-chair", "Katharina", "three", "August 10, 1948", "the holy catholic (or universal) church", "30\u201360%", "Louis Paul Cailletet", "Class I MHC molecules", "crust and lithosphere", "orange", "electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons", "stroke", "13 June 1525", "the type of reduction being used", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "Cho Diergaardt", "September 1947", "Spanish explorers", "Western Australia", "Two Days Before the Day After Tomorrow", "April 10, 2018", "October 1, 2015", "number of games where the player played, in whole or in part", "cat", "a nobiliary particle indicating a noble patrilineality or as a simple preposition that approximately means of or from in the case of commoners", "Tom Brady", "china in chinese is called zhongguo", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Emma", "1960", "Bengal tiger", "Harry Potter and the Deathly Hallows", "a barrier that runs across a river or stream to control the flow of water", "De Wayne Warren", "triacylglycerol", "August 15, 1971", "The word autumn comes from the ancient Etruscan root autu - and has within it connotations of the passing of the year", "Tom Brady", "Margaery Tyrell", "Mount Mannen in Norway and at the Isle of Sheppey in England", "Lake Powell", "Clarence L. Tinker", "Hotel barge   Bed and breakfast   Botel", "Trace Adkins", "Garbi\u00f1e Muguruza", "Keith Richards", "Hercules", "June 12, 2018", "Bart Howard", "111", "Leonard Bernstein", "The Hague", "historic buildings, arts, and published works", "Lisa", "Arnoldo Rueda Medina", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "George Wallace", "Florida", "CNN"], "metric_results": {"EM": 0.5, "QA-F1": 0.6114879172094245}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.75, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.9743589743589743, 1.0, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.4, 0.0, 1.0, 0.45454545454545453, 1.0, 0.0, 0.625, 1.0, 0.4615384615384615, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4074", "mrqa_squad-validation-8581", "mrqa_squad-validation-3474", "mrqa_squad-validation-6618", "mrqa_squad-validation-4952", "mrqa_squad-validation-10483", "mrqa_squad-validation-3385", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-386", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1549", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-13057"], "SR": 0.5, "CSR": 0.6184895833333333, "EFR": 1.0, "Overall": 0.7333854166666666}, {"timecode": 12, "before_eval_results": {"predictions": ["savanna or desert", "Downtown Riverside", "Kurt Vonnegut", "three", "Graz, Austria", "Sky Digital", "until 1796", "Einstein", "Gary Kubiak", "soy farmers", "The Sinclair Broadcast Group", "mercuric oxide (HgO)", "Guglielmo Marconi", "Luther's education", "wages and profits", "southern Europe", "The judicial branch", "biostratigraphers", "1999", "Miami Heat of the National Basketball Association ( NBA )", "the nasal septum", "a warrior, healer, or rogue coming from an elven, human, or Dwarven background", "Bob Dylan", "1799", "interphase", "fixed in United States dollars", "a tree species ( that generally grows in the elevation range of 3,000 to 4,200 metres ( 9,800 to 13,800 ft ) in the Himalayas )", "Roger Federer", "James Corden", "Pasek & Paul", "October 2", "1956", "Thomas Lennon", "Gwendoline Christie", "Orangeville, Ontario, Canada", "Cameron Fraser ( who disappeared with \u00a3 60,000 of her savings )", "random - access memory ( RAM )", "Afghanistan", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Walter Brennan", "James Rodr\u00edguez", "8ft", "various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "1985, 2016, 2018", "any vessel approaching British waters", "April 17, 1982", "Acid rain", "the majority of the chain's Canadian stores by July 1, 2005", "crossbar", "Thespis", "Rafael Barba", "By functions", "Thebes", "a constitutional monarchy in which the power of the Emperor is limited and is relegated primarily to ceremonial duties", "Consular Report of Birth Abroad for children born to U.S. citizens ( who are also eligible for citizenship ), including births on military bases in foreign territory", "Easter", "the hindfoot", "southwestern", "Planet Terror", "Atlantic Ocean", "one", "meander", "snowflake curve, a geometric pattern repeated at smaller scales, has fractional dimensions", "the ruble"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6757372835497836}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 0.0, 0.9777777777777777, 0.5, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.06666666666666667, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8032", "mrqa_squad-validation-9895", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2644", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6998", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-559", "mrqa_newsqa-validation-2782", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-10635"], "SR": 0.609375, "CSR": 0.6177884615384616, "EFR": 0.96, "Overall": 0.7252451923076924}, {"timecode": 13, "before_eval_results": {"predictions": ["return home", "socialist realism", "to spearhead the regeneration of the North-East", "sleep deprivation", "1977", "Anderson", "quantum electrodynamics (or QED)", "provisional elder/deacon", "antigenic variation", "kinematic measurements", "worker, capitalist/business owner, landlord", "the communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Licensed Local Pastor", "driving them in front of the army", "both Kenia and Kegnia believed by most to be a corruption of the Kamba version", "24 March 1879", "Rob Van Winkle", "electrons", "Edie Falco", "Leonard Bernstein", "Empire of the Sun", "Napoleon of the Stump", "Nicaragua", "Afghanistan", "Russia", "the Aladdin Hotel", "Uncle Henry", "MOVIE KISSES", "volatile", "China", "solmn", "the University of Hanoi", "Agliff", "Alexander Ulyanov", "Beatrix Potter", "Seville", "George Washington", "Volvic", "Andrea del Sarto", "Colorado", "Christopher Columbus", "Balfour Declaration", "money damages", "a tortoise", "biretta", "Shinto", "Simpsons", "a genetic disease that prevents blood from clotting", "Beautiful is truth, truth beauty", "silk cord", "Bad Boys", "Philadelphia", "Robert Downey Jr.", "The Fresh Prince of Bel-Air", "abusive husband", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Sylvester J. Pussycat, Sr.", "Daniel Defoe", "Atlantic Ocean", "Battle of Britain and the Battle of Malta", "the U.S. Consulate in Rio de Janeiro", "if we're going to revise our policies here, we need to make it so for all the camps", "three", "in the guest bathroom"], "metric_results": {"EM": 0.625, "QA-F1": 0.7005490390600685}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 0.7407407407407407, 0.4, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-8065", "mrqa_squad-validation-8258", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-10977", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-708", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-4663", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-11170", "mrqa_naturalquestions-validation-8179", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-4717", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-2199"], "SR": 0.625, "CSR": 0.6183035714285714, "EFR": 0.9583333333333334, "Overall": 0.7250148809523809}, {"timecode": 14, "before_eval_results": {"predictions": ["Huguenots", "1281", "Tower Theatre", "the Mi'kmaq and the Abenaki", "basic design typical of Eastern bloc countries.", "Palestine", "a pair of retractable tentacles fringed with colloblasts, sticky cells", "drawn by the convenience of the railroad and worried about flooding, moved to the new community.", "Ireland", "18 April 1521", "the Archangel Michael", "Warszawa", "the Migration period", "2012", "35", "Trump", "a landmark", "the Blue Nile", "Solomon", "Betsey Johnson", "Eragon", "Rawhide", "a miniature peny", "a young Frances", "Paul Hornung", "silk", "Donna Summer", "a galaxy traveler", "John Mahoney", "Murder by Death", "Dave Brubeck", "Emmett Dalton", "Washington", "Gilda", "Gov. Rodney King", "Franklin D. Roosevelt", "rice", "chilled soba", "Sirhan Sirhan", "the Moon", "June Solstice", "a word", "Mountain Dew", "Omaha", "Sammy Hagar", "actress", "the Erie Canal", "Raising Arizona", "Baltic Sea", "held by their state as a whole.", "Magic Johnson", "Filippo Brunelleschi", "hog", "do", "Native American nation from the Great Plains", "Renishaw Hall, Derbyshire, England, UK", "Imola Circuit", "sheep", "to provide travel agencies in Japan with booking and ticketing capabilities for a wider range of international airlines", "Berea College", "hanged in 1979 for the murder of a political opponent two years after he was ousted as prime minister in a military coup.", "Sharon Bialek", "The European Council", "Barbados"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6487959956709957}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285714, 0.8181818181818181, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2943", "mrqa_squad-validation-4621", "mrqa_squad-validation-4585", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-9432", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-4753", "mrqa_triviaqa-validation-1936", "mrqa_hotpotqa-validation-5184", "mrqa_newsqa-validation-846", "mrqa_triviaqa-validation-2317"], "SR": 0.5625, "CSR": 0.6145833333333333, "EFR": 1.0, "Overall": 0.7326041666666666}, {"timecode": 15, "before_eval_results": {"predictions": ["easier and more efficient than anywhere else", "held on a bus and taken to the Nye County seat of Tonopah, Nevada", "Francis Marion", "the death of Elisabeth Sladen in early 2011.", "3.6%", "nearly 42,000", "14", "the United States Census Bureau", "Asia", "19 April 1943", "actions-oriented", "rapidly evolve and adapt", "present-day Upstate New York and the Ohio Country", "the Roman Republic", "Debbie Abrahams", "South Africa", "William David Charles Carling", "The Chatham House Rule", "haiti", "America Online", "Aramis", "bees", "The Firm", "The Streets", "violin", "a blazer", "the last lap", "the Titanic", "inguinal", "the gluteus maximus", "Luigi", "Georgia", "Massachusetts", "the London theatre district", "La Boh\u00e8me", "John Quincy Adams", "kadidlo", "Ethel Skinner", "Queen Victoria", "My Fair Lady", "Austria", "lignin", "Adolphe Adam", "haoy", "Barcelona", "the coelacanth", "mono", "cats", "Love Never Dies", "Elizabeth I", "Harmolodic", "vexillophiles", "the ISS", "the Marcy Brothers", "1770 BC", "costume party", "terrorist activity against Norwegian interests", "Sydney", "German Chancellor Angela Merkel", "CNN", "Myron Leon \" Mike\" Wallace", "the Professor", "Burundi and Rwanda", "The Howard Stern Show"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5728016774891774}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [0.7272727272727273, 0.2, 0.0, 0.7272727272727273, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1830", "mrqa_squad-validation-6702", "mrqa_squad-validation-3118", "mrqa_squad-validation-7872", "mrqa_squad-validation-10108", "mrqa_squad-validation-5893", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-871", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-7024", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3616", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8359", "mrqa_hotpotqa-validation-1537", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-339", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-8487"], "SR": 0.46875, "CSR": 0.60546875, "EFR": 1.0, "Overall": 0.7307812499999999}, {"timecode": 16, "before_eval_results": {"predictions": ["computational complexity theory", "leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies", "tensions over slavery and the power of bishops in the denomination", "mannerist architecture", "489", "the Grainger Town area", "the Great Fire of London", "Theory of the Earth", "Ward", "Newton", "bilaterians", "one of the daughters of former King of Thebes, Oedipus", "the forces of Andrew Moray and William Wallace", "differential erosion", "between two and 30 eggs", "biannually", "Arthur Chung", "18", "9 February 2018", "Yuzuru Hanyu", "marley & Me", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff", "the 2013 non-fiction book of the same name by David Finkel", "Duck", "McKim Marriott", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "31", "in Austin, Texas", "postero - medially towards the optic chiasm", "the Twelvers, and Seven pillars", "a shunting of blood away from the organs not necessary to the immediate survival of the organism and an increase in blood flow to those organs involved in intense physical activity", "attached to another chromosome", "Rockwell", "Allhallowtide", "Sylvester Stallone", "In Time", "Rodney Crowell", "the `` 0 '' trunk code", "in eukaryotic cells", "Melissa Disney", "Darren McGavin", "UNESCO / ILO", "Thaddeus Rowe Luckinbill", "ummat al - Islamiyah", "William DeVaughn", "a Spanish surname", "Brevet Colonel Robert E. Lee, lieutenant colonel of the 2nd U.S. Cavalry Regiment", "The Annunciation", "Santiago Ram\u00f3n y Cajal", "boy", "seven", "novella", "southeast of the city", "21 June 2007", "four", "California", "England", "Roc Me Out", "italy & The Olympians", "root out terrorists within its borders.", "Rudolf Nureyev", "43", "the Kurdish militant group of northern Iraq", "Mohamed Alanssi"], "metric_results": {"EM": 0.5, "QA-F1": 0.60499260611291}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.08695652173913045, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7586206896551725, 0.0, 0.5, 0.0, 0.5714285714285715, 1.0, 0.8, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5882352941176471, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9525", "mrqa_squad-validation-6640", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5348", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-3649", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-4917", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-814"], "SR": 0.5, "CSR": 0.5992647058823529, "EFR": 0.9375, "Overall": 0.7170404411764706}, {"timecode": 17, "before_eval_results": {"predictions": ["Sydney", "Doritos", "Hans Vredeman de Vries", "$20.4 billion, or $109 billion in 2010 dollars", "Lake \u00dcberlingen", "Finsteraarhorn", "seven", "Sherlock Holmes", "the Lippe", "the American Revolutionary War", "25 per cent", "Chuck Noland", "Donna Mills", "March 31, 2013", "pulmonary heart disease ( cor pulmonale )", "before the first year begins", "Kansas", "the President of the United States", "UNESCO / ILO", "asexually", "The Republic of Tecala", "in 2002", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Travis Tritt and Marty Stuart", "the fifteenth full - length studio album by the American band Yo La Tengo,", "Cee - Lo", "The Sun", "a living prokaryotic cell ( or organelle ) by dividing the cell into two parts, each with the potential to grow to the size of the original", "1966", "Charlotte Hornets", "Julie Adams", "April 29, 2009", "J. Presper Eckert", "201", "1983", "Thursdays at 8 : 00 pm ( ET )", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "MacFarlane", "regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Hercules", "Angel Island ( California )", "Rufus and Chaka Khan", "in moist temperate climates", "after Shawn's kidnapping", "7.6 mm", "sorrow regarding the environment", "1955", "John B. Watson", "2020", "continues the pre-existing appropriations at the same levels as the previous fiscal year ( or with minor modifications ) for a set amount of time", "during World War II in Berlin", "the Gospels of Matthew, Mark, Luke and John", "Orangeville, Ontario, Canada", "a o*rab o*la", "Switzerland", "17th President of the United States", "St Augustine's Abbey", "it would welcome the Russian air force, according to Russian news agency Novosti.", "the insurgency", "Joe DiMaggio", "Sandra Bullock", "NBA 2K16", "Baltimore", "September 25, 2017"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6674803591937213}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.72, 1.0, 0.0, 0.0, 1.0, 0.09523809523809523, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.33333333333333337, 1.0, 0.972972972972973, 1.0, 0.5714285714285715, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.41379310344827586, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3953", "mrqa_squad-validation-7700", "mrqa_squad-validation-7288", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-7892", "mrqa_triviaqa-validation-2922", "mrqa_hotpotqa-validation-538", "mrqa_newsqa-validation-3489", "mrqa_hotpotqa-validation-4735"], "SR": 0.5625, "CSR": 0.5972222222222222, "EFR": 0.9285714285714286, "Overall": 0.7148462301587302}, {"timecode": 18, "before_eval_results": {"predictions": ["1904", "to arrest Luther if he failed to recant", "Michael Heckenberger and colleagues of the University of Florida", "declined significantly", "quantum", "prices", "Miller", "Super Bowl XX", "gold", "p", "2017", "to absorb menstrual flow", "protects it from infections coming from other organs ( such as lungs )", "Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "12 to 36 months old", "the mixing of sea water and fresh water", "her abusive husband", "png HTTP / 1.1", "the south coast of eastern New Guinea", "the 2009 model year", "December 2, 1942", "John Cooper Clarke", "1792", "Amitabh Bachchan, Akshay Kumar, Bobby Deol, Divya Khosla Kumar, Sandali Sinha and Nagma", "Annette Strean", "the Bee Gees", "September 25, 1987", "the Surrey Lions defeating the Warwickshire Bears by 9 wickets in the final to claim the title", "the Senate", "a little girl ( Addy Miller )", "Ritchie Cordell", "9.0 -- 9.1 ( M )", "the right to peaceably assemble, or to petition for a governmental redress of grievances", "2010", "James Corden", "the development of electronic computers in the United States", "the external genitalia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "the ark of the covenant", "senators, each of whom represents a single state in its entirety,", "beta decay", "Rachel Sarah Bilson", "the NIRA", "March 5, 2014", "Haikou on the Hainan Island", "19 June 2018", "stromal connective tissue", "the thirteenth series ended on 19 December 2015", "Stephen Curry of Davidson", "Susie's father, Ben Willis", "Rufus and Chaka Khan", "Jodie Foster", "Rory McIlroy", "Aristotle", "the Cubs", "1919", "Lithuania", "President Obama's race in 2008", "Lou and Wilson", "dana point bail", "olympicopa Integrated Care", "Pete Seeger", "Kiss Me Kate", "Wigan"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5525216734159554}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.75, 1.0, 0.0, 0.5714285714285715, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5106382978723404, 1.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.125, 1.0, 1.0, 1.0, 0.4, 0.888888888888889, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.10810810810810811, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2268", "mrqa_squad-validation-10388", "mrqa_squad-validation-845", "mrqa_squad-validation-5", "mrqa_squad-validation-9213", "mrqa_naturalquestions-validation-2406", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2904", "mrqa_triviaqa-validation-1259", "mrqa_hotpotqa-validation-4927", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3307", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-10336"], "SR": 0.4375, "CSR": 0.5888157894736843, "EFR": 0.9444444444444444, "Overall": 0.7163395467836258}, {"timecode": 19, "before_eval_results": {"predictions": ["phagosomal", "well before Braddock's departure for North America", "The Flintstones", "destroyed", "night", "assertive", "generally antagonistic", "whether or not to plead guilty", "parallelogram", "Valley Falls", "22 July 1930", "Sam Raimi", "New Hyde Park", "political thriller", "Timothy McVeigh", "Germaine, Sr.", "1995\u201396", "the NYPD's 83rd Precinct", "Azeroth", "October 5, 1930", "1999", "Musicology", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.", "Michael Sheen", "The rule of three", "the Mayor of the City of New York", "841", "My Boss, My Teacher", "Roy Spencer", "Bardney", "\"Carmen\", perhaps the most famous \"op\u00e9ra comique\"", "Everglades", "Saint Louis County", "1891", "Eielson Air Force Base", "Serhiy Paradzhanov", "Germany", "eight", "Carson City", "Lindsey Islands", "2008", "October 12, 1962", "Eli Roth", "Paige O'Hara", "Northern Ireland", "St. Louis, Missouri", "the first hole of a sudden-death playoff", "Prince Rogers Nelson", "every aspect of public and private life", "Burnley", "Russian", "between 11 or 13 and 18", "the onset and progression of Alzheimer's disease", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "7 June 2005", "Imola", "Sufjan Stevens", "producing rock music with a country influence", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "ballerinas", "Irving Berlin", "$1.5 million", "Sen. Barack Obama", "Tutsi ethnic minority and the Hutu majority"], "metric_results": {"EM": 0.625, "QA-F1": 0.7287326388888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-6024", "mrqa_squad-validation-1551", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-5485", "mrqa_naturalquestions-validation-2629", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-7828", "mrqa_newsqa-validation-3659"], "SR": 0.625, "CSR": 0.590625, "EFR": 1.0, "Overall": 0.7278125}, {"timecode": 20, "UKR": 0.708984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10019", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1939", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2918", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-386", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4867", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15836", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-346", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-4917", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5845", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8487", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9432", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10151", "mrqa_squad-validation-10192", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10381", "mrqa_squad-validation-10388", "mrqa_squad-validation-10399", "mrqa_squad-validation-10408", "mrqa_squad-validation-1062", "mrqa_squad-validation-1082", "mrqa_squad-validation-1131", "mrqa_squad-validation-1180", "mrqa_squad-validation-1248", "mrqa_squad-validation-1272", "mrqa_squad-validation-1334", "mrqa_squad-validation-1348", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-150", "mrqa_squad-validation-1530", "mrqa_squad-validation-1551", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1939", "mrqa_squad-validation-2003", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-217", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2361", "mrqa_squad-validation-2416", "mrqa_squad-validation-2481", "mrqa_squad-validation-2511", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2667", "mrqa_squad-validation-2772", "mrqa_squad-validation-2861", "mrqa_squad-validation-2881", "mrqa_squad-validation-2906", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3148", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3270", "mrqa_squad-validation-3355", "mrqa_squad-validation-3385", "mrqa_squad-validation-3411", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3830", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4159", "mrqa_squad-validation-4186", "mrqa_squad-validation-423", "mrqa_squad-validation-4331", "mrqa_squad-validation-434", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4430", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4681", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-501", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5198", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5265", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-552", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5640", "mrqa_squad-validation-5653", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5849", "mrqa_squad-validation-5893", "mrqa_squad-validation-5986", "mrqa_squad-validation-6024", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6294", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-6728", "mrqa_squad-validation-680", "mrqa_squad-validation-6841", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6917", "mrqa_squad-validation-7029", "mrqa_squad-validation-7164", "mrqa_squad-validation-7175", "mrqa_squad-validation-7302", "mrqa_squad-validation-7362", "mrqa_squad-validation-7366", "mrqa_squad-validation-7435", "mrqa_squad-validation-744", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7709", "mrqa_squad-validation-7740", "mrqa_squad-validation-7766", "mrqa_squad-validation-7775", "mrqa_squad-validation-7818", "mrqa_squad-validation-7821", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8144", "mrqa_squad-validation-8163", "mrqa_squad-validation-828", "mrqa_squad-validation-8336", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8372", "mrqa_squad-validation-848", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8600", "mrqa_squad-validation-8687", "mrqa_squad-validation-8747", "mrqa_squad-validation-8759", "mrqa_squad-validation-8807", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-9050", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9348", "mrqa_squad-validation-9401", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9830", "mrqa_squad-validation-9831", "mrqa_squad-validation-9893", "mrqa_squad-validation-9930", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4825", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7542", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-871"], "OKR": 0.83203125, "KG": 0.4453125, "before_eval_results": {"predictions": ["one way streets", "since at least the mid-14th century", "Marlee Matlin", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight", "Classic", "88", "between 25-minute episodes", "leishmaniasis (Leishmania spp.)", "Romulus, My Father", "Theodore Roosevelt Mason", "The Talented Tenth", "1964", "Kinnairdy Castle", "James Franco", "(Thai: \u0e44\u0e17\u0e22\u0e41\u0e2d\u0e23\u0e4c\u0e40\u0e2d\u0e40\u0e4a\u0e35\u0e22 \u0e40\u05ad\u0e01\u0e0b\u0e4c )", "December 24, 1973", "Freiburg im Breisgau", "James Dean", "Rob Reiner", "Kansas", "Westfield Tea Tree Plaza", "26,000", "the Alemannic and the Bavarian-Austrian dialects of German", "1992", "Darci Kistler", "the EN World web site", "(25 March 1948 \u2212 27 December 2013)", "Pigman's Bar-B- Que", "Columbus, Ohio", "Northern Italy", "Miami Gardens, Florida", "Chaplain to the Forces", "Only Fools and Horses", "churros", "Franklin, Indiana", "Ub Iwerks", "mentalfloss.com", "Omega SA", "Flashbacks", "(1900, Copeville, Texas \u2013 1955)", "Kiernan Brennan Shipka", "1993", "Democratic Republic of the Congo", "Brett Ryan Eldredge", "The pronghorn", "Walcha", "French", "FBI", "BMW X6", "David Michael Bautista Jr.", "the Cherokee River", "15 mi", "Taylor Swift", "to enterprises and offices", "18 Divisional Round", "Batman & Robin", "Melbourne, Australia", "Felipe Massa", "The Washington Post", "Constantinople", "Viva Las Vegas", "bats", "Democritus", "Tennessee"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5314674908424908}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1459", "mrqa_squad-validation-7707", "mrqa_squad-validation-6653", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-5368", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4802", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3052", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3402", "mrqa_searchqa-validation-15983"], "SR": 0.4375, "CSR": 0.5833333333333333, "EFR": 1.0, "Overall": 0.7139322916666666}, {"timecode": 21, "before_eval_results": {"predictions": ["between 1000 and 1900", "Colonialism", "student-teacher relationships", "Approximately one million Protestants", "Corey Brown", "Harrods", "Coptic Cathedral", "Philadelphia", "Disco", "MMA", "the Kentucky River", "Caligula", "1995", "public", "December 21, 1956", "Fitzroya cupressoides", "L\u00e9a Seydoux", "Nanna Popham Britton", "Sada Carolyn Thompson", "Lawrence of Arabia", "Nelson Mandela", "Dissection", "Iranian-German", "The A41", "New South Wales", "Bronwyn Kathleen Bishop", "the Circle-Vision attraction The Timekeeper", "Jena Malone", "water", "Hong Kong First Division League", "Four Weddings and a Funeral", "from 1993 to 1996", "July 16, 1971", "influenced by the music genres of electronic rock, electropop and R&B", "Ted Nugent", "Reverend Timothy \"Tim\" Lovejoy", "Geet", "October 5, 1930", "Pamelyn Wanda Ferdin", "The Ninth Gate", "Comeng and Clyde Engineering", "neuro-orthopaedic", "Saint Motel", "About 200", "War Is the Answer", "Eleanor of Aquitaine", "Trilochanpala", "Rashida Jones", "The Division of Cook", "Peter Townsend", "Blue Origin", "Taylor Swift", "18 December 1975", "convergent plate boundary", "it failed to enforce its rule", "Kiri Te Kanawa", "Christchurch", "30-minute", "the area was sealed off", "Richard Thomas", "the wind band", "`` how now '' is a greeting, short for `` how say you now ''", "23 hours", "Nigel Lythgoe"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6417849511599512}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9904", "mrqa_squad-validation-1873", "mrqa_squad-validation-3060", "mrqa_squad-validation-765", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-4466", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4172", "mrqa_naturalquestions-validation-10448", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-5447", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-2014"], "SR": 0.546875, "CSR": 0.5816761363636364, "EFR": 1.0, "Overall": 0.7136008522727273}, {"timecode": 22, "before_eval_results": {"predictions": ["Zorro", "Zagreus", "January 1985", "Computational complexity theory", "to plan the physical proceedings, and to integrate those proceedings with the other parts", "post-World War I", "Esteban Ocon", "New Orleans Saints", "Clarence Nash", "The Dressmaker", "1,800", "from August 14, 1848", "Urijah Faber", "Mary-Kay Wilmers", "baeocystin", "the authorship of Titus Andronicus", "Vaisakhi List", "the Ars Nova Theater in New York City", "Monticello", "H. Sawin Millett Jr. (born October 8, 1937) is a Maine politician", "\"Menace II Society\"", "16 March 1987", "the Mercedes and stationwagon variants that sell under the luxury Holden Calais (VF) nameplate", "the life of USA Olympian and army officer Louis \"Louie\" Zamperini", "1968", "PlayStation 4", "Andes or Andean Mountains", "the Knight Company", "Parlophone Records", "Nicolas Winding Refn", "South America", "the Kingdom of Morocco", "Gerard Marenghi", "1958", "126,202", "31 July 1975", "orange", "Roseann O'Donnell", "There Is Only the Fight", "SAVE", "29,000", "the extraterrestrial hypothesis", "Larnelle Steward Harris", "4,613", "a morir so\u00f1ando or orange Creamsicle", "Q\u0307adar A\u1e8bmat-khant Ramzan", "Albert", "Tetrahydrogestrinone", "5,042", "Captain B.J. Hunnicutt", "Victoria, Duchess of Kent", "Halt and Catch Fire", "the Gilbert building", "from producers ( plants ) to primary consumers ( herbivores )", "Elena Anaya", "Rome", "The Archers", "the raven", "a particular health ailment or beauty concern", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented", "Daniel Radcliffe", "Milica Bogdanovna", "\"G\"", "a beetle"], "metric_results": {"EM": 0.53125, "QA-F1": 0.639288887064023}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.8, 0.0, 0.0, 0.0, 1.0, 0.6, 1.0, 0.13333333333333333, 1.0, 1.0, 0.125, 0.4615384615384615, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 0.28571428571428575, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6956", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-460", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3419", "mrqa_hotpotqa-validation-3463", "mrqa_naturalquestions-validation-5396", "mrqa_triviaqa-validation-1447", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-11933"], "SR": 0.53125, "CSR": 0.5794836956521738, "EFR": 1.0, "Overall": 0.7131623641304348}, {"timecode": 23, "before_eval_results": {"predictions": ["the growth of mass production", "Edmonton, Canada", "Ollie Treiz", "Mongol", "Richard Trevithick", "coughing and sneezing", "9", "the outdoors", "The Backstreet Boys", "November of that year", "Hawaii", "Tool", "1919", "May 5, 2015", "Arthur Miller", "Edmonton, Alberta", "Brent Robert Barry", "1989 until 1994", "The LA Galaxy", "Nicholas Kristof", "film and short novels", "Flushed Away", "first baseman and third baseman", "River Welland", "The Process", "200,167", "Bohemia and Archduke of Austria", "\"Jawbreaker\"", "Armin Meiwes", "1933", "Dirk Werner Nowitzki (]", "Carl Michael Edwards II", "In a Better World", "the 45th Infantry Division", "Iron Man 3", "Julie 2", "Conservatorio Verdi in Milan", "6'5\"", "a terrible date", "June 26, 1970", "Woking, England", "14 directly elected members", "About a Boy", "1982", "1978", "domestic cat", "Denmark", "1999", "The Ryukyuan people", "Peter Thiel", "Summerlin", "Cersei Lannister", "Kida", "the inner edge of the Nebula Arm", "Mahinda Rajapaksa", "iron", "naples", "\"Big Mac\" McCreedy", "British broadcaster Channel 4 has been criticized for creating a new television show which looks at how children as young as eight would cope without their parents for two weeks.", "\"Osama's alive,\"", "2.5 million", "1,000,000 milligrams", "naples", "naples"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6519345238095238}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.05555555555555555, 0.0, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6220", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-3634", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-4060", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-865", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-3074"], "SR": 0.578125, "CSR": 0.5794270833333333, "EFR": 1.0, "Overall": 0.7131510416666667}, {"timecode": 24, "before_eval_results": {"predictions": ["1965", "A job where there are many workers willing to work a large amount of time (high supply) competing for a job that few require (low demand)", "A tundra", "article 30", "The mermaid (syrenka)", "in Now Zad in Helmand province, Afghanistan.", "a fair and independent manner and ratify successful efforts.", "Andreas, 15, and \"the crew\": Isaac, 8, Hope, 7, Noah, 5, Phoebe Joy, 3, Lydia Beth, 2, Annie", "Kurt Cobain", "seven", "two years", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "piano", "The mother (Charlotte Gainsbourg)", "The Devil Went Down to Georgia.", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "curfew", "Missouri", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics", "five victims by helicopter", "30-minute", "San Diego", "Israel's vice prime minister Silvan Shalom", "cancer", "baseball bat", "U.S. Navy", "$50", "the coalition seeks to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "sportswear", "\"I saw guys who were 34, 35, 36 years old -- still young guys -- about to get out of the game, and I wondered what will they do now?\"", "eight", "the FAA received no reports from pilots in the air of any sightings", "1944", "a Bush-era Justice Department memo released by the Obama administration.", "NATO fighters", "Jaime Andrade", "Somali President Sheikh Sharif Sheikh Ahmed", "\"I wasn't sure whether I was going to return to 'E! News' this week or after the new year.", "that students often know ahead of time when and where violence will flare up on campus.", "the Orbiting Carbon Observatory", "German Chancellor Angela Merkel", "Donald Trump", "Ryder Russell", "Scarlett Keeling", "The U.S. Navy", "Joe Jackson", "Asashoryu", "the driver of that train, who was among the dead, was Jeanice McMillan, 42, of Springfield, Virginia", "Amar bin Laden", "Anil Kapoor", "$3 billion", "question people if there's reason to suspect they're in the United States illegally", "a young girl", "Elizabeth Dean Lail", "1926", "a squall", "Wyoming", "Kenny Everett", "November 23, 1996", "Richa Sharma", "The Frost Place Advanced Seminar", "a criminal empire", "Valentina Tereshkova", "Baccarat"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5342193044846077}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.16, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 0.0, 0.0, 0.5, 1.0, 1.0, 0.06060606060606061, 1.0, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2608695652173913, 1.0, 0.05714285714285714, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.8, 0.10256410256410256, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.10526315789473685, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7407", "mrqa_squad-validation-839", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3010", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6214", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-733", "mrqa_searchqa-validation-14187"], "SR": 0.453125, "CSR": 0.5743750000000001, "EFR": 0.9714285714285714, "Overall": 0.7064263392857143}, {"timecode": 25, "before_eval_results": {"predictions": ["phycobilisomes, and contain chlorophyll b", "WatchESPN", "independent prescribing authority", "10,000", "it infringed on democratic freedoms. The governments of the United States, Britain, Germany and France", "U.N. High Commissioner for Refugees", "a cabin in the town of Argonne", "Paul Ryan", "in the cellar with their mother,", "The Drug Enforcement Administration said that the DEA had contacted the drug maker \"about a specific lot number, and that lot number is not from the two we are recalling.", "his comments", "the U.S. antitrust officials under George W. Bush", "company Polo", "an accidental death in which no law was broken or criminal negligence involved.", "Nkepile M abuse", "a Royal Air Force helicopter", "Samuel Herr,", "McDonald's", "the administration's progress, while we also encourage him to 'evolve faster' on supporting full marriage equality.", "1981", "Jewish", "al Fayed's security team", "Teresa Hairston", "The pilot, whose name has not yet been released,", "18", "July", "\"G gossip Girl\"", "Jason Chaffetz", "debris", "The National Restaurant Association,", "the Form Design Center in Hedmanska Garden.", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "21", "the heart of an urban center like Los Angeles.", "they would not be making any further comments, citing the investigation.", "that a shorter-range test \"could go wrong,\" but for the most part the North Koreans have short and medium missile tests \"down pat,\"", "the 3rd District of Utah.", "Sen. Debbie Stabenow (D- Michigan)", "\"perezagruzka,\"", "one", "Lee Probert.", "1998.", "23", "American", "\"we take this issue seriously,\"", "Monday.", "Frank Ricci", "Old Trafford", "Guinea, Myanmar, Sudan and Venezuela.", "the Louvre", "homicide", "Les Bleus", "Madison", "Gibraltar", "need to repent in time", "John McCarthy", "\"One true Friend,\" by Etta-May Spenser.", "narwhals", "Atlas ICBM", "Lionel Eugene Hollins", "Duchess Eleanor of Aquitaine", "John Irving", "diabetes", "the South African gold mines"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5059405259772907}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [0.33333333333333337, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.5714285714285715, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4444444444444445, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-6354", "mrqa_squad-validation-8392", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-981", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-3726", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2673", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-5238", "mrqa_searchqa-validation-11369"], "SR": 0.421875, "CSR": 0.5685096153846154, "EFR": 1.0, "Overall": 0.7109675480769231}, {"timecode": 26, "before_eval_results": {"predictions": ["Eero Saarinen", "according to a multiple access scheme", "composite number", "Climate fluctuations during the last 34 million years", "Chicago Cubs", "Private Eye", "a brass instrument", "Norman Hartnell", "canoeist", "Millbank in London", "Poland", "Adam Ant", "marine", "Greek centre-right party", "Missouri", "six \"Brandenburg\" Concertos by Johann Sebastian Bach.", "mushrooms", "Turkey", "1960", "the other characters residing outside of it.", "the fictional London Borough of Walford in the East End of London", "(from Middle Latin baccalaureus)", "Laurence Olivier", "four rows", "Leonard Bernstein", "is", "11 and 30 is the same as the number of even numbers between 12 and 31.", "braille", "blood", "aircraft", "passion fruit", "a jumper", "The Lone Ranger", "caridean shrimp", "Yemen", "Welles", "King George IV.", "Barry Briggs", "Joseph Smith,", "Cornwall, Durham, East Riding of Yorkshire, West Midlands and West Yorkshire", "kievan Rus.", "of meat.", "Beaujolais Nouveau", "rowing", "$5 $20 $50 $100)", "(28 July 1925 - 14 April 1988)", "Henry VI", "'Lord Nelson' or a 'Nelson'", "ancient Testament", "Argentina", "bees", "Sinclair Lewis", "Phosphorus pentoxide", "( son of Bindusara )", "Danish", "the County of York", "Cheshire County, New Hampshire", "Atomic Kitten", "Phillip Myers,", "11th year", "Opryland", "(\"communicate with machines and electronics\"", "actress, singer, and Broadway star", "Alzheimer's disease"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5429214015151514}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.1818181818181818, 0.18181818181818182, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-1872", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-397", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-2496", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-2740", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-6073", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4118", "mrqa_naturalquestions-validation-946", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-347", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-1554"], "SR": 0.453125, "CSR": 0.5642361111111112, "EFR": 0.9714285714285714, "Overall": 0.7043985615079366}, {"timecode": 27, "before_eval_results": {"predictions": ["water-cooled", "The Genghis Khan Mausoleum", "the most cost efficient bidder", "February 1, 2016", "Kylie Jenner", "53", "Aman", "2005", "Elizabeth Dean Lail", "Anthony Caruso as Johnny Rivers", "Peter Finch", "Erastus Utoni", "Guant\u00e1namo Bay in Cuba", "19 state rooms", "north end", "23 %", "vivre", "Roanoke", "American", "Central Germany", "King Saud University", "Sherwood Forest", "on the southeastern coast of the Commonwealth of Virginia in the United States", "Orlando", "NHS studies", "Kyrie Irving", "spinal cord", "Paul Hogan", "fifty small, white, five - pointed stars arranged in nine offset horizontal rows", "1996", "stan McGregor", "in capillaries", "September 6, 2019", "two", "Shenzi", "after obtaining the consent of the United Kingdom", "3000 BC", "chalkidice and the Peloponnese", "1976", "on the ureters", "October 2008", "Broken Hill and Sydney", "Latin liberalia studia", "The Hunger Games : Mockingjay -- Part 1 ( 2014 )", "1773", "Escherichia coli", "15th century", "in a M\u00f6ssbauer spectrometer", "Carol Worthington", "when viewed from different points on Earth", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "2014", "english patient", "dorset", "pressure", "Frederick Martin", "Franz Ferdinand", "authoritarian tendencies", "(the Democratic VP candidate delivers a big speech next Wednesday)", "tripplehorn,", "\"Rent,\" \"Cabaret\" and \" Proof,\"", "Portugal", "grow old", "Holstein cow"], "metric_results": {"EM": 0.46875, "QA-F1": 0.575097866042628}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.25, 0.4, 0.5, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.06896551724137931, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.06896551724137931, 0.4347826086956522, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-1180", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-3827", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-3635", "mrqa_searchqa-validation-7864"], "SR": 0.46875, "CSR": 0.5608258928571428, "EFR": 0.8823529411764706, "Overall": 0.6859013918067227}, {"timecode": 28, "before_eval_results": {"predictions": ["the father of the house when in his home", "Welsh", "data link operations", "blue", "Mead", "Vietnam", "John Peel", "Moscow", "insect", "Estonia", "Siberia", "malaria", "on the grounds of a hospital that treated injured war veterans, featured 16 people \u2013 14 men and 2 women \u2013 competing in one sport, archery.", "Kent", "Arthur, Prince of Wales", "Israel", "butterflies", "Hindenburg", "Philippines", "radicalization", "the number thirteen", "bath", "Eric Coates", "long john", "to make a furrow or furrows in", "Brothers In Arms", "Mexico", "Aberystwyth", "Eric Morley", "northern areas", "Frank Spillane", "Erik Aunapuu", "Frank Sinatra", "Alberto Salazar", "the majority of newspaper subscribers in an older and more affluent group.", "Niger", "Lone Gunmen", "David Nixon", "piano", "Triton", "Addis Ababa", "pascal", "heart", "Nova Scotia", "\u201cThe lions of Islam have avenged our Prophet. They are lions. These are the first droplets.", "James Lillywhite", "Nigeria", "Dead Sea", "40", "the Rock", "2010", "penultima", "President Gerald Ford", "Spanish missionaries", "William Wyler", "a split album between Nardwuar the Human Serviette's two bands, The Evaporators, and Thee Goblins.", "1887", "Portal", "on various military check posts in Pakistan's border with Afghanistan", "a plaque", "Phillip A. Myers", "Priscilla Ann Presley", "Cold Mountain", "killers"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6058035714285714}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.6, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2318", "mrqa_squad-validation-4968", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-6573", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-3536", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-3924", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-388", "mrqa_newsqa-validation-2885", "mrqa_searchqa-validation-1846", "mrqa_searchqa-validation-2066"], "SR": 0.546875, "CSR": 0.5603448275862069, "EFR": 1.0, "Overall": 0.7093345905172413}, {"timecode": 29, "before_eval_results": {"predictions": ["Albert C. Outler", "Jakaya Kikwete", "Drogo", "new jersey", "Fitzwilliam", "Australia", "chipmunk", "Amnesty International", "the Scud", "Australia", "Labrador Retriever", "Rio Grande", "Carrie", "times", "Poland", "copenhagen", "Brooklyn", "Nero", "Proverbs", "cooley", "PJ Harvey", "Gryffindor", "arthur", "The French Connection", "Thundercats", "Stanley", "Rapa Nui", "copenhagen", "Azerbaijan", "piero della Francesca", "Oliver!", "Hinduism", "forefoot", "co-founder", "high cooking", "Herbert Henry Asquith", "puddings", "index fingers", "linn", "tlachtli", "short History of Tractors", "9", "function", "shakespears Sister", "purple rain", "Fenn Street School", "Fenella Fielding", "arthur", "copenhagen", "Jean-Paul Sartre", "a guide to the general climate of the regions, based on average annual precipitation, average monthly precipitation, and average monthly temperature", "couscous", "March 31, 2017", "1987", "countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "Dutch", "1981", "Humberside", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "soeoth, pictured here with his wife, says he was injected with drugs by ICE agents against his will.", "three full-length animated films.", "jefferson", "tap", "copenhagen"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5876235305334949}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.10526315789473684, 1.0, 1.0, 1.0, 0.8823529411764706, 1.0, 1.0, 0.33333333333333337, 0.08695652173913043, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7292", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-860", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-7733", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-575", "mrqa_naturalquestions-validation-6764", "mrqa_hotpotqa-validation-2997", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2618", "mrqa_searchqa-validation-10920", "mrqa_searchqa-validation-15291"], "SR": 0.515625, "CSR": 0.5588541666666667, "EFR": 1.0, "Overall": 0.7090364583333333}, {"timecode": 30, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1132", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-2642", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3117", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-394", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-478", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-5368", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-998", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2364", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-913", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5986", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-9310", "mrqa_squad-validation-10141", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-1131", "mrqa_squad-validation-1272", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-1551", "mrqa_squad-validation-1639", "mrqa_squad-validation-1641", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1732", "mrqa_squad-validation-1830", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2849", "mrqa_squad-validation-2881", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3205", "mrqa_squad-validation-3270", "mrqa_squad-validation-3385", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3841", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4166", "mrqa_squad-validation-423", "mrqa_squad-validation-4385", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4655", "mrqa_squad-validation-4673", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4806", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5198", "mrqa_squad-validation-5234", "mrqa_squad-validation-5265", "mrqa_squad-validation-5331", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5590", "mrqa_squad-validation-5640", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5986", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6614", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-668", "mrqa_squad-validation-6727", "mrqa_squad-validation-6894", "mrqa_squad-validation-6956", "mrqa_squad-validation-7029", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-7435", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7740", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8144", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-839", "mrqa_squad-validation-848", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8600", "mrqa_squad-validation-8646", "mrqa_squad-validation-8656", "mrqa_squad-validation-8687", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-915", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-939", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9813", "mrqa_squad-validation-9878", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1021", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2789", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3512", "mrqa_triviaqa-validation-3595", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4717", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4858", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-884"], "OKR": 0.84765625, "KG": 0.44921875, "before_eval_results": {"predictions": ["cellular respiration", "The Greens", "the most cost efficient bidder", "Prussian", "850 m", "armani, Esprit", "media executive and the current chair of Cox Enterprises", "capital of the Socialist Republic of Vietnam", "Stephen Mangan", "saloon-keeper", "James William McCutcheon", "Vixen", "Adult Swim", "Venice", "Ana", "1974", "tokamak", "March 17, 2015", "American", "1958", "China Airlines", "Wayne County", "political office of Keeper of the Great Seal of Scotland", "Republican", "Greek", "Honey Irani", "2000", "Beauty and the Beast", "137th", "Nobel Prize in Physics", "Future", "27 November 1956", "Francis Nethersole", "hiphop", "47,818", "Salisbury", "Lakshmibai", "Tony Aloupis", "sarod", "Anne Arundel County", "Austria Wien", "midnight sun", "Robert A. Iger", "Netherlands", "Dr. Alberto Taquini", "2 March 1972", "Terry the Tomboy", "Gracie Mansion", "parlophone", "R-8 Human Rhythm Composer", "\"Obergruppenf\u00fchrer\"", "World War I", "1983", "Linda Davis", "Sunni Muslim family", "mona Lisa", "eight", "highlands", "an open window", "staff sergeant", "\"It has never been the policy of this president or this administration to torture.\"", "trans fats", "Anna Mary Robertson", "alan eberman"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6321428571428571}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666667, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3524", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-1370", "mrqa_hotpotqa-validation-4594", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-3473", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-2529", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-276", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-4709", "mrqa_newsqa-validation-3819", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-15972"], "SR": 0.546875, "CSR": 0.5584677419354839, "EFR": 1.0, "Overall": 0.7198966733870968}, {"timecode": 31, "before_eval_results": {"predictions": ["the freedom (of the German health clinic) to provide services.", "phagocytes", "Finland", "Gulf of Aden", "Nathaniel Poe", "New York", "Amsterdam", "cellulose", "moles", "Ryan henderson", "Sicily", "Howard Keel", "Lilo & Stitch franchise", "Charlie henderson", "Sweet Home Alabama", "Alopecia or hair loss", "Percy Sledge", "A Kid for Two Farthings", "William Shakespeare Biography - Poem Hunter - Poetry", "Man V Food", "1780s", "80\u2019s", "George Fox", "Croatian", "Manchester City", "mike henderson", "heineken", "Isaac, Patriarch of the Bible", "South Africa", "Fidelio,", "ABBA", "Some Like It Hot", "silver", "Marcus Antonius", "frottage", "Enrico Caruso", "was Hitler right to invade Russia in 1941", "Hydrogen", "Nitric acid", "Tasmania", "Antonio\u2019s flesh", "Mille Miglia", "tiger", "rhododendron", "Uranus", "Utrecht", "mrs heberian Peninsula", "a wedge of hard cheese", "stenographer", "caliper", "Sacred Theology", "Adolf Hitler", "the player to the dealer's right", "March 27, 2017", "Black Mesa Research Facility", "4,613", "Swiss Super League", "Benj Pasek and Justin Paul", "Jenny Sanford,", "The environmental campaign video then has long been a powerful tool for environmental groups to spread their message and raise pubic attention.", "innovative, exciting skyscrapers", "chicago", "Shakespeare's Macbeth,", "mrs henderson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.546812996031746}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6, 0.0, 0.0, 1.0, 0.8571428571428571, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4462", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-3257", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-7201", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-740", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-2298", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7145", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-10606", "mrqa_hotpotqa-validation-1690", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-3", "mrqa_searchqa-validation-199", "mrqa_searchqa-validation-10166"], "SR": 0.453125, "CSR": 0.55517578125, "EFR": 0.9428571428571428, "Overall": 0.7078097098214287}, {"timecode": 32, "before_eval_results": {"predictions": ["The Sarah Jane Adventures", "Thomas Sowell", "lilo", "swimmers", "pangram", "October 31", "Wyoming", "Leicester", "rattlesnakes", "kangaroos", "Lisieux", "1929", "hypopituitarism", "February", "piano", "Gloucestershire", "Jupiter Mining Corporation", "hartman", "come Find Yourself", "Yulia Tymochenko", "france", "Adriatic Sea", "hitler", "fife", "Goran Ivanisevic", "Francis Drake", "nupedia", "Baku", "france", "frasier", "france pampas", "Madness", "Barings", "Anne Boleyn", "the body", "Ken Norton", "Yann Martel", "cabbage", "The Best of John Denver Live", "hawstan's church", "cleabrook", "inch", "france", "hitman", "france", "Edward III", "bill bryson", "American Tobacco Company", "Triumph and Disaster", "france", "Norman Mailer", "a \"major science finding from the agency's ongoing exploration of Mars.\"", "between 1923 and 1925", "Pakistan, India, and Bangladesh", "capillaries, alveoli, glomeruli, outer layer of skin", "sarod", "Laban Movement Analysis", "Sir Matthew Arundell of Wardour", "\"probably from NORAD,\" or the North American Aerospace Defense Command,", "on the bench", "Bright Automotive", "will & Grace", "prehensile", "france"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4692505411255411}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.7272727272727273, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_triviaqa-validation-1725", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-952", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2643", "mrqa_triviaqa-validation-1425", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-7073", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-1770", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-2133", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-3994", "mrqa_searchqa-validation-852"], "SR": 0.421875, "CSR": 0.5511363636363636, "EFR": 1.0, "Overall": 0.7184303977272728}, {"timecode": 33, "before_eval_results": {"predictions": ["the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea,", "1580s", "intelligence", "France", "Ted Heath", "wuthering Heights", "Portugal", "will never play for another Premier League club", "vice-admiral", "Sweeney Todd", "smallbill", "Benfica", "six", "1984", "11", "Buzz Aldrin", "MC Hammer", "Archie Shuttleworth", "hanks", "The IT Crowd", "The Cream of Manchester", "gnu", "Pokemon", "Cold Comfort Farm", "The Isar River", "Ruth Rendell", "wales", "The Call", "eddie", "brouilly", "john Constable", "sheep", "willow", "carmen Miranda", "nottingham", "jump jump", "1882", "Jessica Simpson", "heddlu Dyfed-Powys Police", "sashimi", "sea otter", "dot-coms", "Tunisia", "Notting Hill", "earache", "scar", "Vladimir Putin", "croquet", "low-cost", "The Shard", "the best value diamond for your money", "wigan Warriors", "Taittiriya Samhita", "minimum viable", "The United States Secretary of State", "Jos\u00e9 Bispo Clementino dos Santos", "1979", "Umberto II", "Spc. Megan Lynn Touma,", "suppress the memories", "40", "ID", "The Tonight Show", "copper"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5718005952380952}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [0.47619047619047616, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-7609", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-1809", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-6224", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-11614"], "SR": 0.515625, "CSR": 0.5500919117647058, "EFR": 0.967741935483871, "Overall": 0.7117698944497153}, {"timecode": 34, "before_eval_results": {"predictions": ["Central business districts", "1973", "Brittany", "father's day", "Kenya", "September", "Japanese", "baul sartre", "huggins", "Martin Luther King, Jr.", "petticoat", "indus", "Puerto Rico", "180 degrees", "Charles Taylor", "wurst", "Ireland", "The Savoy", "niki lauda", "Finland", "india", "Brazil", "Massachusetts", "boutros Ghali", "PETER FRAMPTON", "Uranus", "witch", "Aleister Crowley", "Greek", "Spain", "mumbai", "marie", "taj", "cutouts", "primrose", "eriksson", "johan evertsen", "Angus Deayton", "Dean Martin", "Emily Davison", "farbauti and Laufey", "pence", "Constantine", "ethel Skinner", "james stewart", "ghee", "cleopatra Selene", "rambling", "commitment", "trina gulliver", "s\u00e8vres", "Procol Harum", "Victory gardens", "artes liberales", "Mark 3 : 13 -- 19", "AT&T", "1998", "Dachshunds", "\"totaled,\"", "Iran", "\"It was quite surprising to learn of the request,\"", "nicholas", "\"reshit\"", "anemia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5403645833333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-805", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-3914", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-3129", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-7019", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-2249", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-4837", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-16252"], "SR": 0.515625, "CSR": 0.5491071428571428, "EFR": 1.0, "Overall": 0.7180245535714286}, {"timecode": 35, "before_eval_results": {"predictions": ["Seven Days to the River Rhine", "the Seerhein", "1,776 steps", "to collect menstrual flow", "New Jersey Devils of the National Hockey League ( NHL )", "the Near East", "on BBC One on Saturday evenings", "edward Blunt", "Hook", "Brian Steele", "nearby objects show a larger parallax than farther objects when observed from different positions", "four", "Leonard Bernstein", "The Portuguese", "on 9 February 2018", "1970s", "2001", "the 18th century", "her abusive husband", "one of The Canterbury Tales by Geoffrey Chaucer", "in case of passing a constitutional amendment bill, two - third of the total members present and voted in favour of the bill with more than 50 % of the totals members of a house, is required per Article 368", "a federal republic composed of 50 states, a federal district, five major self - governing territories, and various possessions", "July 14, 1969", "Frank Langella", "Tennessee Titan", "Spanish brought the European tradition to Mexico", "April 26, 2005", "Castleford is a town in the metropolitan borough of Wakefield, West Yorkshire, England", "Action Jackson", "New England Patriots", "the world's first collected descriptions of what builds nations'wealth", "Patrick Warburton", "when the cell is undergoing the metaphase of cell division", "`` One Son ''", "Mara Jade", "revenge", "20 year - old Kyla Coleman from Lacey, Washington", "Nathan Hale", "Rachel Kelly Tucker", "outermost layer", "during World War II", "Joe Pizzulo and Leeza Miller", "a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "David Tennant", "Kelly Reno", "Brooke Wexler", "with an initial worldwide gross of over $1.84 billion, Titanic was the first film to reach the billion - dollar mark", "arose separately in England and Wales", "Leonard Nimoy", "Benny Goodman", "2005", "Pangaea or Pangea", "Kent", "petula Clark", "arm", "Tufts University", "2002", "May 4, 2004", "stabilize Somalia and cooperate in security and military operations.", "Communist Party of Nepal (Unified Marxist-Leninist)", "Robert Barnett", "macGyver", "Daniel Boone", "chicago"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6218116906850459}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.1904761904761905, 1.0, 0.0, 0.15000000000000002, 0.2222222222222222, 1.0, 1.0, 0.5, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.3333333333333333, 0.0, 0.2222222222222222, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 0.21052631578947367, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8338", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-6190", "mrqa_triviaqa-validation-702", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2030"], "SR": 0.515625, "CSR": 0.5481770833333333, "EFR": 0.9354838709677419, "Overall": 0.704935315860215}, {"timecode": 36, "before_eval_results": {"predictions": ["$5,000,000", "The Handmaid's Tale", "Austrian", "Archbishop of Canterbury", "1912", "102,984", "Emmanuel Ofosu Yeboah", "Clarence Nash", "Bulgarian-Canadian", "Macomb County", "Dusty Dvoracek", "served as the 45th Vice President of the United States from 1993 to 2001", "1972", "Disney California Adventure", "Indiana", "Travis County, Texas", "The 1996 PGA Championship", "orange", "Regionalliga Nord", "six different constructors taking the first six positions", "Wragby", "life insurance", "Ukrainian", "Cape Cod", "actress and model", "BBC Focus", "George Clooney", "Joe Scarborough", "Tottenham Hotspur F.C.", "the attack on Pearl Harbor", "Alemannic", "Vienna", "Jesper Myrfors", "Paper", "Amway", "Ogallala Aquifer", "What's Up", "News Corp", "the amount charged by a bookmaker, or \"bookie\", for taking a bet from a gambler", "12", "Rockbridge County", "jurisdiction", "Italian", "Gatwick", "Carlos Santana", "two Nobel Peace Prizes", "Dutch", "a game setting", "2013 Cannes Film Festival", "Aiden English", "actor, singer and a DJ", "Bill Patriots", "Achal Kumar Jyoti", "a set of connected behaviors, rights, obligations, beliefs, and norms as conceptualized by people in a social situation", "jazz", "15\u00d715", "a placebo", "a federal judge in Mississippi", "when times get tough,", "\"It was perfect work, ready to go for the stimulus package,\"", "blow fly", "pinnipeds", "the United States", "Harry S. Truman"], "metric_results": {"EM": 0.5, "QA-F1": 0.6134758470695971}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5333333333333333, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.09523809523809525, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3145", "mrqa_naturalquestions-validation-3076", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-1124", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2449", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-13135"], "SR": 0.5, "CSR": 0.546875, "EFR": 0.96875, "Overall": 0.711328125}, {"timecode": 37, "before_eval_results": {"predictions": ["Robert Lane and Benjamin Vail", "pjaro carpintero", "20", "biting", "the College of William and Mary", "China", "bling-bling", "Biggie Smalls", "Fred MacMurray", "chinook", "Feodor Ivanovich", "\"rock stars\"", "Sonnets", "Caesar salad", "a sheath", "David Berkowitz", "Catch Me if You Can.", "Pueblo", "a 3500 lb. sculpture", "licorice stick", "Eugene O'Neill", "\"Sunshine Superman\"", "marie", "Dublin", "mathematical", "King George II", "Suzuki Grand Vitara", "Yogi Bear", "General Emile Lahoud", "a tree", "Christopher", "Stripes", "\"Little Red Riding Hood\"", "a blackbody", "Daryl Hall and John Oates", "Cherokee", "The cause of the French people", "Gettysburg National Military Park", "Morocco", "Jackie Kennedy", "\"Sensible and responsible women don't want to vote.\"", "Orange County", "Oscar Wilde", "Sanders", "Helen", "the Court", "a trap door", "Aaron Burr", "violins", "ethanol", "Adam Smith", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "Paul", "Athens became a breeding ground for disease and many citizens died including Pericles, his wife, and his sons Paralus and Xanthippus", "red", "Cole Porter", "a non-speaking character", "New York City", "Westminster system", "Phil Collins", "nuclear", "Egypt", "\"The Book\"", "from boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round"], "metric_results": {"EM": 0.359375, "QA-F1": 0.42714511183261183}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.4, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.08000000000000002, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_searchqa-validation-12628", "mrqa_searchqa-validation-11078", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12401", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-11435", "mrqa_searchqa-validation-2181", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-7563", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-2565", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-4949", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-8404", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-10312", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-14370", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-14827", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-2068", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-822", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-10156", "mrqa_triviaqa-validation-7414", "mrqa_hotpotqa-validation-4081", "mrqa_newsqa-validation-2534"], "SR": 0.359375, "CSR": 0.5419407894736843, "EFR": 1.0, "Overall": 0.7165912828947368}, {"timecode": 38, "before_eval_results": {"predictions": ["1954", "Zion", "cloves", "Mrs.mrs.", "Abraham Lincoln", "St Martin", "a physician or surgeon", "Greenpeace", "the coelacanth", "Ice Cream", "a canton", "a Palestinian city", "Caracas", "Memphis", "Truman Capote", "The Free Dictionary", "Faneuil Hall", "Babe", "shrimp", "Balsa", "Israel", "prostitutes", "an anteater", "New Orleans Saints", "Diana", "Tasmania", "the Taj Mahal", "Iron", "Louisa May Alcott", "the improvidence of society itself", "Spider-Man", "grease", "glucose", "The Apartment", "Sony", "Van Helsing", "Hugh Grant", "the Great Wall", "hand", "Hormel Foods", "Canada", "Clara Barton", "Kauai", "esophagus", "Joseph", "the National Baseball Hall of Fame", "Fred G. Sanford", "Colombia", "Thomas Paine", "Venezuela", "canticle", "Donna", "before November 1", "first stand - alone instant messenger", "Apollon", "Chicago", "Andrew Lloyd Webber", "1776", "Tim Howard", "Princess Jessica", "Hutus and Tutsis", "The father of Haleigh", "Monday and Tuesday", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5434806034482759}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4827586206896552]}}, "before_error_ids": ["mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-7187", "mrqa_searchqa-validation-8722", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-6833", "mrqa_searchqa-validation-1238", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-9939", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-3861", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-12021", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-11351", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-1716", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1534", "mrqa_searchqa-validation-14173", "mrqa_naturalquestions-validation-215", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-4264", "mrqa_hotpotqa-validation-5676", "mrqa_newsqa-validation-3774", "mrqa_naturalquestions-validation-4021"], "SR": 0.46875, "CSR": 0.5400641025641026, "EFR": 1.0, "Overall": 0.7162159455128205}, {"timecode": 39, "before_eval_results": {"predictions": ["movements of nature", "retirement", "Hill Street Blues", "Tuberculosis", "Ross Perot", "murdered", "Fuchsia", "fracture", "Lance Armstrong", "Kung Fu", "elbow", "Raw sienna", "Hindu", "the Village Voice", "Nacho Libre", "The Beatles", "Cygnus", "Laguna Diamante", "nougat", "a Scotch egg", "the Manhattan Project", "the Eiffel Tower", "Roger Federer", "a sculpere", "a yolk", "a Cheddar", "the Anglo-Iranian", "Florida", "The Virgin Spring", "(Ed) Dostoyevsky", "the Nome Nugget", "Queen Victoria", "Winnipeg", "Deviled Eggs", "Zorro", "Assume", "Jack Sprat", "offbeat", "Uranium", "(Ant) Antigone", "(7)", "Gannett", "( William) Shakespeare", "George Sand", "a glacier", "Dick Gephardt", "Better Grades", "Lord Louis Mountbatten", "a Master of Fine Arts", "the US", "Robert Peary", "down to the ground", "Hellenism", "James W. Marshall", "(A) Ares", "linseed", "The Sixth Sense", "the Soldier Bear", "Montreal, Quebec, Canada", "Broad Sands Bay", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "58 minutes", "\"What she's doing is putting a personal and human face on the issue", "$31,000"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5130208333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-16284", "mrqa_searchqa-validation-2577", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-12788", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-9039", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-4966", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-262", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-4826", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-6258", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-31", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-1152"], "SR": 0.46875, "CSR": 0.53828125, "EFR": 1.0, "Overall": 0.715859375}, {"timecode": 40, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2920", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3586", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-514", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1316", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-13508", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-2066", "mrqa_searchqa-validation-222", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4829", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9908", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10088", "mrqa_squad-validation-10388", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1248", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2608", "mrqa_squad-validation-2831", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3331", "mrqa_squad-validation-349", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4046", "mrqa_squad-validation-4155", "mrqa_squad-validation-4331", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4634", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5248", "mrqa_squad-validation-5307", "mrqa_squad-validation-5389", "mrqa_squad-validation-5469", "mrqa_squad-validation-5506", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5728", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6024", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6224", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6702", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7211", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-801", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-9140", "mrqa_squad-validation-915", "mrqa_squad-validation-9285", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9525", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-1425", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.791015625, "KG": 0.48203125, "before_eval_results": {"predictions": ["toward the center of the curving path", "a fruit machine", "Hinduism", "a turtle", "Donkey", "Jane Eyre", "Aiden", "Alles gut", "France", "Montmartre", "the Dying Swan", "Elvis Presley", "a protractor", "voter registration", "The Kite Runner", "white granite", "Islamabad", "horseshoe", "Stephen Crane", "trespassing", "Jack Dempsey", "beheading", "Val Kilmer", "Pakistan", "Milwaukee", "a kiwi", "Pop-Tarts", "sugar", "Enrico Fermi", "Tiger Woods", "the Madding Crowd", "local broadcasters", "Grace Kelly", "a monkey-tail", "Bilbo", "Oliver Wendell Holmes", "the Constitution", "Proverbs", "a photon", "Maria Montessori", "orchid", "orchid", "the Sun", "Michelangelo", "Spain", "ale", "Superman", "a verb", "Brazil", "Puget Sound", "taxonomy", "Yahya Khan", "Prince Bao", "1038", "Bubba", "Easter Parade", "Thom Yorke", "beer", "Keeper of the Privy Seal of Scotland", "Martin O'Malley", "1983", "The cause of the child's death will be listed as homicide by undetermined means", "five", "Edgehill"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7420386904761904}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-7659", "mrqa_searchqa-validation-12159", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-7222", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-7370", "mrqa_naturalquestions-validation-3485", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-2627"], "SR": 0.6875, "CSR": 0.5419207317073171, "EFR": 1.0, "Overall": 0.7083060213414634}, {"timecode": 41, "before_eval_results": {"predictions": ["metals", "2004", "to capitalize on her publicity", "Karen Gillan", "Moira Kelly", "Nick Kroll", "Albert Einstein", "Miami Heat", "Poems : Series 1", "long - distance two - way communications", "the east African coast across the Indian Ocean", "200 to 500 mg up to 7 mL", "James Madison", "Tom Brady", "asphyxia", "1947", "Thomas Edison", "Unwinding of DNA at the origin", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "on Flag Day in 1954", "Erica Rivera", "Afghanistan", "Gettysburg", "the Geography of Oklahoma", "Eydie Gorm\u00e9", "an epithelial surface by way of a duct", "an oxidant, usually atmospheric oxygen", "1 mile ( 1.6 km )", "In 1871 A.D. Pt. Buddhiballav Pant opened a debating club", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "1968", "solids", "Presley Smith", "a combination of genetics and the male hormone dihydrotestosterone", "Norman's half - brother", "A wide range", "Tami Lynn", "Christianity", "compasses", "1931", "the fifth studio album by English rock band the Beatles", "July 31, 2010", "24 episodes", "August 8, 1945", "A-Lot of Mess", "16 December 1908", "Comprehensive Crime Control Act of 1984", "the meridian", "Buffalo Springfield", "the form is properly a rotationally symmetric saltire", "in technical journals", "the first Plantagenets", "asphalt", "Mar del Sur", "Julie Taymor", "an organ", "Tony Aloupis", "the American Civil Liberties Union", "Nigeria", "Enchautegui's death", "Pancho Gonzales", "William Henry Harrison", "orchids", "Britain"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6355040098790099}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.24000000000000002, 0.0, 0.3076923076923077, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.23076923076923078, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-9237", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-7595", "mrqa_searchqa-validation-2827"], "SR": 0.53125, "CSR": 0.5416666666666667, "EFR": 0.9333333333333333, "Overall": 0.694921875}, {"timecode": 42, "before_eval_results": {"predictions": ["high voltage", "water buffalo", "Texas", "Song of Solomon", "Israel", "Denmark", "Battlestar Galactica", "Buffy Sainte-Marie", "sheep", "Mary Tudor", "\"Lucia di Lammermoor.\"", "blackbird", "Patty Duke", "Gene Hackman", "Judas Iscariot", "3,000th", "grow old along with me", "savanna", "Luciano", "freight ton", "Kellogg\\'s", "Fall Guy", "cape horn", "Elizabeth Barrett Browning", "Judy Garland", "crocodiles", "greece", "Bosnia and Herzegovina", "Loma Linda University", "Morris West", "outta nowhere", "El burlador de Sevilla", "California", "Empire", "League of Nations", "Sally Ride", "bullet", "Queen Elizabeth I", "75%", "patchouli", "the Civil War", "gravitational force", "Greek Meatballs", "Holden Caulfield", "Caucasus mountains", "Firebird", "iambic pentameter", "Lecompton", "Midnight Cowboy", "Rosetta Stone", "Louisiana", "Malayalam", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "destroyed the only boats on the island", "Arabian Gulf", "Ross Kemp", "Marine One", "Juan Manuel Mata Garc\u00eda", "Vietnam War", "Taylor Swift", "Bialek", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "David Beckham", "Alexandros Grigoropoulos, whose death"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5967490842490842}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-15998", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-8429", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-6562", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-11902", "mrqa_searchqa-validation-1970", "mrqa_searchqa-validation-9663", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-448", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-1406", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-1411", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-115"], "SR": 0.484375, "CSR": 0.5403343023255813, "EFR": 0.9696969696969697, "Overall": 0.7019281294045101}, {"timecode": 43, "before_eval_results": {"predictions": ["Southeastern U.S.", "gin", "Leicester", "the mile run", "the Jets", "scurvy", "japan", "azor", "falconry", "Niger", "Norman Brookes", "Billy Crystal", "Jaipur", "Goran Ivanisevic", "14", "Geoffrey Cox", "Spain", "Henry Hudson", "bridge", "a raven", "Much Ado About Nothing", "depth depth", "Louis XV", "Elizabeth Mainwaring", "Australia", "Robert A. Heinlein", "Old Ironsides", "Aug. 24", "fertilization", "a crafty artisan", "Massachusetts", "Sherlock Holmes", "Delaware", "Olivia Smith", "Costa Concordia", "spiral", "orange juice", "the Sun", "mrs henderson presents", "graphite", "Bash Street", "Mercury", "Ireland", "Gandalf", "Moses Sithole", "Michael Curtiz", "Minder", "a star", "a turkey", "Eva depth of her own talent", "Tigger", "Saint Alphonsa", "the Witch", "March 15, 1945", "Portsea", "1861", "McLaren", "\" Maria\"", "Diego Milito", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Zyrtec", "a kid", "diameter", "Leo Frank"], "metric_results": {"EM": 0.546875, "QA-F1": 0.59765625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-294", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7617", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-5446", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-2751", "mrqa_searchqa-validation-14791"], "SR": 0.546875, "CSR": 0.5404829545454546, "EFR": 1.0, "Overall": 0.7080184659090909}, {"timecode": 44, "before_eval_results": {"predictions": ["the inclusion of Lake Constance and the Alpine Rhine is more difficult to measure objectively; it was cited as 1,232 kilometres (766 miles) by the Dutch Rijkswaterstaat in 2010", "Lori McKenna", "Philadelphia's rapid growth into America's most important city", "Xanthippus", "when the forward reaction proceeds at the same rate as the reverse reaction", "Lucknow", "Kristy Swanson", "Hendersonville, North Carolina", "hydrogen ions", "Abraham", "Kelly Osbourne", "1773", "the final episode of the series", "to refer to a god of the Ammonites, as well as Tyrian Melqart", "the Han", "just after the Super Bowl", "1980s", "31 October 1972", "The Italian Agostino Bassi", "graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Germany", "Real Madrid", "LED illuminated", "BC Jean", "about 24 hours", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "2015", "UMBC", "Tommy Shaw", "2018", "Thomas Alva Edison", "B.R. Ambedkar", "1928 national games", "Niles", "Judy Collins", "Oklahoma", "Grand Inquisition", "T'Pau", "San Francisco Bay", "Johannes Gutenberg", "Terry Reid", "masons'marks", "Johannes Gutenberg", "Domhnall Gleeson", "Russia", "the Pandavas", "2020 National Football League ( NFL ) season", "1994 season", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "the church at Philippi", "Chuck Noland", "The Green Mile", "cal Ripken, Jr.", "1905", "Workers' Party", "Revolver", "140 million", "contraband", "the International Space Station", "750", "lew springsteen", "nantucket", "Microsoft", "love Never Dies"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6792410714285714}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 1.0, 0.19999999999999998, 0.5714285714285715, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.5, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9113", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4592", "mrqa_triviaqa-validation-7676", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-1345", "mrqa_searchqa-validation-7952", "mrqa_searchqa-validation-3760"], "SR": 0.5625, "CSR": 0.5409722222222222, "EFR": 0.8928571428571429, "Overall": 0.6866877480158731}, {"timecode": 45, "before_eval_results": {"predictions": ["generally westward", "the United States", "Felipe Massa", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Giovani dos Santos is set to take up the vacant slot alongside Cameroon international Samuel Eto'o and Ivory Coast midfielder Yaya Toure in the non-EU berths permitted under Spanish Football Federation (RFEF) rules", "the Dalai Lama", "\"handful\" of domestic disturbance calls to police since 2000", "President Bush", "Brad Blauser, center, created the program.", "Sunday", "Rush Limbaugh", "at a depth of about 1,300 meters in the Mediterranean Sea", "Jayson Williams", "raping and murdering a woman in Missouri", "Ryder Russell", "in August 11, 12 and 13", "al Qaeda", "the club's board", "insect stings", "it is provocative action", "peter Docter", "Fullerton, California", "Chinese and international laws", "$2.6 million", "Rev. Alberto Cutie", "ceo Herbert Hainer", "Chris Robinson", "269,000", "outer suburbs", "Dharamsala, India", "the state's attorney", "delivered three machine guns and two silencers to the hip-hop star", "\"the need for reconciliation in a country that endured a brutal civil war lasting nearly three decades.\"", "two", "Spanish", "it was a wrong thing to say,", "stolen by the armed robbers.", "Oxbow, Minnesota", "motor scooter", "Joan Rivers", "6,000", "gossip Girl", "Public Citizen", "Diversity", "650", "in the Yemeni port city of Aden", "South Africa", "Daniel Radcliffe", "President Sheikh Sharif Sheikh Ahmed", "cities throughout Canada", "Florida's Everglades", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "Tyrion", "At Apple's Worldwide Developers Conference ( WWDC ) on June 8, 2009", "'Q'", "wolf", "polio", "mathematics", "mountain-climbing", "Mel Blanc", "\"hoover\"", "Colorado", "the CIA", "Billy Bob Thornton"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5026492400573283}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.4, 1.0, 0.5454545454545454, 0.0588235294117647, 1.0, 0.09523809523809525, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.05555555555555555, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.19999999999999998, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9261", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-1120", "mrqa_triviaqa-validation-3086", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-5862", "mrqa_searchqa-validation-6261"], "SR": 0.421875, "CSR": 0.5383831521739131, "EFR": 1.0, "Overall": 0.7075985054347826}, {"timecode": 46, "before_eval_results": {"predictions": ["every four years (quadrennium)", "part - Samoyed terrier", "to oversee the local church", "enterocytes of the duodenal lining", "warmth", "A patent is a set of exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time in exchange for detailed public disclosure of an invention", "prophase I of meiosis", "federal government", "alveolar process", "Katharine Hepburn -- Ethel Thayer", "Thorleif Haug", "multiple origins of replication on each linear chromosome that initiate at different times ( replication timing ), with up to 100,000 present in a single human cell", "England, Northern Ireland, Scotland and Wales", "Gladys Knight & the Pips", "transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "January 2004", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "276", "French CYCLADES project directed by Louis Pouzin", "when each of the variables is a perfect monotone function of the other", "Kansas City Chiefs", "Middlesex County, Province of Massachusetts Bay", "Western cultures", "Husrev Pasha", "2001", "Squamish, British Columbia, Canada", "Mankombu Sambasivan Swaminathan", "either annuity", "1963", "prophets and beloved religious leaders", "Best Picture, Best Director for Fincher, Best Actor for Pitt and Best Supporting Actress for Taraji P. Henson", "interstate communications by radio, television, wire, satellite, and cable", "Patris et Filii et Spiritus Sancti", "William Chatterton Dix", "1987", "Stefanie Scott", "Karen Gillan", "in the fovea centralis", "the seven churches", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Ferraro", "seven", "Uralic", "when an noticing that the person in the photograph is attractive, well groomed, and properly attired", "scrolls", "British and French Canadian fur traders", "Lori McKenna", "2017", "October 27, 1904", "the mid - to late 1920s", "Firoz Shah Tughlaq", "Runic", "backgammon", "Malcolm Bradbury", "Atlanta Braves,", "16th season for the Minnesota Timberwolves in the National Basketball Association.", "Holberg's comedy \"Den V\u00e6gelsindede\"", "Pakistan's", "Malcolm X", "February 12", "Charles Dickens", "the Capulets & the Montagues", "plutonium", "24"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6243321983595289}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.16666666666666666, 0.7272727272727273, 0.6666666666666666, 0.4615384615384615, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.15789473684210525, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4799999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.7499999999999999, 0.09999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473684, 1.0, 1.0, 0.6666666666666666, 0.5531914893617021, 0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-6517", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-1692", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-4464"], "SR": 0.46875, "CSR": 0.5369015957446808, "EFR": 0.9117647058823529, "Overall": 0.6896551353254068}, {"timecode": 47, "before_eval_results": {"predictions": ["Max Martin", "1995 Mitsubishi Eclipse", "21 June 2007", "The Queen of Hearts", "the government - owned corporation of Puerto Rico responsible for electricity generation, power distribution, and power transmission on the island", "Austria - Hungary", "Josie ( Gabrielle Elyse )", "libretto", "Theodore Roosevelt", "The Order of the Phoenix", "1800", "in the dress shop", "201", "Millerlite", "Instagram's own account", "Experimental neuropsychology", "2015", "Selena Gomez", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W", "Dido", "during initial entry training", "Eddie Murphy", "1936", "Bonnie Aarons", "1960", "during the summer of 1979", "Part XI of the Indian constitution", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "Rising Sun Blues", "Chlorofluorocarbons", "the 1980s", "MacFarlane", "18 by Frankie Laine's `` I Believe '' in 1953", "1898", "a business applications to be developed with Flash", "Matt Jones", "a tradition in brass band parades in New Orleans, Louisiana", "The Confederate States Army ( C.S.A. )", "three", "MFSK", "Zachary John Quinto", "Sanchez Navarro", "Have I Told You Lately", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "John Travolta", "Scheria", "1 October 2006", "2017 Georgia Bulldogs", "the cast of do n 't tell mom the babysitter's dead", "Michael Clarke Duncan", "the Origination Clause of the United States Constitution", "Gestapo", "Belgium", "heartburn", "Maryland", "Vanessa Hudgens", "two", "Johannesburg", "a sort of robot living inside.", "hardship for terminally ill patients and their caregivers", "The Odyssey", "Jeff Goldblum", "Prego", "Marillion"], "metric_results": {"EM": 0.46875, "QA-F1": 0.595359788505691}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.2727272727272727, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.8, 0.25, 0.0, 0.5714285714285715, 0.6666666666666666, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6451612903225806, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.5, 0.7000000000000001, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-10381", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-89", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-2606", "mrqa_newsqa-validation-886"], "SR": 0.46875, "CSR": 0.5354817708333333, "EFR": 0.9117647058823529, "Overall": 0.6893711703431372}, {"timecode": 48, "before_eval_results": {"predictions": ["Daylight Saving Time", "Cygnus", "Constantine", "Lautrec", "Nigeria", "Hawaii", "Jeremiah", "a \"park\"", "Don Knotts", "cinnamon Life", "kbenhavn", "Porgy and Bess", "a largest city in South Korea", "Mars", "Cheers", "Robert Frost", "geena Davis", "dungeon", "the transtormation", "Laila Ali", "Fiji Islands", "The Eleventh Night", "border region", "Fat man, you shoot a great game of pool", "to be close to you.", "Led Zeppelin", "the Book of Kells", "grandmother", "nuclear chain reaction", "a Heisman", "Gulf of Tonkin", "Stephen Vincent Bent", "coelacanth", "Prague", "the Federal Reserve", "gasoline, coal, or other fossil fuels", "Afghanistan", "Cheetah", "Ambrose Bierce", "the American Lung Association", "croquet", "Aphrodite", "Your wife comes back to you, your dog returns to life and you get out of prison.", "Chico Rodriguez,", "Budapest", "John Mahoney", "pythons", "Nit-A-Nee", "le Cercle member Robert Moss", "Beverly Cleary", "Afghanistan", "2009", "a recognized group of people who jointly oversee the activities of an organization", "since 3, 1, and 4 are the first three significant digits of \u03c0", "eight", "the YMCA", "Liddell", "the theory of direct scattering and inverse scattering", "43rd", "South America", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "$150 billion", "fill a million sandbags and place 700,000 around our city.", "lewad"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6337797619047618}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-4404", "mrqa_searchqa-validation-4458", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-5114", "mrqa_searchqa-validation-7598", "mrqa_searchqa-validation-8359", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-12165", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-3066", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-58", "mrqa_hotpotqa-validation-1316", "mrqa_newsqa-validation-1791", "mrqa_triviaqa-validation-336"], "SR": 0.53125, "CSR": 0.5353954081632653, "EFR": 1.0, "Overall": 0.707000956632653}, {"timecode": 49, "before_eval_results": {"predictions": ["William Wyler", "Redford's adopted home state of Utah", "pelvic floor", "Kanawha Rivers", "amino acids glycine and arginine", "an alien mechanoid", "1937", "1920s", "the united monarchy of Israel and Judah", "2005", "New York City", "Beorn", "Jonathan Cheban", "1992", "Montreal", "18", "Kristy Swanson", "687 ( Earth ) days", "Camping World Stadium in Orlando", "the Bactrian", "sorrow regarding the environment", "New York Yankees", "Woodrow Wilson", "Phosphorus pentoxide", "Brooklyn, New York", "Republican Secretary of Commerce Herbert Hoover", "the Mishnah", "deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "Tom Brady", "Around 1200", "Richard Dashut", "The Crossing", "to stay, abide", "Taiwan", "origins of replication, in the genome", "people raced modified cars on dry lake beds northeast of Los Angeles under the rules of the Southern California Timing Association ( SCTA ), among other groups", "from 13 to 22 June 2012", "Jackie Robinson", "Coton in the Elms", "the American Civil War", "Pradyumna", "1979", "James Madison", "UVA", "Roger Federer", "17 - year - old", "Rob Davis", "4,840", "pumped through the semilunar pulmonary valve into the left and right main pulmonary arteries", "electron donors", "A rear - view mirror", "Bushisms", "the Constitution", "Lucas McCain", "Martin Scorsese", "2014", "2014", "deviant young men does not provide solutions that prevent gang rape from happening.", "the Listeria monocytogenes bacteria", "United States, NATO member states, Russia and India", "Sagamore Hill", "thyroid", "a parrot", "UVB rays"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6303214820547669}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6976744186046512, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.16, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.3, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9328", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-7770", "mrqa_hotpotqa-validation-558", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2074", "mrqa_searchqa-validation-14922", "mrqa_searchqa-validation-10011", "mrqa_triviaqa-validation-7608"], "SR": 0.546875, "CSR": 0.535625, "EFR": 1.0, "Overall": 0.707046875}, {"timecode": 50, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-561", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9724", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3331", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.826171875, "KG": 0.48984375, "before_eval_results": {"predictions": ["a cappella", "caterpillar", "Zeus", "7", "Lady Gaga", "(John) Entwistle", "kray Brothers", "fearful man, all in coarse gray with a great iron on his leg...who limped and shivered, and glared and growled, and whose teeth chattered in his head as he seized [Pip]by the chin", "morocco", "December 18, 1958", "Cumberland sausage", "Gloucestershire", "Prince Albert", "(RoRo)", "Sherrie Hewson", "Sharjah", "Madagascar", "persian gulf", "Miss Havisham", "oxygen", "Macbeth", "Gentlemen Prefer Blondes", "Khrushchev,", "Dutch", "the hose", "Hudson River", "John Key", "Subway", "geodetics", "Dylan Thomas", "Fred Astaire", "the Tower of London", "Bridge", "a quarter", "cirrus", "Manchester", "Passchendaele", "cheese", "Klaus dolls", "Argentina", "Colin Montgomerie", "James Valentine", "Count Basie Orchestra", "the Behemoth", "Matthew Boulton", "Top Gun", "elbow", "General Paulus", "isohyet", "naples", "Danish", "July 14, 1969", "1923", "Massillon, Ohio", "1937", "LA Galaxy", "2017", "The BBC,", "Frank Ricci", "\"Three Little Beers,\"", "underwear", "Susan B. Anthony", "ivory", "in the north and west of the country,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5686099439775909}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3529411764705882]}}, "before_error_ids": ["mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6534", "mrqa_triviaqa-validation-6964", "mrqa_triviaqa-validation-2757", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-4700", "mrqa_triviaqa-validation-7251", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-5001", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-1896", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-3522", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2793", "mrqa_searchqa-validation-1070", "mrqa_newsqa-validation-2191"], "SR": 0.515625, "CSR": 0.5352328431372548, "EFR": 0.9354838709677419, "Overall": 0.7018777178209994}, {"timecode": 51, "before_eval_results": {"predictions": ["Samson", "smith", "passenger pigeon", "catalytic converter", "Sir Edwin Landseer", "smith", "albert erie", "smith", "scales", "Judy Garland", "dave walliams", "tepuis", "into one of the Vikings nine realms.", "Andy Murray", "horse", "hypopituitarism", "Delaware", "bees", "Treaty of Amiens", "water", "six month", "shirley", "france", "mantle", "fabrics", "algae", "shine", "Algeria", "Churchill Downs", "netherlands", "Leonard Bernstein", "Vladimir Putin", "koi-descent", "japan", "20", "Ken Platt", "smith", "adios", "denmark", "reginald smith", "muppet Christmas Carol", "wiziwig", "jump", "apples", "old chair", "smith, Jr.", "10", "Vancouver Island", "dublin", "Babylonian Empire", "Continental Marines", "Joe Spano", "Gary Grimes", "liver and kidneys", "CBS News", "Herman's Hermits", "United States House of Representatives", "\"Den of Spies\"", "clothes", "2-0 up", "Brunei", "Pringles", "a peanut butter cup", "1983"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5421875}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.8, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-1635", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7762", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-4091"], "SR": 0.46875, "CSR": 0.5339543269230769, "EFR": 1.0, "Overall": 0.7145252403846153}, {"timecode": 52, "before_eval_results": {"predictions": ["horse", "australia", "sweden", "vodka", "Tony Manero", "Snarked", "Prussian 2nd Army", "silversmith", "Joshua Tree National Park", "carpathia", "Superman", "letchworth", "velvet", "1", "australia", "Crackerjack", "white Ferns", "Utah", "carburetors", "as You Like It", "Labrador Retriever", "what", "Delaware", "thieves", "Clara wieck", "squeeze", "Apocalypse Now", "a crew of ten trying to hunt the Snark, an animal which may turn out to be a highly dangerous Boojum", "st moritz", "david hockney", "Scafell Pike", "Edgar Allan Poe", "stanley", "Tony Blackburn", "Donna Summer", "united states", "kiki", "wolf", "The Titanic", "kennifer purdy", "trumpet player", "Andrew Lloyd Webber", "Malawi", "australia", "eric barker", "mrs baryshnikov", "bristol", "Ruth Rendell", "smiths", "ontario", "Cleveland", "Miami Heat", "Joanna Page", "20 July 2015", "1902", "FCI Danbury", "25 June 1971", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "sweden", "a \"happy ending\" to the case.", "suffrage", "jennifer lauer", "james feldman", "Colonel"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6319444444444444}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.1111111111111111, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2767", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-1377", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3898", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-1934", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-11787"], "SR": 0.578125, "CSR": 0.5347877358490566, "EFR": 0.9629629629629629, "Overall": 0.707284514762404}, {"timecode": 53, "before_eval_results": {"predictions": ["albinism", "Jack Ruby", "Google", "Hugh Dowding", "squash", "Tennessee Williams", "Jim Smith,", "injecting a 7 percent solution intravenously three times a day", "David Bowie", "hemp", "canoeist", "Louren\u00e7o Marques", "Christian Louboutin", "Ironside", "the need to toss logs across narrow chasms to cross them", "James Dean", "mars", "every two weeks", "thomas hemingiewicz", "chicken Marengo", "George Orwell", "noel ederick heberian", "college", "Derbyshire", "Cubism", "polynesian", "france", "healdson", "Charlie Brown", "Ruth Rendell", "hemingley", "christopher addams", "hot Chocolate", "Botswana", "1921", "france", "cl Clash", "praseodymium", "Bruce Alexander", "slap", "spinal cord", "azor", "Arthur Hailey", "hedgehog", "Madonna", "little arrows", "a nerve cell cluster", "noel Ephron", "orrest Head", "27", "orchids", "104 colonists and Discovery", "aiding the war effort", "anterograde amnesia and dissociation", "Cesar Millan", "2004 Paris Motor Show", "jiu-Jitsu", "cell phones", "anil kapoor", "drug cartels", "Guinea-Bissau", "steel", "wager", "2012"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6145833333333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-3136", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-31", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1350", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-3119", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-294", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4525", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4442", "mrqa_hotpotqa-validation-3655", "mrqa_searchqa-validation-14449"], "SR": 0.59375, "CSR": 0.5358796296296297, "EFR": 1.0, "Overall": 0.7149103009259259}, {"timecode": 54, "before_eval_results": {"predictions": ["Big Mamie", "dancer", "Canadian", "Sunyani", "six", "basketball", "Adelaide", "Alfred Preis", "the Forest of Bowland", "Pakistan Aeronautical Complex (PAC)", "Daniel Espinosa", "Switzerland", "The Keeping Hours", "Jeffrey Adam \"Duff\" Goldman", "Ang Lee", "Mineola", "67,038", "Kentucky Wildcats", "eileen Atkins", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Harry Potter series", "the Haitian Revolution", "alcoholic drinks", "Frederick Dewey Smith", "John Meston", "York County", "Cleveland Cavaliers", "scotland", "Hillsborough County", "Cartoon Network Too", "San Diego County Fair", "the fictional city of Quahog, Rhode Island", "French", "1970", "George Raft", "Chiba, Japan", "1944", "the British Army", "311", "Jennifer Taylor", "Cartoon Network", "April 19, 1994", "British", "6,241", "October 17, 2017", "Les Clark", "Marilyn Martin", "Hawaii", "Gregg Popovich", "1979", "Vincent Landay", "Lauren Tom", "~ 0.072 mm", "Spektor", "spain", "Montgomery, Alabama", "vanilla", "a Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "4", "The Louvre", "einstein", "hemming", "will", "The Capitoline Wolf"], "metric_results": {"EM": 0.5, "QA-F1": 0.5854166666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-796", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-3161", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-1310", "mrqa_naturalquestions-validation-8962", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5383", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2609", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-1776"], "SR": 0.5, "CSR": 0.5352272727272727, "EFR": 1.0, "Overall": 0.7147798295454545}, {"timecode": 55, "before_eval_results": {"predictions": ["The 2016 United States Senate election in Nevada", "1952", "private Roman Catholic university", "voicing Liquid Snake", "400 MW", "Kerry Marie Butler", "Esp\u00edrito Santo Financial Group", "September 5, 2017", "Tuesday, January 24, 2012, at 8 p.m. ET/PT.", "leopard", "Salman Rushdie", "Tom Rob", "Big Bad Wolf", "Rockland County", "Personal History", "Australian-American", "Holston River", "1943", "1996", "Tamil Nadu", "Tallahassee City Commission", "a type of stochastic recurrent neural network (and Markov Random Field)", "three", "Hechingen in Swabia", "Floyd Mutrux and Colin Escott", "A Little Princess", "Marvel's Agent Carter", "2008", "from July 2, 1967 to August 21, 1995", "Kennedy Road", "Mark Neveldine and Brian Taylor", "the fictional city of Quahog, Rhode Island", "Spiro Agnew", "The String Cheese Incident", "three", "Noel", "Thorgan Ganael Francis Hazard", "1692", "9", "the twelfth title in the mainline \"Final Fantasy\" series", "Saudi Arabian", "26,000", "Northampton Town", "Indian state of Gujarat", "attorney, politician, and the principle founder of the Miami Dolphins", "\"punk rock\"", "\"Little Britain\" star David Walliams", "The 1990\u201391 UNLV Runnin' Rebels", "Bill Clinton", "John D Rockefeller's Standard Oil Company", "Barack Obama's", "pigs", "dystopian science fiction action thriller", "The flag of Vietnam", "pulsar", "The Dogger Bank", "the first eight seasons", "\"totaled,\"", "Steven Gerrard", "ketamine", "the Caspian tern", "The Southern Hemisphere", "Diebold", "Andr\u00e9s Iniesta"], "metric_results": {"EM": 0.546875, "QA-F1": 0.644534632034632}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true], "QA-F1": [0.8333333333333333, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-855", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3340", "mrqa_hotpotqa-validation-5280", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3512", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-2859", "mrqa_hotpotqa-validation-5228", "mrqa_naturalquestions-validation-5394", "mrqa_triviaqa-validation-3487", "mrqa_newsqa-validation-456", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-6593"], "SR": 0.546875, "CSR": 0.5354352678571428, "EFR": 1.0, "Overall": 0.7148214285714285}, {"timecode": 56, "before_eval_results": {"predictions": ["35,000", "Vishal Bhardwaj", "Macau", "Russian film industry", "no. 3", "The Government of Ireland", "Washington Street", "Eielson Air Force Base", "Galo (], \"Rooster\"", "Tom Shadyac", "Michael Phelps", "Richard Strauss", "Marie Heughan", "Iynx", "David Robert Starkey", "Edward III", "Anne Perry", "September 13, 1994, by Bad Boy Records and Arista Records", "Humphrey Goodman", "\"Darconville\u2019s Cat\"", "An invoice, bill or tab", "October 16, 2015", "Dan Brandon Bilzerian", "The Andes", "Srinagar", "STS-51-C", "Gregg Harper", "The Ones Who Walk Away from Omelas", "Minnesota's 8th congressional district", "(Polish: \"Trzy kolory\", French: \" Trois couleurs\"", "November 27, 2002", "Henry Albert \"Hank\" Azaria", "UFC Fight Pass", "The Sun", "Noel Paul Stookey", "The Frost Report", "Ready Player One", "1,925", "7 October 1978", "June 2, 2008", "Ravenna", "John Meston", "the Battelle Energy Alliance", "a farmers' co-op", "The Spiderwick Chronicles", "goalkeeper", "George Orwell", "Croatian", "Labour Party", "Fortunino", "Warsaw", "Texhoma", "small amounts of transcellular fluid such as ocular and cerebrospinal fluids in the `` transcellula compartment ''", "September 2017", "Ken Norton", "Israel", "Anne", "the UK", "Al Nisr Al Saudi", "5 1/2-year-old", "cobalt", "Hollaback girl", "The bassoon", "a time in exchange for detailed public disclosure of an invention"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6834993131868132}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4, 1.0, 0.4615384615384615, 0.8, 1.0, 0.4, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.9285714285714286, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-675", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-7526", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-6632"], "SR": 0.5625, "CSR": 0.5359100877192983, "EFR": 1.0, "Overall": 0.7149163925438596}, {"timecode": 57, "before_eval_results": {"predictions": ["1961", "Ballon d'Or", "Elsie Audrey Mossom", "trans-Pacific flight", "House of Hohenstaufen", "BraveStarr", "White Knights of the Ku Klux Klan", "Benjam\u00edn Arellano F\u00e9lix", "lion", "Tony Award", "\"Lebensraum\" became a geopolitical goal of Imperial Germany in World War I (1914\u20131918) originally, as the core element of the \"Septemberprogramm\" of territorial expansion", "China", "Esperanza Spalding", "1854", "Art Deco-style skyscraper", "Starachowice", "Debbie Harry", "England", "Mineola", "Kagoshima Airport", "Essendon Airport", "McLemore Avenue", "bioelectromagnetics", "German nuclear weapon project", "casting, job opportunities, and career advice", "ten episodes", "2012", "Ford Falcon", "Paul Hindemith", "Euripides", "first and second segment", "Clitheroe Football Club", "Peter Wooldridge Townsend", "Paper", "November 27, 2002", "located at Mississippi State", "1837", "2006", "al-Qaeda", "Humberside Airport", "5,000,000", "an English former footballer", "Nia Kay", "11 November 1918", "1891", "the Bank of China Tower", "Lerotholi Polytechnic Football Club", "private liberal arts college", "872 to 930", "hastings", "Richard Street", "latitude 90 \u00b0 North, as well as the direction of true north", "a radius 1.5 times the Schwarzschild radius", "spot - type detectors", "Japan", "an ulterior motive", "l Leeds", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia", "people are going to look at the content of the speech, not just the delivery", "Fernando Torres", "Alicia Keys", "days Inn", "heavy rain", "Yulia Lipnitskaya"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6506634424603175}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.8, 0.5333333333333333, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.125, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-4341", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4436", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-3499", "mrqa_triviaqa-validation-4259", "mrqa_triviaqa-validation-1256", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2325", "mrqa_searchqa-validation-6491", "mrqa_triviaqa-validation-4287"], "SR": 0.515625, "CSR": 0.5355603448275862, "EFR": 1.0, "Overall": 0.7148464439655172}, {"timecode": 58, "before_eval_results": {"predictions": ["Donald Duck", "an open window that fits neatly around him", "Narayanthi Royal Palace", "Susan Atkins", "a brown coffin containing the remains of Israeli soldiers killed during the 2006 war.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "\"Mammograms are known to be uncomfortable,\"", "\"The mass shooting last week was the deadliest ever on a U.S. military base.", "The iconic Abbey Road music studios", "10 municipal police officers", "his past and his future", "mpire of the Sun", "Nancy Sutley", "These harder they squeeze and squish that breast, the less tissue the X-rays have to go through and the more likely they are to find something.\"", "Haeftling,", "55-year-old", "capital murder and three counts of attempted murder", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "eight Indians whom the rebels accused of collaborating with the Colombian government,", "16th grand Slam title.", "curfew", "\"Nu au Plateau de Sculpteur,\"", "giving birth to baby daughter Jada", "Glasgow, Scotland", "antihistamine and an epinephrine auto-injector for emergencies,", "Damon Bankston", "Afghanistan", "maintain an \"aesthetic environment\" and ensure public safety,", "prevent it at every turn, if we are committed and prepared.", "women to cover their bodies and heads from view,", "humans", "Vancouver, British Columbia", "an independent homeland for the country's ethnic Tamil minority since 1983.", "two years ago.", "\"I saw guys who were 34, 35, 36 years old -- still young guys -- about to get out of the game, and I wondered what will they do now.\"", "Senator. Barack Obama", "social issues like homelessness and AIDS.", "fake his own death", "\"To change public opinion in Turkey is a long-term affair,\"", "one-of-a-kind navy dress with red lining by the American-born Lintner,", "The Rev. Alberto Cutie", "citizenship", "housing, business and infrastructure repairs", "The nation's foremost concert producer, Charles Jubert, died. So did members of four bands who were practicing inside a studio that collapsed.", "a mammoth's skull", "\"I will be asking questions,\"", "the creation of an Islamic emirate in Gaza", "in the north and west of the country,", "Elisabeth,", "2-1", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "Francisco Pizarro", "Owen Vaccaro", "2003", "MCA plant in Maubeuge, France", "cheese", "Alan Freed", "Woolsthorpe-by-Colsterworth", "War Is the Answer", "arts manager", "The Antarctic ice sheet", "Ms.", "Thesaurus.com", "prevent both freezing and melting"], "metric_results": {"EM": 0.296875, "QA-F1": 0.4183465679165176}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.4444444444444445, 0.8, 1.0, 0.0, 0.0, 0.15789473684210525, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.5, 1.0, 0.0, 0.923076923076923, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.1111111111111111, 0.25, 1.0, 0.0, 0.7142857142857143, 1.0, 0.05714285714285714, 0.6666666666666666, 0.5, 0.5, 0.09523809523809525, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.3529411764705882, 0.0, 0.0, 0.08, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2182", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1055", "mrqa_triviaqa-validation-4416", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-4517", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-1521"], "SR": 0.296875, "CSR": 0.5315148305084746, "EFR": 0.9555555555555556, "Overall": 0.705148452212806}, {"timecode": 59, "before_eval_results": {"predictions": ["a female soldier,", "Sunday", "five suspects,", "A witness", "U.N. agencies", "Sen. Barack Obama", "Ken Choi", "second child", "Old Trafford", "leftist Workers' Party.", "Robert", "Christiane Amanpour", "Narayanthi Royal", "Mexico", "five", "a city of romance, of incredible architecture and history.", "five-day retreat,", "Keating Holland", "Takashi Saito,", "Mutassim,", "Dr. Jennifer Arnold and husband Bill Klein,", "to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Sunday.", "future relations with Washington", "prostate cancer", "the southern city of Naples", "at least 300", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "2002", "Spc. Megan Lynn Touma,", "women and breast cancer.", "Some truly mind-blowing structures are being planned for the Middle East.", "strife in Somalia,", "Miami Beach, Florida", "Charles Darwin", "it was unjustifiable \"for a project which does nothing more than perpetuate misconceptions about the state and its citizens.\"", "President Obama.", "Stacey Roundtree", "\"doodles\"", "baseball bat", "Philippines", "OneLegacy,", "Jaime Andrade", "Alfredo Astiz,", "tie salesman", "The station", "Ronald Cummings,", "Amir Zaki", "$8.8 million", "Tuesday", "Alan Graham", "Canada south of the Arctic", "Glenn Close", "Joseph M. Scriven", "Richard Krajicek", "Manchester", "Robert", "John Rich", "Cannes Film Festival", "Mario Winans", "3", "Rooster Cogburn", "Korea", "1936"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5790426587301587}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, false, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.3, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3095", "mrqa_naturalquestions-validation-1872", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6864", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-53", "mrqa_searchqa-validation-11013"], "SR": 0.46875, "CSR": 0.53046875, "EFR": 1.0, "Overall": 0.7138281249999999}, {"timecode": 60, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1899", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-382", "mrqa_hotpotqa-validation-4012", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-892", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-266", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2558", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1143", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5687", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6157", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7783", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.8125, "KG": 0.49140625, "before_eval_results": {"predictions": ["2017", "1648 - 51 war", "Thomas Edison", "season four", "Master Christopher Jones", "William Shakespeare", "the status line", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "The Maginot Line", "1937", "Fix You", "Hirschman", "Bobby Eli, John Freeman and Vinnie Barrett", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "approximately 11 %", "Jonny Buckland", "East Prussia", "Sharyans Resources", "Babe Ruth", "in Seattle, Washington", "more than 420", "Bumping Robinson and Terrence Howard", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province )", "DNA was a repeating set of identical nucleotides", "Magnavox Odyssey", "the ball is fed into the gap between the two forward packs and they both compete for the ball to win possession", "Sunday night", "tolled ( quota ) highways", "under the left shoulder", "innermost in the eye", "Saturday", "Glenn Close", "reservoirs at high altitudes", "aiding the war effort", "into smaller pulmonary arteries that spread throughout the lungs", "Nepal", "the symbol \u00d7", "1974", "Phillip Paley", "10.5 %", "liver and kidneys", "Lee County, Florida", "tenderness of meat", "Stephen Graham", "New York City", "2010", "Jeff Barry and Andy Kim", "infection, irritation, or allergies", "ABC", "August 1991", "The Maidstone Studios", "two", "helium", "cask", "Edward James Olmos", "Alison Sweeney", "Toyota Innova", "Omar bin Laden,", "a man's lifeless, naked body", "great jazz music", "Falkland Islands", "a cement pond", "Seventy-six trombones", "Fifty Shades of Grey"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5541566299039125}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.28571428571428575, 0.6666666666666666, 0.1111111111111111, 0.8695652173913044, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-2425", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-5460", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-6052", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-1584", "mrqa_newsqa-validation-3764", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-15624", "mrqa_searchqa-validation-893"], "SR": 0.4375, "CSR": 0.5289446721311475, "EFR": 0.9722222222222222, "Overall": 0.7035927538706739}, {"timecode": 61, "before_eval_results": {"predictions": ["17 - year - old", "The procedure can be performed at any level in the spine ( cervical, thoracic, or lumbar ) and prevents any movement between the fused vertebrae", "Dido", "international aid", "reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "January 15, 2007", "Gorakhpur railway station", "Roger Federer", "the Second Battle of Manassas", "1933", "it `` never had any meaning other than the obvious one '' and is about the `` loss of innocence in children ''", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "John Travolta", "July 14, 1937", "Rory McIlroy", "Jonathan Breck", "the 4th century", "Smith Jerrod", "Acid rain", "2017", "Mulberry Street", "Zilphia Horton, then - music director of the Highlander Folk School of Monteagle, Tennessee ( an adult education school that trained union organizers )", "the sinoatrial node", "1994", "Dylan Walters", "10 May 1940", "Phillipa Soo", "Kansas City Chiefs", "Judiththia Aline Keppel", "Vincent Price", "the customer's account", "California's Del Norte Coast, Jedediah Smith, and Prairie Creek Redwoods State Parks ( dating from the 1920s )", "The film follows a child with Treacher Collins syndrome trying to fit in", "metaphase chromosome", "Sally Field", "Eric Clapton", "E-1 through E-3", "Aamir Khan", "Billy Colman", "Karen Gillan", "Easter", "Beijing, China", "either a marked ( `` - s '' ) or unmarked plural, as in : `` 1 lakh people ''", "Havana Harbor", "1824", "2017", "Qutab Ud - Din - Aibak", "Prince Henry", "the inferior thoracic border -- made up of the diaphragm", "customary units", "an opinion in a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "floating ribs", "Cowboy Builders", "Romania", "The Gold Coast", "Kathryn Jean Martin \"Kathy\" Sullivan AM", "over 20 million", "to see my kids graduate from this school district.\"", "the use of torture and indefinite detention", "Tuesday's iPhone 4S news,", "Chagas disease", "Jacob Marley", "John Hersey", "22 September 2015."], "metric_results": {"EM": 0.453125, "QA-F1": 0.6247235472193027}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.12903225806451613, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.09523809523809523, 0.07407407407407408, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.10526315789473685, 0.9, 0.6666666666666666, 1.0, 1.0, 0.6, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 0.6666666666666666, 0.25806451612903225, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.4, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10172", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8382", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6596", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-451", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-2373", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2249", "mrqa_searchqa-validation-4844", "mrqa_searchqa-validation-14622"], "SR": 0.453125, "CSR": 0.5277217741935484, "EFR": 0.9428571428571428, "Overall": 0.6974751584101382}, {"timecode": 62, "before_eval_results": {"predictions": ["Christian Dior", "cranberry", "fort boyard", "fibula", "a fort", "a beached whale", "the Mississippi", "Dennis Quaid", "Lil Jon", "the Boers", "the Colosseum", "Goldeneye", "fish stock", "a pest", "Sir Winston Leonard Spencer-Churchill", "Scampton, Lincolnshire", "the Lincoln Tunnel", "Pinta", "Louisiana", "the Kid", "Rembrandt", "Canada", "fort boyard", "Benedict XVI", "Strategic Petroleum Reserve", "Prague", "the hanging gardens of babylon", "electrical energy", "9:30 p.m. on CBS", "Lyra and Will", "Nacho Libre", "the College of William & Mary", "the Rocky", "bassoon", "Montpelier", "fort boyard", "the Pound of flesh", "Iran", "Best Picture", "Hamlet", "Heroes", "Syria", "mead", "the Hawks", "Carnaval", "wood", "fort boyard", "fort Hoffman", "Nittany", "Sicily", "Socrates", "more than 1,000", "the large area needed for effective gas exchange", "Harry Potter's first year at Hogwarts School of Witchcraft", "fort boyard", "fort boyard", "fort boyard", "Esperanza Emily Spalding", "2000", "Sargent Shriver", "Daytime Emmy Lifetime Achievement Award", "The Impeccable", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "fort boyard"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5542410714285715}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6, 0.761904761904762, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.47619047619047616, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-7337", "mrqa_searchqa-validation-13095", "mrqa_searchqa-validation-4677", "mrqa_searchqa-validation-14699", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-1906", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-13940", "mrqa_searchqa-validation-5976", "mrqa_searchqa-validation-1751", "mrqa_searchqa-validation-11584", "mrqa_searchqa-validation-8943", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-16438", "mrqa_searchqa-validation-8676", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-14898", "mrqa_searchqa-validation-2392", "mrqa_searchqa-validation-546", "mrqa_searchqa-validation-7407", "mrqa_searchqa-validation-16785", "mrqa_searchqa-validation-15398", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6028", "mrqa_triviaqa-validation-7643", "mrqa_hotpotqa-validation-2367", "mrqa_hotpotqa-validation-5895", "mrqa_newsqa-validation-3961", "mrqa_triviaqa-validation-1787"], "SR": 0.4375, "CSR": 0.5262896825396826, "EFR": 0.9722222222222222, "Overall": 0.7030617559523809}, {"timecode": 63, "before_eval_results": {"predictions": ["the Voting Rights Act", "a pistol", "disabilities", "Harry Potter and the Chamber of Secrets", "Senator Mitchell", "a soap", "the Bronze Age", "a People's Party", "the Tower of London", "Daniel", "Eckhart didn't want to smoke", "a mall", "California", "Cosmopolitan", "David Beckham", "a Minoan civilization", "Japan", "rodeo", "a screwdrivers", "Vietnam", "Stanford", "a minor", "a prism schism", "Alaska", "Wallis Warfield Simpson", "a centipede", "Greek", "Sisters Rosensweig", "Brooklyn", "Penn State", "Easter Island", "Nasser", "Hell", "Stephen Hawking", "Labour Day", "Mozambique", "landfills", "The Silence of the Lambs", "15", "Al Capone", "Paul McCartney", "Anne Murray", "Tennessee", "Buenos", "contagious", "Jane Austen", "wheat", "baking soda", "peanuts", "philosophy", "Byzantium", "The Paris Sisters", "Rent", "Miami Heat", "gluten", "an embroidered cloth", "Sunday", "2004", "Pacific War", "Tsavo East National Park", "the Dominican Republic", "almost 100", "incentive and a method to reach car owners who haven't complied fully with recalls.", "left fielder"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7487847222222221}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.4, 1.0, 0.22222222222222224, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-3380", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-2172", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-6957", "mrqa_searchqa-validation-15049", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-14995", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-7085", "mrqa_hotpotqa-validation-4005", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2362", "mrqa_hotpotqa-validation-798"], "SR": 0.671875, "CSR": 0.528564453125, "EFR": 1.0, "Overall": 0.7090722656249999}, {"timecode": 64, "before_eval_results": {"predictions": ["Malawi", "Ethiopia", "knife hones", "the short-beaked echidna and the duck-billed platypus", "British Airways", "Jumpman", "1929", "Blades", "the Kelly Gang", "Dante", "repechage", "uranium", "Wildcats", "Milady de Winter", "Abraham Lincoln", "Merchant of Venice", "gallons", "Paul Rudd", "Tanzania", "Julian", "Christian Louboutin", "saxophone", "Firecracker Maria Elena (Penelope Cruz)", "October", "Grantham,", "Muriel Spark", "Turkey", "kvetch", "the Netherlands", "Lome", "Caviar", "Christian Dior", "Bobbi Kristina Brown", "William IV", "Dutch", "Jack Lemmon", "Trainspotting", "Australia", "the north-west corner of the central business district", "Diptera", "India and Pakistan", "obi", "Broccoli", "Heisenberg", "1976", "Cyclopes", "phrenology", "Full Metal jacket", "Tokyo", "California", "Windermere", "third", "the fictional elite conservative Vermont boarding school Welton Academy", "the Old French tailleur ( `` cutter '' )", "Michael Jordan", "11 or 13 and 18", "Franz Ferdinand Carl Ludwig Joseph Maria", "Chile", "the single-engine Cessna 206 went down,", "helping on the sandbags to keep the waters at bay.", "Rachel Carson", "laundry", "Toni Morrison", "Copts"], "metric_results": {"EM": 0.625, "QA-F1": 0.6975378787878788}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-4208", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-825", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1819", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8858", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3410", "mrqa_newsqa-validation-1926", "mrqa_searchqa-validation-4044", "mrqa_newsqa-validation-2435"], "SR": 0.625, "CSR": 0.5300480769230769, "EFR": 1.0, "Overall": 0.7093689903846153}, {"timecode": 65, "before_eval_results": {"predictions": ["amanda barrie", "Temple of the Dog", "spain", "tina Turner", "dreamgirls", "barry Taylor", "1986", "port", "(TLD)", "manson", "Uganda", "Ash", "temperature", "satirical cartoons, drawings, stage design", "Alaska", "iron", "drogba", "c\u00e9vennes", "bagram", "Zeus", "september", "The Pillow Book", "isabella", "aramian", "john denver", "smack", "Massachusetts", "air masses", "money for Nothing", "The Apprentice", "The Altamont Speedway Free Festival", "jennifer luigi", "brixham", "The Marcy Brothers", "Ghana", "alla capella", "Nelson Mandela", "luigi", "Illinois", "sailing", "rain hat", "a mark", "Oman", "CBS", "noah beery, Jr.", "lincoln", "Sarajevo", "luigi", "Beyonce", "Carl Johan", "MI5", "Games played", "Something to Sing About", "certiorari", "\"Little Dixie\" area of western Missouri", "American Way", "Arab", "fluoroquinolone", "Alina Cho", "eight.", "Batman", "Passover", "the Vienna Woods", "John Lennon and George Harrison,"], "metric_results": {"EM": 0.5, "QA-F1": 0.5703125}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-7609", "mrqa_triviaqa-validation-3599", "mrqa_triviaqa-validation-2184", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-7203", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-2918", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-5878", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-6095", "mrqa_naturalquestions-validation-7950", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-150", "mrqa_newsqa-validation-1804", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-14825"], "SR": 0.5, "CSR": 0.529592803030303, "EFR": 1.0, "Overall": 0.7092779356060606}, {"timecode": 66, "before_eval_results": {"predictions": ["SLEEPLESS in SEattle", "Harriet Harman", "Portugal", "typhoid fever", "Sheryl Crow", "Melvil Dewey", "japan", "Pancho Villa", "the Wild Bunch", "Mikhail Gorbachev", "South Korea", "prince bride", "tiptoe through the Tulips", "landerstra\u00dfer", "london", "Greek Goddess of Revenge", "c\u00e9vennes", "Les Invalides", "1861", "brown", "london", "numeric keypad", "arise", "aslan", "26", "the narwhal", "a wishbone", "photography", "Charlie Chan", "taekwondo", "phosphorus", "St Trinian\u2019s", "plutonium", "Mercury", "tanks", "dorset", "J. S. Bach", "Groucho Marx", "baseball", "Queen Elizabeth I", "london", "1825", "london", "Clydebank", "the Mad Hatter", "Claudette Colbert", "heating devices", "Chrysler", "\"The closest approach to the original sound\"", "Gambia", "Carl Sagan and his wife and co-writer, Ann Druyan,", "the end of the series in 2010", "Pepsi Super Bowl LII", "a percentage of ethanol in the blood in units of mass of alcohol per volume of blood or mass ofalcohol per mass of blood, depending on the country", "the FAI Junior Cup", "the Knight Company", "Miami-Dade County", "World leaders", "January 24, 2006.", "giving birth to baby daughter Jada,", "a retronym", "Ovid", "a dream", "Latin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.609167395104895}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.7000000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4413", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-458", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-1834", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-999", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-1794", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-4523", "mrqa_hotpotqa-validation-1030", "mrqa_newsqa-validation-801", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-8081"], "SR": 0.5625, "CSR": 0.5300839552238805, "EFR": 0.9642857142857143, "Overall": 0.7022333089019189}, {"timecode": 67, "before_eval_results": {"predictions": ["the Rheingold", "Mahatma Gandhi", "Battlestar Galactica", "the humpback whale", "South Africa", "Brett Favre", "Peril", "Texas", "angioplasty", "anxiety", "a phaser", "Mary Pickford", "a termite", "Sayonara", "Cats", "india", "the Lone Ranger", "Mars", "the Battle of Verdun", "a statistic", "Asia", "India", "b Botswana", "Houston Rockets", "sirloin", "apartheid", "Boston", "red Dead Redemption", "Van Helsing", "Thomas Hobson", "coin", "Shop", "the Bounty", "Urban Outfitters", "burt baskin", "Andrew Wyeth", "smallpox", "jimmy", "al jolson", "Risk", "a T allele", "Don Quixote", "Richmond", "midnight", "the Age of Innocence", "Students for a Democratic Society", "the Bering Sea", "the Caucasus", "Coretta Scott King", "tritonic", "Sgt. Pepper", "the Mayflower", "After World War II", "acronym", "brighton", "Microsoft", "fleet street", "1966", "Juilliard School", "City of Starachowice", "success as a recording artist", "Mombasa, Kenya,", "United Front for Democracy Against Dictatorship", "peninsulas"], "metric_results": {"EM": 0.625, "QA-F1": 0.6700520833333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-15118", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4010", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-5960", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-10385", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-3274", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-7165", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2674", "mrqa_naturalquestions-validation-2064"], "SR": 0.625, "CSR": 0.5314797794117647, "EFR": 0.9583333333333334, "Overall": 0.7013219975490196}, {"timecode": 68, "before_eval_results": {"predictions": ["Davenport", "a phrase", "Ulysses S. Grant", "gonads", "the High Plains", "the gap", "Grease", "Kansas", "motor neurons", "John Fogerty", "the megaton", "king", "an electron", "the Communist Party", "Alfred Binet", "the wife of Bath", "the Atlantic Ocean", "Billboard", "James Buchanan", "Hinduism", "a stone", "Wayne Brady", "Leon Trotsky", "Arkansas", "embryos", "Cuba", "the Computer Age", "airplanes", "Columbia University College of Physicians and Surgeons", "Thomas Nast", "3", "freezing", "Anja Prson", "the National Security Agency", "Kiss Me Kate", "Honey", "Hercules", "five", "Rod Laver", "antonio", "Ivy Dickens", "Selma", "vote", "Herbert Hoover", "Joe Hill", "IHOP", "the Poet of the American Revolution", "Amyotrophic lateral sclerosis", "Samuel Beckett", "William Pitt the Younger", "August 1947", "Jethalal Gada", "Kevin Sumlin", "Charles Carson", "Albert Einstein", "Orion", "the dodo", "1986", "James Gandolfini", "Mandarin Airlines", "at least two and a half hours.", "sixth-year veteran", "Buenos Aires.", "Citizens for a Sound Economy"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6419270833333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-30", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-15032", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-5152", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-3663", "mrqa_searchqa-validation-6721", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-9373", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-2684", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-14015", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_newsqa-validation-2945"], "SR": 0.59375, "CSR": 0.5323822463768115, "EFR": 1.0, "Overall": 0.7098358242753623}, {"timecode": 69, "before_eval_results": {"predictions": ["Manitoba", "a RAND Corporation", "60 Minutes", "a fruitcake", "midnight", "Socrates", "Al Gore", "the Louvre", "(Henry) II", "the Desert Fox", "Baton Rouge", "Langston Hughes", "the Angels", "Men\\'s Hockey", "Cleveland", "slave", "Santa Fe", "Stevie Wonder", "Typhoid Mary", "an inch", "a dental discount plan", "Tokyo", "Tennessee Williams", "the United Arab Emirates", "Lurch", "the President", "Big Ben", "Old Yeller", "a mammal", "2001: A Space Odyssey", "the Big Bang to Black Holes", "Prince", "Abraham", "Duncan", "Yitzhak Rabin", "Brisbane", "Jason Bourne", "the Flagellant", "a globe", "Iowa", "quarterback Paul", "Samsonite L luggage", "the Marine Corps", "Vietnam", "the Canton of Geneva", "Punxsutawney, Pennsylvania", "Sports Illustrated", "Venus", "Shia LaBeouf", "Annapolis", "Princess Anne", "Hungary ( Hungarian : Magyarorsz\u00e1g z\u00e1szlaja )", "January 15, 2007", "Introverted Feeling", "Aethelbert", "Full Monty", "Stella McCartney", "1898", "Anne of Green Gables", "Daniil Shafran", "a Columbian mammoth", "Pixar", "to cope in prison.", "then-Sen. Obama"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6625}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.16666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-14167", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-3164", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-7011", "mrqa_searchqa-validation-6746", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-2651", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-3564", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3695"], "SR": 0.59375, "CSR": 0.5332589285714286, "EFR": 1.0, "Overall": 0.7100111607142857}, {"timecode": 70, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4260", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5757", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1840", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10156", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-1971", "mrqa_squad-validation-2082", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3215", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4299", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-513", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6067", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6917", "mrqa_squad-validation-6960", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9401", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.84765625, "KG": 0.5078125, "before_eval_results": {"predictions": ["Doc Holliday", "Fritos", "the pale", "the fowls", "a fruitcake", "Alicia", "Day s Night", "Christa McAuliffe", "Kilimanjaro", "Misbegotten", "a pumpkin", "Jumbo", "Stoke-on-Trent", "Jalisco", "Hitler", "Portland", "the imagist movement", "Rudolf Diesel", "a palace", "the brig", "the Spanish Republic", "Ruth", "sugar", "Cheaper by the Dozen", "Hans Christian Andersen", "San Francisco", "John Cale", "Wanted", "the flute", "BORE", "Peel", "the Homestead Act", "Ellis Island", "Max Factor", "plantain", "Marie Curie", "German", "Aladdin", "rain", "Peter the Great", "Toy Story", "the Tea Party", "George Sand", "Jim Jarmusch", "Afghanistan", "Minos", "salamanders", "Charles", "the rose hips", "Led Zeppelin", "Luxes", "Michigan", "Texas A&M Aggies", "Alex Burrall, Jason Weaver and Wylie Draper played Michael Jackson in different eras", "Jackson", "The Kentucky Derby", "Iwo Jima", "Charles L. Clifford", "Vince Staples", "Sam Bettley", "Al-Shabaab", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Lucky Dube,", "Florida"], "metric_results": {"EM": 0.625, "QA-F1": 0.6950980392156862}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4747", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-9874", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-2880", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-2455", "mrqa_searchqa-validation-13954", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-6077", "mrqa_searchqa-validation-8455", "mrqa_naturalquestions-validation-2870", "mrqa_triviaqa-validation-1148", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-585"], "SR": 0.625, "CSR": 0.5345510563380282, "EFR": 1.0, "Overall": 0.7155039612676056}, {"timecode": 71, "before_eval_results": {"predictions": ["The Rolling Stones", "The Andy Griffith Show", "incon impractical", "Pocono Mountains", "trip", "watermelon", "a kart", "tanks", "Simon & Garfunkel", "(J Jos Alberto) Pujols", "the Andean bear", "ordinal", "nebulae", "(George) Stephanopoulos", "Eastwick", "The Who", "Cy Young", "Austin Powers", "a mime instructor", "conga drums", "a redwood", "Nellie Bly", "IBM", "pizza", "A Lesson from Aloes", "Menudo", "debts", "cloven", "Nicole Kidman", "Aristophanes", "Wimbledon", "Prince Siddhartha", "a restrictive type", "the Jungle Book", "turquoise", "Papua New Guinea", "Rooster Cogburn", "Halo 3", "Boz", "Sayonara", "touch", "a crescent", "The Moment of Truth", "aardvark", "Harpy", "Henry Fielding", "(James A. Garfield) Garfield", "a waterfowl", "Howie Mandel", "a Chinese orbital carrier rocket", "(Henry Cavendish) Cavendish", "three", "MFSK", "May 31, 2012", "Syria", "Ida Noddack", "(Nynorsk)", "World War II", "Peter Kay\\'s Car Share", "Oneida Limited", "(Zac) Efron", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "the murders of his father and brother.\"", "mantle"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6627203525641026}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 0.7499999999999999, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-11101", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-1282", "mrqa_searchqa-validation-11039", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-2234", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-10201", "mrqa_searchqa-validation-1831", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-11617", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-2850", "mrqa_searchqa-validation-7941", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-5808", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-2382"], "SR": 0.609375, "CSR": 0.5355902777777778, "EFR": 1.0, "Overall": 0.7157118055555556}, {"timecode": 72, "before_eval_results": {"predictions": ["133", "the United States", "Comoros Islands", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "$2 billion in stimulus funds", "\"Slumdog Millionaire\"", "17", "London's O2 arena,", "Joel \"Taz\" DiGregorio,", "second child", "in her home", "a fake pilot's license,", "U.S. 93 in White Hills, Arizona, near Hoover Dam.", "a tracheotomy", "64,", "Muslim festival", "figure out a way that was practical to get a drum set on a plane.", "a supplemental spending bill", "detainees to appear in his Washington courtroom at 10 a.m. Friday and said he would hold a hearing next week to determine under what conditions they will be settled in the United States.", "a satellite", "Mokotedi Mpshe,", "cross-country skiers", "Lisa Brown", "Saturn Sky", "the reality he has seen is \"terrifying.\"", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "Haeftling,", "severe flooding", "Consumer Reports", "Now Zad in Helmand province, Afghanistan.", "a clear strategy", "second", "through the weekend,", "Sen. Barack Obama", "South Africa's president", "the abduction of minors.", "three", "Fernando Caceres", "Joan Rivers", "the L'Aquila earthquake,", "Sgt. Barbara Jones", "country directors", "Tehran", "\"We must find ways to relieve some of this stress,\"", "When the economy turns unfriendly,", "Filippo Inzaghi", "BMW", "Afghanistan,", "the bombers", "her mother", "Dolgorsuren Dagvadorj", "king", "a form of fixed - mobile convergence ( FMC )", "Ant & Dec", "Robert A. Heinlein", "a falcon", "friends", "IT", "the World Series", "Dumfries and Galloway, south-west Scotland", "popcorn", "a date", "a white elephant", "Lord Salisbury"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6000671203796204}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 0.0, 0.10256410256410256, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1373", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3088", "mrqa_naturalquestions-validation-6294", "mrqa_triviaqa-validation-179", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-2632", "mrqa_hotpotqa-validation-2653", "mrqa_searchqa-validation-8732", "mrqa_hotpotqa-validation-364"], "SR": 0.53125, "CSR": 0.5355308219178082, "EFR": 1.0, "Overall": 0.7156999143835616}, {"timecode": 73, "before_eval_results": {"predictions": ["$40 and a bread.", "The ruling Justicialist Party, or PJ by its Spanish acronym, lost its majority in the Chamber of Deputies after being defeated in 18 of 60 races,", "\"Vaughn,\" which is what co-workers called him,", "Israel's Defense Secretary Ehud Barak, at a meeting with U.N. nuclear watchdog agency said Iran's stock of low-enriched uranium would have to be turned into highly enriched uranium to be weapons-grade material.", "11,", "I've always been fascinated by the political process ever since I was a kid.", "average of 25 percent", "March 22,", "President Robert Mugabe", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "OneLegacy, an organ procurement agency in Southern California,", "an upper respiratory infection,\"", "10:30 p.m. October 3,", "Arabic, French and English.", "U.N. drug chief.", "\"Maude\"", "Bill,", "great tenderness toward the people who devoted themselves to Rin Tin Tin", "ancient Jewish tradition", "\"The Da Vinci Code\"", "J.G. Ballard,", "protective shoes", "17", "2008.", "12 hours", "Now Zad in Helmand province, Afghanistan.", "Russian concerns that the defensive shield could be used for offensive aims.", "American", "Los Alamitos Joint Forces Training Base", "Al-Shabaab,", "where they can learn in safer surroundings.", "Flint, Michigan,", "an upper respiratory infection,\"", "test scores and graduation rates", "Sri Lanka", "\"CNN Heroes: An All-Star Tribute\"", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "$162 billion in war funding", "scientific reasons.", "innovative, exciting skyscrapers set to appear all over the world over the next 10 years.", "concentration camps,", "Pakistani territory", "Benazir Bhutto, 54, spent eight years in self-imposed exile in Great Britain and Dubai after President Farooq Leghari dismissed", "Pew Research Center", "self-styled revolutionary Symbionese Liberation Army -- perhaps best known for kidnapping Patricia Hearst --", "38,", "\"stressed and tired force\"", "speed attempts", "ole Southern boy. He was tried and convicted, based on what most historians say was the perjured testimony of a black man, and sentenced to death.", "At least 38", "Iran", "from the northern terminus of Port Said to the Red Sea through the Isthmus of Suez", "Philadelphia, which is Greek for brotherly love", "September 2017", "Dr John Sentamu", "Tony Cozier", "Sheffield", "Dulwich", "Tom Coughlin", "Paraguayan Guarani", "Vulcan", "Beloved", "a flapjack", "Hungarian"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5833598259379509}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false], "QA-F1": [0.7499999999999999, 0.5454545454545454, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.25, 0.0, 0.8, 0.0, 0.20000000000000004, 1.0, 0.375, 1.0, 1.0, 0.09090909090909091, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.6363636363636364, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-2763", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-6111", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-1383"], "SR": 0.46875, "CSR": 0.5346283783783784, "EFR": 1.0, "Overall": 0.7155194256756757}, {"timecode": 74, "before_eval_results": {"predictions": ["South Korea's", "their surrogate,", "16", "the program was made with the parents' full consent.", "five", "Marie-Therese Walter.", "18", "1979", "federal help for those affected by the fires.", "consumer confidence", "$81,88010", "Climatecare,", "her father's home in Satsuma, Florida,", "state senators", "August 19, 2007.", "Ghana", "6,000", "1-0", "Brian Smith.", "Barack Obama sent a message that fight against terror will respect America's values.", "Port-au-Prince,", "Sunday,", "historical, inspiring, creative, romantic and beautiful.", "1,500 Marines", "At the weekend, the captain appealed urgently to be rescued, fearing the crew could be harmed or killed,", "Bill Haas", "Silicon Valley.", "people switched from the very bad category to the pretty bad category,", "at least nine people", "British troops", "Cash for Clunkers", "Yemen.", "Tibet's", "800,000", "carving a pumpkin.", "Russia", "one", "Democratic VP candidate", "Former U.S. soldier Steven Green", "Cologne, Germany,", "President Clinton.", "Kurdish militant group in Turkey", "6-4", "Karen Floyd", "St Petersburg and Moscow,", "flights", "1-1 draw", "Bahrain", "Alaska or Hawaii.", "Biomedical engineers", "repeal of the military's \"don't ask, don't tell\" policy", "Dollree Mapp", "Matthias Schleiden and Theodor Schwann", "Abraham Lincoln's war goals", "giraffe", "swimming", "the World Bank", "Dallas/Fort Worth Metroplex", "Robert Matthew Hurley", "Ashanti Region", "Michael Kors", "Dr. Alex Karev", "Lenin", "henry"], "metric_results": {"EM": 0.515625, "QA-F1": 0.575689935064935}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.09090909090909091, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-258", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-9093", "mrqa_triviaqa-validation-280", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-5300", "mrqa_searchqa-validation-662", "mrqa_triviaqa-validation-6077"], "SR": 0.515625, "CSR": 0.534375, "EFR": 0.967741935483871, "Overall": 0.7090171370967742}, {"timecode": 75, "before_eval_results": {"predictions": ["Bear Grylls", "Deutsche Mark", "Terry", "the Elbe", "New South Wales", "Sean Connery", "Stockholm", "Leo", "Henry VIII", "a power outage", "Scott Glenn", "Pluto", "Deep Blue", "yellow-brown", "1996", "squid", "James Scott, duke of Monmouth", "Isaac", "New Israel Shekel", "serbia", "The Lost Weekend", "Althorp", "the conquest of Peru", "June", "Persuasion", "Robert Taylor", "the AllStars", "Hannibal", "Norman Mailer", "fishes", "floating", "Florence", "hallmarks", "pascal", "\"gruppetto\"", "Adidas Originals", "11", "Israel", "football", "the Porteous Riots", "English", "Gentlemen Prefer Blondes", "puppies", "Kenya", "Conrad Murray", "Liftoff", "Amelia Earhart", "spinach", "John Gorman", "Benedict", "cirrus", "Ariana Clarice Richards", "Turner Layton", "The Parlement de Bretagne", "Campbellsville University", "Daniel Craig", "Vanilla Air", "HPV", "random events", "21 percent suggesting that", "bottle", "Kansas", "Parkinson\\'s disease", "Sammy Gravano"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5929", "mrqa_triviaqa-validation-4049", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-771", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-5132", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-1221", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7059", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-1490", "mrqa_naturalquestions-validation-7021", "mrqa_hotpotqa-validation-684", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-10356", "mrqa_hotpotqa-validation-4564"], "SR": 0.53125, "CSR": 0.5343338815789473, "EFR": 0.9333333333333333, "Overall": 0.7021271929824561}, {"timecode": 76, "before_eval_results": {"predictions": ["Steve Jobs", "les Bleus", "Islamic", "A prisoner killed in a Maryland county jail on Sunday", "two women killed in a stampede at one of his events in Angola on Saturday,", "seven", "drug cartels", "suicide car bombing", "the Illinois Reform Commission", "vice-chairman of Hussein's Revolutionary Command Council.", "4 meters (13 feet) high", "The recent violence -- which has included attacks on pipelines and hostage-taking --", "183", "80,", "a landlord", "Bob Bogle,", "back at work.", "in Mexico", "The Rev. Alberto Cutie -- sometimes called \"Father Oprah\" because of the advice he gave on Spanish-language media --", "California, Texas and Florida,", "2-1", "Iran of trying to build nuclear bombs,", "\"with a goal of starting to withdraw forces from the country in July 2011.\"", "dancing", "this week's Australian Open,", "wayne mosteller,", "\"It was a wrong thing to say,", "Sheikh Sharif Sheikh Ahmed", "8,", "safety issues", "bipartisan", "Haiti", "Mashhad", "10", "Buenos Aires.", "U.S. Navy helicopter crew", "Natalie Wood's 1981 drowning death,", "oaxacan countryside of southern Mexico", "Kim Clijsters", "Venus Williams", "1.0-magnitude earthquake sent a quarter-mile pier crumbling into the sea along with two of his trucks.", "\"the most dangerous precedent in this country, violating all of our due process rights.\"", "15,000", "the state's first lady,", "on vacation", "The father of Haleigh Cummings,", "\"The most affecting thing about this whole wheelchair for children is when the parents realize the gift that is being given to their children and they reach out to hug you.\"", "E. coli", "12 off-duty federal agents in southwestern Mexico,", "1,500", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "Jyotirindra Basu", "60", "Stax Records songwriters Homer Banks, Carl Hampton and Raymond Jackson", "leeds", "boudicca, Queen of the Iceni (d c. 62)", "seattle", "strings", "Xherdan Shaqiri", "1958", "William Tecumseh Sherman", "buren", "air pressure", "eucritta"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5851607789855072}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0, 1.0, 0.1818181818181818, 1.0, 0.0, 0.0, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.7272727272727272, 0.6666666666666666, 1.0, 0.88, 0.08695652173913043, 0.5, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-990", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2948", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-4796", "mrqa_triviaqa-validation-5828", "mrqa_hotpotqa-validation-1902", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-12792", "mrqa_triviaqa-validation-1454"], "SR": 0.453125, "CSR": 0.5332792207792207, "EFR": 1.0, "Overall": 0.7152495941558442}, {"timecode": 77, "before_eval_results": {"predictions": ["Bill Haas", "a French team", "more than 200.", "the player", "American Civil Liberties Union", "Haleigh Cummings,", "\"The Cycle of Life,\"", "Ralph Lauren", "Charlie Chaplin", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "Columbia, Missouri.", "four", "Jenny Sanford,", "Buenos Aires", "hundreds", "more than 78,000 parents of children ages 3 to 17.", "Miami, Florida,", "The Israeli Navy", "Buddhism", "in Seoul,", "President Obama", "North Korea", "strangulation and asphyxiation and had two broken bones in his neck,", "John and Elizabeth Calvert", "school funding", "hanging a noose in a campus library,", "It represents the second-highest unemployment rate in the European Union, after Latvia,", "Michael Jackson", "Brazil", "Pixar's", "Pixar's", "1,500", "instrumental in many of my accomplishments,\"", "Two United Arab Emirates based companies", "fascinating transformation that takes place when carving a pumpkin.", "over 600 meters", "Transport Workers Union leaders", "Amanda Knox's aunt", "in the Muslim north of Sudan", "Melbourne", "cancer", "an unspecified threat to disrupt the inauguration,", "the ship", "338", "finance", "Haeftling,", "bribing other wrestlers to lose bouts,", "in the northwestern province of Antioquia,", "outfit from designer", "350 U.S. soldiers", "\"Taz\" DiGregorio,", "active osmotic water absorption", "Robin", "1965", "three Worlds", "the Sulu Sea", "commonwealth", "The satirical", "Valhalla Highlands Historic District", "boxer", "push", "rain", "Wynton Marsalis", "argon"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5230790043290043}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.7777777777777777, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2633", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-2997", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-394", "mrqa_triviaqa-validation-699", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12492", "mrqa_searchqa-validation-4358"], "SR": 0.40625, "CSR": 0.531650641025641, "EFR": 1.0, "Overall": 0.7149238782051281}, {"timecode": 78, "before_eval_results": {"predictions": ["3-2", "Aung San Suu Kyi", "1957", "Fayetteville, North Carolina,", "emergency plans", "South Africa", "Kenyan forces", "\"Marketers are finally waking up to it -- you know -- black is beautiful,\"", "assassination of President Mohamed Anwar al-Sadat at the hands of four military officers during an annual parade celebrating the anniversary of Egypt's 1973 war with Israel.", "Diego Milito's", "11 countries,", "Robert Barnett,", "More than 22 million people in sub-Saharan Africa", "the driver", "Southeast Asia and India.", "Elizabeth Taylor did not go into further detail about her heart condition or the medical procedure.", "200", "Phillip Myers,", "The National Association of Broadcasters", "bipartisan", "a rally", "1912.", "Steven Green", "The plane, an Airbus A320-214,", "Casey Anthony,", "Roland S. Martin", "sixties minimalism with a twist.", "some deaths", "a park bench facing Lake Washington", "Andrew Morris,", "Siri", "\"The Real Housewives of Atlanta\"", "an independent homeland since 1983.", "Air traffic delays began to clear up Tuesday evening after computer problems left travelers across the United States waiting in airports,", "Hudson, Wisconsin.", "Barbara Streisand's", "the death of a pregnant soldier", "raping", "five", "Asashoryu", "Bob Johnson", "\u00a320 million ($41.1 million)", "in order to accommodate the elephants,", "natural disasters", "his mother, Katherine Jackson,", "red varnished cover with the word \"Album\" inscribed on it in gold lettering,", "the inspector-general", "anti-Israeli sentiment in Egypt in the past few months", "John Demjanjuk,", "following in Arizona's footsteps would take states in the wrong direction.", "seven", "a couple broken apart by the Iraq War", "antimeridian", "instructions", "The Undertones", "Johnny Mercer", "Fenn Street School", "acting", "the Vietnam War", "1967", "Ukraine", "capitals", "Francis Steegmuller", "genome"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5782111378205128}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.5, 0.8, 1.0, 0.0, 0.4, 0.16, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-374", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-584", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2118", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9852", "mrqa_triviaqa-validation-6200", "mrqa_hotpotqa-validation-5128", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-10570"], "SR": 0.46875, "CSR": 0.5308544303797469, "EFR": 0.9705882352941176, "Overall": 0.7088822831347729}, {"timecode": 79, "before_eval_results": {"predictions": ["Dick Rutan and Jeana Yeager", "20 years from the filing date subject to the payment of maintenance fees", "The eighth and final season of the fantasy drama television series", "Rigg", "March 2016", "importance, honor, and majesty", "the name announcement of Kylie Jenner's first child", "1998", "orange collection boxes", "seven", "February 16, 2018", "Coroebus of Elis", "Jonathan Breck", "George Harrison, his former bandmate from the Beatles", "Howard Ellsworth Rollins Jr", "Ancylostoma duodenale", "January 17, 1899", "eight hours ( UTC \u2212 08 : 00 )", "Thespis ( / \u02c8\u03b8\u025bsp\u026as / ; Greek : \u0398\u03ad\u03c3\u03c0\u03b9\u03c2 ; fl. 6th century BC )", "Eukarya", "Wednesday, September 21, 2016", "Robber baron", "three", "Tbilisi, Georgia", "off the rez", "1895", "the Anglo - Saxon King Harold Godwinson", "Sir Ronald Ross", "May 29, 2018", "1979", "In the television series's fourth season", "ancient Athens", "Russia", "the magnetic stripe `` anomalies '' on the ocean floor", "United Nations", "In late - 2011", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "Real Madrid", "Angel Benitez", "capillary action", "1830", "1st Earl Mountbatten of Burma", "foreign investors", "Bhupendranath Dutt", "June 22, 1942", "775 rooms", "By mid-1988", "senators", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Paul Lynde", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Treaty of Waitangi", "James Garfield", "Burkina Faso", "England", "Sverdlovsk", "47,818,", "seven", "Samoa", "to close their shops during daily prayers,", "Versailles", "Westminster College", "Bob Dylan", "Mount Pelee"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7426144321364909}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false], "QA-F1": [0.5714285714285715, 0.5882352941176471, 0.3636363636363636, 1.0, 1.0, 0.0, 0.7999999999999999, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6, 0.2222222222222222, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727272, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-7848", "mrqa_triviaqa-validation-5704", "mrqa_triviaqa-validation-5769", "mrqa_searchqa-validation-13149", "mrqa_searchqa-validation-13773"], "SR": 0.609375, "CSR": 0.5318359375, "EFR": 0.84, "Overall": 0.6829609375000001}, {"timecode": 80, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11088", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.822265625, "KG": 0.496875, "before_eval_results": {"predictions": ["the 13th century", "14 December 1972 UTC", "China", "1975", "Abraham Gottlob Werner", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "the Afghan - Pakistan border", "September, 2016", "July 2014", "ase", "at a given temperature", "Skat", "Claudia Grace Wells", "Bill Russell", "China in American colonies", "the Reverse - Flash", "A lymphocyte is one of the subtypes of white blood cell in a vertebrate's immune system", "Christopher Lloyd", "four", "1923", "San Francisco, California", "2018", "Lilian Bellamy", "Kansas City Chiefs", "Zoe Badwi, Jade Thirlwall's cousin", "1997", "William Wyler", "Ray Harroun", "one person", "the NIRA", "Santa Fe, New Mexico", "Kaley Christine Cuoco", "Marshall Sahlins", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "in the United Kingdom", "The Gupta Empire", "the oculus, or `` eye point ''", "James Madison", "Matt Monro", "Coton in the Elms", "Algeria", "June 1992", "prokaryotic cell ( or organelle )", "Germany", "the Italian / Venetian John Cabot", "the May Revolution of 1810", "Jason Flemyng", "blue", "unknown origin", "New Zealand to New Guinea", "1960", "Kaiser Chiefs", "Donna Jo Napoli\u2019s Beast,", "claire goose", "\"The 8th Habit\"", "road movie", "Earvin \"Magic\" Johnson", "President Sheikh Sharif Sheikh Ahmed", "python", "posting a $1,725 bail,", "Shimon Peres", "cortisone", "1936", "carbon"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6334425990675991}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4615384615384615, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 0.8, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-5995", "mrqa_triviaqa-validation-4432", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-4303", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-1713", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-11202"], "SR": 0.53125, "CSR": 0.5318287037037037, "EFR": 1.0, "Overall": 0.7143344907407407}, {"timecode": 81, "before_eval_results": {"predictions": ["William the Conqueror", "1962", "Alastair Cook", "The President of Zambia", "Ray Charles", "AD 95 -- 110", "the recommended", "in the 1970s", "Dan Stevens", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "Idaho", "Tatsumi", "1969", "Ethiopia", "1937", "artes liberales", "Debbie Reynolds", "Mediterranean Shipping Company S.A.", "2020", "2007", "Hodel", "the coffee shop Monk's", "Siddharth Arora / Vibhav Roy as Ishaan Anirudh Sinha", "1991", "1900", "the Port Authority of New York and New Jersey's formal name for the new PATH station and the associated transit and retail complex that opened on March 3, 2016", "Munich, Bavaria", "Steve Russell", "October 2, 2017", "Paul Lynde", "regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Frank Oz", "Sam Waterston", "Sunday night", "Johnny Darrell", "the 7th century", "2018", "Will Turpin", "Will Verchere - Gopaulsingh", "2013", "two", "Will Champion", "Bartemius Crouch Jr impersonating Alastor `` Mad - eye '' Moody ( book four )", "Pre-evaluation", "Jackie Robinson", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "Brazil, Turkey and Uzbekistan", "Times Square in New York City west to Lincoln Park in San Francisco", "two", "The pia mater", "United Nations Peacekeeping Operations", "the Kinks", "inishtrahull Island", "the Indus Valley civilization", "the junction with Interstate 95", "Greek mythology", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "$1.45 billion", "Lindsey Vonn", "$55.7 million", "( Hipparchus)", "Dixie", "San Salvador", "candy bar"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6681030851977372}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.10810810810810811, 1.0, 1.0, 0.6666666666666666, 1.0, 0.972972972972973, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.7999999999999999, 0.4, 0.631578947368421, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.4210526315789474, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-5862", "mrqa_hotpotqa-validation-1861", "mrqa_newsqa-validation-3916", "mrqa_newsqa-validation-3156", "mrqa_searchqa-validation-10515", "mrqa_triviaqa-validation-7778"], "SR": 0.515625, "CSR": 0.5316310975609756, "EFR": 0.9354838709677419, "Overall": 0.7013917437057435}, {"timecode": 82, "before_eval_results": {"predictions": ["the Four Seasons", "Kathleen Erin Walsh", "1603", "Stephen Graham", "Opened in 2010", "Tulsa, Oklahoma", "beta decay", "Nickelback", "1987", "John Adams", "Ford", "NCIS Special Agent in Charge", "Jason Clarke", "(Pierre) Renoir", "July 2, 1928", "Dottie West", "in northern China", "Blood is the New Black", "1977", "arm", "Professor Eobard Thawne", "Jack Nicklaus", "trunk", "St. John's, Newfoundland and Labrador", "December 25", "a young husband and wife", "May 29, 2018", "Church of England", "September 27, 2017", "April 10, 2018", "Daniel Suarez", "had sovereignty over some or all of the current territory of the U.S. state of Texas", "January 15, 2007", "United States, its NATO allies and others", "optic chiasma", "Empire of Japan", "Bangladesh -- India border", "Ole Einar Bj\u00f8rndalen", "Jenny Slate", "XLVIII", "Prince William, Duke of Cambridge, the Prince of Wales's elder son", "Nicklaus", "Opened in 1904", "Peter Greene", "Matt Monro", "a little warmth", "in the reverse direction", "currently starring as Phyllis Summers on The Young and the Restless", "energy from light is absorbed by proteins called reaction centres", "in Rome in 336", "September 30", "Charles Darwin", "john Letterman", "Fiat", "five", "Wanda", "April 24, 1934", "40", "55-year-old", "$2 billion", "(Cynthia) Nixon", "coffee", "Have You Ever Really Loved a Woman", "Oldham, in Greater Manchester, England"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6495994916031682}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8235294117647058, 1.0, 0.4444444444444445, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.5, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.8, 0.4, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9687", "mrqa_triviaqa-validation-7718", "mrqa_hotpotqa-validation-1812", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2444", "mrqa_searchqa-validation-13906", "mrqa_searchqa-validation-12962", "mrqa_triviaqa-validation-6822"], "SR": 0.484375, "CSR": 0.5310617469879518, "EFR": 0.9090909090909091, "Overall": 0.6959992812157723}, {"timecode": 83, "before_eval_results": {"predictions": ["American 3D computer-animated comedy film", "1964", "Hindi cinema", "eight", "Flula Borg", "Comanche County, Oklahoma", "Harry Winston, Inc.", "1999", "Congo River", "4,613", "George Washington Bridge", "Harry Robbins \"Bob\" Haldeman", "Guardians of the Galaxy Vol.  2", "2.1 million", "6'5\" and 190 pounds", "526 people per square mile", "Cheshire, North West England", "casting, job opportunities, and career advice", "Saint Elgiva", "Scottish", "2006", "Carrefour", "infinite sum of terms", "Stravinsky's \"The Rite of Spring\"", "Wayne Rooney", "second half of the third season", "Johnson & Johnson", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "Brown Mountain Lights", "Backstreet Boys", "45th", "Foxborough, Massachusetts", "Wichita", "animation", "Sir John Major", "sandstone", "Reginald Engelbach", "Dutch", "BC Dz\u016bkija", "Anne", "Derek Jacobi", "Rebirth", "\"Big Mamie\"", "British-Irish folk rock band The Pogues", "Maxwell Atoms", "Prince Alexander of Denmark", "King of France", "Alemannic", "Larry Alphonso Johnson Jr.", "Filipino-Swiss driver", "Taylor Swift", "June 5, 2017", "tenderness of meat", "Tigris and Euphrates rivers", "Mozambique Channel", "Astor family", "Tashkent", "Pakistan intelligence institutions and its army", "Francisco X. Pacheco,", "black bear", "Speed Racer", "Yogi Bear", "sakura", "to function like an endocrine organ, and dysregulation of the gut flora has been correlated with a host of inflammatory and autoimmune conditions"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6375528665413535}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false], "QA-F1": [0.7499999999999999, 1.0, 0.6666666666666666, 0.5, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.33333333333333337, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-690", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-5216", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-138", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1512", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-4696", "mrqa_triviaqa-validation-5311", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3680", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5496", "mrqa_naturalquestions-validation-7393"], "SR": 0.515625, "CSR": 0.5308779761904762, "EFR": 1.0, "Overall": 0.7141443452380953}, {"timecode": 84, "before_eval_results": {"predictions": ["As of January 17, 2018, 201 episodes", "a group of seemingly unconnected people in Atlanta", "around 1872", "1978", "Hodel", "Central Germany", "Kida", "the Outfield", "infant, schoolboy, lover, soldier, justice, Pantalone and old age", "a central place in Christian eschatology", "Bill Irwin", "artes liberales", "through the right atrium to the atrioventricular node, along the Bundle of His and through bundle branches", "to become oxidized and cease functioning", "Ellen is restored to life and is married to Bobby", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "from January to May 2014", "A vanishing point", "Janie Crawford's `` ripening from a vibrant, but voiceless, teenage girl into a woman with her finger on the trigger of her own destiny", "UTC \u2212 09 : 00", "mining", "Scottish post-punk band Orange Juice", "ocular and cerebrospinal fluids", "39 %", "Missouri River", "iOS", "Jason Marsden", "The Chipettes", "American country music artists", "Fred E. Ahlert", "Thespis", "Bill Belichick", "May 19, 2017", "season two", "about 375 miles ( 600 km ) south of Newfoundland", "the status line", "Samantha Jo `` Mandy '' Moore", "Dr. Lexie Grey", "Nodar Kumaritashvili", "TLC - All That Theme Song 1 : 04", "Welch, West Virginia", "Nicki Minaj", "Steve Russell", "Danielle Rose Russell", "the church at Philippi", "a bandwidth of 2.048 Mbit / s", "to solve its problem of lack of food self - sufficiency", "Bed and breakfast", "the Middle East", "five", "Geothermal gradient", "a contact lens", "zak Starkey", "fire insurance", "Seoul, South Korea", "Saturday", "Al Capone", "14 years", "ambassadors", "Russia", "the Dustbin", "the Equator", "( Claudius) Caesar", "There's no chance"], "metric_results": {"EM": 0.578125, "QA-F1": 0.644507012085137}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.25, 1.0, 1.0, 0.125, 0.0, 0.0, 1.0, 0.0, 1.0, 0.09090909090909091, 1.0, 1.0, 0.5714285714285715, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-1767", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-16205", "mrqa_newsqa-validation-2213"], "SR": 0.578125, "CSR": 0.5314338235294118, "EFR": 0.9629629629629629, "Overall": 0.706848107298475}, {"timecode": 85, "before_eval_results": {"predictions": ["the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "the : The uvea", "25 June 1932", "Felicity Huffman", "a site for genetic transcription that is segregated from the location of translation in the cytoplasm, allowing levels of gene regulation that are not available to prokaryotes", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Roger Nichols and Paul Williams", "peninsular", "at least 28 vowel forms", "letter series", "eleven separate regions of the Old and New World", "As of January 17, 2018, 201 episodes", "the Tennessee Titans", "Monk's", "B.R. Ambedkar", "the septum", "Cecil Lockhart", "Executive Residence of the White House Complex", "David Motl", "cephalopods", "Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "in the beta cells of the islets of Langerhans", "the New Croton Reservoir in Westchester and Putnam counties", "Professor Eobard Thawne", "Freddie Highmore", "USA Today", "1939", "over 74 languages", "during prenatal development", "Zedekiah", "A footling breech", "Robert Gillespie Adamson IV", "Waylon Jennings", "rearview mirror", "the fourth ventricle", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Doug Diemoz", "$19.8 trillion", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "Zappa", "1956", "Camping World Stadium in Orlando, Florida", "Louis XV", "the final episode of the series", "Jonathan Breck", "Ludacris", "British Kennel Clubs", "Wilt Chamberlain", "James Chadwick", "Athens", "Lula ( Lesley Ann Warren )", "Swiss", "Trade Mark Registration Act 1875", "Las Vegas", "Hindi", "Brookhaven", "2004", "space shuttle Discovery", "dismissed all charges", "\"Empire of the Sun,\"", "frittata", "a crab", "a meter", "Matlock"], "metric_results": {"EM": 0.625, "QA-F1": 0.7145317983912933}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6086956521739131, 0.0, 0.36363636363636365, 1.0, 1.0, 0.0, 1.0, 0.8, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6485", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2863", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-7912", "mrqa_triviaqa-validation-2264", "mrqa_hotpotqa-validation-861", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-10904"], "SR": 0.625, "CSR": 0.5325218023255813, "EFR": 0.9583333333333334, "Overall": 0.706139777131783}, {"timecode": 86, "before_eval_results": {"predictions": ["Los Angeles", "a Polaroid picture", "(M Mikhail) Baryshnikov", "flibutor", "The Gates", "Top Gun", "(Cyrus) McCormick", "the Follies", "ballpoint pen", "the tabernacle", "a caterpillar", "tequila", "the Hudson River Valley", "Albert Einstein", "American infantryman", "a roller coaster", "President George H.W. Bush", "Elvis Presley", "a toilet", "an end-use product", "Gogol", "the Hudson River", "Banquo", "Leo", "Texas", "pardon", "the alligator", "General Mills", "the comb", "Ferdinand Magellan", "France", "The Tavern Kitchen & Bar", "\"The Drowsy Chaperone\"", "the Tragedy of Macbeth", "the Greyhounds", "a periodicity", "John Molson", "a lottery", "Al Lenhardt", "Tanzania", "Colorado", "Christopher Columbus", "the Atlantic Ocean", "Slovakia", "polygons", "Frederic Remington", "The biography written by daughter Julie", "Evan Almighty", "toothache", "64", "a pillar of salt", "Johannes Gutenberg", "203", "4 January 2011", "Neighbours", "catslide", "Parsley the Lion", "Norway", "Taylor Swift", "The Campbell Soup Company", "Stoke City.", "The leftist guerilla group, which goes by its Spanish acronym FARC,", "Los Angeles", "Venezuela"], "metric_results": {"EM": 0.5, "QA-F1": 0.5906498015873016}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10854", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-6274", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-11180", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-7345", "mrqa_searchqa-validation-2269", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-4790", "mrqa_searchqa-validation-632", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-13536", "mrqa_searchqa-validation-5092", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6649", "mrqa_hotpotqa-validation-5209", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-1516"], "SR": 0.5, "CSR": 0.5321479885057472, "EFR": 1.0, "Overall": 0.7143983477011495}, {"timecode": 87, "before_eval_results": {"predictions": ["Shopgirl", "Easter Island", "True", "The Bridge on the River Kwai", "Edwin Starr", "Virgin Atlantic", "cheddar", "Indira Gandhi", "pew", "Alfred Hitchcock", "the circulatory system", "the London Bridge", "Ho Chi Minh", "Mount Vesuvius", "Himalaya", "Kodachrome", "\"House M.D.\"", "Paul Simon", "Pakistan", "A Prairie Home Companion", "Concord", "a bug", "the Thames", "Taipei", "Dame Nellie Melba", "the Caspian Sea", "Babe Ruth", "the Edo era", "a recipe", "a apple", "chocolates", "bay", "the Canterbury Tales", "Japan", "Ben & Jerry", "a Sweater", "Krakauer", "Ali", "Sweden", "John Glenn", "the Andes", "the cuttlefish", "Mitt Romney", "Goofy", "Peter Jackson", "an inch", "Neptune", "Earhart", "a manta ray", "Simon Cowell", "the Northern Mockingbird", "an enzyme known as ligase", "in serial format in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "57 days", "Mr. Humphries", "James Mason", "Tinie Tempah", "Richard Arthur", "Ghana", "Labour", "because he was depriving his wife of the liberty to come and go with her face uncovered,", "1,500", "the power-sharing deal with the MDC offshoot", "Louis XV"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7284455128205127}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3533", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-7940", "mrqa_searchqa-validation-13410", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-9282", "mrqa_searchqa-validation-13168", "mrqa_searchqa-validation-8024", "mrqa_searchqa-validation-2075", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-6808", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-6778", "mrqa_searchqa-validation-6947", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-1277", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3391"], "SR": 0.609375, "CSR": 0.5330255681818181, "EFR": 1.0, "Overall": 0.7145738636363637}, {"timecode": 88, "before_eval_results": {"predictions": ["Charles Dickens", "a fish", "Sherlock", "Grant Wood", "(Jeff) Probst", "\"Twelfth Night\"", "the Black Sea", "Eggs Benedict", "a lovebird", "Agatha Christie", "a church", "(Lauren) Bacall", "the Mossad", "a backstroke", "Swahili", "a code", "a tropical depression", "a \"ball in tube\"", "a proscenium arch", "(Thunnus) alalunga", "Pocahontas", "sinuses", "Africa", "Jane Eyre", "Kandahar", "(George) Mogridge", "tofu", "(Henry) Harrison", "clay", "the Jutland Peninsula", "(Alma) Thomas", "the Fourteen Points", "Misery", "(Steven) Waddington", "(Bo) Diddley", "the Osmonds", "a guitar", "Roberta Flack", "Belgium", "SHIELD", "Chicago", "an actuary", "Latkes", "Montana", "a dulcimer", "McCarthy", "lead", "the Apocrypha", "a Discobolus", "Ayahuasca", "Berlin", "a chimera", "via redox ( both reduction and oxidation occurring simultaneously ) reactions", "Charles Frederickson", "horse racing", "Uruguay", "Mauna Kea", "Deftones", "The Design Inference", "Dutch", "(Norra Grangesbergsgatan 4)", "3-2", "bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "Ronald Lyle \" Ron\" Goldman"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6386160714285715}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.9047619047619047, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-16778", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-13978", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-10698", "mrqa_searchqa-validation-8423", "mrqa_searchqa-validation-10749", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-12604", "mrqa_searchqa-validation-2486", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-16009", "mrqa_searchqa-validation-13957", "mrqa_searchqa-validation-7883", "mrqa_naturalquestions-validation-181", "mrqa_naturalquestions-validation-6711", "mrqa_triviaqa-validation-5284", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-2402", "mrqa_hotpotqa-validation-3258", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-692", "mrqa_hotpotqa-validation-2410"], "SR": 0.546875, "CSR": 0.5331811797752809, "EFR": 1.0, "Overall": 0.7146049859550562}, {"timecode": 89, "before_eval_results": {"predictions": ["Four bodies", "The son of Gabon's former president", "African National Congress Deputy President Kgalema Motlanthe,", "Ron Parris, 25, and Rob Lehr, 26,", "held for security reasons and not because of their faith.", "18th", "the monarchy's", "that he believed he was about to be attacked himself.", "Dave Bego,", "near the equator,", "U.S. senators", "\"we have more work to do,\"", "a one-shot victory in the Bob Hope Classic on the final hole to join his father as a winner of the tournament.", "that she was lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "Citizens", "death", "the Perot Foundation", "his business dealings", "plastic surgery", "Meira Kumar", "The exact cause of IBS remains unknown,", "a nearby day care center whose children are predominantly African-American.", "The Mexican military", "The FBI's Baltimore field office", "the UK", "a floating National Historic Landmark,", "Adidas,", "can be traced, in part, to a \"stressed and tired force\" made vulnerable by multiple deployments,", "Zach Efron", "he wants a \"happy ending\" to the case.", "1 percent", "4.6 million", "Miss USA Rima Fakih", "the Airbus A330-200", "the Internet", "228", "London's O2 arena,", "The Georgia Aquarium", "London Heathrow's Terminal 5.", "$7.8 million in cash", "Bob Dole,", "Section 60", "Muqtada al-Sadr,", "Leaders of more than 30 Latin American and Caribbean nations", "At least 38", "two years", "heavy turbulence", "Alwin Landry's supply vessel Damon Bankston", "Florida", "tax", "The Human Rights Watch organization", "The genome", "2010", "Manchuria", "the Black Sea", "William WymarkJacobs", "serbia", "wine", "Tsavo East National Park", "\"Lucky\"", "bananas", "magpies", "Midas", "Home Alone 2: Lost in New York"], "metric_results": {"EM": 0.5, "QA-F1": 0.5739635121544804}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5217391304347826, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4210526315789474, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.125, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1996", "mrqa_naturalquestions-validation-5449", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-1968", "mrqa_hotpotqa-validation-3474", "mrqa_triviaqa-validation-5705"], "SR": 0.5, "CSR": 0.5328125, "EFR": 1.0, "Overall": 0.7145312500000001}, {"timecode": 90, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-11678", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.8359375, "KG": 0.52890625, "before_eval_results": {"predictions": ["1974", "ball", "\"Antz\"", "\"Switzerland of England\"", "Ford Island", "the Food and Agriculture Organization", "Beno\u00eet Jacquot", "Lincoln", "World War II", "Les Temps modernes", "Agra", "1986", "\"King of Cool\"", "841", "Missouri River", "1.23 million", "32", "Clarence Nash", "2015", "James Edward Franco", "1970", "Sam Kinison", "1918", "Robert Gibson", "3730 km", "Washington State Cougars", "1939", "Forbes", "Univision", "an English Anglican cleric and theologian", "1865", "Dallas", "237", "Tennessee", "Orlando", "Oklahoma City", "Suspiria", "The School Boys", "Amble", "2007", "China", "fixed-roof", "Singapore", "balloons Street, Manchester", "\"The Orville\"", "The National League", "Anne Erin \"Annie\" Clark", "Atlantic Ocean", "Samantha Spiro", "Fort Hood, Texas", "Hong Kong", "Clarence Darrow", "Pierre Chambrin", "Instagram's own account", "(Rory) Bremner", "Edward Elgar", "geese", "Bill Klein,", "last summer.", "speed attempts", "Andrew Wyeth", "a calico", "biddy", "donated earlier this month to the U.S. Holocaust Memorial Museum"], "metric_results": {"EM": 0.625, "QA-F1": 0.7008356227106227}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-238", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5341", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-2970", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-7442", "mrqa_newsqa-validation-2420"], "SR": 0.625, "CSR": 0.5338255494505495, "EFR": 0.9166666666666666, "Overall": 0.7056453182234432}, {"timecode": 91, "before_eval_results": {"predictions": ["Leo Tolstoy", "Montana", "Tigger", "Huguenots", "Nintendo", "Lexington", "a ray", "mint", "Roxanne", "Salutatorian", "Roald Dahl", "ER", "Buffalo Bill", "a pager", "Hawaii", "Sir Isaac Newton", "Radiohead", "Cain", "Lignite", "the Vietnam War", "Algebra", "Catherine", "paul mcc McCartney", "Blondie", "Drumline", "Donnie Wahlberg", "cytokinesis", "the Unabomber", "Tom Petty", "Harry Potter", "The Sixth Sense", "New Wave", "Gin", "Santa Clause", "Prada", "Billy the Kid", "Stone", "(Cecil) Rhodes", "James Garner", "Inn", "Michelle Pfeiffer", "a double take", "Jacob Webster", "Michael Phelps", "Papua New Guinea", "Santa Ana winds", "spectroscope", "sesame", "a quart", "hock", "David Letterman", "UNICEF's global programing", "defense against rain rather than sun", "Vijaya Mulay", "andean", "Istanbul", "caspian seas", "Dante", "George Raft", "Grace O'Malley", "Zed", "Communist", "empty water bottle", "new wave rock band The Fixx"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7698660714285714}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9909", "mrqa_searchqa-validation-12917", "mrqa_searchqa-validation-5080", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-14637", "mrqa_searchqa-validation-12260", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-2465", "mrqa_searchqa-validation-13343", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-11934", "mrqa_searchqa-validation-3638", "mrqa_searchqa-validation-6925", "mrqa_searchqa-validation-11404", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-2238", "mrqa_triviaqa-validation-3479", "mrqa_hotpotqa-validation-2012"], "SR": 0.671875, "CSR": 0.5353260869565217, "EFR": 0.9523809523809523, "Overall": 0.7130882828674948}, {"timecode": 92, "before_eval_results": {"predictions": ["John Foster Dulles", "aluminum", "\" Look, Momno cavities!\"", "The New York Times", "the PATRIOT", "to pawn", "John Madden", "Hemingway", "a rubaiyat", "a federal judge", "Pattani", "a yam", "Mariner 2", "the Chunnel", "President Lincoln", "Smithfield", "Scorsese", "Poland", "Ausolus", "a bagel", "the Tabernacle", "the U.S. Open", "the Galapagos", "Boston", "Nautilus", "the Phoenicians", "parez", "a troll", "Athens", "a cologne", "Cherry Jones", "Pennsylvania", "Cotton Mather", "the Berlin Wall", "Suzanne Valadon", "Sexuality", "Elephants", "a axe", "Ernest Hemingway", "Navarre", "wheat", "Vermont", "Mending Wall", "Thomas Jefferson", "copper", "Wrigley", "rum", "towels", "Brian C. Wimes", "steel", "Zane Grey", "Barry Mann", "A turlough, or turlach", "8,850", "Tutankhamun", "Funchal", "Ruth Rendell", "Anthony Lynn", "The 2016 United States Senate election", "Martin Scorsese", "African National Congress Deputy President Kgalema Motlanthe,", "Angela Merkel", "September,", "Somali-based"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6059027777777777}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-13482", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-5947", "mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-15747", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4976", "mrqa_searchqa-validation-4760", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-1937", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-10940", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-2767", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-335", "mrqa_newsqa-validation-1382"], "SR": 0.5625, "CSR": 0.5356182795698925, "EFR": 1.0, "Overall": 0.7226705309139785}, {"timecode": 93, "before_eval_results": {"predictions": ["a sleepover", "Romeo & Juliet", "doughboy", "Georgetown", "the Dalmatian", "Cricket", "a duke", "Frontier", "a mummies", "Lot", "a hull", "aluminum", "Kevin Smith", "Jabez Stone", "Major", "the Monitor", "Louis Pasteur", "vingtaines", "Hudson", "wheat", "Edgar Allan Poe", "Edison", "Manhattan", "Curly Lambeau", "T.E. Lawrence", "Blake Lively", "an ape", "Eliot Spitzer", "flute", "the union", "a toque", "Edgar Allan Poe", "a pick-me-up", "impeachment", "Santo Domingo", "a Dagger", "Washington", "Nightingale", "'The Inside Story of the Wendy\\'s", "Harvey Milk", "butterflies", "a computer", "the devil's", "Goodyear", "corpulent", "Edinburgh", "a crumpet", "panniers", "the Great Smoky Mountains", "Walter Scott", "Hindi", "October 1986", "in the duodenum", "Anthony Hopkins", "six", "Dubai", "horse racing", "Abdul Razzak Yaqoob", "England", "Brotherly Leader", "first", "Haiti,", "whether findings revealed if Gadhafi suffered the wound in crossfire or at close-range", "Roberto Micheletti,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6043530254467755}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.8, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.15384615384615385, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-13621", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-15297", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-8858", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3704", "mrqa_searchqa-validation-3150", "mrqa_searchqa-validation-1995", "mrqa_searchqa-validation-14177", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-7436", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-6686", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-12720", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-5257", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-11673", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-221", "mrqa_hotpotqa-validation-3442", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2385"], "SR": 0.515625, "CSR": 0.535405585106383, "EFR": 1.0, "Overall": 0.7226279920212766}, {"timecode": 94, "before_eval_results": {"predictions": ["John Mayer", "Benjamin Franklin", "the Amstel River", "Constantinople", "Georgie Porgie", "Chad", "Adam", "Puerto Rico", "hearth", "Quebec", "Belshazzar", "the Cincinnati Reds", "Once", "Shelley", "China", "Frederick Douglass", "Sitka", "the Amazons", "Debussy", "a prince", "Bojangles", "Aunt Jemima", "Frank Sinatra", "a flying saucer", "Hawaii", "KLM", "(Frederick Victor) Zeller", "a carriage", "a vest", "The Adventures of Sherlock Holmes", "Ned Kelly", "a prostitution scandal", "World of Warcraft", "Shakespeare", "the Inca", "Alaska", "Sam Kinison", "the 360", "the high jump", "Champagne", "a new broom", "Danica Patrick", "an animal's pancreas", "Midway", "stars", "Henry Cisneros", "sacristy", "the Great Seal", "Rihanna", "\"24\"", "Tom Brady", "a star", "the final scene of the fourth season", "Billy Hill", "iron", "Henry Ford", "Saint Bartholomew's Day", "1943", "Battle of Britain and the Battle of Malta", "Kaep", "not", "Kaka,", "the vicious brutality which accompanied the murders of his father and brother.\"", "Lana Clarkson"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7628501400560225}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10187", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-9190", "mrqa_searchqa-validation-13912", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-15733", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-615", "mrqa_searchqa-validation-15473", "mrqa_searchqa-validation-4359", "mrqa_searchqa-validation-16221", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-13716", "mrqa_naturalquestions-validation-5305", "mrqa_triviaqa-validation-4019", "mrqa_hotpotqa-validation-5383", "mrqa_newsqa-validation-1330"], "SR": 0.71875, "CSR": 0.5373355263157895, "EFR": 0.8888888888888888, "Overall": 0.7007917580409356}, {"timecode": 95, "before_eval_results": {"predictions": ["The Buckwheat Boyz", "1998", "between the Eastern Ghats and the Bay of Bengal", "The Indian Ocean is the largest among the tropical oceans, and about 3 times faster than the warming observed in the Pacific", "The india's fastest train now called Gatiman express its ranges 160km / hour between Delhi to Agra", "Dr. Rajendra Prasad", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "no more than 4.25 inches ( 108 mm )", "the Jews", "art", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "typically the player to the dealer's right", "his guilt in killing the bird", "February 6, 2005", "a global cultural icon of France", "In the episode `` Kobol's Last ''", "Philippe Petit", "Terrell Suggs", "anion", "four", "1952", "The User State Migration Tool", "2017", "restoring someone's faith in love and family relationships", "an illustration by Everest creative Maganlal Daiya back in the 1960s", "if the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "solemniser", "1961", "36 months", "an integral membrane protein that builds up a proton gradient across a biological membrane", "11 p.m. to 3 a.m", "Ethel `` Edy '' Proctor", "Randy VanWarmer", "Isle Vierge ( 48 \u00b0 38 \u2032 23 '' N 4 \u00b0 34 \u2032 \u00b0 N 4.63972 ; - 4.57028 ) to Lands End", "Kevin Spacey", "Miller Lite", "the body - centered cubic ( BCC ) lattice", "in 1986", "their son Jack ( short for Jack - o - Lantern ) is born on Halloween 2023", "Spektor", "9pm ET ( UTC - 5 )", "in February 2017 in Japan and in March 2018 in North America and Europe", "July 21, 1861", "her attractive Tatted neighbour ( Holden Nowell )", "Muhammad Yunus", "Jules Shear", "Juliet", "Texas, Oklahoma, and the surrounding Great Plains", "Ethel Merman", "The New Croton Aqueduct", "Judith Cynthia Aline Keppel", "Kenny Everett", "Trinidad and Tobago", "Ghost", "Hawaii", "William Bradford", "Chuck Noll", "Italian Serie A", "three", "suicides", "John Henry", "compost", "Two and a Half Men", "the 2004 Paris Motor Show"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6817455828485239}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.15384615384615383, 0.23529411764705882, 1.0, 0.9428571428571428, 1.0, 0.0, 0.0, 1.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 0.5454545454545454, 0.2857142857142857, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.4999999999999999, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.36363636363636365, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9683", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-10294", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-6035", "mrqa_triviaqa-validation-4071", "mrqa_hotpotqa-validation-4553", "mrqa_newsqa-validation-2754"], "SR": 0.546875, "CSR": 0.5374348958333333, "EFR": 0.8275862068965517, "Overall": 0.688551095545977}, {"timecode": 96, "before_eval_results": {"predictions": ["trinidad", "art", "the A state worker unfurls a rainbow flag in front of the Washington state Capitol,", "Pisces", "The Law Society", "Russell Crowe", "two", "Khomeini\u2019s Iran", "Clint Eastwood", "1921", "france", "David Bowie", "Shetlands", "German state of North Rhine-Westphalia", "Volcanoes", "Porridge", "south africa", "New Orleans", "the eye", "pooh and his friends", "Ringo Starr", "John Mortimer", "bushfires", "Boise", "Danny DeVito", "Sweden", "four players", "the AllStars", "Rastafari", "trinidad", "the 800m", "Leo Tolstoy", "Sir Stirling Craufurd Moss", "liberty pole", "Yes", "Benghazi", "Brazil", "The Mary Tyler Moore Show", "Gordon Jackson", "scotlands", "Beyonce", "West Sussex", "Laputa", "Colombia", "the Addams", "rabia", "Tanzania", "Darby and Joan", "Robert Boyle", "Thailand", "Hugh Laurie", "Tokyo", "sometime between 124 and 800 CE", "the beginning of the American colonies", "1 January 1788", "guitar feedback", "Stephen Lee", "her husband", "artificial intelligence.", "North Korea is technically capable of launching a rocket in as little as two to four days,", "the Vietnam War", "white granite", "the humerus", "Pull"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6359722222222222}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4799999999999999, 0.4, 0.8, 1.0, 0.5, 0.6666666666666666, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-1554", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3638", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-6139", "mrqa_triviaqa-validation-6359", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-2617", "mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-1866", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-1433", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-1941", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-1662", "mrqa_searchqa-validation-4903"], "SR": 0.546875, "CSR": 0.5375322164948453, "EFR": 1.0, "Overall": 0.723053318298969}, {"timecode": 97, "before_eval_results": {"predictions": ["40 militants and six Pakistan soldiers dead,", "recall notices", "burgess meredith", "Nechirvan Barzani,", "Spain,", "The monarchy's end after 239 years of rule", "Kerstin and two of her brothers,", "56,", "\"Freshman Year\" experience", "more than 200.", "Princess Diana", "1831", "maintain an \"aesthetic environment\" and ensure public safety,", "Caylee Anthony,", "The elections are slated for Saturday.", "Alexandre Caizergues, of France,", "Arroyo and her husband", "\"It was terrible,", "hackers", "India", "non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "Haleigh Cummings,", "Glenn Corker, Senate president pro tempore,", "toffelmakaren.", "ended his playing career", "researchers have developed technology that makes it possible", "between 1917 and 1924", "will be available under the inverted glass pyramid of the Louvre.", "3,000", "10 percent", "$250,000", "April 22.", "a depth of about 1,300 meters in the Mediterranean Sea.", "Yusuf Saad Kamel", "Obama", "flights affected", "try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Jeffrey Jamaleldine", "as spies for more than two years,", "it was unjustifiable", "up to $50,000 for her,", "Citizens", "41,280 pounds", "Adam Lambert", "President George H.W. Bush", "allegedly members of five organized crime rings with ties to Europe, Asia, Africa and the Middle East.", "Iran's nuclear program.", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "133", "\"He tried to suppress the memories and to live as normal a life as possible;", "deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "Richard Parker", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "the stable balance of attractive and repulsive forces between atoms, when they share electrons, is known as covalent bonding", "ACC", "groszy", "roman numbers", "Hawaii", "stolperstein", "Paper", "Las Vegas", "the hypothalamus", "the orangutan", "germany"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5512533067370302}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [0.923076923076923, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9411764705882353, 1.0, 0.0, 1.0, 0.20689655172413793, 1.0, 0.7272727272727273, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.12244897959183672, 0.8, 0.12121212121212123, 1.0, 0.2666666666666667, 0.0, 1.0, 0.2553191489361702, 0.08695652173913045, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-1451", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-4055", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-7701", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-1532"], "SR": 0.421875, "CSR": 0.5363520408163265, "EFR": 1.0, "Overall": 0.7228172831632652}, {"timecode": 98, "before_eval_results": {"predictions": ["it does not", "Swedish Prime Minister Fredrik Reinfeldt", "Denver, Colorado.", "Friday,", "Iran", "Daniel Radcliffe", "Yusuf Saad Kamel", "digging at the site", "Susan Atkins,", "Pakistan's High Commission in India", "\"Wicked,\"", "magazine's Friday edition.", "Tsvangirai", "The oceans", "Asashoryu", "The plane famously landed with 155 people aboard in the frigid river waters by Capt. Chesley \"Sully\" Sullenberger", "autonomy.", "South Africa", "\"A good vegan cupcake has the power to transform everything for the better,\"", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "between 1917 and 1924", "Les Bleus", "Too many glass shards left by beer drinkers in the city center,", "ammonia.", "40-year-old", "Krishna Rajaram,", "the island's dining scene", "in the U.S.", "clogs", "Kurt Cobain's", "Turkey,", "Moody and sinister,", "United States, NATO member states, Russia and India", "Ryder Russell,", "spend billions to improve America's education, infrastructure, energy and health care systems.", "Palestinian Islamic Army,", "Haeftling,", "off the coast of Dubai", "change course", "Sri Lanka,", "eight-week", "drug cartels", "head for Italy.", "Anil Kapoor.", "\"a fantastic five episodes.\"", "38 feet", "St. Louis, Missouri.", "64,", "ultra-high-strength steel and boron", "September 21.", "order", "1977", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Hanna Alstr\u00f6m", "Nasdaq", "Van Rijn", "the kalavinka", "Little Big League", "hulder", "Lucille Ball", "Ziploc", "republic of south african", "Ivory", "Kraftwerk"], "metric_results": {"EM": 0.6875, "QA-F1": 0.725}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-452", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-2399", "mrqa_searchqa-validation-1338"], "SR": 0.6875, "CSR": 0.5378787878787878, "EFR": 1.0, "Overall": 0.7231226325757576}, {"timecode": 99, "UKR": 0.671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1232", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13865", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14527", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-622", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8108", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9502", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1713", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3633", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3883", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.845703125, "KG": 0.5125, "before_eval_results": {"predictions": ["\" Number Ones\"", "\"She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "$40 billion", "\"I am sick of life -- what can I", "10 municipal police officers", "Lifeway's 100-plus stores nationwide", "truck safer,", "2009", "two years,", "A group of college students of Pakistani background", "gasoline", "Her husband and attorney, James Whitehouse,", "North Korea,", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "Omar bin Laden,", "nude beaches.", "1-1", "Jason Chaffetz", "Martin \"Al\" Culhane,", "President Obama", "A member of the group dubbed the \"Jena 6\"", "Animal Planet", "Minerals Management Service Director Elizabeth Birnbaum", "Mandi Hamlin", "41,280", "Arlington National Cemetery's Section 60,", "Tim Clark, Matt Kuchar and Bubba Watson", "Caster Semenya", "his health", "Elisabeth Fritzl,", "in the neighboring country of Djibouti,", "123 pounds of cocaine and 4.5 pounds of heroin,", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Brazil, Argentina, Mexico, Colombia and Venezuela.", "second-degree aggravated battery.", "two", "Madonna's", "The forward's lawyer", "Turkish President Abdullah Gul.", "the children of street cleaners and firefighters.", "secure more funds", "five female pastors", "14", "South Carolina Republican Party Chairwoman Karen Floyd", "African National Congress Deputy President", "1913", "second", "Chinese President Hu Jintao", "Michael Arrington,", "asylum in Britain.", "Indian Army", "New York City", "2009", "two of whom were his sons", "Phil Mickelson", "the entire Quran", "Shanghai", "University of the District of Columbia", "a wooden Indian", "zona glomerulosa", "freezing", "Power", "Maine", "November 17, 2017"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7630843115218116}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3895", "mrqa_naturalquestions-validation-644", "mrqa_triviaqa-validation-3743", "mrqa_hotpotqa-validation-810"], "SR": 0.671875, "CSR": 0.5392187500000001, "EFR": 1.0, "Overall": 0.713859375}]}