{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=3e-5_ep=5_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=5.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=3e-5_ep=5_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2080, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "the United States", "damaged film", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "underpinning", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "only son", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "Many who had believed in Spiritualism wrote most pathet- ically", "What separates a Cyberpunk setting from a futuristic setting", "unemployment benefits"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7458733974358975}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.0, 0.16666666666666666, 0.0, 0.0, 0.25, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-7291", "mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.6875, "CSR": 0.765625, "EFR": 0.9, "Overall": 0.8328125}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "delay in the CSM caused by the fire", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61 per cent", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "It requires the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NFL", "1724 to 1725", "Two thirds of the water flow volume of the Rhine", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene welding", "war, famine, and weather", "Wesel-Datteln Canal", "TLC", "on the south side of the garden", "novel medications", "friendly and supportive", "Eero Saarinen", "Newton", "41", "communications from another planet", "The Lodger", "1954", "Thursday, September 15th 2016 C.C.", "Fondue", "Bruce Lee", "the scrum-half", "Danskin", "London - Wikipedia  London Listen/lndn/ is the capital and most populous city of England and the United... London has a diverse range of people and cultures, and more than 300", "the Old French and Latin words meant \"bloody, blood-colored\"", "New Hampshire", "Sequoyah Nuclear Plant", "a boardinghouse for beagles or borzois (6)", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7305914750957855}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0689655172413793, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-2783", "mrqa_squad-validation-9227", "mrqa_squad-validation-5525", "mrqa_squad-validation-1529", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.703125, "CSR": 0.7447916666666667, "EFR": 0.9473684210526315, "Overall": 0.8460800438596492}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "rule", "four", "San Joaquin Light & Power Building", "1972", "three", "science fiction", "behavioral and demographic", "the SNP", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "technical problems and flight delays", "the United States", "violence", "zeta", "those who proceed to secondary school or vocational training", "139th", "eight", "kinetic friction", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "private citizen", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the freedom (of the German health clinic) to provide services", "BBC HD", "Byker", "Genoa, Italy", "the United States", "Chickamauga", "a reddish-brown horse", "National Center for Physical Acoustics", "Gaius Maecenas", "Christopher", "Sweden", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "the Chicago White Stockings", "The Diary of a Young Girl", "Orwell", "the Barbizon school", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6796875}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_squad-validation-1662", "mrqa_squad-validation-9533", "mrqa_squad-validation-2088", "mrqa_squad-validation-2283", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-412"], "SR": 0.640625, "CSR": 0.71875, "EFR": 0.782608695652174, "Overall": 0.7506793478260869}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "Lord's Prayer", "$5 million", "hypersensitive response of plants against pathogen attack", "6,100.43 square kilometres (2,355.39 sq mi)", "Industry and manufacturing", "non-violent", "Parish Church of St Andrew", "1262", "New Orleans", "April 1523", "radiometric isotopes stop diffusing into and out of the crystal lattice", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Monday", "Pickawillany", "plan the physical proceedings, and to integrate those proceedings with the other parts", "the Autons with the Nestene Consciousness and Daleks in series 1, Cybermen in series 2, the Macra and the Master in series 3", "graduate and undergraduate students", "16", "standard", "Lucas\u2013Lehmer", "Level 3 Communications", "the Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "450 feet", "opera seria", "Okinawa", "pentameter", "kidney", "fried in oil", "Basin Street", "Tarsus", "Macy's", "Woody Allen", "Louisa May Alcott", "San Orleans", "Treasure Island", "the Skagway", "John Gutzon Borglum", "Absinthes", "white", "Miss You Already", "1960s", "TV", "Alistair Grant", "they had arrested Samson D'Souza, 29"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7026348039215686}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-1082", "mrqa_squad-validation-117", "mrqa_squad-validation-4932", "mrqa_squad-validation-7729", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_naturalquestions-validation-1549", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.65625, "CSR": 0.70625, "EFR": 0.8636363636363636, "Overall": 0.7849431818181818}, {"timecode": 5, "before_eval_results": {"predictions": ["ash leaf", "75,000 to 100,000 people", "By the 1970s", "Gilgamesh of Uruk and Atilla the Hun", "The individual is the final judge of right and wrong", "Hendrix v Employee Insurance Institute", "all matters that are not specifically reserved are automatically devolved to the Scottish Parliament", "SAP Center in San Jose", "one-eighth", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea", "closed", "21 to 11", "crustal rock", "to formalize a unified front in trade and negotiations with various Indians", "ten to fifteen", "the network and the connected users via leased lines", "a separate condenser", "to the North Sea", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element", "39", "Romana (Mary Tamm and Lalla Ward)", "metals", "abortion, broadcasting policy, civil service, common markets for UK goods and services, constitution, electricity, coal, oil, gas, nuclear energy, defence and national security, drug policy, employment, foreign policy and relations with Europe", "C\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "wicket", "Donner", "Colonel (Tom) Parker", "New Netherland", "Monrovia", "umpire", "Taiwan", "Omaha", "Beniamino Gigli", "Nez Perce", "George Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "Jack Bauer", "Inchon", "February 29", "beetles", "Alabama", "Bennington", "Giorgio Armani", "the 1960s", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "David F. Wherley Jr."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6353122942553089}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.07407407407407407, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-6975", "mrqa_squad-validation-457", "mrqa_squad-validation-2976", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-4829", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.515625, "CSR": 0.6744791666666667, "EFR": 0.9032258064516129, "Overall": 0.7888524865591398}, {"timecode": 6, "before_eval_results": {"predictions": ["Central Region", "Fred Singer", "north", "for Lutheran views", "Bible", "water pump", "874.3 square miles", "53% in Botswana to -40% in Bahrain", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger NFL", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "British and Europeans", "Judith Merril", "Routing a packet requires the node to look up the connection id in a table", "Von Miller", "weekly screenings of all available classic episodes", "type III secretion system", "10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "oxygen concentration is too high", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church.", "global village", "Sun City", "Freeport, Maine", "a small type of hippo", "auction community", "Statue of Liberty", "the closest living relative or relatives to a person", "the American Psychiatric Association", "Lenin", "Abilene", "National Rail Passenger Corporation", "the Pioneer Log House", "The Pianist", "Patty Duke", "the king", "a Mackintosh", "Richard Cory", "Homer J. Simpson", "South Africa", "the greyhound", "Beany and Cecil", "in a miner's hut high in the mountains of Eastern Nevada", "Trenton", "nickel", "different philosophers and statesmen have designed different lists of what they believe to be natural rights", "guitar", "margarita", "prostate cancer", "DNA's unique double-helix structure", "Andorra"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6716801572270323}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.29629629629629634, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7449", "mrqa_squad-validation-9334", "mrqa_squad-validation-87", "mrqa_squad-validation-5589", "mrqa_squad-validation-8923", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-7474"], "SR": 0.578125, "CSR": 0.6607142857142857, "EFR": 0.8148148148148148, "Overall": 0.7377645502645502}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "Works Council Directive", "Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "Denver Steelers", "McManus", "Gemini", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "high energy single terminal vacuum tube of his own design that had no target electrode", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland, terminating Tesla's relationship with Morgan", "LOVE Radio", "Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "civil disobedients", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "holy catholic", "competition", "1516", "an increase in skilled workers,", "Prudhoe Bay", "a cat's eye", "cigar", "Percy Bysshe Shelley", "Lucy Hayes", "ribonucleic acid", "Grapes of Wrath", "Eight Is Enough", "Stockholm", "Bacall", "William of Baskerville", "Thomas Paine", "sea horses", "Thor", "G4", "Thor CINEMA", "Julius Caesar", "malaria", "Adele Parks", "Hairspray", "Johann Wolfgang von Goethe", "mask", "Oneida Community", "battleships", "Sherman Antitrust Act", "hf", "Grace Zabriskie", "Harold Bierman, Jr.", "Winnie the Pooh", "Ryder Russell", "economic opportunities", "Joe Harn", "Reid's dismissal"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6175771365309408}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.11111111111111112, 0.0, 0.07407407407407407, 1.0, 0.5714285714285715, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-1407", "mrqa_squad-validation-1467", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_squad-validation-9930", "mrqa_squad-validation-7439", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-1453", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.546875, "CSR": 0.646484375, "EFR": 0.8620689655172413, "Overall": 0.7542766702586207}, {"timecode": 8, "before_eval_results": {"predictions": ["the 1970s and sometimes later", "Madison Square Garden.", "the Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "safaris, diverse climate and geography, and expansive wildlife reserves", "Silk Road", "E. W. Scripps Company", "8", "1.6 kilometres", "the deportation of the French-speaking Acadian population from the area. Monckton's forces, including companies of Rogers' Rangers, forcibly removed thousands of Acadians", "Ryan Seacrest", "a piece of paper", "buildings, infrastructure and industrial", "broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "his friendship", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "leptospirosis", "the Little Engine That Could", "a dwarf planet", "tango tango", "a cave", "bamboo", "Nevil Shute", "Livia", "Vlad III", "a rail", "ginseng", "Coffee", "Depeche Mode", "pepsi Benches Its Drinks", "a pacemaker-like device that sends electrical signals to brain areas responsible for body movement.", "Pat Sajak", "a hippopotamus", "1492", "the Madding Crowd", "(M Mikhail) Baryshnikov.", "Mars", "John Adams", "grass pollens", "a Hardmode gun", "Venice", "Mayoor", "Jimmy Durante", "Carl Sagan", "her transfer was a backstop and that she was actually put on a task force in the Middle East with Mateo Cruz ( who later became the section chief of the BAU following Strauss's death )", "General Paulus", "John Ford", "Cirque du Soleil", "a donor molecule to an acceptor molecule", "Sylvester Stallone", "the Mongol - led Yuan dynasty ( 1271 -- 1368)"], "metric_results": {"EM": 0.515625, "QA-F1": 0.601131095467033}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false], "QA-F1": [0.4, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5384615384615384, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-6058", "mrqa_squad-validation-6402", "mrqa_squad-validation-10273", "mrqa_squad-validation-2476", "mrqa_squad-validation-8864", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3478", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6321"], "SR": 0.515625, "CSR": 0.6319444444444444, "EFR": 0.9354838709677419, "Overall": 0.7837141577060931}, {"timecode": 9, "before_eval_results": {"predictions": ["Metropolitan Police Authority", "Jack Jouett", "parallel importers", "unmanned", "the Tangut relief army", "five", "governmental", "the Great Yuan", "Mario Addison", "adaptive immune system", "more than 70", "movements of nature", "1850s", "2000", "Bruno Mars", "electric lighting", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "2,700,000 sq mi", "an adjustable spring-loaded valve", "classical position variables", "the Left Hand of Darkness", "Henry Gondorff", "George Jetson", "deus ex machina", "an arboretum", "pommel horse", "President William McKinley", "PSP", "Daphne du Maurier", "Turkish", "antonyms", "the saguaro cactus", "Daughters of the American Revolution", "Morrie Schwartz", "Jimmy", "Mercury and Venus", "Tokyo", "an entry-level restaurant job", "gorillas", "the Pentagon", "oats", "4", "China", "Gone With the Wind", "A Delicate Balance", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "pancreas", "in the mid-1990s", "Hudson Bay", "Johnsonkip", "Melpomene", "Boston", "James Lofton", "people who haven't bought converters", "he was letting the likes of Mr. Clemmons out."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6144431089743589}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-664", "mrqa_squad-validation-1290", "mrqa_squad-validation-1849", "mrqa_squad-validation-4402", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_naturalquestions-validation-4124", "mrqa_triviaqa-validation-2735", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.546875, "CSR": 0.6234375, "EFR": 0.8275862068965517, "Overall": 0.7255118534482758}, {"timecode": 10, "UKR": 0.791015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.88671875, "KG": 0.465625, "before_eval_results": {"predictions": ["Mike Figgis", "around 1.7 billion years ago", "southern", "technical problems and flight delays", "the fact (Fermat's little theorem)", "Virgin Media", "unless he were removed from the school, Tesla would be killed through overwork.", "Times Square Studios", "Philip Webb and William Morris", "service to the neighbor in the common, daily vocations of this perishing world.", "Amtrak San Joaquins", "refusing to make a commitment", "regulations and directives", "already-wealthy individuals or entities", "26", "\"physical control or full-fledged colonial rule\"", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "a third group of pigments found in cyanobacteria", "a paired light source", "Hana", "an anatomic structure that serves as a place to collect or retain fluid.", "The Femme Fatale", "stability control", "a mated with the top", "the Black Death", "Aluminium", "Taylor Swift", "the Phanerozoic Era", "Africa", "Reddi-wip", "a cat", "tea", "Larry Fortensky", "oxygen", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time Magazine", "Lionel's Problem", "the Sopranos", "The Crucible", "Liston", "\"Impressionists\"", "Willa Cather", "Aida", "Walden", "lamb", "the right to Free Expression", "(intr) to take rest or recreation, as from work", "zero", "Australian & New Zealand", "Maine", "Doug Diemoz", "can easily flow from the faucet into the sink", "Hal Ashby", "John Ford", "119", "CR-X", "a skilled hacker", "Frank Ricci"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5973011363636364}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-9023", "mrqa_squad-validation-1326", "mrqa_squad-validation-7546", "mrqa_squad-validation-9734", "mrqa_squad-validation-8839", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8157", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400"], "SR": 0.515625, "CSR": 0.6136363636363636, "EFR": 0.8709677419354839, "Overall": 0.7255926961143695}, {"timecode": 11, "before_eval_results": {"predictions": ["study of rocks", "imperialist", "A plant cell which contains chloroplasts", "the vBNS came on line in April 1995 as part of a National Science Foundation (NSF) sponsored project to provide high-speed interconnection between NSF-sponsored supercomputing centers", "allowing the lander spacecraft to be used as a \"lifeboat\"", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "oppidum Ubiorum", "The entrance to studio 5 at the City Road complex", "1.7 million", "August 4, 2000", "an al Qaeda leader Abu Zubaydah", "free", "Bob Dole", "1959", "WikiLeaks", "three", "137", "the green grump", "Opryland", "Asashoryu", "Conway", "How I Met Your Mother", "three", "opium", "Chinese", "\"a crusade\" and \"Islamofascism\"", "war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "Hearst Castle", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis", "Rev. Alberto Cutie", "Aeneh Bahrami", "military trials for some Guant Bay detainees.", "opium", "Obama's race in 2008.", "Italy and Japan", "Hawass", "Arabic, French and English", "a baseball analyst", "seven", "President Jose Manuel Zelaya", "Abu Sayyaf", "four", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "Mike Meehan", "middle of the 15th century", "1966", "J. S. Bach", "Brainy", "Fitzroya cupressoides", "Stephanie Plum", "Sweeney Todd", "Andorra", "The Rise and Fall of Eliza Harris"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5755426225820962}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.9333333333333333, 1.0, 0.0, 0.10256410256410256, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-10090"], "SR": 0.515625, "CSR": 0.60546875, "EFR": 0.8709677419354839, "Overall": 0.7239591733870968}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "Prime ideals", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "Korea", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.Pennsylvania was awarded $49,297,140 for damage from Hurricane Irene", "two", "CNN", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi, who is the best-selling poet in America.", "Suwardi", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi, who is the best-selling poet in America.", "U.S. senators", "died peacefully with Mildred and two other females. Breeders are hoping he'll show interest in Lucy, who is about the same age as Mildred, later this year.", "Muslim", "California, Texas and Florida", "Robert De Niro", "Argentina", "three searches", "creation of an Islamic emirate in Gaza", "Somalia", "Patonio Maria Costa,", "Pope Benedict XVI", "he castigates U.S. policies and deplores Israel's offensive in Gaza that started in late December 2008 and continued into January", "He met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "Apple employees", "a German citizen, one of an estimated 20,500 \"green-card warriors\" in the military.", "Haiti", "The Screening Room", "Iran test-launched a rocket capable of carrying a satellite", "Nieb\u00fcll", "Juan Martin Del Potro.", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state,", "Seoul", "Wayne Michaels", "Afghanistan", "seven", "Johan Persson and Martin Schibbye", "Fix You", "Bobb McKittrick", "Ytterby", "George III", "Philadelphia, Pennsylvania", "Alien Resurrection", "Warren Joslyn", "Moscow", "A dressage horse performing at his peak levels will be calm, supple, and in complete harmony"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6103961074561404}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.15789473684210525, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3157894736842105, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_naturalquestions-validation-4193", "mrqa_hotpotqa-validation-5014", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.5625, "CSR": 0.6021634615384616, "EFR": 0.8928571428571429, "Overall": 0.7276759958791209}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I,", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation were in error", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "City of Edinburgh Council", "the son of the most-wanted man in the world", "rural California,", "Hearst Castle", "\"Larry King Live.\"", "CNN's Charlie Moore", "north coast of Puerto Rico", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane,", "Gadahn,", "the iPods", "southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "Barack Obama", "South Africa", "in his favor in 2006, granting him co-writing credits and a share of the royalties.", "Iran's nuclear program.", "North Korea", "Sunday,", "\" random events are very challenging to prevent and difficult to deal with when they occur.", "Haeftling", "i report form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\" to the case.", "San Diego", "Ralph Lauren", "At least 40", "$1,500", "43,000", "137", "suppress the memories and to live as normal a life as possible;", "Muslim and a Coptic family", "poor", "Tom Hanks", "The Louvre", "27-year-old", "165", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\" Frankie Neylon, the town's mayor said.", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Tyler, Ali, and Lydia", "Kansas", "September", "dance", "\"Inside No. 9\"", "Lusitania", "Eratosthenes", "Coronation Street", "Turkey, Saudi Arabia, and Pakistan"], "metric_results": {"EM": 0.46875, "QA-F1": 0.596937003968254}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.9333333333333333, 0.6666666666666666, 0.5, 1.0, 0.5555555555555556, 0.0, 1.0, 0.14285714285714288, 0.8, 1.0, 0.25, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2009", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-1028", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.46875, "CSR": 0.5926339285714286, "EFR": 0.8823529411764706, "Overall": 0.7236692489495798}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart,", "between September and November 1946,", "$2.50 per AC horsepower royalty", "1990s", "organic", "Stagg Field", "2010", "Reuben Townroe", "the Black Death", "a water pump,", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "in Bangladesh,", "The victims had been hit by rocks, glass bottles, birdshot and Molotov cocktails,", "bankruptcy", "Inter Milan", "98", "As soon as 2050, some scientists say.", "merit-based civil service system.", "The Ski Train", "severe", "The six bodies were found Saturday at about 6:30 p.m.", "Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "the \"surge\" strategy he implemented last year.", "the port remains shut down, and desperately needed aid cannot be unloaded quickly.", "onstage demos.", "Tim O'Connor,", "impeachment", "Kearny, New Jersey", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "the #JustSayin", "gang rape", "high tide", "killing", "genocide, crimes against humanity, and war crimes.", "The oldest documented bikinis", "Fullerton, California,", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "repeal of the military's \"don't ask, don't tell\" policy", "Consumer Reports", "a woman", "Sheikh Abu al-Nour al-Maqdessi,", "the remaining rebel strongholds in the north of Sri Lanka,", "The Everglades, known as the River of Grass,", "six-year veteran", "\"The jet, which was flying at 35,000 feet and at 521 mph, also sent a warning that it had lost pressure, the Brazilian air force spokesman added.", "ninth w\u0101", "Magnavox Odyssey", "The Overture", "robin", "Russell Humphreys", "The Guest", "\"The Longview\"; \"Welcome to Paradise\"; \"Basket Case\"; \"When I Come Around\"", "a platinum cast of a human skull", "2019", "6 January 793"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5331043956043956}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.25, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_triviaqa-validation-2022", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.453125, "CSR": 0.5833333333333333, "EFR": 0.9142857142857143, "Overall": 0.7281956845238096}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966,", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education", "proportionally", "North", "Mohammed Mohsen Zayed,", "\"we are \"still trying to absorb the impact of this week's stunning events,\"", "President Obama", "Friday,", "CNN affiliate WFTV.", "The cause of the deaths has not been determined,", "The station", "sculptures", "Atlantic Ocean.", "the 725-mile Veracruz regatta", "200.", "through Greece, the birthplace of the Olympics,", "Patrick McGoohan,", "parents", "a class A traffic violation that can command a fine of $627,", "27-year-old's", "Virgin America", "know what's important in life,", "g gossip Girl", "ketchum, Idaho.", "at my undergrad alma mater, Wake Forest,", "Sporting Lisbon", "tie salesman", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Secretary of State Hillary Clinton,", "The collider's ALICE experiment will look at how the universe formed by analyzing particle collisions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "Dancing With the Stars.", "two women killed in a stampede at one of his events in Angola on Saturday,", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "1.2 million people.", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "her mother.", "pigs", "Matt Flinders", "Isar", "East of Eden", "Sam Bettley.", "14 directly elected members, 12 indirectly elected members representing functional constituencies and 7 members appointed by the chief executive.", "the Sea of Galilee", "liquid", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.515625, "QA-F1": 0.656367184533303}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.5454545454545454, 0.9166666666666666, 0.0, 1.0, 0.5, 0.7272727272727273, 1.0, 1.0, 0.0, 0.0, 0.5, 0.28571428571428575, 1.0, 0.6666666666666666, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 1.0, 0.33333333333333337, 0.8181818181818181, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.10526315789473685, 0.8, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_hotpotqa-validation-4463", "mrqa_searchqa-validation-5504", "mrqa_triviaqa-validation-5573"], "SR": 0.515625, "CSR": 0.5791015625, "EFR": 0.8387096774193549, "Overall": 0.712234122983871}, {"timecode": 16, "before_eval_results": {"predictions": ["the fact (Fermat's little theorem) that np\u2261n (mod p) for any n if p is a prime number.", "adjustable spring-loaded valve, which is locked such that operators may not tamper with its adjustment unless a seal illegally is broken.", "George Low", "Synthetic aperture radar (SAR) and Thematic Mapper (TM)", "A fundamental error", "recant his writings.", "diversity", "can include arbitrarily many instances of 1 in any factorization,", "136,", "unions", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "Rima Fakih", "He was sentenced to five years in jail and is eligible for parole in 18 months.", "2nd Lt. Holley Wimunc.", "the Spanish flu pandemic of 1918-1919.", "a pair of Mathematics geniuses.", "U.S. Holocaust Memorial Museum,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "the first American team to win yachting's most prestigious trophy since 1992.", "U.S. senators who couldn't resist taking the vehicles for a spin.", "Between 1,000 and 2,000", "Larry Ellison,", "Taher Nunu", "President Obama", "Karen Floyd", "U.S. Chamber of Commerce.", "Kim Il Sung died", "Daniel Nestor, from Canada,", "Caylee Anthony,", "because its facilities are full.", "25 dead", "more than 200.", "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.", "South African", "in Seoul,", "Haiti", "The United States", "will be speaking to a small group of friends, colleagues and close associates.", "Daytime Emmy Lifetime Achievement Award.", "Republican", "Teen Patti", "Eleven", "Hugo Chavez.", "Four bodies", "attached to another chromosome", "starch", "the UK", "Diptera", "100th anniversary of the first \"Tour de France\" bicycle race,", "British acid techno and drum and bass electronic musician.", "fibrous tissue", "Johannes Brahms,", "by the end of the 17th century.", "Orson Welles."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6004226223655572}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.2608695652173913, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.6153846153846153, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.06666666666666668, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.19999999999999998, 0.7142857142857143, 0.0, 1.0, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3407", "mrqa_squad-validation-3975", "mrqa_squad-validation-4509", "mrqa_squad-validation-2788", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_searchqa-validation-2260", "mrqa_hotpotqa-validation-4478"], "SR": 0.484375, "CSR": 0.5735294117647058, "EFR": 0.9393939393939394, "Overall": 0.731256545231729}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "multi-cultural", "the father of the house when in his home", "John Fox", "US$1,000,000", "Annual Conference", "Colonel Monckton,", "thermodynamic", "\"CNN Moscow Correspondent at Star City, the Russian cosmonaut training facility.", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "northwestern Montana", "a delegation of American Muslim and Christian leaders", "South Africa", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "Sunday,", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Louisiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2006,", "the FBI.", "as many as 250,000 unprotected civilians", "release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "allegations that a dorm parent mistreated students at the school.", "Pakistan", "Columbia, Illinois,", "\"I'm just getting started.\"", "Pittsburgh", "flooding and debris", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "a former Navy captain whose boyish looks and deceitful ways", "a \"aesthetic environment\" and ensure public safety,", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford,", "Ginger Rogers", "five", "Marine Corps", "Garfield", "pickpocket", "seven", "a vigorous deciduous tree", "a transistor,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.726923076923077}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425"], "SR": 0.609375, "CSR": 0.5755208333333333, "EFR": 0.96, "Overall": 0.7357760416666667}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical climate (K\u00f6ppen Cfa)", "American Sign Language", "Fort Caroline,", "specialty pharmacy", "Doctor of Theology", "God's", "The Prince of P\u0142ock,", "multi-stage centrifugal pumps", "Pet Sounds", "40", "Sax Rohmer,", "Aug 24,", "\"algebra\"", "a real whale", "\u00ef\u00bf\u00bd", "Naboth", "Jeffrey Archer", "C N Trueman", "Anne Boleyn", "Golda Meyerson", "a round, slightly tapered,", "Alan Greenspan", "Thai", "Parsley the Lion", "Japan", "Runic", "plutonium", "Carlos Tevez", "blancmange", "baloney cubed", "\" automatic\" method of creative production", "recorder", "\"People\u2019s decathlon", "Microsoft", "Austria", "Isambard Kingdom Brunel", "Edward Lear", "Jamaica", "John Ford", "Petronas", "Beyonce", "Microsoft", "Charlemagne", "Praseodymium", "The Battle of the Three Emperors,", "southern Pacific Ocean,", "Trimdon, County Durham,", "Midnight Cowboy,", "Surrealist", "FIFA World Cup 2010", "Southwest Airlines,", "Afghanistan", "Matt Jones", "Rudolf H\u00f6ss", "3 May 1958", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "off Somalia's coast.", "canibalism", "Kent", "Ford Motor Company,", "Banff", "bull"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6197048611111111}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7210", "mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-3824", "mrqa_naturalquestions-validation-4731", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.546875, "CSR": 0.5740131578947368, "EFR": 0.7241379310344828, "Overall": 0.6883020927858439}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Wi-Fi or Power-line", "ash tree", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds", "Tony Blair", "The Flintstones", "9-1-1", "Jonathan Swift", "South Sudan", "Maria Bueno", "treen", "Frankie Laine", "July 28, 1948", "Thor", "Austria", "Goosnargh", "dilly griffith", "dna's structure", "Montreal", "ruda", "kippis", "Rocky and Bullwinkle Show", "Ben Drew", "muktar al-Bakri", "Poland", "Indiana Jones", "Sousa", "duke of Wellington", "Sydney", "Alabama", "Jura", "armoured car", "dike", "a meteoroid", "Norman Brookes", "bobbyjo", "lola", "Bodhidharma", "Klaus dolls", "Albert Reynolds", "gaff", "r\u00fcgen", "Singapore", "cathead", "yellow", "cat food", "Vespa", "Squamish", "65", "Theme Park World", "Cape Cod", "\"Itsy bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "10", "Tommy Tutone", "dill", "dilla", "small intestine", "Siddhartha"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5036458333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-3139"], "SR": 0.453125, "CSR": 0.56796875, "EFR": 0.7714285714285715, "Overall": 0.6965513392857143}, {"timecode": 20, "UKR": 0.76953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.859375, "KG": 0.45546875, "before_eval_results": {"predictions": ["red algal derived chloroplast", "non-specific", "1525\u201332", "a few", "solution", "2011", "random noise", "trans-Atlantic wireless telecommunications facility", "h.G. Wells", "SE Ethiopia", "the Washington Post", "honshu", "Steve Biko", "pottery", "a pennsylvanica", "acute", "soil-tilling machine", "a salt", "Kate Smith", "Norman Kingsley Mailer", "Oliver!", "Lone Ranger", "Bolton", "Hawaii", "tsarevitch", "le Roy", "junk Planet", "Hartford", "Your Excellency", "King george", "Lincoln", "River Severn", "Canada", "pon farr", "small islands in the Caribbean", "Tony Blair", "Jesse Garon Presley", "Kopassus", "lithium", "40", "fASHION", "Zig and Zag", "white", "China", "Salt Lake City,", "m  edusa", "Capricorn", "a short", "Sergio Garc\u00eda Fern\u00e1ndez", "butterfly", "a cousin, Carlo,", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Twitch Interactive,", "right-wing extremist groups.", "cantaloupes", "Heartbreak Hotel", "leopard", "Wes Craven", "Australian", "King Kelly"], "metric_results": {"EM": 0.484375, "QA-F1": 0.55390625}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2513", "mrqa_squad-validation-1384", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3117", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3822"], "SR": 0.484375, "CSR": 0.5639880952380952, "EFR": 0.8484848484848485, "Overall": 0.6993695887445888}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois architecture", "confrontational", "San Francisco Bay Area", "gold", "Chinese", "Surrey", "Telstar", "wED", "Buzz Aldrin", "l Lois", "Niger", "Backgammon", "Instagram", "Home alone", "Columbus", "T.S. Eliot", "Venus", "Bob Marley & the Wailers", "the Crusades", "jockey nicky Henderson", "curb-roof", "jagger", "dana", "mezzo-piano", "Socrates", "selenium", "Stephen King", "chestnut", "Catskill Mountains", "jeno depp", "a kilovolt", "fluid", "Jordan", "James Garner", "London", "chainsaws", "Poland", "treble clef", "forehead", "dill", "sacrament of Holy Communion", "100 years", "jerry", "Washington", "jcadilly Circus", "saffron", "Melbourne, Victoria,", "meadowbank", "Tangled", "Vincent Motorcycle Company", "Melissa Duck", "inner core", "novella", "The Prodigy", "John Anthony \"Jack\" White (n\u00e9 Gillis; July 9, 1975)", "Michelle Rounds", "21-year-old", "jimlin", "Daytona Beach", "jordan bostick", "Mickey's philharMagic", "hiphop producer"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5494318181818182}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-927", "mrqa_squad-validation-170", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-1488", "mrqa_searchqa-validation-13792", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-550"], "SR": 0.484375, "CSR": 0.5603693181818181, "EFR": 0.9696969696969697, "Overall": 0.7228882575757576}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "being drafted into the Austro-Hungarian Army", "63,523", "faith alone", "Ticonderoga Point", "seal", "Season 4", "Tyrion", "1972 -- 81", "dottie West", "May 1980", "doting wife", "the Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "2018", "Malibu, California", "variation in plants", "Baltimore, Maryland", "beginning of the American colonies", "Second Battle of Manassas", "Paspahegh Indians", "left atrium and ventricle", "Mayflower", "start with the building of local isolated wooden wagonways starting in 1560s", "Davos", "Prince James", "jazz", "2002", "U.S. service members who have died without their remains being identified", "March 16, 2018", "Narendra Modi", "Sohrai", "explosion", "heartbreak", "Annette", "The fourth season premiered on September 21, 2017,", "yowgli", "ABC", "eukaryotic cells", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "between 8.7 % and 9.1 %", "hero", "37.7", "1954", "1922 to 1991", "\"Shine", "petowo", "dawa", "Mountain West Conference", "Sydney", "yasiin Bey", "look at how the universe formed by analyzing particle collisions.", "five female pastors", "returning combat veterans", "mill on the Floss", "Antarctica", "cherry bombs"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5497903138528139}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.6666666666666666, 0.5, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.5714285714285715, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 0.6, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2919", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6295", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-2275"], "SR": 0.453125, "CSR": 0.5557065217391304, "EFR": 0.9142857142857143, "Overall": 0.7108734472049689}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "vicious and destructive", "60%", "girls", "in the 1980s", "picturebook Shiji no yukikai", "almost 3,000", "Chinese flower shop", "T'Pau", "Bud Light", "comedy web television series", "Universal Pictures and Focus Features", "LED illuminated display", "committed and effective Sultans", "If there are no repeated data values", "Mangal Pandey", "North Carolina", "the retina", "IBM", "Felicity Huffman", "Djokovic", "84", "the United States economy", "in Wales and Yorkshire", "Since 1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Nalini Negi", "very important", "the United States", "Jodie Foster", "Frederick Chiluba, Levy Mwanawasa, Rupiah Banda, Michael Sata, and current President Edgar Lungu", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "`` It Ain't Over'til It's Over ''", "Massillon Jackson High School", "white rapper B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "giant planet", "the RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "10,000 BC", "New York City", "Germany", "1961 during the Cold War", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "lung cancer", "Pakistan", "Sam Raimi", "7 October 1978", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "Mississippi", "wiki", "general contractor"], "metric_results": {"EM": 0.5, "QA-F1": 0.6164847883597884}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.0, 1.0, 0.2857142857142857, 0.8, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.1, 0.6666666666666666, 0.07407407407407407, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9047619047619047, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.5, "CSR": 0.5533854166666667, "EFR": 0.78125, "Overall": 0.6838020833333334}, {"timecode": 24, "before_eval_results": {"predictions": ["22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors. There is a strong presence of Border Reiver surnames, such as Armstrong, Charlton, Elliot, Johnstone, Kerr, Hall, Nixon, Little and Robson", "a three-stanza confession of faith prefiguring Luther's 1529 three-part explanation of the Apostles' Creed in the Small Catechism", "April 20", "Tanzania", "October 2", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "1928", "Samaria", "northern China", "Missouri River", "Hagrid", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1950, 1955, 1956, 1974, 1975, 1985, 2000", "May 3, 2005", "Jason Flemyng as Dr. Henry Jekyll / Edward Hyde", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "1977", "Cody Fern", "22 November 1970", "Reveille", "2007", "Camping World Stadium in Orlando, Florida", "Aldis Hodge", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "James", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "a single, very long DNA helix on which thousands of genes are encoded", "Philippines", "R.E.M.", "a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "Juliet compares Romeo to a rose saying that if he was not named Romeo he would still be handsome and be Juliet's love", "colonialism was coming to an end worldwide, France fashioned a semi-independent State of Vietnam", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "rocks and minerals", "various submucosal membrane sites of the body", "in Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "its vast territory was divided into several successor polities", "Tremont neighborhood of Cleveland, Ohio", "a hooker and addict", "Kingsholm Stadium and Sandy Park", "Ahmad ( Real ) selected Doll", "a `` skin - changer '', a man who could assume the form of a great black bear", "Robert Plant", "a large beetle in Mt Coot-tha.", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Bow River", "41,", "Fareed Zakaria", "Afghan National Security Forces", "John Cotton (minister)", "a Welch rabbit", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.5, "QA-F1": 0.5906639561708216}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.06666666666666667, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.11764705882352941, 0.08695652173913045, 0.15384615384615385, 1.0, 1.0, 0.0, 0.8571428571428571, 0.41379310344827586, 1.0, 0.7741935483870968, 1.0, 0.10526315789473682, 0.0, 0.0, 0.4, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_triviaqa-validation-5910", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.5, "CSR": 0.55125, "EFR": 0.84375, "Overall": 0.695875}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "8:10 p.m.", "about 5 nanometers across, arranged in rows 6.4 nanometers apart,", "1894", "the means of production", "Atlanta, Georgia", "Thunder Road", "Acid rain", "Bette Midler", "gathering money from the public, which circumvents traditional avenues of investment", "the pyloric valve", "Ben Faulks", "Julia Ormond", "anvil", "The Satavahanas", "March 16, 2018", "Hathi Jr", "by capillary action", "twice", "Asuka", "when matching regions on matching chromosomes break and then reconnect to the other chromosome", "Hathi Jr.", "the Kananaskis", "development of electronic computers in the 1950s", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Madison, Wisconsin, United States", "to weaken the British by cutting off its imports, and strike a winning below with German soldiers transferred from the Eastern front, where Russia had surrendered", "March 21, 2016", "1981", "USS Chesapeake", "arcade mode -- an offline single player or local co-op where players can choose which side to play on and which battle to play in", "repudiation, change of mind, repentance, and atonement", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Mad - Eye Moody and Hedwig", "David Seaman", "house edge of between 0.5 % and 1 %, placing blackjack among the cheapest casino table games", "Burnham Beeches in Buckinghamshire was used for the outlaws'encampment, Aysgarth Falls in Yorkshire for the fight scene between Robin and Little John, and Hardraw Force in North Yorkshire", "1898", "Clarence Anglin, John Anglin", "April 1st", "12.65 m ( 41.50 ft ) long, weighed about 21.5 t ( 47,000 lb ), and had a girth of 7 m ( 23.0 ft )", "Northeast Monsoon or Retreating Monsoon", "Michael Crawford", "1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "the 17th episode in the third season of the television series How I Met Your Mother and 61st overall", "The Parlement de Bretagne", "Joe Davis", "phosphorus", "Spencer Perceval", "a variety of ancient herding dogs, some dating back to the Roman occupation, which may have included Roman Cattle Dogs, Native Celtic Dogs and Viking Herding Spitzes", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley,", "Venezuela", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Stark County, Ohio", "Edward VI", "Dumaine"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5342750334998423}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.28571428571428575, 0.33333333333333337, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8695652173913044, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.07142857142857144, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.5, 1.0, 0.7499999999999999, 0.0, 0.4444444444444445, 0.0, 0.5, 0.6666666666666666, 1.0, 0.09523809523809525, 1.0, 1.0, 0.6666666666666666, 1.0, 0.1290322580645161, 0.47619047619047616, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-8869", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1263", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-4972"], "SR": 0.40625, "CSR": 0.5456730769230769, "EFR": 0.868421052631579, "Overall": 0.6996938259109312}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "Interstate 9", "already-wealthy individuals or entities", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "England", "virtual reality simulator", "the five - year time jump", "seven years earlier on Christmas Eve ( as the setting is Christmas Eve 1843, this would have made the date of his passing December 24, 1836 )", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions", "the outside world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "Ali Skovbye", "Pete Seeger", "Richard Stallman", "Santa Monica", "Afghanistan, Bangladesh, Bhutan, Maldives, Nepal, India, Pakistan", "December 15, 2017", "Ed Sheeran", "Johnson", "in skeletal muscle, while the remainder is distributed in the blood, brain, and other tissues", "the lumbar cistern", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "India", "Geoffrey Zakarian", "Tommy James and the Shondells", "Hammond, Louisiana", "Bonnie Aarons", "April 13, 2018", "Jay Baruchel", "a rag tag bunch of trash - talking, street-wise, inner city kids who live in the projects, where people have to sit on the floor in their apartments to avoid stray bullets", "2004", "rear - view mirror", "Portuguese and Spanish - French origins", "2011", "The terrestrial biosphere", "1937", "2017", "Beijing", "the court from its members for a three - year term", "to convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "75", "Famous Players-Lasky Corporation", "Tiffany & Company", "45th Vice President of the United States from 1993 to 2001", "villanelle", "lifeless, naked body", "lifeless, naked body", "four months ago", "magnesium", "Christopher Newport", "rotunda"], "metric_results": {"EM": 0.4375, "QA-F1": 0.579132913752777}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 0.23076923076923078, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.721311475409836, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.125, 0.25, 0.9, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.2222222222222222, 0.7272727272727273, 1.0, 1.0, 0.0, 0.4, 0.0, 0.33333333333333337, 0.5, 0.8571428571428571, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4753", "mrqa_squad-validation-7547", "mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-692", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.4375, "CSR": 0.5416666666666667, "EFR": 0.8333333333333334, "Overall": 0.691875}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "the next architect to work at the museum was Colonel (later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "the Seminole Tribe", "one out of every 17 children under 3 years old", "Tuesday", "Dan Parris, 25, and Rob Lehr, 26,", "the estate with its 18th-century sights, sounds, and scents.", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "reaffirmed commitment to lesbian, gay, bisexual and transgender Americans.", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "step up", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "Juliet", "at a Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "a small text inside the item lays out the prisoners story, including the name, where he is in jail and how long the term will be.", "blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "a national telephone survey", "did not speak to those who had gathered but shadow-boxed to spectators and cameras before meeting his distant relatives.", "Kgalema Motlanthe,", "Ankara", "Bill Stanton", "humans", "Herman Thomas", "Werder Bremen,", "a lightning strike", "Deputy Treasury Secretary", "Columbia, Illinois,", "Arizona", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "al Qaeda,", "Tom Hanks", "outside his house in Najaf's Adala neighborhood after returning from Friday prayers.", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "Both men were hospitalized and expected to survive, according to David Peterka, who was part of the film crew, but was not aboard the plane.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Briton Carl Froch", "Abdullah Gul,", "1979", "Lynne Tracy", "Lauren Blumenfeld", "Jughead Jones", "Sarah Josepha Hale", "1998", "violinist.com", "a single arrow pointing to the left and is used to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Johnny Torrio", "a cabinetmaker", "shrimp", "tentacles"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5315884058071558}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5925925925925926, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909093, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1604", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522"], "SR": 0.453125, "CSR": 0.5385044642857143, "EFR": 1.0, "Overall": 0.7245758928571429}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars", "Nepali", "German", "President Sheikh Sharif Sheikh Ahmed", "off east  Africa", "from Thursday and Friday", "Illinois Reform Commission", "gasoline", "Union Station in Denver, Colorado.", "Dolgorsuren Dagvadorj,", "not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "broke his long silence with a detailed account of that day in \"Goodbye Natalie, Goodbye Splendour,\" a book he wrote with his friend Marti Rulli.", "Kurt Cobain", "Peshawar", "The Casalesi Camorra clan", "President Clinton", "he regrets describing her as \"wacko.\"", "Nick Adenhart", "The nation's foremost concert producer, Charles Jubert,", "more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods.", "environmental", "2009", "problems with the way Britain implements European Union employment directives.", "France's", "More than 15,000", "Tens of thousands of new voters became the key", "0-0 draw", "Spain", "air support", "$249", "Amsterdam,", "Juan Martin Del Potro.", "the last person known to have seen Haleigh the night she disappeared from the family's rented mobile home.", "the finding of \"a whole new treasure hoard of fossils\"", "Israel's vice prime minister compared Iran to Nazi Germany", "Sharon Bialek", "the Kurdish militant group in Turkey", "President Obama and Britain's Prince Charles", "41,", "the job bill's controversial millionaire's surtax, which would increases taxes on those with incomes of more than $1 million.", "Sabina Guzzanti", "Booches Billiard Hall,", "More than 15,000", "21 percent", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "the presence of correctly oriented P waves on the electrocardiogram ( ECG )", "the town of Acolman, just north of Mexico City", "1973", "rugby", "rabies", "Parkinson's", "The 254th episode overall,", "Disha Patani", "Anah\u00ed Giovanna Puente Portilla de Velasco", "Gordon Brown", "The Passing of Arthur", "witchcraft"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6235171657046656}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.18181818181818182, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07407407407407408, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7142857142857143, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1368", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-2861", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.515625, "CSR": 0.5377155172413793, "EFR": 0.6129032258064516, "Overall": 0.6469987486095662}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "quarterback Denver Broncos", "teach by rote", "He was inspired to start regularly holding vegan bake sales after joining the first worldwide bake sale last year.", "The premier of \"Dance\" rated highly for Oxygen, with more than 1 million viewers tuning in.Oxygen also found success earlier with \"Mo'Nique's F.A.T. Chance,\"", "Robert Barnett,", "business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "British troops", "Jacob Zuma,", "Simon Cowell.", "jazz", "\"falling space debris,\"", "Obama", "30", "Monday night", "prison inmates.", "Franklin, Tennessee,", "British broadcasters,", "the coalition", "sexual assault on a child.", "Brian David Mitchell,", "Christmas", "football", "consumer confidence", "Republican", "normal maritime traffic", "Dean Martin, Katharine Hepburn and Spencer Tracy", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan: the IV cafe.", "the helicopter crashed in northeastern Baghdad as a result of clashes between U.S.-backed Iraqi forces and gunmen.The Iraqi officials said the area was sealed off, so they did not know casualty figures.One Iraqi official", "twice.", "The EU naval force", "chairman of the House Budget Committee,", "top designers, such as Stella McCartney,", "about 5:20 p.m. at Terminal C", "Mexico's attorney general's office responded with a statement saying that it would investigate the video and any group that tries to take justice into its own hands.", "Darrel Mohler", "Casalesi Camorra", "Obama", "Sen. Barack Obama", "steep embankment in the Angeles National Forest", "more than 30 Latin American and Caribbean nations", "Empire of the Sun", "30-minute recorded message", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday,", "Ghana", "Caylee", "The nation's congress, in consultation with the supreme court, must approve Zelaya's return to power.", "6-4 loss,", "between Glen Miller Road in Trenton and the Don Valley Parkway / Highway 402 Junction in Toronto", "the United States, its NATO allies and others", "annually in late January or early February", "Galileo Galilei", "A.D.", "paper sales company", "Chancellor Christian Kern", "Indianola", "Wayne County, Michigan", "fast 29, 2006", "emperor of Japan.", "Dorothy Parker"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5952080390779976}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.14285714285714285, 0.08333333333333333, 0.6666666666666666, 0.125, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.16, 0.3157894736842105, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14814814814814814, 1.0, 1.0, 0.4, 1.0, 0.923076923076923, 0.5454545454545454, 1.0, 0.5, 0.5, 1.0, 1.0, 0.25, 1.0, 1.0, 0.6666666666666666, 0.08695652173913043, 0.0, 0.9333333333333333, 0.4444444444444445, 0.4444444444444444, 1.0, 0.0, 0.5, 0.8, 0.0, 0.8, 0.0, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614"], "SR": 0.390625, "CSR": 0.5328125, "EFR": 0.9230769230769231, "Overall": 0.7080528846153846}, {"timecode": 30, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.79296875, "KG": 0.475, "before_eval_results": {"predictions": ["Super Bowl XX,", "undermining the communist ideology", "67.9", "letters", "Wendell, North Carolina", "Queen Mary II", "Pula Arena", "Maggie", "Google", "Boundless", "HIV", "a chela", "Jeopardy!", "The Last Starfighter", "having a tendency", "the House of Romanov", "\"No\" are not seen as weaknesses.", "fermentation", "Thomas Becket", "Morocco", "Little Red Riding Hood", "\"troubling\"--the fun part", "The Simpsons Movie", "Clara Barton", "Earhart", "Minnesota", "Geena Davis", "Han Solo", "Gutzon Borglum", "Catherine of Aragon", "Paris", "St. Mark", "Oklahoma", "Salman Rushdie", "United Nations Organisation", "Tycho Brahe", "an American sitcom that aired on CBS from February 8, 1974 to August 1, 1979", "the Interior", "elephants", "cloister", "\"to the Chief\"", "Pakistani Newspapers", "Idiot's", "Clue", "Heath", "Beautiful Rita", "Ellen Wilson", "animal cookies", "tornado", "Omaha, Nebraska", "The Greatest gift", "Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Popowo", "Bobby Kennedy", "Mercury", "a split album between Nardwuar the Human Serviette's two bands, The Evaporators, and Thee Goblins", "Nivetha Thomas", "1975", "four people believed to be illegal immigrants were found in a canyon east of San Diego.", "CEO of an engineering and construction company", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5515625}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-13866", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-1834", "mrqa_newsqa-validation-1432"], "SR": 0.484375, "CSR": 0.53125, "EFR": 0.9696969696969697, "Overall": 0.6971425189393939}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "the quotient", "Carson Palmer", "hail", "Sierra Nevada", "Florida", "the Hippocratic Oath", "Latifah", "a golden Retriever", "Shropshire", "the Aegean Sea", "nails", "a bogey", "It Can't Happen Here", "crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a conscientious objector", "Trans Alaska Pipeline", "trout", "the 13th", "Taking the Long Way", "Bob Woodward", "a buffalo", "America", "Istanbul", "Blue Horse", "a star", "\"Rehab\"", "the Golden Hind", "Administrative Professionals' Day", "Gamal Abdel Nasser", "Van Halen", "a black bear", "dams", "Djibouti", "pyrite", "a cyclone", "Ted Morgan", "cashmere", "Diana", "spilled milk", "Grasshopper", "carat", "Robin Hood", "the White Cliffs", "tendang", "September 29, 2017", "almost entirely in Wake County", "December 1800", "Nicolas Sarkozy", "Democratic", "a double whole note", "Rabies", "Environmental Protection Agency", "Robert Gibson", "Mogadishu", "cardio for 45 minutes, five days a week", "three years"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7140625}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5555555555555556, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-15945", "mrqa_searchqa-validation-9922", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.59375, "CSR": 0.533203125, "EFR": 1.0, "Overall": 0.70359375}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuro immune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "tornado", "Trump Taj Mahal", "plantain", "broiling", "John", "Liverpool", "The Andy Griffith Show", "the Bahamas", "the Mediterranean", "Celsius", "Janet Reno", "the Spanish American War", "Seinfeld", "corticosteroids", "Atlantic City", "\"Who is John Galt?\"", "President George W. Bush", "Iraq", "the taro", "Sans Souci", "\"Mr. Incredible\"", "Pyotr Ilyich Tchaikovsky", "Malle Babbe", "the Stone Age", "\"Some Things Bear Fruit\"", "Billy Pilgrim", "Louis XIII", "it wasn't meaty enough", "Prince Charles", "the Sacred Heart", "whiskers", "a cigarette lighter", "Elmer's", "the Cretaceous", "Peggy Fleming", "Panama", "the metric system", "France", "Castle Rock", "fuchsia", "the Mediterranean Sea", "George W. Bush", "The Fabulous Baker Boys", "Sinclair Lewis", "Dame Daphne du Maurier", "Starsky & Hutch", "King Willem - Alexander", "the New England Patriots", "comprehend and formulate language", "Damon Albarn", "krak\u00f3w", "Ken Burns", "the Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "along the equator between South America and Africa.", "four decades"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5268685741341992}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.8750000000000001, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-7455", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-16617", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-11721", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-9880", "mrqa_triviaqa-validation-1459", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.40625, "CSR": 0.5293560606060606, "EFR": 0.9210526315789473, "Overall": 0.6870348634370016}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual teachers", "echinos", "poker", "tuna", "Nigeria", "the Mycenaean palatial civilization", "Sulphur Island", "Thomas Merton", "ex-wife", "the phantom", "the Crystal Car Fathers Day Auto Show", "The New York Times Crossword", "47.3 years", "Dunkin' Donuts", "volcanic rock", "deor", "German", "volcanoes", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "birds", "Columbia University", "Halloween", "Sexuality", "Greece", "the Inca Empire", "contagious", "Toorop", "the mob", "New Mexico", "the French Revolution", "the Purple Heart", "Arkansas", "the 7090 mainframe computer", "\"Sorry folks, park's closed.", "wakizashi", "the sender", "Jean Lafitte", "the Komodo dragon", "Italian", "Churchill", "knitting", "Cecilia Tallis", "receipt", "Damascus", "kung", "Innsbruck", "deluge", "SeaWorld", "the back of the head", "Article Two", "Andy Cole", "Genghis Khan", "Roy Rogers", "violet", "the Great Northern Railway", "25 October 1921", "East Germany", "\"The Orchid thief\"", "shot him,", "cardiac arrest"], "metric_results": {"EM": 0.5, "QA-F1": 0.587673611111111}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-9258", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13753", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-11473", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-6427", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.5, "CSR": 0.5284926470588236, "EFR": 0.90625, "Overall": 0.6839016544117646}, {"timecode": 34, "before_eval_results": {"predictions": ["independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other", "cortisol and catecholamines", "Moon River", "\"Mighty Joe Young\"", "Leif Ericsson", "the West India Company", "\"Who is Dickens\"", "luffa", "Hershey's", "a snail", "crossword", "Muhammad Ali", "deodorant", "the Supreme Court", "the north magnetic pole", "Putin", "thunderstorms", "Kennebunkport", "a rocket", "Black Death", "Devon", "Earhart", "Hoover Dam", "Panty Raid", "French", "cricket", "\"Stagdell\"", "\"NYPD Blue\"", "Tonto", "a beaver", "white", "getting There.", "a keypunch", "the Amazons", "The Fugitive", "China", "blak-smith", "Harpers Ferry", "computer vision", "lilac", "a crossword", "Tampa", "ductile", "The Lord Chamberlain's Men", "Leo", "first anniversary", "nautilus", "salaam", "Bigfoot", "Juris Doctorate", "buy back the option for a much lower price", "The Thing", "Sebastian Lund ( Rob Kerkovich )", "Stephen Curry", "Kusha", "Mars", "Captain America", "Black Tuesday", "South America,", "1998", "Picric acid", "Nineteen", "emergency aid", "Siri"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5482886904761906}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10398", "mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-8656", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8526", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_newsqa-validation-3365"], "SR": 0.46875, "CSR": 0.5267857142857143, "EFR": 0.9117647058823529, "Overall": 0.6846632090336134}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "one", "How I Met Your Mother", "the two-state solution", "in-cabin lighting system", "little blue booties", "forgery and flying without a valid license,", "Kurdish militant group in Turkey", "the underprivileged", "end of a biology department faculty meeting", "Malawi", "fusion teams", "Her husband and attorney, James Whitehouse,", "a shortfall in their pension fund and disagreements on some work rule issues", "Muslim", "Muslim festival", "the IAAF", "Kishan Kumar,", "GospelToday", "death of cardiac arrest", "\"Drugs not only poison people, but they poison economies and governments, and it is in everyone's interest to stop this proliferation.\"", "rural Tennessee.", "The BBC", "Haiti", "a lukewarm CinemaScore", "seven", "Karen Floyd", "Expedia", "Kenneth Cole", "a much greater presence of U.S. companies in his country", "Harvard Law School", "hand-painted Swedish wooden clogs", "July for A Country Christmas,", "the bottom of the hill", "piano", "Amy Bishop Anderson,", "Jennifer Arnold and husband Bill Klein,", "her landlord", "job training", "State Department employee", "two years,", "criminal or disciplinary proceedings initiated", "Diego Maradona", "21-year-old", "bartering -- trading goods and services without exchanging money", "Rawalpindi", "the need for reconciliation in a country that endured a brutal civil war lasting nearly three decades.", "Leo Frank", "Miami", "Buddhism", "Russian bombers", "President Bill Clinton", "independently in different parts of the globe, and included a diverse range of taxa", "Sophocles", "a charbagh", "Vito Corleone", "Caribbean", "Valletta", "the Old Executive Office Building (OEOB)", "Premier League club Tottenham Hotspur and the England national team", "February 22, 1968", "Palatine", "Scones", "\"12 Years a Slave\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5692622048090799}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.12121212121212123, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.923076923076923, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.05555555555555555, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666665, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-2855", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-3265", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-5633"], "SR": 0.484375, "CSR": 0.5256076388888888, "EFR": 0.8787878787878788, "Overall": 0.6778322285353535}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "sustain future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "Vernon Forrest,", "without bail and will be arraigned June 25,", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "\"Top Gun\"", "step up.", "glass shards", "one Iraqi soldier,", "Jaipur", "Obama", "1994", "the Democratic VP candidate", "Cologne, Germany,", "34", "20,000-capacity O2 Arena.", "wrote a letter to Pakistan's High Commission in India", "Obama", "Eric Besson", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Twitter", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Evans", "Some truly mind-blowing structures are being planned for the Middle East.", "FARC rebels.", "Dan Brown", "The pilot,", "Paul McCartney and Ringo Starr", "Booches Billiard Hall,", "the National Guard reallocated reconnaissance helicopters and robotic surveillance craft", "\"She was focused so much on learning that she didn't notice,\"", "a Starbucks", "finance", "Friday.", "diagnosed with skin cancer.", "Vicente Carrillo Leyva", "the hunt for Nazi Gold and possibly the legendary Amber Room", "more than 5,600 people every year, and about 10 percent of those cases are hereditary.", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "21 percent suggesting that", "Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "at least $20 million to $30 million,", "a vigilante group whose goal is the eradication of the Zetas", "the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "the fovea centralis", "10 years", "Jeffrey Archer", "a palla", "Peter O'Toole", "Flatbush ZOMBiES", "Crane Wilbur", "Venice", "a bag", "Special Boat Teams", "Magic Johnson Jr.", "Fix You"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5671145260989011}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.125, 0.1, 0.0, 0.0, 0.4444444444444445, 0.0, 0.9846153846153847, 0.8, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-383", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1127"], "SR": 0.453125, "CSR": 0.5236486486486487, "EFR": 0.9714285714285714, "Overall": 0.695968569015444}, {"timecode": 37, "before_eval_results": {"predictions": ["in all health care settings,", "Ricardo Valles de la Rosa,", "an Italian and six Africans dead.", "Sunni Arab and Shiite tribal leaders", "Hollywood headquarters of Capitol Records,", "African National Congress Deputy President Kgalema Motlanthe,", "ferry", "1994,", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "Cain", "U.S. filmmakers", "B-movie queen", "CEO of an engineering and construction company", "the British capital's other two airports,", "40 lash after he was convicted of drinking alcohol in Sudan where he plays for first division side Al-Merreikh of Omdurman.", "take immunosuppression drugs for life so that the body does not reject the donated tissue,", "almost 9 million", "the soldiers", "NATO fighters", "low-calorie meals", "1,500", "Grayback Forestry in Medford, Oregon,", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "U.S. forces in Afghanistan are doing everything possible to free Bergdahl,", "Stuntman: Yakima Canutt", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "about 3,000 kilometers (1,900 miles)", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "nuclear", "Iran's parliament speaker", "No 4, the highest ever position", "\"services to film, theater and the arts and to activism for equal rights for the gay and lesbian community.\"", "have chosen their rides based on what their cars say", "10", "artificial intelligence.", "\"There's no chance of it being open on time. Work has basically stopped,\"", "10 percent", "April 13,", "Samuel Herr,", "London's", "Obama", "16", "Ralph Lauren", "$10 billion", "more than 2,800", "almost 2,500", "David Ben - Gurion", "Kiss", "20 years from the filing date", "Bennifer 2,", "Noises Off", "aeoline", "Mauthausen", "Delilah Rene", "first-year", "Pope John Paul II", "art deco", "the Invisible Man", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.5, "QA-F1": 0.6540122463675853}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.888888888888889, 1.0, 0.888888888888889, 1.0, 0.9090909090909091, 0.4444444444444445, 0.0, 1.0, 0.11764705882352941, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.08695652173913042, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.36363636363636365, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.5714285714285715, 0.08695652173913043, 0.18181818181818182, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.5, "CSR": 0.5230263157894737, "EFR": 0.84375, "Overall": 0.6703083881578947}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "environmental and political events.", "U.S. Holocaust Memorial Museum", "Ireland.", "At least 33 people", "2007", "heavy turbulence", "Liza Murphy", "Opryland.", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "1980", "Haiti", "Hanin Zoabi,", "Desmond Tutu", "84-year-old", "John Kiriakou.", "President George Bush", "humans", "Sylt's dining scene", "will not support the Stop Online Piracy Act,", "Vice's broadband television network.", "President Robert Mugabe's", "he'd begin sending the additional troops \"at the fastest pace possible\" starting in early 2010 \"with a goal of starting to withdraw forces from the country in July 2011.\"", "more than 30", "Brown and her family", "133 people in 26 states have been infected, according to the CDC. Additionally, a woman who was pregnant at the time of her illness had a miscarriage.", "it would investigate the video and any group that tries to take justice into its own hands.", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "1-0 win at Siena on Sunday.", "a bump on his head that turned him into a genius,", "last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and bus ticket to Johannesburg.", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic on the final hole to join his father as a winner of the tournament.", "such joint exercises between nations are not unusual. \"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "terrorists and Osama (bin Laden)", "two courses on the Black Sea coast in Bulgaria.", "first grand Slam,", "the MS Columbus", "taking a turn as the ultimate evil, the thrill of continuing a slasher legacy and why he's really not that bloody nice.", "The local Republican Party", "1 October 2006", "1834", "cell surface ( particularly caveolae internalization ) as well as at the Golgi apparatus", "guitar", "Scafell Pike", "Alzheimer's disease", "the University College of North Staffordshire", "9,984", "Smithfield, Rhode Island, U.S.", "helium", "Donna Rice Hughes", "albatross", "actor"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6018926448821329}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.11428571428571428, 0.5, 0.3333333333333333, 0.16, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5217391304347826, 0.65, 0.0, 0.2222222222222222, 0.8571428571428571, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3468", "mrqa_hotpotqa-validation-5393", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185"], "SR": 0.484375, "CSR": 0.5220352564102564, "EFR": 0.8181818181818182, "Overall": 0.664996539918415}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "Arroyo and her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week", "ties", "Addis Ababa,", "then-Sen. Obama", "ethnic Uighurs,", "Leo Frank", "the market makers", "\"It appears that they just made those numbers up,\"", "the Dancy-Power Automotive Group showroom", "the fact that the teens were charged as adults.", "Palestinian-Israeli issue", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "Saturday", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "Robert", "suicides", "\"Let it Roll: Songs by George Harrison\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades. He was extradited from the United States to Israel,", "Oprah Winfrey.", "Too many glass shards left by beer drinkers in the city center,", "over 1,000 pounds", "two satellites", "Half Moon Bay", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three out of four questioned", "$199", "at the Lindsey oil refinery", "1,300 meters in the Mediterranean Sea.", "Twitter", "Pakistan", "Thursday", "\"We are here to cooperate with anyone and everyone that will help us find the guilty party and return Lisa home safely,\"", "fluoroquinolone", "to ensure that detainees are not drugged unless there is a medical reason to do so.\"", "Empire of the Sun", "digging", "1000 square meters in forward deck space,", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001 -- 2002 season", "786 -- 802", "31 March 2018", "Muhammad Ali", "tallest building in the world", "1929", "goalkeeper", "the Secret Intelligence Service", "75 mi southeast", "a chef's salad", "grasshopper", "the Knesset", "Secretary of the Interior"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7070309157521328}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.125, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.4799999999999999, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.5263157894736842, 0.0, 0.5714285714285715, 0.7142857142857143, 0.0, 0.0, 1.0, 0.07407407407407407, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_triviaqa-validation-7335", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-667", "mrqa_searchqa-validation-6954"], "SR": 0.59375, "CSR": 0.523828125, "EFR": 0.6538461538461539, "Overall": 0.6324879807692307}, {"timecode": 40, "UKR": 0.630859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.7578125, "KG": 0.43046875, "before_eval_results": {"predictions": ["1985", "a nurse who tried to treat Jackson's insomnia with natural remedies", "20 times during the 1992 campaign.", "Austin Wuennenberg,", "in a canyon in the path of the blaze.", "machine guns and two silencers", "former Procol Harum bandmate", "CNN's Larry King", "NATO", "Lieberman", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony called police four months ago, directing them three times to same site,", "the Gulf", "Haiti.", "northwest Pakistan", "Basel", "Pyongyang and Seoul.", "\"It feels good for me to talk about her,\"", "Kurt Cobain's", "pulling on the top-knot of an opponent,", "1983", "22-10.", "Los Ticos in Cairo.", "Miss USA Rima Fakih", "\"the Democratic VP candidate delivers a big speech", "a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "Justicialist Party, or PJ by its Spanish acronym,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas in Argentina,", "86 passengers", "future relations between the Middle East and Washington.", "\"It takes one crooked prison worker to populate a whole prison unit with them,\"", "Six", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "19-year-old", "The cars run on electricity and biofuels as well as gasoline.", "The first of 1,500 Marines will be part of the initial wave of President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "\"Perfidia,\" \"Walk Don't Run '64\" and \"Diamond Head.\"", "people with snorkels typically launch into the reefs from Cairns, Queensland.", "Communist", "the journalists and the flight crew will be freed,", "Haitians", "Sri Lanka's", "he said Chaudhary's death should serve as a warning to management,", "summer", "Rev. Alberto Cutie", "since 1983.", "witnesses spotted Caylee since her disappearance.", "the content of the speech,", "\"We need a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.\"", "Afghanistan", "Tsetse also have a long proboscis, which extends directly forward and is attached by a distinct bulb to the bottom of their heads.", "1957", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Nicol Williamson", "born September 6, 1967", "Latin American culture", "Dolly Parton", "the book", "a martian", "Nippon Professional Baseball"], "metric_results": {"EM": 0.40625, "QA-F1": 0.588229931832873}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.923076923076923, 0.5, 0.0, 0.8, 1.0, 0.5, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.8750000000000001, 0.9411764705882353, 0.4, 0.25, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.14285714285714288, 0.125, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.5454545454545454, 0.11764705882352941, 1.0, 0.9743589743589743, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-4093", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_newsqa-validation-430", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-5265", "mrqa_hotpotqa-validation-5556"], "SR": 0.40625, "CSR": 0.5209603658536586, "EFR": 0.9473684210526315, "Overall": 0.657493882381258}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "Los Angeles, California,", "an 88-year-old white supremacists who stepped into the museum with a rifle and began firing.", "A Brazilian supreme court judge", "The prince, second in line to the throne, landed a Chinook helicopter", "KBR's", "The drama that pulls in the crowds", "across Greece", "monthly allowance,", "Coast Guard helicopters and boats, as well as vessels from other agencies,", "their \"Freshman Year\" experience", "Marcell J Hansen", "not guilty of affray", "the Brundell family", "outside the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago.", "\"You cannot have eradication in isolation. If we don't give them the support to produce alternative crops, then by wiping out their opium fields, you are only creating enemies for the future,\"", "sailboat", "The FBI's Baltimore field office", "Tuesday in Los Angeles.", "Honduran", "curfew in Jaipur", "The group, Lashkar-e-Jhangvi,", "Robert", "as he exercised in a park in a residential area of Mexico City,", "16 times.", "\"Toy Story\"", "the picturesque Gamla Vaster neighborhood", "Russian air force,", "an Italian and six Africans", "three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre", "Missouri.", "the Dalai Lama", "ketamine,", "Haleigh", "two and a half hours.", "Bobby Darin,", "Queen Elizabeth's birthday", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama", "an obscure story of flowers", "Kevin Kuranyi", "Kris Allen,", "on the family's blog", "2", "Supplemental oxygen", "Iran", "Batman: The Animated Series", "Tom Mix", "George Washington", "lion", "German", "Forbes", "black magic or of dealings with the devil", "cholesterol", "Orlando", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5416819748234222}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.4, 0.09523809523809525, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.8421052631578948, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-13584", "mrqa_naturalquestions-validation-8733"], "SR": 0.40625, "CSR": 0.5182291666666667, "EFR": 0.8157894736842105, "Overall": 0.6306318530701754}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "minimum viable product that addresses and solves a problem or need that exists", "Freddie Highmore", "Elvis Presley", "divergent tectonic plate boundary", "Stefanie Scott", "Tanvi Shah", "Kida", "1991", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Palmer Williams Jr.", "Chicago metropolitan area", "Coldplay", "$19.8 trillion or about 106 % of the previous 12 months of GDP", "3,000 metres ( 9,800 ft )", "Ann Gillespie", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Doc '' Brown, Ph. D.", "The cella of the Parthenon housed the chryselephantine statue of Athena Parthenos sculpted by Phidias and dedicated in 439 or 438 BC", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Fred E. Ahlert", "Institute of Chartered Accountants of India ( ICAI )", "2012", "Bette Midler", "push the food down the esophagus", "Walter Mondale", "Nick Sager", "long - standing policy of neutrality was tested on many occasions during the 1930s", "the 18th century", "Graham McTavish", "1963", "Julie Adams", "Odoacer", "Michael Madhusudan Dutta", "one", "Neal Dahlen", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Brobee", "January 15, 2007", "John Garfield", "SURFACE HEREA of ROOTS", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "geophysicists", "Billy Colman", "360 members", "November 17, 2017", "Lulu", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay.", "1932", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Evey's mother", "Switzerland", "supermodel", "\"I remember growing up in the Middle East, influenced, pondered, and paid tribute to pop legend Michael Jackson,", "surrogate", "salt", "Rocky Marciano", "consumer confidence"], "metric_results": {"EM": 0.5625, "QA-F1": 0.670131917392463}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 0.7499999999999999, 1.0, 0.19999999999999998, 0.3076923076923077, 0.4, 1.0, 0.9523809523809523, 0.5, 0.1, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5882352941176471, 0.16, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-8424", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.5625, "CSR": 0.5192587209302326, "EFR": 0.7857142857142857, "Overall": 0.6248227263289037}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational.", "A witness", "34", "Miami Beach, Florida,", "doctors thought she would adhere to medications after the surgery.", "He blogs at http://americanexception.com/William Jelani Cobb says Somalia's piracy problem was fueled by environmental and political events.", "Cash for Clunkers", "Clijsters", "it has seen \"nothing out of the ordinary\" off Haiti's coast in recent days.", "Current TV", "It is I, the chief executive officer, the one on the very top,", "Kuranyi", "Tim Clark, Matt Kuchar and Bubba Watson", "Columbia", "Omar Bongo,", "active athletes,\"", "mother.", "Madrid", "1940's", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "ketamine.", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "up three", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Larry Ellison,", "Mexican military", "Sporting Lisbon", "The Kirchners", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "in July 1999,", "Piers Morgan Tonight", "\"I could have turned everything into a crime scene like O.J.", "London's O2 arena,", "90", "Elspeth Cameron-Ritchie,", "\"Most of those who managed to survive the incident hid in a boiler room and storage closets during the rampage.", "his parents", "nearly 28 years.", "above zero (3 degrees Fahrenheit),", "Claude Monet", "Princess Diana", "Consumer Reports", "Cash for Clunkers", "nine-wicket", "Iowa,", "Plymouth Rock", "died Wednesday night from injuries he suffered in a single car wreck in Cheatham County, Tennessee.", "Michael Schumacher", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "federal matching funds", "Julia Roberts", "line code", "the Narrator", "Muffin Man", "Childeric I", "Roots: The Saga of an American Family", "Almeda Mall", "a greek cheese made from either sheeps or goats milk", "FRAM", "the Ross Ice Shelf", "Bonita Melody Lysette"], "metric_results": {"EM": 0.453125, "QA-F1": 0.574782949841912}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.7407407407407407, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.1111111111111111, 1.0, 0.9090909090909091, 0.6666666666666666, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.05555555555555555, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6792452830188679, 0.23076923076923078, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.453125, "CSR": 0.5177556818181819, "EFR": 0.9428571428571428, "Overall": 0.655950689935065}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000.", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40 militants and six Pakistan soldiers", "700", "officers", "breast cancer.", "Alfredo Astiz,", "$5.5 billion", "Suzan Hubbard, director of the Division of Adult Institutions,", "15", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally. It also targets those who hire illegal immigrant day laborers or knowingly transport them.", "The short version of my version was I heard they were doing a new \"Friday the 13th,\" and I've never tried to pursue a role before and I went,", "Sunday,", "Stuttgart", "27", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "unclear what, if any, action might be taken against the mother.", "South Africa,", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "her mom,", "a federal judge in Mississippi on March 22,", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Kim Jong Un issued his first military orders as leader of North Korea just before the death of his father was announced,", "cocaine and 4.5 pounds of heroin,", "3-0", "70,000 or so", "citizenship", "The leftist guerilla group, which goes by its Spanish acronym FARC,", "2,700-acre", "his comments", "two weeks after Black History Month", "G Chat away message.", "Wanda E Elaine Barzee.", "Molotov cocktails, rocks and glass.", "Kim Jong Un", "3,000 kilometers (1,900 miles),", "taking the product off the market would result in hardship for terminally ill patients and their caregivers,", "typically closes for two and half weeks in late summer so it can be converted into the Haunted Mansion Holiday", "The euro", "Asia", "FROMBRY", "The Bible", "Gen. Douglas MacArthur", "PlayStation 4", "ITV,", "cockfighting", "The Goonies", "Galileo Galilei", "Carson McCullers", "fearful man, all in coarse gray with a great iron on his leg...who limped and shivered, and grewled, and whose teeth chattered in his head as he seized [Pip]by the chin"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5835355236259842}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.631578947368421, 0.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727273, 0.4, 0.060606060606060615, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.0, 0.4, 0.07407407407407407, 1.0, 1.0, 0.9090909090909091, 0.08333333333333333, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-3192", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10531", "mrqa_triviaqa-validation-3284"], "SR": 0.46875, "CSR": 0.5166666666666666, "EFR": 0.9411764705882353, "Overall": 0.6553967524509805}, {"timecode": 45, "before_eval_results": {"predictions": ["sports", "1-1", "Aung San Suu Kyi", "stayed strong deep into their long runs.", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Islamist militia", "a treadmill", "Bahrain", "Piers Morgan", "Mary Phagan,", "well over two decades.", "100,000", "drowned in the Pacific Ocean", "more than a million residents who have been displaced by fighting in Somalia, including 100,000 who fled to neighboring countries last year alone, according to the United Nations.", "5-0 from the first leg,", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her;", "\"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "15-year-old's", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "what's it really like to be a new member of the world's most powerful legislature?", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "The Rosie Show", "helicopters and unmanned aerial vehicles", "racial intolerance.", "a model of sustainability.", "Rolling Stone.", "dogs who walk on ice in Alaska.", "Ralph Lauren", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"As the U.N. resolutions prohibit (North Korea) from engaging in ballistic missile activities,", "\"a striking blow to due process and the rule of law.\"", "surrender.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Lindsey Vonn", "November 26,", "Rwanda", "cancer", "The OAS, the United Nations, the European Union and the United States condemned the military-backed coup and demanded that Zelaya be reinstated.", "around 10:30 p.m. October 3,", "onto the college campus.", "200", "a full garden and pool, a tennis court, or several heli-pads.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "was depressed over a recent breakup, grabbed the gun and  took her own life.", "Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "December 2, 2013", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "Christopher Lloyd", "Nero", "Ethiopia", "Chile and Argentina.", "River Shiel", "7 miles", "Burnley", "O. Henry", "Robert Downey Jr.", "P.M.S. Blackett", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6855650252525253}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.42857142857142855, 1.0, 0.5, 0.9600000000000001, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.7142857142857143, 1.0, 0.13333333333333333, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0909090909090909, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.8363636363636363, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.25]}}, "before_error_ids": ["mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-13808", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.5625, "CSR": 0.5176630434782609, "EFR": 0.8571428571428571, "Overall": 0.6387893051242236}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "cortisone", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "the war years,", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight.", "around Ciudad Juarez, across the border from El Paso, Texas.", "former U.S. secretary of state.", "Sri Lanka,", "Communist", "Gainsbourg", "U.N.", "Ike,", "The military commissions are inherently illegitimate, unconstitutional and incapable of delivering outcomes we can trust,\"", "41,", "Tuesday,", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "al Qaeda", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "8,", "new materials", "a president who understands the world today, the future we seek and the change we need.", "in the neighboring country of Djibouti,", "in the mouth.", "over 1000 square meters in forward deck space,", "Alfredo Astiz,", "\"SHARMHAI HERE miss YOU! NOT just SHANGHAI!", "14 years", "1979", "nearly 100", "100% of its byproducts", "prostate cancer,", "EU naval force", "highest ranking former member of Saddam Hussein's regime still at large, salutes the \"People of Palestine\" and calls on them to fight back against Israel in Gaza.", "Michelle Obama", "fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "black, red or white,", "Seoul.", "try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Muqtada al-Sadr,", "Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$81,88010.", "Magyarorsz\u00e1g z\u00e1szlaja", "more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Ernest Hemingway", "n\u00famero", "Ellie Kemper", "President's Volunteer Service Award", "nursery rhyme", "Equatorial Guinea", "St. Mary's", "Holly Golightly", "Lundy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6067563797605027}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0909090909090909, 0.0, 0.0, 0.9565217391304348, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.20689655172413793, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.20689655172413793, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.7499999999999999, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-4204", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-1315"], "SR": 0.484375, "CSR": 0.5169547872340425, "EFR": 0.9393939393939394, "Overall": 0.6550978703255964}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"The Cycle of Life,\"", "\"a striking blow to due process and the rule of law.\"", "make the new truck safer, but also could make it more expensive to repair after a collision.", "200", "Alexey Pajitnov,", "1959.", "lightning strike", "Iron Eyes Cody", "at least 18 federal agents and two soldiers", "$17,000", "\"The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "Columbian mammoth fossil \"Zed.\"", "$40 billion,", "Les Bleus", "Samoa", "more than 100.", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "Portuguese water dog", "Long Island", "arrested, arraigned and jailed, with bail set at $500,000 each.", "Damon Bankston", "Fayetteville, North Carolina,", "clogs", "\"The Rough Guide to Climate Change\"", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "\"Hawaii Five-O\"", "more than 9,500 energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals. The LEDs will use the same amount of electricity as about 10 toasters,", "Deputy Treasury Secretary", "an Italian and six Africans", "supply vessel Damon Bankston was alongside Deepwater Horizon at the time of the blast.", "warning", "London", "eradication of the Zetas cartel from the state of Veracruz, Mexico,", "get out of the game,", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick. That's when I see the mud coming out of the top", "The food, music, culture and language of Latin America", "Gary Brooker", "No 4,", "Tuesday", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "Miguel Cotto", "Zac Efron", "US Airways Flight 1549", "269,000", "view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "the most recent Super Bowl champions", "the Sultanate", "czarevitch", "auk", "Tennessee", "from 1993 to 1996", "Minette Walters", "Noam Chomsky", "Linda", "photoelectric", "US"], "metric_results": {"EM": 0.5, "QA-F1": 0.6289345315254653}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.08695652173913043, 0.0, 0.8571428571428571, 1.0, 0.0, 0.19047619047619047, 0.0, 0.21428571428571425, 1.0, 1.0, 0.125, 0.0, 1.0, 0.8, 0.0, 0.962962962962963, 1.0, 0.5, 1.0, 0.0, 0.12121212121212123, 1.0, 1.0, 1.0, 0.0, 0.8, 0.22222222222222218, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7763", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_naturalquestions-validation-177"], "SR": 0.5, "CSR": 0.5166015625, "EFR": 0.6875, "Overall": 0.6046484375000001}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "The U.S. intelligence official does not believe North Korea intends to launch a long-range missile in the near future,", "Vonn", "Salt Lake City, Utah,", "actress in \"Barbarian Queen\" and \" Amazon Women on the Moon.\"", "Wake Forest,", "\"It seems an unlikely musical style for the Pittsburgh native to pursue. Enka's fan based comes generally from an older generation and is practically unknown outside of Japan, with simple song themes about love and loss.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "Peppermint oil, soluble fiber, and antispasmodic drugs", "fake his own death by crashing his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "pizza, the other for the drug ketamine.", "Kris Allen,", "her fetus were found beneath in a fire pit January 11 in Marine Cpl. Cesar Laurean's backyard.", "4-1 Serie A win at Bologna", "Haiti's", "suppress the memories and to live as normal a life as possible; the culture of his time said that he was fortunate to have survived and that he should get on with his life,\"", "1981,", "Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help as factors contributing to the suicides.", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "commander of the current space shuttle mission to upgrade the Hubble Space Telescope.", "at Hansa (Malmborgsgatan 6) and Triangeln (Sodra Forstadsgatan 41)", "it really like to be a new member of the world's most powerful legislature?", "at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "NATO fighters", "Michelle Obama", "two people, including the spokesman for an extremist group called Ansar ul Islam. Two suspects are in custody.", "$250,000", "the WBO welterweight title from Miguel Cotto", "Courtney Love,", "Hu Jintao", "Tim Cahill", "54", "\"Slumdog Millionaire,\"", "murder in the beating death of a company boss who fired them.", "African National Congress", "walk on ice in Alaska.", "Russell", "maintain an \"aesthetic environment\" and ensure public safety,", "Kansas ( 29.6 % )", "season seven", "BeBe Winans", "Sir Harry Secombe", "Claire Goose", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "deadly", "small-town rabbi", "Cheers", "Coleman Randolph Hawkins"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5727975316715915}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.7692307692307693, 0.6666666666666666, 0.75, 0.0, 1.0, 0.052631578947368425, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 1.0, 0.2, 1.0, 0.0, 1.0, 0.33333333333333337, 0.5714285714285715, 0.0, 0.0, 0.0, 0.06060606060606061, 1.0, 0.125, 0.8, 0.1739130434782609, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0606060606060606, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-212", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_searchqa-validation-12017", "mrqa_searchqa-validation-11020", "mrqa_hotpotqa-validation-864"], "SR": 0.421875, "CSR": 0.5146683673469388, "EFR": 0.918918918918919, "Overall": 0.6505455822531716}, {"timecode": 49, "before_eval_results": {"predictions": ["a delegation of American Muslim and Christian leaders", "\"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.", "35,000.", "curfew in Jaipur", "Muslim revolutionary named Malcolm X", "Four", "its nude beaches.", "The Falklands,", "between Pyongyang and Seoul", "in Japan: the IV cafe.", "Africa", "Haiti", "current and historic conflict zones,", "cancerous tumor.", "Brett Cummins,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "a racially-tinged remark made by his former caddy,", "David McKenzie", "\"If we're going to revise our policies here, we need to make it so for all the camps,\"", "Daniel Radcliffe", "\"The Da Vinci Code\"", "exotic sports", "The Da Vinci Code", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "Rwandan declared a cease-fire in", "$60 million", "4,000 credit cards and the company's \"private client\" list,", "Alison Sweeney,", "At least 33", "Carrousel du Louvre,", "137", "bartering", "Austin Wuennenberg,", "in self defense in punching businessman Marcus McGhee.", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher", "in to combat Mafia crime in southern Italy,", "\"brain hacking\"", "Saturday", "Both women", "Andy Serkis", "late 1989 and 1990", "Davos", "Malm\u00f6", "Richard Attenborough and wife Sheila Sim", "an eclipse", "\"novel with a key\"", "London", "Comanche County", "Kevin Nealon", "the Roman Catholic Church", "Tammy Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.625, "QA-F1": 0.7370969742063492}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.08333333333333333, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.625, 0.4444444444444445, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.8, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.25, 0.16666666666666669, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-6362", "mrqa_hotpotqa-validation-703", "mrqa_searchqa-validation-1891"], "SR": 0.625, "CSR": 0.516875, "EFR": 0.8333333333333334, "Overall": 0.6338697916666667}, {"timecode": 50, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.78125, "KG": 0.503125, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria.", "11", "July 1999,", "Emmy-winning", "Haiti.", "May 4", "Turkey,", "11", "Chinese Communist outfit", "\"revolution of values\"", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "Islamabad", "March 8", "female soldier, missing", "remote highway in Michoacan state,", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the Little Rock Nine,", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "10 municipal police officers", "more than 80 features to his name,", "117-111, 115-113 and 116-112.", "Arabic, French and English,", "40", "Johannesburg", "L'Aquila", "Dr. Death in Germany", "North Korea,", "at least 27", "former caddy,", "Amsterdam, in the Netherlands,", "burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "North is actually preparing to test-fire a long-range missile under the guise of a satellite launch.", "posting a $1,725 bail,", "Cal", "78,000 parents of children ages 3 to 17.", "Apple Inc.", "London's", "U.S. program to assassinate terrorists in Iraq.", "martial arts,", "couple's", "Operation Crank Call,\"", "Orwell", "Guwahati", "the winter solstice", "French", "sheep", "daisy", "1853", "Musicology", "1902", "The State of Alaska", "Baylen,", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6502819069636504}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9743589743589743, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.25, 0.8181818181818181, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.10526315789473684, 0.0, 0.0, 0.875, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-306", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1384", "mrqa_triviaqa-validation-7329", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.53125, "CSR": 0.517156862745098, "EFR": 0.6333333333333333, "Overall": 0.6373636642156862}, {"timecode": 51, "before_eval_results": {"predictions": ["U.N. charter allowing military action in self-defense against its largely lawless neighbor.", "\"disagreements\" with the Port Authority of New York and New Jersey,", "New Zealand and not in the bubble of Wales", "\"revolution of values\"", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali", "meter reader", "Diego Maradona", "London and Buenos Aires", "near Grand Ronde, Oregon.", "in rural Tennessee.", "Fakih", "as", "14", "Former Mobile County Circuit Judge", "18", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook tablet", "Amado Carrillo Fuentes,\"", "Dolgorsuren Dagvadorj,", "\"I haven't seen any violence. I know [Wimunc's husband] was not living here anymore, but that's all I know,\"", "41,", "the comedy \"Badhaai Ho Badhaai,\"", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "pirates", "the estate with its 18th-century sights, sounds, and scents.", "Olivia", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "the helicopter went down in Talbiya,", "served in the military,", "helicopters and unmanned aerial vehicles", "the prime minister's handling of the L'Aquila earthquake,", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "nearly 92 %", "Charlton Heston", "administrative supervision over all courts and the personnel thereof ''", "L\u00ea L\u1ee3i", "national militia", "Monopoly", "fourth-largest media group", "Kentucky, Virginia, and Tennessee.", "1999", "beans", "Mountain Dew", "fat", "Japan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6480435924369747}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.4, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8571428571428571, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.11764705882352941, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_hotpotqa-validation-2623", "mrqa_searchqa-validation-13313"], "SR": 0.5625, "CSR": 0.5180288461538461, "EFR": 0.8928571428571429, "Overall": 0.6894428228021978}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "education", "$55.7 million", "A family friend of a U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "\"in bad condition at the scene,\"", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "United", "planned attacks", "\"falling space debris,\"", "Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides, but certainly we will look at every opportunity,\" Collier County Sheriff Kevin Rambosk", "Kingman Regional Medical Center,", "bronze medal in the women's figure skating final,", "Long Island", "5,600", "\"I get positive feedback because everybody around me likes Obama,\"", "\"Sharon Bialek is the first woman to publicly accuse Cain of sexual harassment after last week's disclosure that the National Restaurant Association", "\"One man, Brad Blauser,", "two", "humans", "Muslim", "certain pieces of evidence presented by prosecutors were prejudicial", "Kevin Evans", "near the Somali coast", "in the $24,000-30,000 price range.", "2008,", "killing rampage.", "\"Twilight\" book series.", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "Obama and McCain camps", "flooding", "\"Five members of the Mohler family of Lafayette County, Missouri, were arrested earlier this week after six alleged victims, who are relatives of the five suspects, made accusations of sexual abuse.", "\"The sole survivor of the crash that killed Princess Diana has told a court he still cannot remember the incident but does not support the conspiracy theories surrounding it.", "Dubai", "June 6, 1944,", "\"surge\" strategy", "Free skiing Michigan Technological University", "true", "reproductive system", "Aidan Gallagher", "Rebecca Adlington", "Bedfordshire", "15.", "Consigliere", "2007", "The entity", "TriviaBistro.com", "a 1992 American erotic thriller film based on John Lutz's novel SWF Seeks Same.", "launch one ship.", "northern latitudes"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5736234322444351}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 0.12500000000000003, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5581395348837209, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2424242424242424, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.484375, "CSR": 0.5173938679245282, "EFR": 0.8484848484848485, "Overall": 0.6804413682818753}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lance Cpl. Maria Lauterbach", "his version that he acted in self defense in punching businessman Marcus McGhee.", "Argentina", "Ferraris, a Lamborghini and an Acura NSX", "death", "1983", "the simple puzzle video game,", "\"Dancing With the Stars.\"", "African National Congress", "across Greece Monday in a third day of rioting over Saturday's killing of a 15-year-old boy that has left dozens injured and scores of properties destroyed.", "morphine", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "when he failed to return home,", "Jiverly Wong,", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday's", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "$10 billion", "Mokotedi Mpshe,", "April 22.", "Mitt Romney", "twice.", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Mary Phagan,", "pesos", "judge", "Herman Cain,", "60 euros", "Michigan,", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "in the north of Sri Lanka,", "Nafees A. Syed,", "Sunday", "a share in the royalties for the tune.", "Vicente Carrillo Leyva,", "in a canyon in the path of the blaze Thursday.", "a number of calls,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "Henry Higgins", "shoes", "Herbert Lom", "the Japanese conquest of Burma.", "Shawnee Mission Parkway", "Nick Hornby", "the Intrigue card", "siegfried", "Rhonda Revelle", "Kwame Nkrumah"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6682299777428454}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.2857142857142857, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8823529411764706, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_hotpotqa-validation-1265", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-8941"], "SR": 0.5625, "CSR": 0.5182291666666667, "EFR": 0.8928571428571429, "Overall": 0.689482886904762}, {"timecode": 54, "before_eval_results": {"predictions": ["$50", "diabetes and hypertension,", "Jet Republic,", "Muslim", "at least 27", "last week,", "Peru's", "Joan Rivers", "\"Watchmen's\"", "sovereignty over them.", "NATO's Membership Action Plan,", "Bangladesh", "250,000 unprotected", "complicated and deeply flawed", "had Thilan Samaraweera caught in the leg-trap for one just before the tea interval.", "Jenny Sanford,", "\"build a fortress around America; to stop trading with other countries, shut down immigration, and rely on old industries.\"", "voluntary voluntary voluntary manslaughter", "dancing", "South Africa", "The noose incident", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "head injury.", "500 feet down an embankment", "Marxist guerrillas", "1918-19.", "Rwanda is now considered one of the central African nation.", "The UNHCR", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "6-2 6-1", "\"I wanted to come here, and I wanted to see my kids graduate from this school district.\"", "CNN", "Jobs", "bribing other wrestlers to lose bouts,", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "every soccer game and knowing what his kids like to eat for breakfast,\"", "Sasha and Malia", "Tuesday,", "Stuntman: Wayne Michaels", "The UNHCR recommended against granting asylum,", "Al-Shabaab", "his health", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "2017", "March 29, 2018", "quartz", "Kursk", "squash", "Caroline Garcia", "Caesars Entertainment Corporation.", "Premier League club Manchester United", "March", "Eudora Welty", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5987973707403056}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.16216216216216214, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.53125, "CSR": 0.5184659090909092, "EFR": 0.8, "Overall": 0.6709588068181818}, {"timecode": 55, "before_eval_results": {"predictions": ["gun charges,", "Republicans, for the most part, have stuck with Bush on the war.", "Mexico", "The worst snowstorm to hit Britain", "five", "customers are lining up for vitamin injections", "a thorough understanding of the dogs' needs,", "writer and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "\"We want to reset our relationship and so we will do it together.'\"", "Cambodian territory", "general astonishment", "65 years ago", "\"If you have a massive electrical problem it's possible that you could cut off all the commands out to the control surfaces,\"", "twice", "Sen. Barack Obama", "money or other discreet aid", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Steve Williams", "preserved corpses having sex", "Elisabeth", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw", "the 3rd District of Utah.", "Steve Williams", "organizing the distribution of wheelchairs,", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "\"She was focused so much on learning that she didn't notice,\"", "NASA's latest missions indicate the moon is much more than a dead, unchanging satellite orbiting Earth.", "punish participants in this week's bloody mutiny,", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "Alaska or Hawaii.", "Robert Park", "Djibouti,", "many people enthusiastically embraced it as a wonder drug. Dr. Kevin Ault, associate professor of gynecology and obstetrics at Emory University's School of Medicine,", "Six", "Bahrain", "delivers a big speech", "Facebook and Google,", "Somali", "2006,", "18th", "March 24,", "The father of Haleigh Cummings,", "a senior at Stetson University studying computer science.", "Saturday,", "NATO fighters", "\"Empire of the Sun,\"", "New Zealand", "\"Reusable Lessons - Step onto the campus of a school that's a model of sustainability.", "The Jewel of the Nile", "summer", "79", "furniture", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World", "Frank Sinatra", "mass", "a snout beetle", "Sir Kingsley Wood"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5991443452380952}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.8750000000000001, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.42857142857142855, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2418", "mrqa_naturalquestions-validation-10114", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.546875, "CSR": 0.5189732142857143, "EFR": 0.8620689655172413, "Overall": 0.683474060960591}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "those traveling near the Somali coast", "\"To My Mother\"", "the burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "Worry Free Dinners,", "Rod Blagojevich,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "bodyguard Trevor Rees", "the most-wanted man in the world", "the Carrousel du Louvre,", "suicide vests", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "Washington State's", "shows the world that you love the environment and hate using fuel,\"", "The apartment building collapsed together with two other buildings on March 3.", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Inter Milan", "strife in Somalia,", "cancerous tumor.", "provided Syria and Iraq 500 cubic meters of water a second,", "Abdullah Gul,", "three empty vodka bottles,", "11th year in a row.", "the journalists and the flight crew will be freed,", "CNN's Campbell Brown", "national telephone", "the thoroughness of the officers involved as they were acting to protect the passengers and crews of the flights departing Lubbock that day.", "the AR-15 and two other rifles and left the cabin.", "Ben Roethlisberger", "Oracle", "Newcastle", "228", "it's historical, inspiring, creative, romantic and beautiful.", "gasoline", "in a Utah jail", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "militants", "July 18, 1994,", "a Celtic people living in northern Asia Minor", "diastema ( plural diastemata )", "to manage the characteristics of the beer's head", "cryogenics", "Cambridge", "Mercury", "13 October 1958", "bassline", "omnisexuality", "\"Invisibility\"", "(1784-1850)", "Battlestar Galactica", "m Marilyn Monroe"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6609158202908203}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.2222222222222222, 0.6153846153846153, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2727272727272727, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-6999", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-2826", "mrqa_searchqa-validation-10329", "mrqa_searchqa-validation-2760", "mrqa_triviaqa-validation-3538"], "SR": 0.5625, "CSR": 0.5197368421052632, "EFR": 0.8928571428571429, "Overall": 0.6897844219924811}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "\"Mad Men\"", "5,600", "the European Commission", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "Madonna", "did not speak", "\"Draquila", "al Qaeda,", "U.S. Chamber of Commerce", "Carol Browner", "U.N. Security Council", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "the actor who created one of British television's most surreal thrillers,", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle", "a U.S. helicopter crashed in northeastern Baghdad as", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "an acid attack", "Congress", "southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "The model set up the foundation after her near-death experience", "South Africa", "Somali,", "returning combat veterans could be recruited by right-wing extremist groups.", "opposition supporters in Libreville, Gabon.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "the iPods", "a treadmill", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "the RheinEnergieStadion.", "free milk.", "the 45th-ranked women's player according to the World Tennis Association, qualified to compete in this week's Barclays Dubai Tennis Championships", "No. 1", "Republican Gov. Jan Brewer.", "Boundary County, Idaho, which borders Canada and abuts the area where the attack took place.", "securities", "$150 billion", "the Berlin School of experimental", "Michael Crawford", "beginning of the American colonies", "other countries", "Fenn Street School", "external ear canal", "Australian", "Argentinian", "a fibre optic cable", "rap", "inducere", "Harvard", "129,007,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7291508245044429}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.5833333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473684, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.4, 0.0, 1.0, 0.5, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-2824", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3677", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174"], "SR": 0.65625, "CSR": 0.5220905172413793, "EFR": 0.7727272727272727, "Overall": 0.6662291829937305}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "A planned missile defense system in Eastern Europe poses no threat to Russia,", "Six people", "rarely seen portrait of Michael Jackson is on display inside a Harlem luxury car dealership. Macky Dancy, a partner at Dancy-Power Automotive, said, he received a phone call from representatives of Livingstone-Strong,", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers dead,", "a good opening bit and a couple surprise things throughout. We want to balance respecting the show and the doling out of the awards with the sort of random things that will keep the audience's attention in other ways.", "Arthur E. Morgan III,", "Jason Chaffetz is a freshman Republican congressman representing the 3rd District of Utah.", "\"a very thorough, 78-page decision by the district court\" and followed an established precedent.", "Casey Anthony,", "The Ski Train", "bronze medal in the women's figure skating final,", "No 4, the highest ever position", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"a chicken soaked in the rain,\"", "President Obama.", "Jacob Zuma,", "December 7, 1941", "help rebuild the nation's highways, bridges and other public-use facilities.", "18", "into the Southeast, with a cold high-pressure center expected to remain over the area through Saturday, the National Weather Service said.", "\"Up,\"", "more disposable income: used luxury cars.", "that fascinating transformation that takes place when carving a pumpkin. Instead of picking up a few pumpkins from the grocery store, however, we decided to work on six giant pumpkins, specially delivered from nearby Half Moon Bay", "school, their books burned,", "a motor scooter", "learn in safer surroundings.", "$50 less,", "J.Crew outfits", "$106.5 million", "Nearly eight in 10", "credit card information", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July for A Country Christmas, and the festivities run from mid-November until the holidays end.", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\"", "\"black box\" label warning", "that drugs are funding the insurgency, NATO has a self-interest in supporting Afghan forces in destroying drug labs, markets and convoys,\"", "Olivia Newton-John", "Virgin America", "her husband, and she's there,\"", "It's so weird. There's two different versions of my version of how it went about, and there's the producer's version.", "Kenyan and Somali", "drug trade,", "Wednesday evening", "an angry mob.", "Africa", "Osama bin Laden.", "left - sided heart failure", "After Shawn's kidnapping, Juliet goes to his apartment with Gus to search for clues", "Optimus", "Madness", "(Jelly Roll) Morton (ca. October 20, 1890 - July 10, 1941)", "vice-admiral", "Lawrence Mikan, Jr. (June 18, 1924 \u2013 June 1, 2005), nicknamed Mr. Basketball,", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy", "United We Stand, Divided We Fall", "Professor Henry Higgins"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6220910530593879}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true], "QA-F1": [0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.05405405405405405, 0.0, 0.923076923076923, 0.0, 0.4, 0.3076923076923077, 0.7777777777777778, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9411764705882353, 1.0, 0.1111111111111111, 1.0, 0.0, 0.1111111111111111, 0.4, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 0.0, 1.0, 0.8, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.12903225806451615, 0.0, 1.0, 0.0, 0.918918918918919, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4615384615384615, 1.0, 0.3529411764705882, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3829", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2193", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-3611", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951"], "SR": 0.4375, "CSR": 0.5206567796610169, "EFR": 0.8611111111111112, "Overall": 0.6836192031544256}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings for possible securities violations", "1913,", "$40 and a loaf of bread.", "14-day", "U Win Tin,", "543 elected", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "11 healthy eggs", "four", "64,", "The mammoth's fossil was found along with 16 other deposits at the site that paleontologists \"tree-boxed\" along with the surrounding dirt, creating 23 massive crate weighing between 5 and 53 tons", "at least two and a half hours.", "partially submerged in a stream in shark River Park in Monmouth County", "improve the environment", "the Obama girls from Sen. Ted Kennedy.", "Kurt Cobain's", "More than 15,000", "0300 (All times GMT)", "Nafees A. Syed,", "CNN's \"Piers Morgan Tonight\"", "Illness", "Basel", "\"It feels good for me to talk about her,\"", "Strategic Arms Reduction Treaty and nonproliferation.", "sumo wrestling", "10 below", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall notices", "Roy", "Grayback forest-firefighters", "Kerstin", "Marxist guerrillas", "Greeley, Colorado,", "five", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "an unprecedented wave of buying amid the elections.", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "the Muslim north of Sudan", "18 federal agents and two soldiers", "Bahrain", "33", "Kenneth Cole", "Devastator", "Brazil", "Theodore Roosevelt", "vice-admiral", "Phillies", "Jape", "Greek-American", "feats of exploration.", "uncle", "Monarch", "Yale", "Nixon", "Lo\u00efc Duval"], "metric_results": {"EM": 0.546875, "QA-F1": 0.64039548992674}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false], "QA-F1": [0.6, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9, 0.3076923076923077, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.08, 0.6666666666666666, 0.0, 0.0, 0.3, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2588", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2817", "mrqa_triviaqa-validation-105", "mrqa_triviaqa-validation-2582", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.546875, "CSR": 0.52109375, "EFR": 0.8620689655172413, "Overall": 0.6838981681034483}, {"timecode": 60, "UKR": 0.69140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.771484375, "KG": 0.46796875, "before_eval_results": {"predictions": ["183", "Ed McMahon,", "fastest circumnavigation of the globe in a powerboat", "\"several pieces of aircraft equipment were at fault or had broken down.\"", "Paul McCartney and Ringo Starr", "ballots", "\"Smoke and dust fill the platform area at Liverpool Street Station on July 7, 2005, after a bomb exploded.", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "Iran's", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Sixteen", "Clinton", "Matthew Chance", "34", "five victims by helicopter, one who died, two in critical condition and two in serious condition.", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South Africa", "Vertikal-T,", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "\"It hurts my heart to see him in pain, but it enlightenedens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "authorizing killings and kidnappings by paramilitary death squads.", "Ozzy Osbourne", "it is not just $3 billion of new money into the economy.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "he wants a \"happy ending\" to the case.", "Obama and McCain camps", "Africa", "in a hotel,", "the only goal of the game", "France", "workers", "U.S. security coordinator and chief of the Office of Military Cooperation.\"", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Pipeline Express.\"", "Islamabad", "man's lifeless, naked body", "Conway", "the roof", "Lalo Schifrin", "April 17, 1982", "Billy Idol", "sexual arousal", "Ken Clarke", "every ten years since 1801,", "five", "\"The Dragon\"", "1994", "a magnolia", "1st August", "Jupiter", "mural"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6422679151169963}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.6666666666666666, 0.4, 0.4210526315789474, 0.0606060606060606, 0.0, 1.0, 0.2105263157894737, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.888888888888889, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2768", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5502", "mrqa_triviaqa-validation-1114", "mrqa_hotpotqa-validation-1812", "mrqa_searchqa-validation-16357"], "SR": 0.546875, "CSR": 0.521516393442623, "EFR": 0.7931034482758621, "Overall": 0.649095843343697}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "South Dakota State Penitentiary", "$8.8 million", "Friday,", "the 11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "a drug lord with ties to paramilitary groups,", "a baseball bat", "six", "a book.", "Venezuela", "Kerstin Fritzl,", "$1.45 billion", "Iranian consulate,", "VoteWoz.com", "Janet Napolitano", "Malawi.", "Harry Potter star Daniel Radcliffe", "the attacks", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Shanghai", "the BBC's central London offices", "reduced their carbon footprint by 132 tons.", "an engineering and construction company", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "her performance,", "9:20 p.m. ET Wednesday.", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "people who think things are going very badly has dropped from 40 percent in December to 32 percent now,\"", "The island's dining scene", "carving a pumpkin.", "prisoners at the South Dakota State Penitentiary", "Intensifying violence, food shortages and widespread drought", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "that he spent the first night in his car.\"", "hiring veterans as well as job training for all service members leaving the military.", "his home was destroyed and his business is shattered,", "the UK", "\"We need a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.\"", "has a thicker consistency and a deeper flavour than sauce", "skeletal muscle and the brain", "1985 -- 1993", "Dublin", "Goldfinger", "Lidice", "Baltimore", "Wynonna", "is one of the youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure.", "the Italian occupation", "a hog", "Canada", "Bolton"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6512047318525513}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.18181818181818182, 1.0, 0.4, 0.2857142857142857, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 0.3529411764705882, 0.09999999999999999, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-431", "mrqa_naturalquestions-validation-2943", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.5625, "CSR": 0.5221774193548387, "EFR": 0.9285714285714286, "Overall": 0.6763216445852535}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "the sperm and ova", "Michael Buffer", "pH ( / pi\u02d0\u02c8e\u026at\u0283 / ) ( potential of hydrogen ) is a logarithmic scale used to specify the acidity or basicity of an aqueous solution", "16,801 students in 12 separate colleges / schools, including the Leonard M. Miller School of Medicine in Miami's Health District, a law school on the main campus, and the Rosenstiel School of Marine and Atmospheric Science", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "in the 1820s", "the Tigris and Euphrates rivers", "third", "Andrew Garfield", "The Fixx", "in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "4 in ( 10 cm )", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison, his former bandmate from the Beatles", "Kristy Swanson", "the appointment tends to be from within the Bank, with the incumbent grooming his or her successor", "mathematical model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "Vasoepididymostomy", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "C\u03bc and C\u03b4", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions forms the joint venture Bullwinkle Studios", "Tbilisi, Capital of Georgia", "dry lake beds northeast of Los Angeles", "autopistas, or tolled ( quota ) highways", "issuance of birth certificates is a function of the Vital Records Office of the states, capital district, territories and former territories", "outside cultivated areas", "Frank Theodore `` Ted '' Levine", "IIII", "extremely slowly in the absence of a catalyst", "The Maginot Line", "Gustav Bauer", "James Watson and Francis Crick", "Franklin Roosevelt", "card verification number", "unbiased relationships between exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Sondheim", "Tasmania", "Laura Robson", "Afghanistan.", "Todd McFarlane,", "Massachusetts", "one", "\"significant skeletal remains\" consistent with those of a small child on the outer perimeter of the", "The forward's lawyer", "the giant mega-yacht 'Wally Island'", "maple syrup", "palate", "loco", "December 1974"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6104877085326544}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.12121212121212122, 0.9166666666666666, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.9411764705882353, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.13793103448275862, 0.4, 0.0, 0.33333333333333337, 0.4347826086956522, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-8026", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-5652"], "SR": 0.453125, "CSR": 0.5210813492063492, "EFR": 0.8857142857142857, "Overall": 0.667531001984127}, {"timecode": 63, "before_eval_results": {"predictions": ["Lady Agnes", "Coriolis force", "1776", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile ( 1.6 km )", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Jessica Sanders", "Article 1, Section 2, Clause 3", "Rick Rude", "November 2, 2010", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "a jackpot share", "Mark Lowry", "1877", "24 judges, against a maximum possible strength of 31", "c. 1000 AD", "a bow bridge with 16 arches shielded by ice guards", "Dick Rutan and Jeana Yeager", "near major hotels and in the parking areas of major Chinese supermarkets", "July 1790", "King Saud University", "Hugo Weaving", "Book of Exodus", "is a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Toby Kebbell", "the end of 1066", "James", "Stefanie Scott", "amino acids glycine and arginine", "the art of the Persian Safavid dynasty from 1501 to 1722, in present - day Iran and Caucasia", "Stephen A. Douglas", "the Dolby Theatre in Hollywood, Los Angeles, California", "24 -- 3", "The Republic of Tecala", "during prophase I of meiosis", "July -- October 2012", "Andy Serkis", "ancient cult activity", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1990", "dollhouse", "Berlin", "Marjorie McGinnis", "Saxe-Coburg and Gotha", "fourth-ranking", "Anne Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone: The Movie", "\"Mulholland Drive,\"", "the No Child Left Behind Act", "part of the proceeds"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5957649988899989}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.15384615384615385, 0.0, 0.0, 0.2222222222222222, 1.0, 0.3636363636363636, 0.5714285714285715, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.15384615384615383, 0.4, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8, 0.0, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-171", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-311", "mrqa_searchqa-validation-7843", "mrqa_searchqa-validation-7607"], "SR": 0.46875, "CSR": 0.520263671875, "EFR": 0.8529411764705882, "Overall": 0.6608128446691176}, {"timecode": 64, "before_eval_results": {"predictions": ["winter solstice", "19 July 1990", "senators", "Rex Harrison", "maquila", "Turducken", "Patrick Warburton", "chief priests", "1936", "President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "The hitchhiking scene with Elvis and Gary Lockwood was filmed near Camarillo, California, as were some of the flying scenes", "Yuzuru Hanyu", "Tracy McConnell", "Kenny Rogers", "The small intestine or small intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "Rumplestiltskin", "Sylvester Stallone", "from 35 to 40 hours per week", "Naomi", "belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all and in all ''", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "1996", "The first feature film originally presented as a talkie was The Jazz Singer, released in October 1927", "Far Away", "Sarah Silverman", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves on the electrocardiogram", "Brenda", "After the Battle of Culloden", "Cyanea capillata", "Bonnie Lipton", "2009", "Tom Brady", "Dawn French", "a universal translator", "Ut\u00f8ya island", "125 lb", "Old World fossil representatives", "1992", "National Infrastructure Program,", "North Korea", "\"E! News\"", "Carbon", "former presidents", "The Greatest Show on Earth", "mary Stuart"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6679543641170302}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.8000000000000002, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 0.14634146341463417, 1.0, 0.0, 1.0, 1.0, 0.0, 0.10526315789473682, 1.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4435", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-7290", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-2069", "mrqa_newsqa-validation-3451", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.5625, "CSR": 0.5209134615384615, "EFR": 0.8571428571428571, "Overall": 0.6617831387362637}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "In `` The Crossing ''", "The first installment, Divergent ( 2014 )", "Jocelyn Flores", "1956", "November 25, 2002", "zinc", "Pebe Sebert", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae,", "Lesley Gore", "Paul", "a comic book series", "the warships and the naval bases of the belligerents", "ingredients", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "December 1, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "Melanie Martinez", "the Director of National Intelligence,", "Liam Cunningham", "Elliot Scheiner", "a cylinder of glass or plastic that runs along the fiber's length", "Ace", "Goths", "the only acid excreted as a gas by the lungs", "StubHub Center in Carson, California", "Mayor Hudnut", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea", "Norman given name Robert", "the start of the 20th century", "Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "performance marker", "following the 2017 season", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "Columbia River Gorge", "Setsuko Thurlow", "John Joseph Patrick Ryan", "In 1936", "Luke 6 : 12 -- 16, and Acts 1 : 13", "Ric Flair", "Around 1200, Tahitian explorers found and began settling the area. This began the rise of the Hawaiian civilization. It remained isolated from the rest of the world for another 500 years", "Pangaea or Pangea ( / p\u00e6n\u02c8d\u0292i\u02d0\u0259 / ) was a supercontinent to have existed and the first to be reconstructed by geologists", "2008 NBA Finals", "Adam Werritty", "the Jets", "\u201cThe Seven Year Itch\u201d", "Kim Jong-hyun", "Alphonso", "The flagship store on London's Oxford Street", "\"Most of my friends have put in at least a couple hours,\"", "tax incentives for businesses hiring veterans as", "Arnold Drummond", "Nixon", "Great Expectations", "cathode", "\"Lucky\""], "metric_results": {"EM": 0.421875, "QA-F1": 0.5537879609425662}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428572, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.044444444444444446, 0.3157894736842105, 0.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2200", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_hotpotqa-validation-3792", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1827"], "SR": 0.421875, "CSR": 0.5194128787878788, "EFR": 0.8108108108108109, "Overall": 0.6522166129197379}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "October 1980", "IIII", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "the Near East", "considered harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "decreases as the soil becomes saturated", "Kim ( Kathy Najimy )", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "mortality or morbidity", "Richard Crispin Armitage", "Himalayas", "Harry Potter and the Deathly Hallows, with Death - Eaters in charge of the school, the subject is renamed the Dark Arts, and involves pupils practicing the Cruciatus Curse on those who have earned detentions", "volcanic activity", "In 1837", "late - September through early January", "1991, with L.A. Reid and Babyface during sessions for the Dangerous album, but didn't make the final cut", "Joseph Sherrard Kearns", "Union", "on 3 September, after a British ultimatum to Germany to cease military operations was ignored, Britain and France declared war on Germany", "a loop ( also called a self - loop or a `` buckle '' )", "Carlos Alan Autry Jr. ( also known for a period of time as Carlos Brown ; born July 31, 1952 )", "fictional town of West Egg on prosperous Long Island in the summer of 1922", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "writ of certiorari", "after World War II", "Guwahati and Kuladhar Chaliha as its president", "the largest Greek island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Cheap trick", "October 29, 2015", "Pir Panjal Range", "16 for females and 18 for males", "3.5 million years old from Idaho, USA", "federal government", "Tigris and Euphrates rivers ; and the Levant, the eastern coast of the Mediterranean Sea", "bicameral Congress", "In the year 2026", "Holly Marie Combs", "utopian novels of H.G. Wells, including A Modern Utopia ( 1905 ) and Men Like Gods ( 1923 )", "Michael Crawford", "Microsoft Windows", "the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Los Angeles", "moral tale", "Lana Del Rey", "NBA", "a picky eater,", "Aristotle", "Northwest Mall", "Supergirl", "Field Marshal Lord Gort", "\"From a young age, you know, I used to have the video game,\"", "gun", "the Swat Valley", "Odysseus", "cajun", "Scouts of America", "three empty"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5798436198620919}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.38095238095238093, 0.8181818181818181, 0.45454545454545453, 1.0, 0.0, 0.25, 0.9361702127659575, 1.0, 0.0, 0.6, 0.25, 0.7272727272727273, 0.0, 0.5333333333333333, 1.0, 0.8, 0.0, 0.5263157894736842, 1.0, 0.0, 0.653061224489796, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-3858", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-397", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-3688", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-4320", "mrqa_newsqa-validation-3067"], "SR": 0.40625, "CSR": 0.5177238805970149, "EFR": 0.8947368421052632, "Overall": 0.6686640195404555}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "July 2010", "Clarence Darrow", "John B. Watson", "Spanish explorers", "Tara / Ghost of Christmas Past", "The film follows a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "March 6, 2018", "Erica Rivera", "Bill Irwin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "Amartya Sen", "Georgia", "Domhnall Gleeson", "Alex Drake", "March 11, 2016", "March 11, 2018", "Thomas Mundy Peterson", "not in our stars, / But in ourselves, that we are underlings", "boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "Michael Jackson and Lionel Richie ( with arrangements by Michael Omartian )", "Amybeth McNulty", "the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "John Goodman", "the intermembrane space", "February 25, 2004 ( Ash Wednesday, the beginning of Lent )", "the breast or lower chest of beef or veal", "state's DMV", "Dr. Hartwell Carver", "two occasions", "following the 2017 season", "Arunachal Pradesh", "Charles R Ranch, County Road 24, Las Vegas, New Mexico, USA", "It is a work of social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "his brother", "Washington metropolitan area", "the euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking song", "tissues of the outer third of the vagina,", "Bergen County", "John R. Dilworth", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "Hippos & baboons", "the Republic of Belarus", "tommy hilfiger", "jug"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6966105577536499}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.2105263157894737, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 0.9824561403508771, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.846153846153846, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.1818181818181818, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-7922", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-3449", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-808"], "SR": 0.546875, "CSR": 0.5181525735294117, "EFR": 0.8275862068965517, "Overall": 0.6553196310851926}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Chris Sarandon", "Olivia Olson", "21 June 2007", "Paul Rudd", "Kaitlyn Maher", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Bindusara", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "NFL coaches", "Davos", "Neil Patrick Harris", "1900", "Joel", "inwards towards the pith, and secondary phloem growth outwards to the bark", "either late 2018 or early 2019", "R.E.M.", "the Jews", "as a lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant ( the Aron Habrit in Hebrew )", "Homer Banks", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "Polk County, Florida", "Iran, and there is evidence of sheep farming in Iranian statuary dating to that time period", "2001", "the inventor Bi Sheng", "Electors", "1799, in the fourth Anglo - Mysore war during which Tipu Sultan was killed", "Kid Creole & The Coconuts", "a god of the Ammonites, as well as Tyrian Melqart", "late - night programming block Adult Swim", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "1770 BC", "Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "12-fret", "1825", "Miracle", "Dumfries and Galloway, south-west Scotland", "the Crab Orchard Mountains", "President Obama and Britain's Prince Charles", "NATO fighters", "19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "lullaby", "E. E. Cummings", "Elizabeth Birnbaum"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6358901935103201}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 0.15384615384615385, 0.8571428571428571, 0.0, 0.5, 0.8837209302325582, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.26666666666666666, 1.0, 0.16666666666666669, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8659", "mrqa_triviaqa-validation-2420", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5586", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013"], "SR": 0.484375, "CSR": 0.5176630434782609, "EFR": 0.8787878787878788, "Overall": 0.6654620594532279}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Old Trafford", "the Coercive Acts", "skeletal muscle and the brain", "the libretto", "prophets and beloved religious leaders", "1947", "the St. Louis Cardinals", "Andy Serkis", "Panning", "September 21, 2017", "Baez", "Virginia Beach is an independent city located on the southeastern coast of the Commonwealth of Virginia in the United States", "the sidewalk between Division Street and East Broadway", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "Eastern Redbud", "eleven", "the tax rate paid by a small business", "Roger Dean Stadium", "`` Blood is the New Black ''", "Otis Timson", "four distinct levels", "Benjamin Franklin", "routing information base ( RIB ), is a data table stored in a ISPs or a networked computer that lists the routes to particular network destinations, and in some cases, metrics ( distances ) associated with those routes", "James Rodr\u00edguez", "AD 95 -- 110", "Johnson", "2,500 locations", "from the top of the leg to the foot on the posterior aspect", "Walmart", "Ashoka", "the dermis", "Hodel", "October 27, 2017", "Howard Caine", "a popular medieval given throughout Europe, coming from the biblical name, Thomas being one of Jesus'disciples", "April 10, 2018", "the fourth C key from left", "Aegisthus", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the Supreme Court of Canada", "September 29, 2017", "around 10 : 30am", "Algeria", "Russia", "Manley", "December 15, 2017", "Wyatt", "December 31", "God We Trust", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street", "workers", "At least 40", "Juan Martin Del Potro.", "the Aral Sea", "Sweden", "photoelectric", "Botswana"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7109094719618617}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.625, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8571428571428571, 1.0, 0.8, 0.4615384615384615, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-7612", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-8395", "mrqa_triviaqa-validation-5834"], "SR": 0.578125, "CSR": 0.5185267857142857, "EFR": 0.9259259259259259, "Overall": 0.6750624173280423}, {"timecode": 70, "UKR": 0.64453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.759765625, "KG": 0.4734375, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as well as at the Gol", "Aamir Khan stars along with Gracy Singh, with British actors Rachel Shelley and Paul Blackthorne playing supporting roles", "Super Bowl XXXIX in Jacksonville", "almost exclusively land based powers, able to summon large land armies that were very nearly invincibleable", "September 2017", "Kanawha River", "12.65 m ( 41.5 ft )", "1820s", "from the customer's account immediately, and assumes the responsibility for covering the cashier's check", "D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "as a prison from 1100 ( Ranulf Flambard ) until 1952 ( Kray twins ), although that was not its primary purpose", "the Supreme Court of Canada", "July 1, 1923", "Aibak, first ruler of the Delhi Sultanate", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "irsten Simone Vangsness", "Frankie Laine's `` I Believe ''", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "2002 Tamil film Ramanaa, directed by A.R. Murugadoss, which was later remade in Telugu as Tagore in 2003 and in Kannada as Vishnu Sena in 2005", "RAF Coningsby in Lincolnshire", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha", "vernacular Italian", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Sebastian Vettel", "Tiger Woods", "2018", "Speaker of the House of Representatives, President pro tempore of the Senate, and then the heads of federal executive departments who form the Cabinet of the United States", "The couple will reconcile briefly in the final scene of the fourth season", "Lord's, on 15 July 2004 between Middlesex and Surrey", "Mercedes -Benz G - Class", "Ingrid Bergman", "Malayalam Odakkuzhal", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "The terrestrial biosphere", "the outlaw couple Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "eyes", "Vietnam", "Rutger Hauer", "Canada", "Colonel Patrick John Mercer, OBE", "in Srinagar", "all faiths", "the Dalai Lama's current \"middle way approach,\"", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "willahelm", "electric currents and magnetic fields"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6042151626051082}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9538461538461539, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 0.1, 1.0, 0.5, 0.2, 1.0, 1.0, 1.0, 0.21739130434782608, 0.8363636363636363, 0.6666666666666666, 0.6666666666666665, 0.4, 0.0, 0.07407407407407407, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3571428571428571, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.9142857142857143, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.4375, "CSR": 0.5173855633802817, "EFR": 0.75, "Overall": 0.6290239876760564}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Justin Timberlake", "the following day", "Labour Party", "Judi Dench", "a scuffle with the Beast Folk", "six degrees of freedom", "Spanish moss ( Tillandsia usneoides )", "Matt Monro", "1990", "Friedman Billings Ramsey", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "drivers who meet more exclusive criteria", "Charles Carroll of Carrollton", "1959", "many forested parts", "Hermia", "in and around an unnamed village", "Bart Millard", "Lagaan", "Super Bowl XIX", "2007", "Toto", "Taittiriya Samhita", "the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Lorazepam", "April 8, 2016", "absolute temperature", "via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "on the new moon between January 21 and February 20", "Phosphorus pentoxide", "the best selling cake or biscuit in the United Kingdom", "1890", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "on the base of the right ventricle", "Steve Russell", "2016", "1799", "a boy name and a girl name", "Zachary Taylor", "Oscar Wilde", "the Galaxy S6", "The New Yorker", "Citgo", "school in South Africa", "The e-mails", "Rolling Stone", "nuggets", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6993500071479948}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 0.6666666666666666, 1.0, 0.25, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.08333333333333334, 1.0, 1.0, 0.9090909090909091, 1.0, 0.16666666666666666, 1.0, 0.11764705882352941, 0.4, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-199", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_searchqa-validation-10641"], "SR": 0.59375, "CSR": 0.5184461805555556, "EFR": 0.7692307692307693, "Overall": 0.633082264957265}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "General George Washington", "Louis XIV", "Ed", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "President Lyndon Johnson", "16 seasons", "Panama Canal Authority", "John Brown", "nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season of the television series How I Met Your Mother", "Kansas City Chiefs", "Yuzuru Hanyu", "Kevin McKidd", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "The House of Representatives", "Lisa Stelly", "Ali", "Meg Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a forest", "Seton Hall Pirates", "six season", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Scott Schwartz", "Japan -- Korea Protectorate Treaty", "Djokovic", "White won gold in the half - pipe", "Judy Collins", "2002", "Georgia Groome as Georgia Nic Nicholson : The main character, a 14 - year - old girl who falls in love with Robbie and tries to get him to be her boyfriend throughout the film", "Incudomalleolar joint", "London, United Kingdom", "President of the United States", "Rack of lamb", "Ross MacManus", "York", "Hamburger Sport-Verein e.V.", "2", "The Los Angeles Dance Theater", "100 meter", "President Sheikh Sharif Sheikh Ahmed", "Miami Beach, Florida,", "Suntory", "Victoria", "a yoke", "multiplayer online role-playing video game"], "metric_results": {"EM": 0.625, "QA-F1": 0.7105205813385961}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.9090909090909091, 0.4, 1.0, 0.2352941176470588, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-9877", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-13611", "mrqa_hotpotqa-validation-1074"], "SR": 0.625, "CSR": 0.5199058219178082, "EFR": 0.8333333333333334, "Overall": 0.6461947060502283}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "the efferent nerves that directly innervate muscles", "Johannes Gutenberg of Mainz", "Shawn Wayans", "the United States is the world's third - or fourth - largest country by total area and the third-most populous", "regulatory site", "3", "near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Brazil", "March 31 to April 8, 2018", "American Indian allies", "the radius R of the turntable", "the United Kingdom ( UK ) against large - scale attacks by Nazi Germany's air force, the Luftwaffe", "1945", "CeCe Drake", "April 4, 2017", "post translational modification", "1960 Summer Olympics in Rome", "naturalization law", "September 6, 2019", "Bulgaria", "Michael Douglas", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "from Hebrew \u05d4\u05d5\u05e9\u05d9\u05e2\u05d4 \u05be \u05e0\u05d0, \u05d4\ufffd\u05e9\u05e2\u05e0\u05d0 ( \u02be\u014dsha\u02bfn\u0101 ) meaning `` save, rescue, savior ''", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff / \ufefc 26.617 \u00b0 N 81.617", "German engineer Werner Ruchti", "Brooklyn, New York", "British singer - songwriter Chris Rea", "Julie Adams", "pneumonoultramicroscopicsilicovolcanoconiosis", "In 2010", "British regulars", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350 at the 2010 census", "Uruguay", "to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority ''", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2002", "Anna Faris and Allison Janney", "Cress", "Montr\u00e9al", "Queen Victoria", "Gerald Ford", "Bank of China Tower", "Mumbai, Maharashtra", "Corendon Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "King of England", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6960301876432391}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.823529411764706, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.4545454545454545, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.53125, "CSR": 0.5200591216216216, "EFR": 0.7, "Overall": 0.6195586993243244}, {"timecode": 74, "before_eval_results": {"predictions": ["Lgion d'honneur", "Shaft", "retronym", "kleopatra", "pharaoh", "Tony Dungy", "the Rolling Stones", "opera", "cayenne", "cell", "universal and equal suffrage", "60", "enigma", "tornado", "dance", "Lord Tennyson", "Laryngitis", "Gentle Ben", "terraces", "voodoo", "aquiline", "Hair", "a cozy", "Jalisco", "Davenport", "Sammy Sosa", "Suzuki", "One billion", "the green-eyed monster", "Mount Parnassus", "haematoma", "Conquest", "Coral", "General William Tecumseh Sherman", "Fess Parker", "a duvet", "Baltimore", "crayfish", "Japan", "liberty, equality", "the African Union", "Vernon Kell", "Nepal", "USDA", "cat scratch fever", "freezing", "elvis monroe", "kangaro court", "Whatchamacallit", "Charles Edward Anderson", "Fantasy Island", "humans", "between the Eastern Ghats and the Bay of Bengal", "oneness", "benjamin franklin", "Soricide", "Saint Aidan", "Sulla", "Switzerland", "Parlophone Records", "keyboardist", "150", "mental health", "the contestant"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5649403561827957}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.125, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-8968", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-6289", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-7901", "mrqa_triviaqa-validation-3029", "mrqa_triviaqa-validation-1931", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-5636"], "SR": 0.484375, "CSR": 0.5195833333333333, "EFR": 0.9696969696969697, "Overall": 0.6734029356060607}, {"timecode": 75, "before_eval_results": {"predictions": ["Hip Hop", "paul newman", "Louisiana", "a clapper", "Tombs of Kobol", "A Tale Told By An Idiot, Full of Sound and Fury", "a Gourmet Sandwich Shoppe", "seven", "Cosmo Kramer", "Poetic Justice", "(VICT) Hugo", "Colossus", "(Hugh) Jackman", "silver", "the Republic of Lebanon", "the eagle", "the Dandolo family", "comedian", "(Hamlet) Claudius", "Mussolini", "Margot Fonteyn", "Alfred Nobel", "lifejackets", "an adjective", "General Mills", "Emmitt Smith", "a hair style", "a black hole", "Kampala", "Department of the Clerk of the U.S. House", "Heisenberg", "Sin City", "David Hyde Pierce", "the period of program music", "Old North Church", "the hematopoietic growth factors", "Bull Flugtag", "a jolly Roger", "the North West Territories", "Alaska", "the Electric Company", "Vienna", "the City of Bridgeport", "Red", "a plant", "( Ellen) Wilson", "Esau", "a will", "Agatha Christie", "Ronald Reagan", "Ford", "1947", "Sally Dworsky", "Zuzu", "Mt Kenya", "Christian Wulff", "Zelle", "Princess Aisha bint Hussein", "French", "Oxford", "Kaka", "133", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.5, "QA-F1": 0.5841746794871795}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-6199", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-11498", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_naturalquestions-validation-8847", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-1497", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.5, "CSR": 0.5193256578947368, "EFR": 0.875, "Overall": 0.6544120065789474}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "Department of the Treasury", "Montserrat", "a cyclone", "Starland Vocal Band", "gallows", "ohm", "Roll of Thunder, Hear My Cry", "earthquakes", "the Potomac", "Oregon", "Mary Stuart", "Hulk Hogan", "sciphile.org", "Russia", "Adam Sandler", "David Letterman", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "sack dress", "Don't Worry, Be Happy: Mcferrin: 9780941807517", "the Portsmouth Navy Yard", "Hill East", "a glider", "the heart", "Guyana", "jelly", "camels", "droughttolerant", "ad verbera", "Jonathan Winters", "Pink", "Rhode Island", "Newton", "Algeria", "Smith", "Theodore Roosevelt", "gold", "Moses", "Jamestown", "Lignite", "Seymour Cray", "Private Practice", "steroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "Neptune", "Scotland", "brown", "Lakeside Shopping Centre", "SBS", "Eternal Flame", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "benzodiazepines"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7731770833333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-11245", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-6310", "mrqa_searchqa-validation-14675", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-12072", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_hotpotqa-validation-1377", "mrqa_hotpotqa-validation-512"], "SR": 0.703125, "CSR": 0.5217126623376623, "EFR": 0.9473684210526315, "Overall": 0.6693630916780589}, {"timecode": 77, "before_eval_results": {"predictions": ["Charles Darwin", "Eskimos", "Rome", "Billy the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan", "Edward VI", "Leon Trotsky", "Belgium", "Sister Wendy Beckett", "1066", "ibuprofen", "filibustero", "Carver", "the Bulldog Drummond", "Spooky Salem, MA", "the Tigris River", "the Baltic Sea", "\"Nolo contendere\"", "gum", "Abel", "Louis XV", "Wayne Gretzky", "Anna Karenina", "Sacramento", "the Cordillera", "jury", "Sigmund Freud", "Pantaloons", "Abraham", "Paul Newman", "Brian C. Wimes", "wine glasses", "Rhode Island", "The Simple Life", "Laos", "the Vietnam War", "the Philippines", "Kellogg's", "The Backstreet Boys", "Cairo", "Latin", "Venus", "the Hawthorne", "the Congo", "the duchy", "Horatio Nelson,", "caiman", "Ferrari", "iris", "John Adams", "1886", "Ali", "Cairo", "the German Kaiserliche Marine,", "Hedonismbot", "College Football Friday Primetime", "American R&B vocal group from Philadelphia, Pennsylvania, best known for emotional ballads and \"a cappella\" harmonies", "Kansas Joe McCoy", "protective shoes", "Diego Maradona", "Transportation Security Administration", "silver"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5774866854636591}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.4210526315789474, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4029", "mrqa_searchqa-validation-7100", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-9691", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5367", "mrqa_searchqa-validation-7197", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-4756", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319"], "SR": 0.453125, "CSR": 0.5208333333333333, "EFR": 0.9142857142857143, "Overall": 0.6625706845238095}, {"timecode": 78, "before_eval_results": {"predictions": ["the twins", "March", "Christmas Eve", "The Firm", "Schwalbe", "Circumnavigate", "Marilyn Monroe", "Cheddar", "comet", "wings", "Enigma", "surface-to-air missile", "the igloo", "Deimos", "a dermatologist", "Kramer vs. Kramer", "The Tempest", "yellow", "Annie's Song", "tire", "Schwarzenegger", "Lafayette", "John Bayley", "Ironman", "the Swahili", "NHL", "bactrim", "a course", "wives", "The Thousand and Second Tale of Scheherazade", "Scott McClellan", "Jeremiah", "Thomas Edison", "The Chorus Line", "Guadalajara", "Sydney", "Flavors", "the largest city in South Korea", "Gideon v. Wainwright", "the Alamo", "cereal", "Zlatan", "Pell grants", "Eric Clapton - Tears In Heaven", "being buried alive", "Swan", "KU", "Helsinki", "kidney", "One Flew Over the Cuckoo's Nest", "the Nobel Prize", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "the Standard Motor Company", "Portugal", "cooperative", "Big John Studd", "Juan Manuel Mata Garc\u00eda", "Madeleine L'Engle", "British troops in Iraq", "three", "$3 billion,", "Tom Ewell, Sheree North, and Rita Moreno"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6616590007215007}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.4444444444444445]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1169", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-4458", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010", "mrqa_hotpotqa-validation-4597"], "SR": 0.609375, "CSR": 0.5219541139240507, "EFR": 0.92, "Overall": 0.6639376977848102}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "\"The 2 top-selling U.S. magazines with a specific sport in their names are about this sport", "Peter", "litter", "New Zealand", "John", "California", "Nero", "the Dalmatians", "Cecil Day-Lewis", "cotton", "Bridget Fonda", "South Africa", "blackjack", "the Mediterranean", "Catherine de' Medici", "sour cream", "the adder", "a Saturday morning", "the Thames", "(PIE) FLINGING", "Pitcairn", "Adam Sandler", "Mayo", "\"You complete me\"", "(Arrested) Development)", "aissance", "German", "Rodeo", "marry quickly, without thinking carefully, we may be sorry", "Denzel Washington", "vichy", "nougat", "Truman", "rani", "Tiffany", "Louise", "hair", "Hillary Clinton", "globalization", "Van", "the Rhine", "rock salt", "luggage", "Chile", "aleik", "(John) Dalton", "pearls", "Norse", "Niagara Falls", "the Bronx", "the National Football League ( NFL ) for the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins", "Ray Henderson", "Forbes Burnham", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "237", "over two decades.", "it does not", "14", "8th and 16th"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6291220238095239}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23999999999999996, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-3732", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-15867", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-9317", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-8019", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.546875, "CSR": 0.522265625, "EFR": 0.896551724137931, "Overall": 0.6593103448275862}, {"timecode": 80, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.80859375, "KG": 0.50078125, "before_eval_results": {"predictions": ["Washington", "the National Hockey League (NHL)", "blue", "Georgia", "William Devereaux", "scalpels", "the English Channel", "William Shakespeare", "a phonology", "Thornton Wilder", "Baton Rouge", "a cupboard", "frittata", "pardon", "Bartholomew", "fever", "Target", "Regrets", "a possum", "The Cinephiliacs", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "a staycation", "it is best not to take risks even when it seems boring or difficult", "Makkedah", "Yogi Bear", "northern Idaho", "Georgia", "a carpool", "11:00 am", "Benjamin Harrison", "skyscraper", "Billy the Kid", "Gloria Emerson, Cambodia", "Oliver Twist", "jeopardy/2546_Qs.txt at master", "lamb", "a bread", "Boston", "Martinique", "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb", "the Grand Canal", "the Sons of Liberty", "a telescope", "Catholic", "a trumpet", "a long pass completion", "a cube", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II", "eight", "dragonflies", "cranberries", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3, asleep in her crib,", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "2006", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted."], "metric_results": {"EM": 0.625, "QA-F1": 0.6867559523809523}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-8785", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.625, "CSR": 0.523533950617284, "EFR": 0.875, "Overall": 0.6818161651234569}, {"timecode": 81, "before_eval_results": {"predictions": ["order", "Warsaw", "Katrina And The Waves", "the French and Indian War", "Brady", "philosophy", "the American Red Cross", "\"Primun non nocere\"", "Bonnie Raitt", "As Good as It Gets", "chutneys", "Artemis", "neurons", "Evian", "a fashions", "The Mayor of Casterbridge", "the olfactory nerve", "a window", "Isaac Newton", "Buzzbee", "Alexander Hamilton", "Colorado", "Dune", "an opera", "YouTube", "heresy", "The Office", "Charlie Watts", "Black widow", "a button", "Virginia", "abundant", "Albert Schweitzer", "The hemisphere of the brain", "dive bomber", "Henri de Toulouse-Lautrec", "Helen Hayes", "Dada", "biddies", "Herbert George Wells", "\"Sex In Crazy Places\"", "Bill & Melinda Gates", "Hippopotamus", "Friedrich Wilhelm Nietzsche", "\"It's a dog eat dog world, Woody, but it kind of falls apart at the end.\"", "Alexander Hamilton", "American", "Niagara Falls", "a boat", "carrots", "Flintstone", "Abanindranath Tagore", "at slightly different times when viewed from different points on Earth", "the trunk", "Carrefour", "Barack Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano", "national telephone", "the Catholic League", "Quentin Tarantino"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6246164374840846}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.5, 0.8571428571428571, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.8, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.060606060606060615, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-13622", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-1250", "mrqa_searchqa-validation-11852", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846", "mrqa_triviaqa-validation-5750"], "SR": 0.484375, "CSR": 0.5230564024390244, "EFR": 0.9393939393939394, "Overall": 0.6945994433665927}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "The Big Easy", "Oregon", "Dorothy", "Survivor: Fiji", "Wild Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Eternity", "Marvell", "Quiz Show", "the Collegiate American Football", "acetone", "Donald Trump", "Psycho", "Napoleon", "a lullaby", "capuchins", "Napoleon", "the West", "reticulated", "Munich", "digestif", "a suffix", "Pope Benedict XVI", "Los Alamos Scientific Laboratory", "Somerset Maugham", "sapphire", "Three Coins in the Fountain", "ER", "Goldenrod", "Luke", "the distal colon", "Stress Management", "frequency", "Grease", "a salamander", "Solzhenitsyn", "Eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "Makepeace Thackeray", "the Big Sky Conference", "Beavers", "Boston", "Michelle Pfeiffer", "a tube", "Sweden", "Ajay Tyagi", "the 17th episode", "94 by 50 feet", "Salix", "the 7th", "Utsire", "the University of Kentucky", "Sting", "1988", "Hollywood", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "$10 billion", "her boyfriend, Mohamed al Fayed,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7286210317460318}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-9737", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1561", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.609375, "CSR": 0.5240963855421688, "EFR": 0.88, "Overall": 0.6829286521084338}, {"timecode": 83, "before_eval_results": {"predictions": ["the East Sea", "Stitch", "( Joe) Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafarianism", "cinnamon", "The Pirates of Penzance", "Extreme", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Apollo", "troll", "The Flying Dutchman", "Dan Quayle", "Ruth", "William Faulkner", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "the Marshall Islands", "Nepal", "Palladio", "the names of God", "American Graffiti", "Hair", "cicadas", "Jersey Shore", "the shaft which flies In darkness", "wren", "Zappa", "Hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "a loaf of bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Porridge", "Thermopylae", "Magdalene Laundries", "\"$10,000 Kelly\"", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "in solitary confinement", "Kurdish militant group in Turkey", "1937"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7408854166666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-4151", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-993", "mrqa_newsqa-validation-1509"], "SR": 0.6875, "CSR": 0.5260416666666667, "EFR": 1.0, "Overall": 0.7073177083333334}, {"timecode": 84, "before_eval_results": {"predictions": ["faster than the 40 WPM average", "a Crescent", "a trident", "Abercrombie & Fitch", "H. L. Hunley", "Standard Oil", "Crustacea", "Laura Ingalls Wilder", "a carriage", "Monet", "gasoline, coal, or other fossil fuels", "Gerald R. Ford", "Louis Rukeyser (W$W)", "Jupiter (or Jove)", "Clinton", "Truisms", "Tin", "Stephen Hawking", "Kilimanjaro", "Munich", "London", "the Nunavut", "Georgia Bulldogs (Southeastern Conference)", "Giacomo Puccini - La bohme (English)", "abbreviated", "Heroes", "Summer Health Myths", "Kublai Khan", "Jean Lafitte", "the Flushing River", "a relic", "cyclosporine", "the Northern Mockingbird", "CLAUSE", "Comedy", "the Kittiwake species", "the area of the rectangle", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "a colony of Northern Gannet (Morus bassanus)", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteian Trilogy", "Scotland", "the Big Dipper", "1924", "741 weeks", "January 17, 1899", "Douglas MacArthur", "Project Gutenberg", "New Guinea", "Latin American culture", "a farmers' co-op", "John Landis", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6742559523809524}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.8, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-9175", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-1304", "mrqa_searchqa-validation-6747", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-6336", "mrqa_searchqa-validation-3503", "mrqa_searchqa-validation-6009", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-6220", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1148"], "SR": 0.578125, "CSR": 0.5266544117647058, "EFR": 1.0, "Overall": 0.7074402573529411}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Madeleine Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Eva D. Bowles", "The Prince & the Pauper", "Pushing Daisies", "July", "the Reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "the Muskellunge", "Krispy Kreme", "Luxury Real Estate", "Luther", "rice", "Frasier", "Kansas City", "the arteries", "\"Chinatown.\"", "comedy", "Hamlet", "lime", "The Aviator", "alkaline", "Robert Duvall", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "Chapter 6", "wealth", "Pitcairn Island", "ice hockey", "press", "Mars", "the shell", "David", "a potato Dish,", "a cookie jar", "Babe Ruth", "a cheesesteak", "Hilton", "a `` no - compete '' clause he was unable to wrest,", "SummerSlam", "Jessica Simpson", "William Schuman", "the Ash", "Robert Plant", "Oklahoma", "138,535 people", "Martin Scorsese", "I know my son. He has great values, lots of integrity.", "reptiles,", "Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6867050438596491}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3157894736842105, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-11330", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13590", "mrqa_searchqa-validation-12814", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-6317", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-6049", "mrqa_hotpotqa-validation-1363", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.609375, "CSR": 0.5276162790697674, "EFR": 0.96, "Overall": 0.6996326308139535}, {"timecode": 86, "before_eval_results": {"predictions": ["dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "(Tachito) Somoza", "Chastity", "Frank Sinatra", "Mendeleev", "Norman Mailer", "Blitzkrieg", "light", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "Jones", "The Rolling Stones", "Bridge to Terabithia", "Samuel A. Alito", "kings", "Civic", "Hermann Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Rich Girl", "Yogi Berra", "courage", "a shot glass", "rice", "a constitution", "the eastern Mediterranean", "virtual reality", "the bass", "The Last Remake of Beau Geste", "hot air balloons", "Tarzan & Jane", "RBIs", "David Berkowitz", "oblique", "Pecan-nuts", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Philadelphia", "The Matrix", "the Bolsheviks", "April 17, 1982", "Garden of Gethsemane", "the Vi\u1ec7t Minh and France", "James Cameron", "My Sweet Lord", "Japan", "The Lost Battalion", "Kingdom of Dalmatia", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7229319852941176}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.4, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-16572", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-2007", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4669"], "SR": 0.65625, "CSR": 0.5290948275862069, "EFR": 0.8181818181818182, "Overall": 0.671564704153605}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "Cotton-spinning", "onerous", "Clown portrait", "Fargo", "the Dailies", "fibreboard", "River Thames", "Napster", "a member of the musical Partridge family", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "mental", "exposure", "Lowest point", "the Golden Fleece", "satisfaction", "caution", "Macaulay Culkin", "the Tom Thumb", "Edwards", "Hawaii", "Jack Leon Ruby", "Daniel Boone", "cab", "hemoglobin", "Nancy Sinatra", "ear infection", "a fox", "tabby", "Amerigo Vespucci", "Wisconsin", "Jordan", "Canada", "bipolar", "a brownie", "anvil", "Alexander Calder", "honey", "Matthew Broderick", "Christopher Columbus", "Spider-Man", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "Detroit", "an axiom", "electors", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "kosher", "Albatrosses", "Medea", "Agent Carter", "the Sasanian Empire", "\"Kill Your Darlings\"", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.640625, "QA-F1": 0.689236111111111}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-14457", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10767", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-680", "mrqa_hotpotqa-validation-172", "mrqa_newsqa-validation-4165"], "SR": 0.640625, "CSR": 0.5303622159090908, "EFR": 0.9565217391304348, "Overall": 0.6994861660079051}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "high chairs", "Biggie", "Jesus Lord", "John Paul II", "Evita", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "James Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "a gambler", "sleepover", "Spain", "Scrabble", "the Caspian Sea", "Missouri", "Los Angeles Angels of Anaheim", "Covent Garden", "the Hollywood Blacklist", "12:49", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "the captain", "Transamerica", "Xinjiang", "the 1976 Democratic National Convention", "the Delacorte", "Henry Clay", "the wire loop", "Wal-Mart", "On the Origin of Species", "Electric Avenue", "an APA", "Jerusalem", "Vanna White", "Toyota", "a church bell", "Istanbul", "F. Scott Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "the House of Saxe-Coburg-Gotha", "the Knight in Shining Armor", "purification", "the following day", "1960s", "Taron Egerton", "a linesider", "William Marshal", "The Undertones", "Groupe PSA", "Premier Division", "The SoLow Project", "led from a Los Angeles grand jury room after her indictment in the 1969", "Herman Cain", "Yellowstone National Park", "Kevin Costner"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5268229166666666}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-16186", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-3943", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-5520", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-3680", "mrqa_triviaqa-validation-7327"], "SR": 0.453125, "CSR": 0.5294943820224719, "EFR": 0.8, "Overall": 0.6680082514044944}, {"timecode": 89, "before_eval_results": {"predictions": ["the ermine", "Finding Nemo", "pencil", "the body of a deceased person", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "Wales", "Denmark", "the saguaro", "Saigon", "kami-no-michi", "\"reshit\"", "Zephyr", "the eye ball", "Chanel Iman", "Armistice", "Toilet paper", "the Panama Canal", "Cesare Borgia", "pearl", "Cognac", "Hangman", "Bleak House", "October", "Camptown Races", "Henrik Ibsen", "Linkin Park", "dogies", "Hurricane Matthew", "the lungs", "an inclined plane", "Captain Cook", "Robert I", "Marlon Brando", "Abraham Lincoln", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-n-Harmony", "zebras", "Helio Castroneves", "Richard III", "Hugh Grant", "waiting for Godot", "voyeurism", "the Articles of Confederation", "Pavlov", "a hull", "Hot Wings", "all UK permanent residents that is free at the point of use, being paid for from general taxation", "James Madison", "The Firm", "Harriet Tubman", "Hebrew", "\" Finding Nemo\"", "Chris Evans (actor)", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.625, "QA-F1": 0.6926495927318295}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.21052631578947367, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-438", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-5512", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-12554", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-2583", "mrqa_searchqa-validation-15695", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.625, "CSR": 0.5305555555555556, "EFR": 0.8333333333333334, "Overall": 0.6748871527777778}, {"timecode": 90, "UKR": 0.69140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.7890625, "KG": 0.5078125, "before_eval_results": {"predictions": ["Wisconsin", "Gonzo", "a stagecoach", "Henry Winkler", "faction & action", "Hasta la vista", "the United States", "the guillotine", "bats", "Abu Dhabi", "a plexus", "a rattlesnake", "Peter the Great", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "\"AA\"", "Catherine of Aragon", "flag", "Ravi Shankar", "Bangkok", "Spain", "archery", "oblique", "( Joe) Torre", "meatballs", "Kennedy Space Center", "the Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "paddy", "Matt Leinart", "Alabama", "a bat", "Queen Anne", "the banjo", "the B movie", "Lolita", "a coyote", "the Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime minister", "Fi", "Telma Hopkins, Joyce Vincent Wilson and her sister Pam Vincent", "in AD 95 -- 110", "pepsinogen", "Jorge Lorenzo", "1919", "Paris", "Point Place", "11", "National Aviation Hall of Fame", "Thursday", "78,000 parents of children ages 3 to 17", "prisoners at the South Dakota State Penitentiary", "Anne of Cleves"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8452137706043956}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-10419", "mrqa_newsqa-validation-3194", "mrqa_triviaqa-validation-448"], "SR": 0.796875, "CSR": 0.5334821428571428, "EFR": 0.7692307692307693, "Overall": 0.6581988324175824}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a Chile Relleno", "Oliver Twist", "Vampire Slayer", "the Vistula", "Coriolanus", "ING Clarion Partners", "an aide-de-camp", "fracture", "Roman Polanski", "Court TV", "Sharia", "Jake La Motta", "blog", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Islam", "Madeleine Albright", "the Turpan", "the Harlem Renaissance", "Calamity Jane", "John Lennon", "Richard Branson", "MVP", "lights", "Tarzan of the Apes", "Once", "Warren G. Harding", "Daniel & Philip", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Friday", "Lord North", "Doublemint", "the euro", "the narwhal", "the wall", "John", "Wyatt Earp", "Punjabi", "Gylippus", "Department of Agriculture", "heels", "Frottage", "vertical", "1999", "cheated on Miley", "2012", "Oskar Schindler", "Henry Hunt", "Estonia", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million in America,", "Charlotte Gainsbourg", "more use of nuclear, wind and solar power.", "Audrey Roberts"], "metric_results": {"EM": 0.625, "QA-F1": 0.7019097222222223}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-1050", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10428", "mrqa_triviaqa-validation-6374", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.625, "CSR": 0.5344769021739131, "EFR": 0.9166666666666666, "Overall": 0.6878849637681159}, {"timecode": 92, "before_eval_results": {"predictions": ["the Cordillera Blanca", "Hodel", "Muhammad Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Edith Wharton", "Captains Courageous", "handles", "Central Park", "the piers", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "a conifer", "Al Gore", "an asteroid", "first base", "leather", "Ichabod Crane", "rex", "\"Chinatown.\"", "a butterfly", "Annabelita", "Nacre", "the tango", "Wesley K. Clark", "a porterhouse", "antonyms", "Margaret Court", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canali", "the Galatians", "Lewis Carroll", "meters", "corn on the cob", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "Edouard Manet", "sons", "The Hunter", "Jason Flemyng as Dr. Henry Jekyll / Edward Hyde", "over 100 countries", "citizens of other Commonwealth countries who were resident in Scotland", "Nicholas Garland", "Lincoln", "France", "1968", "Vytautas \u0160apranauskas", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive, a small carmaker from Anderson, Indiana,", "Harry Nicolaides", "September 1947"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5960069444444445}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-16368", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-7464", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-137", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-5306", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-14833", "mrqa_searchqa-validation-8145", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-306", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236", "mrqa_newsqa-validation-2928", "mrqa_naturalquestions-validation-2586"], "SR": 0.53125, "CSR": 0.5344422043010753, "EFR": 0.9333333333333333, "Overall": 0.6912113575268817}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet on the Western Front", "the Juba & the Main", "Kingston", "Cheers", "Indiana", "Walt Kelly", "a kidney", "Paris", "Gangbusters", "China", "Maine", "Gertrude Stein", "The Sun Also Rises", "bathrooms", "Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "Eastern Air Lines", "Notre Dame", "Tiberius", "Jupiter", "loverly", "scrum", "Falkland Islands", "The Producers", "Iceland", "Nancy Drew", "a chessboard", "Uneven Heating", "Jonathan Swift", "Miracle on 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "odor", "the Mesozoic", "Eisenhower", "For What It's Worth", "the Fourteen Points", "Freddie Mercury", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "Wiley Post", "Mistry Mountains", "cantaloupe", "London", "Carl Sandburg", "state governments", "The Enchantress", "James Earl Jones", "the medical profession", "Kevin the Gerbil", "The Treaty of Waitangi", "Jessica Lange", "Heinkel He 178", "Kenan & Kel", "304,000", "one", "Thursday", "digging ditches"], "metric_results": {"EM": 0.609375, "QA-F1": 0.656547619047619}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11159", "mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-10151", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-5792", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-591"], "SR": 0.609375, "CSR": 0.5352393617021276, "EFR": 0.96, "Overall": 0.6967041223404256}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Logan's Run", "Muqtada al-Sadr", "zoo", "Omega", "Nixon", "Hudson River", "rodents", "Luxembourg", "Doolittle", "a riot", "Lon Chaney", "New York", "the Coen brothers", "Sicily", "Boston Celtics", "sugar", "Enron", "the fulcrum", "the Central African Republic", "Rudolf Hess", "pitch", "the hippopotamus", "an eye", "Bech", "Walter Mondale", "Washington Irving", "a tree", "the Nubia", "Existentialism", "mezcal", "Scarface", "(John Cornyn", "Mathers", "Nine to 5", "the U.S. Department of Housing and Urban Development", "Extradition", "the head", "the Nutty Professor", "Michael Collins", "The Sopranos", "The Sound And The Fury", "a pair", "Brazil", "obsessive", "Michelle Pfeiffer", "o oats", "arteries", "1773", "a Joule", "the Justice Department", "20 November 1989", "25 September 2007", "the forces of Andrew Moray and William Wallace defeated the combined English forces of John de Warenne, 6th Earl of Surrey, and Hugh de Cressingham", "Nafea Faa Ipoipo", "a window", "St. Agatha", "Newtonian mechanics", "PETE", "SKUM", "backbreaking labor", "Joan Rivers", "second", "Mary Rose Foster"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6128605769230768}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.4666666666666667, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-9412", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-10541", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-1950", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-2638"], "SR": 0.484375, "CSR": 0.534703947368421, "EFR": 0.9393939393939394, "Overall": 0.692475827352472}, {"timecode": 95, "before_eval_results": {"predictions": ["Petro Poroshenko", "London Broils", "the Communist Party", "The Goonies", "Velvet Revolver", "Haunted Mansion", "the Continental Congress", "Robert Johnson", "Mahlemuts", "a brisket", "fish", "parens", "Casablanca", "The Dutchess", "Detroit River", "Mmeatta", "Northern Exposure", "Mount Kilimanjaro", "(Nebuchanezzar)", "a flip", "the Komodo", "Mordecai Richler", "The Simpsons", "The West Wing", "pears", "ravens", "Mexico", "Ladd-Franklin", "Pocahontas", "vii", "John Hersey", "Patricia Arquette", "Ernie Banks", "Grotta Azzurra", "Prince Harry", "Elizabeth Barret Browning", "Hades", "Whigs", "Capone, Alphonse", "Maria Callas", "Wakame", "Ren Faire Jousts", "Antony", "Alfred, Lord Tennyson", "National Geographic", "Starlite Orchestra & Singers", "Jerusalem", "the nativity scene", "the Edict of Nantes", "Hector", "Omega", "before the first letter of an interrogative sentence or clause to indicate that a question follows", "six doctors from Seattle Grace Mercy West Hospital who are victims of an aviation accident fight to stay alive", "since 3, 1, and 4 are the first three significant digits of \u03c0", "Nikolaus Esterh\u00e1zy", "exponentiation", "Worcestershire", "1754", "three", "Lowe's Companies, Inc.", "snow,", "Fernando Gonzalez", "Chester Arthur Stiles, 38,", "wasps"], "metric_results": {"EM": 0.5, "QA-F1": 0.5753446115288221}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.631578947368421, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-1719", "mrqa_searchqa-validation-15023", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-1048", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-827"], "SR": 0.5, "CSR": 0.5343424479166667, "EFR": 0.84375, "Overall": 0.6732747395833334}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "pasta tubes", "assemble, and to petition", "hot air balloons", "fallacy", "Nomar Garciaparra", "John Glenn", "the heron", "Gus Grissom", "The White Company", "New Balance", "General Andrew Jackson", "Joan of Arc", "the finale", "molluscus", "Camille Claudel", "the East", "caricare", "the Seven Years' War", "Meg Tilly", "The Wizard of Oz", "madding", "tribes", "Richard Branson", "Argentina", "Woodrow Wilson", "the Osmonds", "LA TRAVIATA", "the Tribbles", "Billy Joel", "Wyoming", "Tigger", "Geneva", "Frank Sinatra", "pasta", "a leader of congregational prayer", "anchor", "Makkah", "Sydney", "Dermatology", "Solomon", "Mikey: Help!", "Chirac", "20", "snowmobilers", "To Carrie and Irene Miner", "Surinam", "a", "Slovakia and the Czech Republic", "the Corinthians", "a dilithium", "Help!", "1997", "2010", "1215", "Conchita Wurst", "The American Revolution", "Gurgaon, Haryana, India", "Robert Gibson", "three", "a surrogate", "\"The Screening Room\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6433779761904761}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true], "QA-F1": [0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-13848", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-10078", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-11872", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-1445", "mrqa_searchqa-validation-12162", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387"], "SR": 0.578125, "CSR": 0.5347938144329897, "EFR": 0.9629629629629629, "Overall": 0.6972076054791906}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomads", "Washington", "tribbles", "San Jose", "The Two Gentlemen of Verona", "Cobb", "the Hydra", "Gulliver's Travels", "Distant Early Warning Line", "Tordis", "peanut butter", "Xinjiang-Uygur Autonomous Region", "sonic boom", "Fergie", "Sacramento", "emeralds", "Swiss Cheese", "Ernest Hemingway", "Blue Mountain", "Annika Sorenstam", "stars", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the toothache", "Henry Shrapnel", "Venezuela", "Arethusa", "Oklahoma City", "Brazil", "Bob Fosse", "Dugong", "Treading Water", "the 1870s", "the French & Indian War", "checkerboard", "Waterloo", "a waterbed", "mulatta", "a bagel", "propellers", "a bonnet", "an acre", "Alexander Calder", "a cruller", "Helium", "Tokyo", "mozzarella", "Charles Perrault", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "pharyngitis", "Big Dipper", "\"Sofia the First\"", "Africa", "Ben Elton", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.625, "QA-F1": 0.6614583333333333}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-906", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-13536", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-16676", "mrqa_searchqa-validation-13438", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_searchqa-validation-9638", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.625, "CSR": 0.5357142857142857, "EFR": 0.7916666666666666, "Overall": 0.6631324404761905}, {"timecode": 98, "before_eval_results": {"predictions": ["Jacob Marley", "magnum", "the Ottoman Empire", "Helen of Troy", "whale", "New York", "Himalayas", "Wayne's World", "Poland", "Kwanzaa", "ballistic missile submarine", "Russell Crowe", "Splash Down", "a Shelby GT350", "tears", "roulette", "W. Somerset Maugham", "Christo", "Matisse", "the sea", "All Quiet on the Western Front", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "Spain", "Ford", "Sidney Sheldon", "surround", "Faraday", "breakfast", "Krispy Kreme", "doges", "Stanton Avery", "the Death Valley", "the Cumberland Gap", "yolk", "the Navy", "a pie", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Chirac", "Grover Cleveland", "Destiny's Child", "Luxor", "Spain", "Changes", "croutons", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger", "King James I", "Macbeth", "Carol Ann Duffy", "Ravenna", "travel", "having trained law enforcement personnel watching people as they enter the mall.", "the 3rd Platoon, A Company, 2nd Light Tactical Reconnaissance Battalion,", "Bahrami", "make life a little easier"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7025240384615385}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.59375, "CSR": 0.536300505050505, "EFR": 0.7692307692307693, "Overall": 0.6587625048562549}, {"timecode": 99, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.796875, "KG": 0.51015625, "before_eval_results": {"predictions": ["the Hundred Years War", "a vertebral column", "Alfred Binet", "Venial", "a caveat", "\"There's no place like home\"", "cows", "Spanish", "Vanessa Hudgens", "Mighty Joe Young", "Magic", "Japan Monkey Centre (JMC)", "Rhiannon", "Scotland", "leave It to Beaver", "Kurdish", "Ann Richards", "a jack", "American Samoa", "Langston Hughes", "Coke", "The Color Purple", "THX-1138", "Macbeth", "El Greco", "General Motors", "sexy", "a shark", "Frankie Valli", "a Dagger", "a backpacking route", "pineapple", "Buffalo nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "Fondue", "VOD", "Schwarzenegger", "Edison", "Animal Crackers", "oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Sweden", "Students for a Democratic Society", "All the King's Men", "Ramey", "by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 14, 2017, by 20th Century Fox", "James Mason", "slide trumpet", "Anne Frank", "winner (band)  winner (Hangul: \uc704\ub108)", "Nova Scotia", "Rochdale", "Matamoros, Mexico,", "Florida", "Capitol Hill", "775"], "metric_results": {"EM": 0.625, "QA-F1": 0.6921875}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-3260", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-4773", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-1302", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-8862", "mrqa_triviaqa-validation-2452", "mrqa_hotpotqa-validation-2", "mrqa_newsqa-validation-1996"], "SR": 0.625, "CSR": 0.5371874999999999, "EFR": 0.875, "Overall": 0.68134375}]}