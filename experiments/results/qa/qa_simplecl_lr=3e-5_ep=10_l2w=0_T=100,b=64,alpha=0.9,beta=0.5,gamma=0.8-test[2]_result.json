{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4090, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Jelme and Bo'orchu", "gauge bosons", "consumer prices", "Albert C. Outler", "a computational problem", "1521", "River Tyne", "Boston", "San Jose Marriott", "illegal boycotts", "Mitochondria", "bilaterians", "Alexandre Yersin", "Methodists today", "Beyonc\u00e9", "the Rhine and its downstream extension", "7\u20134\u20132\u20133", "Horniman Museum", "400 AD to 1914", "early 1526", "The individual is the final judge of right and wrong", "five", "Battle of B\u1ea1ch \u0110\u1eb1ng", "Time Lady", "oxygen-16", "The Day of the Doctor", "Sierra Sky Park", "James Clerk Maxwell", "Bill Clinton", "in areas its forces occupied in Eastern Europe", "20,000", "Queen Elizabeth II", "The Daleks", "gas turbines", "Newton", "Miasma theory", "Ealy", "several medals", "remaining in black and white", "computability theory", "autoimmune", "American Sweetgum", "Pleistocene epoch", "Feynman diagrams", "orange", "oxygen compounds", "four", "Fort Caroline", "counties or powiats", "chemical bonds", "2015", "France's claim to the region was superior to that of the British", "double", "helps many proteins bind the polypeptide", "Islamism", "lines or a punishment essay", "mercuric oxide", "released Islamists from prison and welcomed home exiles", "Washington and Thomas Gage", "The individual", "Bruno Mars", "the public", "Thomas Edison and George Westinghouse", "a freshwater lake"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8040364583333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6848", "mrqa_squad-validation-9923", "mrqa_squad-validation-100", "mrqa_squad-validation-9434", "mrqa_squad-validation-6966", "mrqa_squad-validation-7725", "mrqa_squad-validation-8538", "mrqa_squad-validation-1703", "mrqa_squad-validation-3511", "mrqa_squad-validation-3597", "mrqa_squad-validation-9567", "mrqa_squad-validation-6973", "mrqa_squad-validation-2025"], "SR": 0.796875, "CSR": 0.796875, "EFR": 1.0, "Overall": 0.8984375}, {"timecode": 1, "before_eval_results": {"predictions": ["the General Sejm", "232", "New Holland", "the \"Rhine knee\"", "the U.S. South", "the Schmalkaldic League", "January 1985", "an Executive Committee", "King Sancho VI of Navarre", "the Arizona Cardinals", "36", "Chloroplasts", "Sydney", "the Panic of 1901", "Muslim medicine", "the Silk Road", "silicon dioxide", "statocyst", "Several thousand", "the Fourth Intercolonial War and the Great War for the Empire", "medieval", "30\u201360% of Europe's total population", "the laws of physics", "the Ten Commandments", "the San Fernando Valley", "Roger NFL", "Hugh L. Dryden", "metals", "1.5 gigatons", "Denver Broncos", "\u00a31 of capital", "the 2010 series", "megaprojects", "1024-bit primes", "the portrait of Fran\u00e7ois, Duc d'Alen\u00e7on by Fran\u00e7ois Clouet, Gaspard Dughet", "Africa", "the Electorate of Saxony", "ice-sheets", "the lion, leopard, buffalo, rhinoceros, and elephant", "A plea of no contest is sometimes regarded as a compromise between the two", "seven", "Demaryius Thomas", "Napoleon's", "the Santa Clara Marriott", "kinematic measurements", "shipping", "12th", "helical thylakoid model", "the A69", "14%", "Thomas Edison", "Toshiba", "detention", "antigen presentation", "hunter's garb", "The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League", "British Gas plc", "Demaryius Thomas", "Homebrewing", "Joe Scarborough", "became a politician", "Gareth Jones", "Sivakumar, S. V. Subbaiah, Jayachitra, Srividya, Shubha, Kamal Haasan and Jayasudha", "Teresa Hairston"], "metric_results": {"EM": 0.75, "QA-F1": 0.7791752518315018}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25000000000000006, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-1117", "mrqa_squad-validation-3664", "mrqa_squad-validation-85", "mrqa_squad-validation-4482", "mrqa_squad-validation-8978", "mrqa_squad-validation-5490", "mrqa_squad-validation-8446", "mrqa_squad-validation-8278", "mrqa_squad-validation-6914", "mrqa_squad-validation-7155", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-2275"], "SR": 0.75, "CSR": 0.7734375, "EFR": 0.9375, "Overall": 0.85546875}, {"timecode": 2, "before_eval_results": {"predictions": ["Ugali with vegetables, sour milk, meat, fish or any other stew", "TFEU article 294", "over $20 billion", "was a major source of water pollution", "unity of God", "all war", "1000 and 1900", "Gamal Abdul Nasser", "R\u00fcdesheim am Rhein", "minor", "1162", "was lost in the 5th Avenue laboratory fire of March 1895", "ABC Cable News", "22 May 2006", "Germany and Austria", "Golden Gate Bridge", "the Welsh", "acquiring nutrients", "Muslims in the semu class", "the Chinese", "temperature and light", "12 May 1999", "1852", "the development of safety lamps", "stabilize the rest of the chloroplast genome", "The Mongols' extensive West Asian and European contacts", "24%", "Milton Friedman Institute", "Donald Davies", "three", "student motivation and attitudes towards school", "1560", "1891", "Lutheran views", "electron", "fear of their lives", "Science", "John Pell, Lord of Pelham Manor", "Cam Newton", "Osama bin Laden", "international drug suppliers", "President", "expelled Jews", "arid and semi-arid areas with near-desert landscapes", "Yosemite Freeway", "Annan and his UN-backed panel and African Union chairman Jakaya", "The Warsaw Stock Exchange", "it becomes an Act of the Scottish Parliament", "certification by a recognized body", "a chain or screw stoking mechanism", "Battle of B\u1ea1ch \u0110\u1eb1ng (1288)", "silver", "1947", "1860", "The club will participate in the Premier League, FA Cup, EFL Cup (as holders), UEFA Champions League and UEFA Super Cup.", "Detroit, Michigan", "Emancipation Proclamation", "26,000", "Pakistan A", "Ricky Skaggs", "Saturday Night Live", "the last living pilot of the X-15 program", "Two new U.S. representatives are teaming up with CNN.com to report their \"Freshman Year\" experience through videos and commentaries", "250,000"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8395968614718615}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.9523809523809523, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.1818181818181818, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9343", "mrqa_squad-validation-9388", "mrqa_squad-validation-5157", "mrqa_squad-validation-8399", "mrqa_squad-validation-4562", "mrqa_squad-validation-8383", "mrqa_squad-validation-9499", "mrqa_squad-validation-8222", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-368"], "SR": 0.78125, "CSR": 0.7760416666666666, "EFR": 0.9285714285714286, "Overall": 0.8523065476190477}, {"timecode": 3, "before_eval_results": {"predictions": ["10 February 1763", "good, clear laws, fairly and democratically", "shaping ideas about the free market", "SAP Center in San Jose", "older", "university and military academy", "Foreign Protestants Naturalization Act", "inequality", "jigg TV", "three", "permafrost", "Silas B. Cobb", "the traditional salute of a knight winning a bout", "Jane Kim", "the Presiding Officer", "lipophilic alkaloid toxins", "three", "William Rainey Harper", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "June", "Huguenot", "the 2007 election aftermath", "Ralph Woodward", "Susan Foreman", "clinical pharmacists", "teleforce", "British failures in North America, combined with other failures in the European theater", "300", "66 million years ago", "Hong Kong", "in commerce, schooling and government", "Krak\u00f3w", "France", "three", "power outage", "the preservation of public order is easier and more efficient than anywhere else", "Muslim and Chinese", "free trade", "15,100", "Cuba", "high pressure shock waves", "28,000", "21 to 11", "Cam Newton", "128,843", "Van Gend en Loos v Nederlandse Administratie der Belastingen", "four years", "Howard Keel", "half of the Pangaea supercontinent", "Mel Gibson", "George Washington", "John Uhler Lemmon III", "six", "Belstar F1", "Charles, Eric Clapton, Bob Dylan and Johnny Cash", "Henry Kelly", "hedgehogs", "George IV", "The Time Machine", "the Granite City", "the natural world and mysticism", "more funds", "Brad Blauser", "$1.45 billion"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7354166666666666}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1831", "mrqa_squad-validation-4971", "mrqa_squad-validation-5975", "mrqa_squad-validation-6223", "mrqa_squad-validation-9570", "mrqa_squad-validation-3044", "mrqa_squad-validation-8326", "mrqa_squad-validation-1830", "mrqa_squad-validation-4065", "mrqa_squad-validation-973", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-1148"], "SR": 0.6875, "CSR": 0.75390625, "EFR": 0.9, "Overall": 0.826953125}, {"timecode": 4, "before_eval_results": {"predictions": ["vote clerk", "estimated $200,000", "carbohydrates", "redistributive taxation", "the League of Nations", "two", "\"The Time of the Doctor\"", "Nairobi", "Missy", "free", "7:00 to 9:00 a.m.", "Strommen et al.", "lipid monolayer", "2009", "whether a state or threat of war existed", "member states", "Jin", "greater equality but not per capita income", "John Houghton", "carbohydrates", "immediately", "America's Funniest Home Videos", "42%", "19", "specialised education and training", "layered basaltic lava flows", "October 2007", "Robert Maynard Hutchins", "Shoushi Li", "clerical marriage", "46%", "Kevin Harlan", "200 Troupes de la marine and 30 Indians", "Half", "Independence Day: Resurgence", "duty", "complex silicates (in silicate minerals)", "Worldvision Enterprises", "hunter's garb", "in the Channel Islands", "\"Blue Harvest\" and \"420\"", "1225", "georgie", "george jackson", "Mississippi River", "\"Hey there Delilah, I know... God speed your love to me\"", "pharoah", "jackson jackson", "king jackson", "feminine nouns", "on the right side", "george jackson", "billy preston", "billy jackson", "billy jackson king", "billy Christian", "\"From the Halls of Montezuma to the shores of Tripoli", "Topix", "London", "Britomart", "Casey Beane", "Islamabad", "The Jefferson Memorial", "murder"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6201388888888889}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2491", "mrqa_squad-validation-3948", "mrqa_squad-validation-8234", "mrqa_squad-validation-7551", "mrqa_squad-validation-3507", "mrqa_squad-validation-1131", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-2118", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-14197", "mrqa_naturalquestions-validation-5040", "mrqa_newsqa-validation-839"], "SR": 0.59375, "CSR": 0.721875, "EFR": 1.0, "Overall": 0.8609375}, {"timecode": 5, "before_eval_results": {"predictions": ["Justifying Grace", "neuronal dendrites", "an electrical generator", "Doctor Who", "coronary thrombosis", "his grandson", "San Francisco", "consumer prices", "2016", "as soon as they enter into force", "colonizing empires", "\"hockey stick graph\"", "OpenTV", "intuition", "1720", "around 300", "2001", "The Chase", "cortisol and catecholamines", "Economist Intelligence Unit", "the Decalogue (the Ten Commandments) and the Lord's Prayer", "Paramount Pictures", "Thomas Coke", "The Neighbors", "the waldzither", "the United States", "\u20ac53,423", "the helicosproidia", "to build their own dedicated networks", "2001", "the High Rhine", "Justin Tucker", "Colorado Springs", "2004", "Newton", "26", "University College London", "the Broncos", "(Freddy and the Ignormus)", "the cube root of a negative number", "Claude Wheeler", "in a perfect... processes can be determined by having that system brought", "(Lewis and Clark)", "Republican", "the pope", "manganese", "a Fokker", "(Unch) Bach", "(2008)", "Ian Fleming", "manganese", "The Thing", "Trincomalee", "Moses", "the Bolshevik party", "the duke's honor", "The Legend of Holy Archer", "The Tale of Genji", "\"The Daily Show\"", "Ant & Dec", "manganese", "Cherokee Nation", "$250,000", "UFC 50: The War of '04"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6154778079710145}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1739130434782609, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999]}}, "before_error_ids": ["mrqa_squad-validation-6267", "mrqa_squad-validation-9865", "mrqa_squad-validation-7827", "mrqa_squad-validation-2391", "mrqa_squad-validation-4874", "mrqa_squad-validation-5214", "mrqa_squad-validation-778", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-8713", "mrqa_triviaqa-validation-3042", "mrqa_hotpotqa-validation-1190"], "SR": 0.578125, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["ten", "", "Miller", "downward pressure on wages", "Catholic", "nine", "11:28", "chest pains", "March 1896", "T cells", "economically", "reach locations not on the private network", "the college", "Yes\u00fcgei", "research", "toward the end of his life", "Clinton", "Treiz", "esoteric", "San Andreas Fault", "30%\u201350%", "double coronation", "ESPN", "p", "plantar fasciitis", "Peter Capaldi", "6000 Da", "Queen Victoria and Prince Albert", "cortisol and catecholamines", "Manakintown", "1985", "a stronger, tech-oriented economy", "stream capture", "Shor's algorithm", "identity documents", "The Bronx County District Attorneys Office", "two women", "Nothing But Love", "a man's lifeless, naked body", "Pakistan", "a legitimate forum for prosecution", "the Department of Justice", "8 p.m. local time", "\"Jersey Shore\"", "diabetes and hypertension", "6-4", "beautiful", "Shalom", "2009", "said, \"It has never been the policy of this president or this administration to torture.\"", "Himalayan", "Wednesday", "2,000 euros", "Siri", "88", "\"Mad Men\"", "13", "NATO", "Representatives", "Bangladesh", "Argentinian", "lion", "Department of Homeland Security", "Moldova"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6166087962962963}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.962962962962963, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10413", "mrqa_squad-validation-3680", "mrqa_squad-validation-6008", "mrqa_squad-validation-2122", "mrqa_squad-validation-9213", "mrqa_squad-validation-6696", "mrqa_squad-validation-3193", "mrqa_squad-validation-2835", "mrqa_squad-validation-1834", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3491", "mrqa_naturalquestions-validation-3569", "mrqa_triviaqa-validation-2240", "mrqa_hotpotqa-validation-985", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-9754"], "SR": 0.515625, "CSR": 0.671875, "EFR": 1.0, "Overall": 0.8359375}, {"timecode": 7, "before_eval_results": {"predictions": ["Blaydon Race", "The Central Region", "ten", "viral", "100\u2013150", "late 1886", "\u00a334m per year", "Alvaro Martin", "more efficient solutions", "Schedule 5", "BBC 1", "victory at Fort Niagara successfully cut off the French frontier forts further to the west and south.", "cantatas", "January 30", "seven", "\u20ac25,000 per year", "St. Bartholomew's Day massacre", "9th", "principle of equivalence", "The Entertainment Channel", "when they improve society as a whole", "1724", "the incentive for the democratic changes", "The St. Johns River", "Life", "priest", "Jan Andrzej Menich", "Jane Kim", "~74,000 (BP = Before Present)", "biomass", "1562", "9 a.m.-1 p.m.", "Empire of the Sun", "22", "four", "Ross Perot", "Baghdad", "$1.45 billion", "in her home", "more than 2,800", "in a tenement in the Mumbai suburb of Chembur", "trail the illegal traffic", "it was \"sick of running,\"", "July 4.", "Evan Bayh", "Hu Jintao", "April 24", "the president would legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "scientific reasons", "around 1918 or 1919", "the first or second week in April", "Pakistan", "Pakistan", "jazz", "appealed against the punishment", "the used-luxury market", "Steve Williams", "Ali", "February", "Mickey's PhilharMagic", "Sugar Ray", "Oedipus Rex", "guitar feedback", "(Cs, Cs-137)"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7670415521978022}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.846153846153846, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.16666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10269", "mrqa_squad-validation-2419", "mrqa_squad-validation-2422", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-2809", "mrqa_triviaqa-validation-955", "mrqa_hotpotqa-validation-3972"], "SR": 0.703125, "CSR": 0.67578125, "EFR": 1.0, "Overall": 0.837890625}, {"timecode": 8, "before_eval_results": {"predictions": ["nolo contendere", "5,984", "Jacksonville", "fish stocks to collapse by eating both fish larvae and organisms that would otherwise have fed the fish", "applied force", "DuMont Television Network", "Amtrak San Joaquins", "teaching", "the traditional salute of a knight winning a bout.", "a ribosome in the cytosol", "British", "1887", "February 7, 2016", "Cargill Meat Solutions and Foster Farms", "Henry Plitt", "1978", "Von Miller", "about 0.7% of the human population's wealth", "water", "The Electronic Frontier Foundation", "the Tyndale Bible", "specialised education and training", "Il milione", "Guo Shoujing", "1908", "1560", "reason", "only \"essentials\"", "30,000", "a Taliban member who had come for the talks about peace and reconciliation, and detonated the explosives as he entered the home.", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Keating Holland.", "Al Gore.", "Rima Fakih", "his business dealings for possible securities violations", "summer", "the Dalai Lama's current \"middle way approach,\"", "his quiet, friendly nature.", "a body", "Brian Mabry", "completely changed the business of music,", "more than 4,000", "consumer confidence", "autonomy", "1996", "1831", "Russian Foreign Minister Sergey Lavrov", "a ruthless cartel", "the clothes we make for the runway", "Arizona", "Muslim festival of Eid al-Adha.", "BET", "FBI recordings of his phone calls.", "the use of torture and indefinite detention", "Mugabe's opponents", "The Casalesi Camorra clan", "a impromptu memorial for the late singer", "China", "the first locomotive to have reached that speed.", "Prime Minister Margaret Thatcher and her cabinet", "a square simula-", "1898", "11 : 40 p.m. ship's time", "18th century"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6895693542568543}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1212121212121212, 0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.22222222222222224, 0.7272727272727273, 0.0, 0.5, 0.0, 0.16]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8960", "mrqa_squad-validation-7552", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-776", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-1352", "mrqa_naturalquestions-validation-8279", "mrqa_triviaqa-validation-2510", "mrqa_hotpotqa-validation-3165", "mrqa_searchqa-validation-12759", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-3505"], "SR": 0.609375, "CSR": 0.6684027777777778, "EFR": 1.0, "Overall": 0.8342013888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["the Yassa", "An attorney", "The Quasiturbine", "s = \u22122, \u22124,...", "creates immunological memory", "three", "coal from Tyneside to Newcastle Quayside", "pseudorandom", "average teacher salaries", "the same message routing methodology as developed by Baran", "Mongols beyond the Middle Kingdom saw them as too Chinese.", "Temecula and Murrieta", "antibodies", "Korean", "the revolution could only succeed in Russia as part of a world revolution.", "William of Orange", "the Horn of Africa", "lipid monolayer", "Hymn for the Weekend", "Colonel Monckton", "Gymnosperms", "7 January 1943", "1.7 million", "ABC News Now", "Boomer Esiason and Dan Fouts", "Eintracht Frankfurt", "former U.S. secretary of state", "Raymond Soeoth,", "10", "nearly 28 years of rule.", "Islamabad", "Haleigh Cummings,", "90", "U.S. Ambassador to Zimbabwe James McGee", "the 12th on the Blue Monster course at Doral", "terminal brain cancer.", "death squad killings", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Leaders of more than 30 Latin American and Caribbean nations", "the Beatles", "Citizens are picking members of the lower house of parliament,", "The Ski Train begins its round-trip journey to Winter Park at Union Station in Denver, Colorado.", "CNN's", "the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "led the weekend box office,", "Some truly mind-blowing structures", "Daryeel Bulasho Guud", "United Arab Emirates", "Bobby Louisiana", "Polo because \"it was the sport of kings.", "the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating.", "made out of either heavy flannel or wool -- fabrics that would not be transparent when wet", "Aniston, Demi Moore and Alicia Keys", "The National Infrastructure Program", "(l-r)", "Pakistan", "Landon Jones", "embryo", "Live and Let Die", "Ben Hogan", "400", "\"Nebo Zovyot\"", "the Chesapeake Bay", "the list of dos & don'ts,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6242627506813363}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.36363636363636365, 1.0, 0.125, 1.0, 0.6153846153846153, 0.0, 1.0, 0.631578947368421, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.9, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5113", "mrqa_squad-validation-10128", "mrqa_squad-validation-3304", "mrqa_squad-validation-9912", "mrqa_squad-validation-8880", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2879", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-2016", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-2219"], "SR": 0.53125, "CSR": 0.6546875, "EFR": 1.0, "Overall": 0.82734375}, {"timecode": 10, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3412", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4048", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-100", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1886", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-8001", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9754", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10128", "mrqa_squad-validation-10155", "mrqa_squad-validation-10162", "mrqa_squad-validation-10167", "mrqa_squad-validation-1018", "mrqa_squad-validation-10198", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10269", "mrqa_squad-validation-10272", "mrqa_squad-validation-1029", "mrqa_squad-validation-103", "mrqa_squad-validation-10310", "mrqa_squad-validation-10315", "mrqa_squad-validation-10326", "mrqa_squad-validation-10345", "mrqa_squad-validation-1036", "mrqa_squad-validation-10380", "mrqa_squad-validation-10413", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10476", "mrqa_squad-validation-1048", "mrqa_squad-validation-1053", "mrqa_squad-validation-1088", "mrqa_squad-validation-1097", "mrqa_squad-validation-1119", "mrqa_squad-validation-1131", "mrqa_squad-validation-1197", "mrqa_squad-validation-1222", "mrqa_squad-validation-1231", "mrqa_squad-validation-1255", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-139", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1521", "mrqa_squad-validation-1537", "mrqa_squad-validation-1546", "mrqa_squad-validation-1561", "mrqa_squad-validation-1592", "mrqa_squad-validation-1611", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1831", "mrqa_squad-validation-1834", "mrqa_squad-validation-1876", "mrqa_squad-validation-1940", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-20", "mrqa_squad-validation-2048", "mrqa_squad-validation-2048", "mrqa_squad-validation-2087", "mrqa_squad-validation-2116", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2188", "mrqa_squad-validation-2235", "mrqa_squad-validation-2250", "mrqa_squad-validation-2374", "mrqa_squad-validation-239", "mrqa_squad-validation-2391", "mrqa_squad-validation-2403", "mrqa_squad-validation-2419", "mrqa_squad-validation-2422", "mrqa_squad-validation-2447", "mrqa_squad-validation-2462", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2580", "mrqa_squad-validation-2640", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2723", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-2797", "mrqa_squad-validation-282", "mrqa_squad-validation-2835", "mrqa_squad-validation-2848", "mrqa_squad-validation-2870", "mrqa_squad-validation-2873", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-30", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3048", "mrqa_squad-validation-3084", "mrqa_squad-validation-3086", "mrqa_squad-validation-3141", "mrqa_squad-validation-316", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3299", "mrqa_squad-validation-3304", "mrqa_squad-validation-3309", "mrqa_squad-validation-3319", "mrqa_squad-validation-3358", "mrqa_squad-validation-3368", "mrqa_squad-validation-3390", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3511", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3849", "mrqa_squad-validation-3932", "mrqa_squad-validation-3948", "mrqa_squad-validation-4032", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4165", "mrqa_squad-validation-4176", "mrqa_squad-validation-4186", "mrqa_squad-validation-4248", "mrqa_squad-validation-4265", "mrqa_squad-validation-4274", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4413", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4482", "mrqa_squad-validation-4488", "mrqa_squad-validation-4493", "mrqa_squad-validation-4562", "mrqa_squad-validation-4611", "mrqa_squad-validation-4623", "mrqa_squad-validation-4627", "mrqa_squad-validation-465", "mrqa_squad-validation-4698", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-4971", "mrqa_squad-validation-4976", "mrqa_squad-validation-501", "mrqa_squad-validation-506", "mrqa_squad-validation-5079", "mrqa_squad-validation-5113", "mrqa_squad-validation-5133", "mrqa_squad-validation-5150", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5214", "mrqa_squad-validation-5230", "mrqa_squad-validation-5295", "mrqa_squad-validation-5343", "mrqa_squad-validation-5355", "mrqa_squad-validation-5457", "mrqa_squad-validation-5478", "mrqa_squad-validation-5490", "mrqa_squad-validation-5499", "mrqa_squad-validation-55", "mrqa_squad-validation-5544", "mrqa_squad-validation-5563", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5642", "mrqa_squad-validation-5664", "mrqa_squad-validation-567", "mrqa_squad-validation-5698", "mrqa_squad-validation-5708", "mrqa_squad-validation-5762", "mrqa_squad-validation-5820", "mrqa_squad-validation-5835", "mrqa_squad-validation-586", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5978", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6008", "mrqa_squad-validation-6011", "mrqa_squad-validation-6079", "mrqa_squad-validation-6109", "mrqa_squad-validation-6124", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-616", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6223", "mrqa_squad-validation-6247", "mrqa_squad-validation-6267", "mrqa_squad-validation-6273", "mrqa_squad-validation-6284", "mrqa_squad-validation-6350", "mrqa_squad-validation-6362", "mrqa_squad-validation-6382", "mrqa_squad-validation-6421", "mrqa_squad-validation-6452", "mrqa_squad-validation-6475", "mrqa_squad-validation-6509", "mrqa_squad-validation-6535", "mrqa_squad-validation-6561", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6643", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6869", "mrqa_squad-validation-6879", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-7021", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7062", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7250", "mrqa_squad-validation-7306", "mrqa_squad-validation-7474", "mrqa_squad-validation-7521", "mrqa_squad-validation-7540", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7591", "mrqa_squad-validation-7592", "mrqa_squad-validation-7598", "mrqa_squad-validation-7653", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7733", "mrqa_squad-validation-7738", "mrqa_squad-validation-7751", "mrqa_squad-validation-7758", "mrqa_squad-validation-7775", "mrqa_squad-validation-778", "mrqa_squad-validation-7827", "mrqa_squad-validation-7842", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7937", "mrqa_squad-validation-7941", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8023", "mrqa_squad-validation-8028", "mrqa_squad-validation-8066", "mrqa_squad-validation-813", "mrqa_squad-validation-8132", "mrqa_squad-validation-8174", "mrqa_squad-validation-8213", "mrqa_squad-validation-8221", "mrqa_squad-validation-8222", "mrqa_squad-validation-824", "mrqa_squad-validation-8298", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8436", "mrqa_squad-validation-8446", "mrqa_squad-validation-8458", "mrqa_squad-validation-8466", "mrqa_squad-validation-8475", "mrqa_squad-validation-85", "mrqa_squad-validation-8505", "mrqa_squad-validation-8507", "mrqa_squad-validation-8533", "mrqa_squad-validation-8538", "mrqa_squad-validation-855", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8606", "mrqa_squad-validation-8636", "mrqa_squad-validation-8656", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8790", "mrqa_squad-validation-8790", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8836", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8880", "mrqa_squad-validation-890", "mrqa_squad-validation-8941", "mrqa_squad-validation-8960", "mrqa_squad-validation-8962", "mrqa_squad-validation-8978", "mrqa_squad-validation-9008", "mrqa_squad-validation-9101", "mrqa_squad-validation-9144", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9297", "mrqa_squad-validation-9308", "mrqa_squad-validation-9343", "mrqa_squad-validation-9388", "mrqa_squad-validation-9431", "mrqa_squad-validation-9470", "mrqa_squad-validation-9499", "mrqa_squad-validation-9567", "mrqa_squad-validation-9638", "mrqa_squad-validation-9661", "mrqa_squad-validation-9692", "mrqa_squad-validation-973", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9865", "mrqa_squad-validation-9912", "mrqa_squad-validation-9923", "mrqa_squad-validation-9935", "mrqa_squad-validation-9975", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-955"], "OKR": 0.875, "KG": 0.41875, "before_eval_results": {"predictions": ["France", "Turkey", "League of Augsburg", "Marburg Colloquy", "1951", "nearly three hundred years", "enter the priesthood", "32%", "Southwest Fresno", "receptions, gatherings or exhibition purposes", "marry one of his wife's ladies-in-waiting", "Prime numbers", "six membraned chloroplast", "Timucua people,", "capturing prey", "Presiding Officer", "1521", "5 million,", "supervisory church body", "the member state cannot enforce conflicting laws,", "Ed Asner", "bitstrings", "\"Quiet Nights,\"", "Friday,", "\"momentous discovery\"", "We Found Love", "five", "\"It was a comment that shouldn't have been made and certainly one that he wished he didn't make.\"", "Karl Kr\u00f8yer", "Austin Wuennenberg,", "133", "\"Barbarian Queen\" and \" Amazon Women on the Moon.\"", "\"Mad Men\"", "the area where the single-engine Cessna 206 went down,", "record flood levels on several rivers.", "Barney Stinson,", "Mutassim, and his former defense minister, Abu Baker Yunis.", "\"The Little Couple,\"", "\"the incitement of sectarian hatred or involved in the acts of violence\"", "ambassadors", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.\"", "the picturesque Gamla Vaster neighborhood", "17", "\"I told them she never looked depressed around me.", "women and breast cancer.", "make an emotional connection to their lost loved ones.", "an affair with a woman in Argentina.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "Fullerton, California,", "the BBC's central London offices", "45 minutes, five days a week.", "his past and his future", "Arab Emirates", "Lashkar-e-Tayyiba", "July in the Philippines", "\"Personnel on the aircraft were initially treated on site and evacuated to the nearest medical facility for further treatment,\"", "San Antonio", "February 6, 2005", "Falkland Islands", "the products of all such acid-base reactions", "\"Lions for Lambs\"", "Efrem Zimbalist Jr.", "an improvement of 160 SAT points or 4 ACT points on your score,", "funk rock band"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6063589015151515}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.08333333333333333, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9047", "mrqa_squad-validation-2468", "mrqa_squad-validation-4272", "mrqa_squad-validation-1676", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-2195", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-976", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-1473", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-5158", "mrqa_hotpotqa-validation-1398", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-15716"], "SR": 0.5625, "CSR": 0.6463068181818181, "EFR": 0.9285714285714286, "Overall": 0.7331006493506493}, {"timecode": 11, "before_eval_results": {"predictions": ["manned lunar landings", "The Better Jacksonville Plan", "the perceived difficulty of its tune", "zero", "Hymn for the Weekend", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "\"Blue Harvest\" and \"420\",", "$105 billion", "Samarkand", "27", "an occupancy permit", "1774", "circuit switching", "TEU articles 4 and 5", "plastoglobulus", "Pakistan", "San Jose State", "the mouth and pharynx", "allowed local area networks to be established ad hoc without the requirement for a centralized modem or server", "erosion", "Kim Clijsters", "African National Congress Deputy President", "12-hour-plus shifts", "Kit of Elsinore", "28", "the president", "Kim Clijsters", "not feelMisty Cummings has told them everything she knows.", "Sub-Saharan Africa", "the North Korean regime intends to fire a missile toward Hawaii", "the man was dead,", "Dr. Jennifer Arnold and husband Bill Klein,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "North Korean Foreign Ministry spokesman described U.S. Vice President Dick Cheney as a \"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "his parents", "International Polo Club Palm Beach", "an antihistamine and an epinephrine auto-injector", "UNICEF", "Casey Anthony,", "a skilled hacker", "Leo Frank,", "the Niger Delta", "Cash for Clunkers", "Turkey,", "environmental videos", "can use solar and renewable energy at home everyday,\"", "41,280 pounds", "Natalie Cole", "not", "Fayetteville, North Carolina", "25 dead", "Rolling Stone", "the child's body partially submerged in a stream in shark River Park in Monmouth County on Tuesday afternoon,", "\"Stagecoach\"", "Las Vegas", "The Man", "ensure party discipline in a legislature", "the driver", "Tommy Sheridan", "Lincoln Memorial University", "Ant Timpson, Ted Geoghegan and Tim League.", "Ural owl", "sack- cloth", "20 - year period"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7317611511752137}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.4, 0.4444444444444445, 0.0, 0.23076923076923078, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 1.0, 0.72, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6248", "mrqa_squad-validation-4789", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3035", "mrqa_triviaqa-validation-319", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-2726"], "SR": 0.578125, "CSR": 0.640625, "EFR": 1.0, "Overall": 0.7462500000000001}, {"timecode": 12, "before_eval_results": {"predictions": ["green spaces", "to make the hosts responsible for reliable delivery of data, rather than the network itself,", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "15 June 1899", "Ismail El Gizouli", "by department", "the Rhine-Ruhr region", "the trans-Atlantic wireless telecommunications facility", "a bachelor's degree", "two", "Death wish Coffee", "The French Protestant Church of London", "James Gamble", "Francisco de Orellana", "\"missing self.\"", "Coldplay", "Sybilla of Normandy", "the population growth", "plate tectonics", "the liver and kidneys", "InterContinental Hotels Group", "Walter Brennan", "Warren Hastings", "in Poems : Series 1,", "April 1917", "the New Testament", "Aristotle", "in a thousand years", "a No. 16 seed", "2015", "April 2, 2018", "help batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship", "Zoe", "16 June", "the head of Lituya Bay in Alaska", "Thomas Jefferson", "1927", "HTTP / 1.1", "Labour Party", "Roger Dean Stadium", "May 2002", "in the polar climate there is more ice or snow on the ground, and this reflects the solar radiation onto the skin", "a ranking used in combat sports", "the portal tomb", "the National Football League Draft", "peninsular mainland", "159", "Missouri River", "Brazil", "318", "Kim Basinger", "Domhnall Gleeson", "the chest, back, shoulders, torso and / or legs", "the skin", "religious Hindu musical theatre styles", "jack jackson", "duck", "Kim Ki-bum", "Nanyue", "18", "Iran", "hockey", "the New York City Russian-Jewish community", "drew their"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6036440122377622}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0909090909090909, 0.3846153846153846, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1040", "mrqa_squad-validation-4786", "mrqa_squad-validation-1384", "mrqa_squad-validation-5422", "mrqa_squad-validation-103", "mrqa_squad-validation-1020", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-976", "mrqa_triviaqa-validation-6508", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-4596", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-3679", "mrqa_searchqa-validation-9108"], "SR": 0.53125, "CSR": 0.6322115384615384, "EFR": 0.9333333333333333, "Overall": 0.7312339743589744}, {"timecode": 13, "before_eval_results": {"predictions": ["the Ominde Commission", "2,000", "2005", "the university's off-campus rental policies", "seven", "Thomas Edison and George Westinghouse", "southern Suriname", "dispensing substandard products", "automobiles", "the first NASA scientist astronaut to fly in space, and landed on the Moon on the last mission, Apollo 17.", "Cam Newton", "KGPE", "the difference in potential energy between two different locations in space,", "prime elements and prime ideals", "Julia Butterfly Hill", "five", "a coomb or combe", "Vienna", "40", "Sir John Nott,", "the Spice Girls", "the hose", "Sandi Toksvig,", "Salvador Allende", "Paris", "Arkansas", "\"The Blind Side\"", "a negative effect on your quality of life", "Dennis Potter,", "Burma\u2019s", "peregrines", "a foliar spray of ethephon (Ethrel) may be used to promote early, uniform ripening and coloring, or to ripen the partially roasted fruit remaining at the end of the harvest season.", "a leucistic female", "MauritaniaMauritania", "a Chopin prelude in a public concert by the end of the year,", "Angus Hudson", "James Carville", "Charlie Sheen", "a full fat pasteurised cow's milk soft cheese", "sheep", "Concepcion,", "the Republic of Upper Volta", "Laurie Lee", "Karl Marx and Friedrich Engels", "John Mortimer", "Burgundy", "Humphrey Bogart", "corey kiedis", "Kansas", "corey Woodpecker", "Carl Sagan and his wife and co-writer, Ann Druyan,", "Alex Turner", "a St. Tropez drag-show nightclub owned by Georges and headlined by his longtime love Albin.", "commitment", "his brother", "2001 -- 2002 season,", "\"Back to December\"", "Michael Lewis Greenwell", "an independent homeland since 1983.", "an angry mob.", "a domestic cat", "a doctor that specializes in...", "a 1992 American action-thriller film directed by Andrew Davis and written by J.F. Lawton.", "International Boxing Hall of Fame"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6246103896103896}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6399999999999999, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6342", "mrqa_squad-validation-3899", "mrqa_squad-validation-9087", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-4427", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-546", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-4630", "mrqa_hotpotqa-validation-3507", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-530", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-14224", "mrqa_hotpotqa-validation-5043"], "SR": 0.578125, "CSR": 0.6283482142857143, "EFR": 0.9629629629629629, "Overall": 0.7363872354497355}, {"timecode": 14, "before_eval_results": {"predictions": ["Iberia", "contemporary accounts were exaggerations", "as an auditor", "Victoria", "erosion", "Cricket", "2", "Gabriel Zwilling", "\"The Day of the Doctor\"", "through confirmation and sometimes the profession of faith", "in the city of Deabolis", "Wijk bij Duurstede", "June 6, 1951", "24 September 2007", "18", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "Teri Garr", "Malayalam", "lamb", "the red - bed country of its watershed", "DNA was the genetic material", "Steve Russell", "Pangaea", "The first Twenty20 match held at Lord's, on 15 July 2004 between Middlesex and Surrey", "Dalveer Bhandari", "boy", "Ingrid Bergman", "Michael Jackson and Lionel Richie", "6 March 1983", "a crossed telephone connection", "Glenn Close", "Andreas Vesalius", "1959", "on location", "in the year 2026", "1901", "Gorakhpur", "electors", "1834", "Indian Standard Time", "New York University", "The Ecology", "Sophia Akuffo", "Hagrid", "2", "FY2010", "regulatory", "a means of restarting play after a minor infringement", "the Infamy Speech of US President Franklin D. Roosevelt", "Wembley Stadium", "1992", "Jonathan Cheban", "green", "4", "mandala", "in Arabah", "May 27, 2016", "a frigate", "some of them are pretty common and not dangerous, like plantar warts or warts on your hand.", "Lonnie", "anima1s", "angus", "Lake Placid, New York", "Russell T Davies"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6253605769230769}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1076", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-4998", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-3977", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-11598", "mrqa_hotpotqa-validation-2357"], "SR": 0.546875, "CSR": 0.6229166666666667, "EFR": 0.9310344827586207, "Overall": 0.7289152298850575}, {"timecode": 15, "before_eval_results": {"predictions": ["The Writers Guild of America", "every five years", "NFIL3", "826", "Treaty of Logstown", "mid-Cambrian", "Cabot Science Library, Lamont Library, and Widener Library", "Palestine", "PNU and ODM camps", "The waxy cuticle of many leaves", "2,200", "KMBC-TV and KQTV", "whether he stood by their contents", "jedoublen", "the \"Hudson River Bridge\"", "jordan", "Helen", "The Physical Basis of Long-Range Weather Forecasts", "Southern elephant", "Sally Margaret", "Thurgood", "The Mysterious Island", "jordan", "Raspberry Pi", "Agamemnon", "jedoublen/jeopardy", "to raise money for the Muscular Dystrophy Association", "jordan", "Boeing", "Alberta", "Kareem Abdul-Jabbar", "Masada", "undercard", "Willa Cather", "jEWELRY", "keith urban", "French", "deer", "jedoublen", "drowsy", "change to a boat", "five", "Rick Springfield", "Lubnan", "100", "sweet, musical, or pleasant to hear", "jordan", "Pulex irritans", "sefirot", "palomino", "piedmont", "Kate Warne", "Elisabeth", "Rent", "Camping World Stadium in Orlando, Florida", "John Cooper Clarke", "President Woodrow Wilson", "Nicola Adams", "1943", "Pearl Jam", "\"The Tennessee Valley Authority, which supplies power to almost 9 million Americans, \"has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\"", "there is not a process to ensure that auto owners comply with recalls", "two", "16\u201321"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4760251696832579}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0784313725490196, 0.15384615384615385, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4802", "mrqa_squad-validation-6435", "mrqa_searchqa-validation-12940", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-12272", "mrqa_searchqa-validation-9570", "mrqa_searchqa-validation-8796", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-10319", "mrqa_searchqa-validation-4536", "mrqa_searchqa-validation-1446", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-4133", "mrqa_searchqa-validation-16715", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-13578", "mrqa_searchqa-validation-1624", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-9561", "mrqa_searchqa-validation-2200", "mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-14789", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2358", "mrqa_hotpotqa-validation-5438"], "SR": 0.40625, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.74}, {"timecode": 16, "before_eval_results": {"predictions": ["within the Church of England", "Lenin", "a qualified majority vote", "36", "Brough Park in Byker", "2012", "Stress", "physics", "Jonathan Stewart", "George Westinghouse", "human", "British military resources in the colonies", "Aquitaine", "(B) Baggins", "Florida", "forests", "the Netherlands", "the Ritz-Carlton Hotel Company", "kiss a fool", "masks", "(J) K. Polk Middle School", "the Sons of Liberty", "a cinema", "National Security Agency", "Ugly Betty", "a vagrant opinion without visible means of support", "(Odocoileus virginianus)", "The All-New Blue Ribbon Cookbook", "Flowerbomb", "Pheonix", "Amy Fisher", "A Portrait of the Artist as a Young Man", "a tumbler", "guttural", "polio", "Meg Tilly", "the Mausoleum of Halicarnassus", "(George) III", "Annie Braddock", "(George) Francis", "the Firmament", "dark places", "(Jerry) Maguire", "Sister Wendy", "(Matthew) Broderick", "the flyer", "\"Bewitched, Bothered and Bewildered\"", "4 p.m.", "Samuel Goldwyn", "Annika Sorenstam", "(George) De Gustibus Cooking School", "Thurman Munson", "Washington, D.C.", "flip-flops", "the International Border", "1783", "chile stew", "(Walker) Smith, Jr.", "McComb, Mississippi", "Bea Arthur", "Tutsi and Hutu rivalry", "last week.", "18", "\"Twilight\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.6314562447786132}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.10526315789473682, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4216", "mrqa_squad-validation-5456", "mrqa_squad-validation-10388", "mrqa_squad-validation-10158", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-11531", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-16832", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-768", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-366", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9957", "mrqa_searchqa-validation-4484", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-2627", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-608"], "SR": 0.53125, "CSR": 0.6047794117647058, "EFR": 0.9666666666666667, "Overall": 0.7324142156862745}, {"timecode": 17, "before_eval_results": {"predictions": ["49\u201315", "113", "protein structure prediction", "Deformational events", "2004", "Department of State Affairs", "Prague", "attempted to enter the test site knowing that they faced arrest", "governments", "M\u00f6ngke Khan", "by using net wealth (adding up assets and subtracting debts),", "Danny Lee", "snow", "Liberation-Marines", "Romeo and Juliet", "Jane Addams", "Rand McNally & Company", "President Harry S. Truman", "the pound sterling", "Auguste Rodin", "the Andes", "Sherlock Holmes", "the Taj Mittal", "the trampoline", "an axe", "rice", "Constantine", "Daniel Inouye", "krypton", "a Buddhist Abbey", "Kung Fu", "The Star-Crossed Stars of Showgirls", "a cocktail", "The GNTCE", "Milwaukee", "silver", "Bangkok", "Soviet Union", "a vodka", "King Henry V", "a doses", "Frank Sinatra", "Christopher Columbus", "the King of the Hill", "Schtze Benjamin", "Donald and Nellie Ruth Pillsbury King", "George Gordon Byron", "Japan", "Joan of Arc", "Jaguar", "orange skin and green hair", "a hound", "(R) Stanton Avery", "U.S. Bank Stadium", "The Golden Gate Bridge", "Jim hacker", "Christopher Hancock", "1976", "(IATA: VNO, ICAO: EYVI)", "Cipro, Levaquin, Avelox, Noroxin and Floxin", "murder", "Parlophone Records", "five times", "known as La R\u00e9sistance"], "metric_results": {"EM": 0.5, "QA-F1": 0.6346726190476191}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.8, 1.0, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2819", "mrqa_squad-validation-6702", "mrqa_squad-validation-7554", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-7825", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-2546", "mrqa_searchqa-validation-5491", "mrqa_searchqa-validation-4402", "mrqa_naturalquestions-validation-5674", "mrqa_triviaqa-validation-6228", "mrqa_hotpotqa-validation-3728", "mrqa_newsqa-validation-57", "mrqa_hotpotqa-validation-5499"], "SR": 0.5, "CSR": 0.5989583333333333, "EFR": 1.0, "Overall": 0.7379166666666667}, {"timecode": 18, "before_eval_results": {"predictions": ["hunter's garb", "from the official declaration of war in 1756 to the signing of the peace treaty in 1763", "two forces", "a computational problem where a single output (of a total function) is expected for every input", "10 to 15 million", "the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal", "Establishing \"natural borders\"", "Houston Street lab", "vary by geographic area and subject taught", "\"Turks\" (Muslims) and Catholics", "provides the public with financial information about a nonprofit organization", "the Northeast Monsoon or Retreating Monsoon", "April 3, 1973", "Fa Ze YouTubers", "July 14, 1969", "Krypton", "Mandarin", "in the bone marrow", "Ukraine", "Coldplay", "Yugoslavia", "head coach", "July 20, 2017", "T - Bone Walker", "April 2, 2018", "Rose Stagg ( Valene Kane )", "Doug Diemoz", "Iran", "southern Anatolia", "classical architecture", "the pyloric valve", "1546", "100,000 writes", "the temple", "16 seasons", "Long Island", "A lacteal", "Mel Jones", "1987", "Jim Capaldi, Paul Carrack, and Peter Vale", "the Hallertau", "Jikji", "Panning", "the RAF", "the National Football League ( NFL )", "Detective Superintendent Dave Kelly", "Morshower", "Ray Henderson", "provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "New Orleans", "the Outback", "an unmasked and redeemed Anakin Skywalker", "Cliff's father, Russell Huxtable", "the Republic of Namibia", "bruschetta", "all-time leader", "Rawlings", "eight", "Los Angeles", "North by Northwest", "Naples", "Etna", "King Edward VI", "North Rhine-Westphalia"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5552649318274319}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.962962962962963, 0.6666666666666666, 0.2666666666666667, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10121", "mrqa_squad-validation-10395", "mrqa_squad-validation-1600", "mrqa_squad-validation-3300", "mrqa_squad-validation-2054", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-3727", "mrqa_hotpotqa-validation-466", "mrqa_searchqa-validation-3761", "mrqa_searchqa-validation-9438"], "SR": 0.453125, "CSR": 0.591282894736842, "EFR": 0.9714285714285714, "Overall": 0.7306672932330828}, {"timecode": 19, "before_eval_results": {"predictions": ["Thomas Vasey and Richard Whatcoat", "reminding their countrymen of injustice", "sex offenders register", "Kenya", "the violence that subsequently engulfed the country", "aristocracy", "1905", "Saul Alinsky", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "U + 002A * Asterisk", "February 1834", "Jonathan Goldstein", "Yahya Khan", "milk", "BC Jean and Toby Gad", "Nacio Herb Brown", "Anatomy", "Pyeongchang County, Gangwon Province, South Korea", "Steffy Forrester", "Oregon Ringwald", "2017", "follows a child with Treacher Collins syndrome trying to fit in", "Andaman and Nicobar Islands", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals ), blood plasma and lymph", "member states", "in the United Kingdom", "Rocinante", "1978", "May 1, 2018", "Ron Clements and John Musker", "pictura", "Cheitharol Kummaba", "1", "alveolar process", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "October 2017", "16 for females and 18 for males", "July 21, 1861", "Vatican City", "45 %", "in the bible", "a list of autopistas, or tolled ( quota ) highways", "actress Niketa Calame", "Robert E. Lee", "Clarence L. Tinker", "Soviet Russia defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "the church at Philippi", "a federal republic", "Thomas Edison", "mitosis", "hydrogen and oxygen", "pit road speed", "Pyeongchang County, Gangwon Province, South Korea", "Glory", "Gianni Versace", "David Simon", "1999", "Madrid", "Seoul", "an Airborne", "serving the tea", "three", "the Dalai Lama", "Christopher Savoie"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6364285960120238}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.9473684210526316, 0.8, 0.41379310344827586, 0.4444444444444445, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 0.0, 1.0, 0.0, 0.25, 0.8, 0.7499999999999999, 0.4615384615384615, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8370", "mrqa_squad-validation-9640", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-8075", "mrqa_triviaqa-validation-2196", "mrqa_newsqa-validation-649", "mrqa_searchqa-validation-10274", "mrqa_searchqa-validation-2516"], "SR": 0.484375, "CSR": 0.5859375, "EFR": 0.9696969696969697, "Overall": 0.7292518939393939}, {"timecode": 20, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1705", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4329", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-466", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-12", "mrqa_naturalquestions-validation-1436", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3376", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9809", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3578", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10708", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-11360", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11598", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16636", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1924", "mrqa_searchqa-validation-1928", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2887", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-3899", "mrqa_searchqa-validation-4133", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4750", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6233", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9561", "mrqa_searchqa-validation-9570", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10158", "mrqa_squad-validation-10162", "mrqa_squad-validation-10198", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10471", "mrqa_squad-validation-1076", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1188", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1330", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1424", "mrqa_squad-validation-1506", "mrqa_squad-validation-1540", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1611", "mrqa_squad-validation-1703", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1834", "mrqa_squad-validation-1908", "mrqa_squad-validation-1976", "mrqa_squad-validation-2015", "mrqa_squad-validation-2025", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2111", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2250", "mrqa_squad-validation-2395", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2532", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-3001", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3193", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-331", "mrqa_squad-validation-3368", "mrqa_squad-validation-3449", "mrqa_squad-validation-3493", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3626", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3948", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4159", "mrqa_squad-validation-4176", "mrqa_squad-validation-4248", "mrqa_squad-validation-4248", "mrqa_squad-validation-4272", "mrqa_squad-validation-4274", "mrqa_squad-validation-4301", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4686", "mrqa_squad-validation-4698", "mrqa_squad-validation-4765", "mrqa_squad-validation-4789", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-501", "mrqa_squad-validation-5133", "mrqa_squad-validation-5157", "mrqa_squad-validation-5214", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-55", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5664", "mrqa_squad-validation-5715", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5897", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6251", "mrqa_squad-validation-6253", "mrqa_squad-validation-6264", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6435", "mrqa_squad-validation-6452", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7191", "mrqa_squad-validation-7226", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7592", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7751", "mrqa_squad-validation-7775", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7889", "mrqa_squad-validation-7932", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8010", "mrqa_squad-validation-8019", "mrqa_squad-validation-8199", "mrqa_squad-validation-8213", "mrqa_squad-validation-826", "mrqa_squad-validation-8278", "mrqa_squad-validation-8298", "mrqa_squad-validation-830", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8383", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-861", "mrqa_squad-validation-8612", "mrqa_squad-validation-8636", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8786", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9308", "mrqa_squad-validation-9315", "mrqa_squad-validation-9322", "mrqa_squad-validation-9388", "mrqa_squad-validation-9405", "mrqa_squad-validation-9431", "mrqa_squad-validation-9495", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9640", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9865", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1646", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5140", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6531", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-88"], "OKR": 0.841796875, "KG": 0.43828125, "before_eval_results": {"predictions": ["the dukes", "saw the Muslim faith as a tool of the devil, he was indifferent to its practice", "Ed Lee", "40%", "a post-World War I environment", "8000", "can produce both eggs and sperm at the same time", "\u00d6gedei Khan", "June 24, 1935", "Kohlberg K Travis Roberts", "John McClane", "International Writings by Steve Biko", "1910s", "between 11 or 13 and 18", "Song Il-gon", "Let's Make Sure We Kiss Goodbye", "Martin \"Marty\" McCann", "a book written by Mercury Seven astronaut Alan Shepard, with NBC News correspondent Jay Barbree and Associated Press space writer Howard Benedict.", "Paul John Mueller Jr.", "December 17, 1974", "\"The Royal Family\"", "Sir Seretse Khama", "Mike Holmgren", "S7", "a British archaeologist, military officer, diplomat, and writer", "Donald Richard \"Don\" DeLillo", "University of Nevada, Las Vegas", "a co-op of grape growers", "Brazil", "five", "Heart", "Bank of China", "Harry Meadows", "Bisexuality", "Winnie the Pooh", "Excalibur Hotel and Casino", "Rigoletto", "Knoxville, Tennessee", "Americana Manhasset", "Omega SA", "1978", "Pim Fortuyn List", "eastern shore", "Todd McFarlane", "M. Night Shyamalan", "Magdalen College", "Eric Allan Kramer", "22 April 1894 in Kuld\u012bga, Russian Empire, now Latvia \u2013 29 January 1969 in New York City, United States", "Province of New York", "a palace on US soil", "Jamaica", "Ashley Leggat", "1933", "mind your manners", "Charles Darwin", "the Football-Stadiums.co.uk", "Katherine Parr", "his past and his future", "British", "Engelbert Humperdinck", "the Colorado River", "John Denver", "The Princess bride", "a republic in W Africa"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5656536737718804}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.125, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.17391304347826084, 0.25, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.36363636363636365, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.375, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2293", "mrqa_squad-validation-1608", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-891", "mrqa_naturalquestions-validation-4109", "mrqa_triviaqa-validation-7452", "mrqa_newsqa-validation-3889", "mrqa_triviaqa-validation-6564"], "SR": 0.484375, "CSR": 0.5811011904761905, "EFR": 1.0, "Overall": 0.7124702380952381}, {"timecode": 21, "before_eval_results": {"predictions": ["two forced fumbles", "ended the true Islamic system, something for which it blames \"the disbelieving (Kafir) colonial powers", "pharmacy practice science and applied information science", "thought it may have been a combination of anthrax and other pandemics", "1912", "rubisco", "the Huguenot rebellions", "Psych", "Ladin", "August 11, 1946", "Protestant Christian", "Queens, New York", "Erreway", "Oakland County", "Madeleine L'Engle", "FAI Junior Cup", "1966 US tour", "February 18, 1965", "Sydney", "Cuban descent", "1928", "51s Janine Helfer", "Mickey's Twice Upon a Christmas", "Eielson Air Force Base", "Taylor Swift", "Italy", "Tel Aviv University", "the Donny & Marie Showroom, at the Flamingo Las Vegas", "John of Gaunt", "William Clark Gable", "Kmart", "C. J. Cherryh", "Spanish", "KDCU-DT", "Axl Rose", "Daniil Shafran", "Sharman Joshi", "1912", "four", "13 October 1958", "70", "3 May 1958", "The 2007 Trail Appliances Autumn Gold Curling Classic", "Vancouver", "Marlborough", "Fountains of Wayne", "mid-ninth-century Viking chieftain", "sulfur mustard", "The Saturdays", "Chick tract", "southwestern", "The Campbell Soup Company", "The Gang", "2010", "The vascular cambium", "A", "dark, spicy", "Rod Blagojevich", "Iraq", "bhakti", "Harley-Davidson", "last day at the MCG", "$60 billion", "taste a hamburger and pizza, and drink coffee from a cup, the \"things we take for granted every day,\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5551058395838441}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.47058823529411764, 1.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.4, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.20689655172413793]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_squad-validation-9918", "mrqa_squad-validation-5029", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-1287", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-51", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1435", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2301", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1784", "mrqa_searchqa-validation-7453", "mrqa_searchqa-validation-2193", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-1093"], "SR": 0.453125, "CSR": 0.5752840909090908, "EFR": 1.0, "Overall": 0.7113068181818182}, {"timecode": 22, "before_eval_results": {"predictions": ["one of the daughters of former King of Thebes, Oedipus", "environmental determinism", "simulcast the game on its sister station WBT-FM (99.3 FM), which is based in Chester, South Carolina", "Articles 106 and 107", "infected corpses", "Cobb Lecture Hall", "Cortina d'Ampezzo", "Eric Edward Whitacre", "1983", "EQT Plaza in Pittsburgh, Pennsylvania", "Ruth Westheimer", "Giotto", "Nickelodeon", "Brad Wilk", "Robert Matthew Hurley", "11", "Journal for Writers and Readers", "March 19, 2017", "Disney California Adventure", "Anah\u00ed Giovanna Puente Portilla de Velasco", "Nicholas John \"Nic\" Cester", "Anne, Princess Royal", "Harry Robbins \"Bob\" Haldeman", "247,597", "tales of various deities, beings, and heroes", "directed several episodes of the popular sitcom \"Friends\"", "40 million", "Billund, Denmark", "in Africa", "William Cavendish", "20 March to 1 May 2003", "23 July 1989", "Kinnairdy Castle", "Ken Rutherford", "\"The Catcher in the Rye\"", "Indianapolis Motor Speedway", "Marjorie Jacqueline \"Marge\" Simpson", "May 4, 1924", "Transporter 3", "Richard Allen Street", "\"Queen In-hyun's Man\"", "Steve Carell", "Green Chair", "The Princess and the Frog", "24 April 1882", "German", "November 10, 2017", "CTV Television Network", "Australian", "Rafael Palmeiro", "Eric Allan Kramer", "\"Orchard County\"", "can raise the relative humidity to 100 % and create clouds", "Reproductive system", "Coroebus of Elis", "\u201cMy Favorite Martian,\u201d", "Disraeli", "Jennifer Ellison", "crossfire", "5 1/2-year-old", "one day,", "an orchestra", "the Supreme Court Building", "Pledge of Allegiance"], "metric_results": {"EM": 0.515625, "QA-F1": 0.640327380952381}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.4, 0.0, 1.0, 0.8333333333333333, 0.2, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_squad-validation-688", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5636", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-4204", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3629", "mrqa_naturalquestions-validation-581", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-4189", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-8667"], "SR": 0.515625, "CSR": 0.5726902173913043, "EFR": 1.0, "Overall": 0.7107880434782609}, {"timecode": 23, "before_eval_results": {"predictions": ["Article 17(1)", "Great Khan", "the Department of Justice", "tidal currents", "returned the ball 19 yards to the Panthers 39-yard line", "The Eleventh Doctor", "Malware", "Claims adjuster", "1995", "Caleb", "Arunachal Pradesh", "1998", "Richard of Shrewsbury", "the Roman Empire", "1985", "Stevie Wonder", "18", "to form a higher alkane", "Percy Jackson", "Gil", "final scene of the fourth season", "UMBC", "the customer's account", "the New Croton Reservoir", "Elena Anaya", "January 2018", "a prison", "Jaffa Cakes", "Woodrow Wilson", "negotiates treaties with foreign nations", "Waylon Jennings", "the English", "the New York Yankees", "The virion must assemble a stable, protective protein shell to protect the genome", "Turducken", "a balance sheet", "San Francisco Bay", "Mickey Mantle", "embroidered skill badges", "1956", "After being absent for a time, they were reintroduced to grocery stores under the Popsicle brand name", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "2014 Olympic Games in Sochi", "David Gahan", "the town of Acolman", "South Asia", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "turkey", "octave", "New England Patriots", "Tristan Rogers", "novella", "the Brady Bunch", "Oliver Stone", "Mexico", "Apsley George Benet Cherry-Garrard", "the Ecumenical Award", "Brig Gen Augustine Warner Robins", "Florida", "Department of Homeland Security", "Elisabeth", "the highest state", "gusto", "Ulysses"], "metric_results": {"EM": 0.53125, "QA-F1": 0.621984708050907}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true], "QA-F1": [0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9411764705882353, 0.33333333333333337, 0.5, 0.0, 1.0, 0.6976744186046512, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4296", "mrqa_squad-validation-8230", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-10586", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-14304"], "SR": 0.53125, "CSR": 0.5709635416666667, "EFR": 0.9666666666666667, "Overall": 0.7037760416666667}, {"timecode": 24, "before_eval_results": {"predictions": ["the third and fourth series respectively", "Doctor of Theology", "c1750", "60%", "a deterministic Turing machine", "Henry III of England", "Henkel", "David Bowie", "George Frideric Handel", "Pol Pot, butcher of Cambodia", "the Coma Pedrosa", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Motel 6", "a downtown restaurant in New York\u2019s Greenwich Village", "Virginia", "Isaac", "a ball", "Lilliput", "the gallbladder", "Thabo Mbeki", "1921", "a dog", "Robert Schumann", "The Benedictine Order", "(George) Pavin", "translator", "King County Executive", "Scotland", "The Penguin", "The Sahara", "Zelle, who was shot by the French as a spy on 15 October 1917.", "Saturn\u2019s Rings", "the Brisbane River", "The Aidensfield Arms", "armada", "Liechtenstein", "I was born in a cross-fire hurricane and raised by a toothless bearded hag", "Rodney", "L. Cochran Jr.", "the UK", "Prokofiev", "horses", "(Antony) Hawksworth, better known as Tony Hawks, (born 1960)", "Kansas", "Australia", "EGBDF", "\"Little Red Rented Rowboat\"", "the sense of smell", "Saul", "Miss Daisy is a 1989 American comedy-drama film adapted from the Alfred Uhry play of the same name", "The Isles of the Blessed", "Anthropocene", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Darren McGavin", "Sir Henry Cole", "Scottish", "Sarah Hurst", "1964", "Lyon", "Jason Chaffetz", "the martyrs of their tribe", "an analog watch", "(Bill Murray)", "Alexander Haig Jr."], "metric_results": {"EM": 0.40625, "QA-F1": 0.5124190301120448}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false], "QA-F1": [0.25, 0.3333333333333333, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.4, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-7762", "mrqa_squad-validation-2126", "mrqa_squad-validation-1819", "mrqa_triviaqa-validation-5569", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-2203", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-1702", "mrqa_naturalquestions-validation-3189", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-1053", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-2385", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-11092", "mrqa_searchqa-validation-1228"], "SR": 0.40625, "CSR": 0.5643750000000001, "EFR": 0.9736842105263158, "Overall": 0.7038618421052633}, {"timecode": 25, "before_eval_results": {"predictions": ["commissioned the impressive staircase that rises the full height of the building, made from Cadeby stone the steps are 7 feet (2.1 m) in length", "floor function", "QuickBooks", "Westinghouse Electric", "The Bachelor", "statue", "2018", "Georgia Bulldogs", "Honor\u00e9 Mirabeau", "England and Wales", "Eleanor Roosevelt", "radioisotope thermoelectric generator", "1908", "honey bees", "Samantha Jo `` Mandy '' Moore", "John Roberts", "Mangal Pandey", "Woodrow Strode", "Woodrow Wilson", "four", "tissues", "season two", "January 17, 1899", "Ptolemy's geocentric model", "left Behind is a series of 16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "British Ultra code - breaking intelligence", "602", "inverted", "5.7 million customer accounts", "Vincent Price", "Steve Russell", "Janis Joplin", "1871", "September 9, 2012", "Ahmad Givens ( Real )", "Haiti", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "October 14, 2017", "by 2026", "2018", "The Outback", "Deputy Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "2004", "Australia", "eleven", "Master Christopher Jones", "around 124 and 800 CE", "12.65 m", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "2017", "in the Tremont neighborhood of Cleveland, Ohio", "1986", "Italy", "music (to be performed) in a fiery manner", "Culture Club", "These Are Special Times", "Indian", "Sofia the First", "software magnate", "Hurricane Gustav", "21", "Joseph Heller", "austria", "uranium"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6215186403508772}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.8, 0.0, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-6935", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7128", "mrqa_hotpotqa-validation-3423", "mrqa_hotpotqa-validation-3842", "mrqa_newsqa-validation-1234", "mrqa_searchqa-validation-3483", "mrqa_searchqa-validation-9088"], "SR": 0.546875, "CSR": 0.5637019230769231, "EFR": 0.9655172413793104, "Overall": 0.7020938328912467}, {"timecode": 26, "before_eval_results": {"predictions": ["the U.S.", "Thomas Commerford Martin", "seven", "capital equipment for labor inputs (workers)", "1996", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Indraprastha", "John Quincy Adams", "third season", "Christopher Columbus", "a child with Treacher Collins syndrome trying to fit in", "an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "President of the United States", "It is presided over by a Chief Justice and is composed of fifteen ( 15 ) Justices, including the Chief Justice", "in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape ''", "Mandy", "king Harold Godwinson", "during meiosis", "producer Norman Whitfield", "Spanish / Basque origin", "in early evenings to call ( in spring and summer ) and hunt for food", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "January 2, 1971", "Hirschman", "in the books of Exodus and Deuteronomy", "at Lucknow in September 1906", "Neuropsychology", "The User State Migration Tool", "Effy", "May 1, 2018", "291 episodes in Japan", "in 1790 passed the first naturalization law for the United States, the Naturalization Act of 1790", "flawed democracy", "in Pashto and Persian", "last Ice Age", "Ren\u00e9 Descartes", "in people and animals that collects and stores urine from the kidneys before disposal by urination", "The First Battle of Bull Run", "Justinardo de Stinky", "in The Empire Strikes Back", "Germany", "in London, United Kingdom", "Jeff East", "President Yahya Khan", "in Greek theology, the evil eye or vaskania ( \u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1 ) is considered harmful for the one whose envy inflicts it on others as well as for the sufferer", "in the 2nd century", "Arnold Schoenberg", "~ 3.5 million years old from Idaho, USA", "111", "when a population temporarily exceeds the long term carrying capacity of its environment", "Mike Leeson and Peter Vale", "gross, that is, it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "Joan Crawford", "Andorra", "Ned Sherrin", "Band-e Amir National Park", "Cartoon Network", "Chrysler", "in a firefight Friday in Afghanistan,", "more than 100", "to comfort those in mourning", "Roanoke Colony", "with the New York City Ballet", "apples"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6599498614308397}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 0.15999999999999998, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.08, 0.9777777777777777, 1.0, 1.0, 0.2857142857142857, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.4, 0.17391304347826086, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.8571428571428571, 1.0, 0.8, 0.15384615384615385, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.5714285714285715, 0.9130434782608696, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.888888888888889, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7185", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-9812", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-2887", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-1694", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-7787"], "SR": 0.46875, "CSR": 0.5601851851851851, "EFR": 1.0, "Overall": 0.7082870370370371}, {"timecode": 27, "before_eval_results": {"predictions": ["eight days after their initial broadcast", "after sustaining an injury which would be fatal to most other species", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "highly conserved among land plants, and accumulate few mutations", "Naples", "black, red or white,", "2009", "the motherless cub defended by Elphaba in \"Wicked.\"", "Diego Maradona", "Harkat-ul-Jihad al-Islami", "\"I'm certainly not nearly as good of a speaker as he is.\"", "\"I don't watch TV,\"", "golf", "AOL Autos", "Floxin", "Coptic", "February 12", "Roberto Micheletti", "in the last year,", "Shiza Shahid,", "Akio Toyoda", "two awards.", "African-Americans", "environmental and political events.", "\"I'm going to deny that motion,\"", "three empty vodka bottles,", "Euna Lee,", "Angela Merkel", "the Ku Klux Klan", "dental work done, including removal of his diamond-studded teeth.", "Manuel Mejia Munera", "requires police to question people if there's reason to suspect they're in the United States illegally.", "UNICEF", "club managers,", "used", "\"I've got a good group of Marines that are behind me, so I'm real excited about the deployment,\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "suicides", "\"Singh is Kinng,\"", "Karen Floyd", "Mary Phagan", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "Cogentin and Haldol,", "about 1,400 people at Hanford, including Joe Gill who manages a team that is tearing down equipment that is heavily contaminated by radiation.", "\"A flight engineer, whose name was not released, was critically injured and was in a coma, Peterka said.", "Krishna Rajaram,", "flooding was so fast that the thing flipped over,\"", "In the 57th-minute when striker Milito collected a pass from fellow-Argentine Javier Zanetti, before firing home a shot with the outside of his right foot.", "Dubai", "up three of the last four months.", "Sunday,", "12-hour-plus", "three times", "Canada", "South Dakota", "Carolus Linnaeus", "potash", "Something In The Air", "16 March 1987", "Peter Kay's Car Share", "Cyclic Defrost", "pfeffernuesse", "skip school", "octave"], "metric_results": {"EM": 0.46875, "QA-F1": 0.509491047745358}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [0.2857142857142857, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 1.0, 1.0, 0.46153846153846156, 0.42857142857142855, 0.07692307692307691, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5843", "mrqa_squad-validation-4648", "mrqa_squad-validation-8704", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2325", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-2456", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7659", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-5378", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-697"], "SR": 0.46875, "CSR": 0.5569196428571428, "EFR": 1.0, "Overall": 0.7076339285714286}, {"timecode": 28, "before_eval_results": {"predictions": ["deserts", "political parties", "The crew that flew the first Earth orbital test mission Apollo 7, Walter M. Schirra, Donn Eisele, and Walter Cunningham,", "Amos McCracken,", "Cherokee Nation", "Flamingo Las Vegas", "Gran Sasso d'Italia,", "Southern Africa", "2013,", "Luis Edgardo Resto", "the Salzburg Festival", "Jay Park", "Blackpool Football Club", "New Orleans Saints", "2012", "The WB supernatural drama series \"Charmed\"", "Dundalk, County Louth, Ireland", "Ashley Jensen", "Syracuse University", "Eileen Atkins,", "Mollie Elizabeth King (born 4 June 1987)", "Esteban Ocon", "the flags of dependent territories", "Ouse and Foss", "Northern", "\"Casablanca\"", "The Go-Go's", "1943", "Sleepy Hollow", "Ronnie Schell", "Wandsworth, London", "Christopher Lloyd Smalling (born 22 November 1989)", "Chevron", "World Music Awards", "La Liga", "French", "Floyd Nathaniel \"Nate\" Hills", "Fort Hood, Texas", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay, and Keller", "Lauren Alaina", "1961", "London", "Preston, Lancashire, UK", "Prudential Center in Newark, New Jersey", "\"Krabby Road\"", "dimensionless quantity", "from 1945 to 1951", "Mexico", "Chevy Motor Car Company", "Jack Rabbit (Seabreeze)", "Disco", "Theodor W. Adorno", "re-education", "Sir Rowland Hill", "compound sentence", "Jimmy Carter", "John Logie Baird,", "1066", "Joan Rivers", "her heart condition or the medical procedure.", "auction off one of the earliest versions of the Magna Carta later this year,", "the Wall of the Moon", "Windows 7", "corn"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5871489621489621}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2791", "mrqa_squad-validation-4060", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-2699", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4611", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-1926", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-1667", "mrqa_naturalquestions-validation-8329", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-9671", "mrqa_searchqa-validation-14497", "mrqa_searchqa-validation-14791"], "SR": 0.46875, "CSR": 0.5538793103448276, "EFR": 0.9705882352941176, "Overall": 0.7011435091277891}, {"timecode": 29, "before_eval_results": {"predictions": ["some teachers and parents", "July 1969", "glaucophyte", "God and the just cause", "Mercury Records", "Oksana Grishuk", "Nye County", "Karolina Dean", "Firestorm", "ten", "White Knights of the Ku Klux Klan", "the Chechen Republic", "Green Lantern", "Kansas City", "March 16, 1927", "English", "Food and Agriculture Organization", "Idaho", "Jeff Meldrum", "Crossed: Dead or Alive", "Romance language", "Philip K. Dick", "80%", "English", "CN Too", "David Starkey", "Cherokee River", "pop music and popular culture", "Field Marshal Lord Gort", "Hopeless Records", "Razor Ramon", "Godspell", "8 August 1907", "Boeing 757", "The 7.63\u00d725mm Mauser (.30 Mauser Automatic)", "Bangkok", "51st Disney animated feature film.", "his exploration and settlement", "August 28, 1774", "Afghanistan", "British", "Potomac River", "the Netherlands", "Love the Way You Lie", "Rio Gavin Ferdinand", "Boston", "Las Vegas", "pilot and aviation", "Washington and Lee University", "St. Louis, Missouri", "Tsung-Dao Lee", "Bay Ridge, Brooklyn", "Human anatomy", "Tyler, Ali", "Pacific Grove", "AFC Wimbledon", "The Duke of Plaza Toro", "2", "Meira Kumar", "Public Citizen", "bartering -- trading goods and services without exchanging money", "spoiled", "malted", "Iceland"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6572296626984128}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-4268", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-3362", "mrqa_triviaqa-validation-6131", "mrqa_triviaqa-validation-4462", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-719", "mrqa_searchqa-validation-11933"], "SR": 0.59375, "CSR": 0.5552083333333333, "EFR": 1.0, "Overall": 0.7072916666666667}, {"timecode": 30, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1079", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1524", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2823", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3382", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3950", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4058", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4334", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4953", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-5313", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-932", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-999", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9867", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1582", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-719", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-958", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-1228", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1828", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9613", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1379", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1546", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1600", "mrqa_squad-validation-1751", "mrqa_squad-validation-1819", "mrqa_squad-validation-1908", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-2025", "mrqa_squad-validation-2106", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-2848", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-3001", "mrqa_squad-validation-3103", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3449", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-4065", "mrqa_squad-validation-4132", "mrqa_squad-validation-4159", "mrqa_squad-validation-4216", "mrqa_squad-validation-4248", "mrqa_squad-validation-4274", "mrqa_squad-validation-4472", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4698", "mrqa_squad-validation-4736", "mrqa_squad-validation-4765", "mrqa_squad-validation-4772", "mrqa_squad-validation-4789", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5270", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5908", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6382", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7043", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7217", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7564", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7775", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7937", "mrqa_squad-validation-8010", "mrqa_squad-validation-8023", "mrqa_squad-validation-826", "mrqa_squad-validation-8298", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8466", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-8612", "mrqa_squad-validation-8665", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9308", "mrqa_squad-validation-9499", "mrqa_squad-validation-9594", "mrqa_squad-validation-9638", "mrqa_squad-validation-9918", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-4504", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6173", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7452", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-88"], "OKR": 0.81640625, "KG": 0.44453125, "before_eval_results": {"predictions": ["U.S. Information Agency", "immediately", "a second Gleichschaltung", "Las Vegas Hilton", "the Recording Industry Association of America", "between 7,500 and 40,000", "mountaineer", "Belgian", "Eve Hewson", "\"Slaughterhouse-Five\"", "BBC Formula One coverage on TV, radio and online.", "William Jefferson Clinton (born William Jefferson Blythe III; August 19, 1946) is an American politician who served as the 42nd President of the United States from 1993 to 2001.", "Oldham County", "sandstone", "Channel 4", "25 December 2009", "a priest", "punk rock", "Robert \"Bobby\" Germaine, Sr. (October 1, 1925 \u2013 April 1986), the son of French-Canadian immigrants, was a drug trafficker, burglar, and freelance writer", "Lord Byron", "Laura Dern", "Carrefour", "burlesque", "Venancio Flores", "Forever Living Products International", "FBI", "The Saturdays", "Indianapolis", "French", "1968", "Edinburgh", "Charles Bronson", "Oklahoma Sooners", "Orson Welles", "Sharyn McCrumb", "National Archives", "1.23 million", "Ford Motor Company", "J. K. Rowling", "Sullivan University College of Pharmacy", "Blue Dragon (anime)", "Salford", "January 28, 2016", "Martin Scorsese", "1979", "Charlie Kaufman", "Merrimack County", "RAF Tangmere", "\"Brotherly Leader\"", "Suicide Kings", "Digby, Lincolnshire", "A stolperstein", "Earl ( John Doe )", "Montgomery", "Bart Howard", "South Park", "a pest", "Andre Agassi", "10 below", "Asashoryu", "heavy turbulence", "a string", "Engelbert", "The Secret"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6706333268025078}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.20689655172413793, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9772", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-4470", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-5278", "mrqa_hotpotqa-validation-528", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-7912", "mrqa_triviaqa-validation-7167", "mrqa_newsqa-validation-1077", "mrqa_searchqa-validation-12237", "mrqa_searchqa-validation-8942"], "SR": 0.578125, "CSR": 0.5559475806451613, "EFR": 1.0, "Overall": 0.7098613911290321}, {"timecode": 31, "before_eval_results": {"predictions": ["NCAA Division I", "1985", "Lord President of the Council and Lord Lieutenant of Ireland", "2006", "over 20 million", "18", "Thomas Robert \"Tom\" Kitt", "Cressida Bonas", "Harry Potter Baddeley", "Game Informer", "Fort Albany", "Thorgan Ganael Francis Hazard", "Robert Marvin \"Bobby\" Hull, OC", "Love Actually", "Larnelle Harris", "Queensland", "Southbank", "the Commanding General of the United States", "2018", "Sean Yseult", "1998", "Benjamin Andrew \" Ben\" Stokes", "newspapers, television, radio, cable television, and other businesses", "Francis Keogh Gleason", "Royal", "\"The Land of Enchantment\".", "$10\u201320 million", "the Cumberland Plain", "The DS Virgin Racing Formula E Team", "some artists' lofts and art galleries", "Province of Canterbury", "the Anhaltisches Theater", "Alemannic", "1932", "128", "Telugu", "1937", "Windermere", "a career-ending knee injury", "Marco Fu", "a South Korean horror film", "Isabella (Belle) Baumfree", "Katherine Murray Millett", "the American comedy-drama series \"Gilmore Girls\"", "Premier League", "Aqua", "St. Louis, Missouri", "Dan Castellaneta", "Bury St Edmunds, Suffolk", "Philip K. Dick", "Labour Party", "Greater Manchester, England", "Toby Keith", "Rigg", "January 15, 2010", "Kenya", "Macbeth", "Sir William Hamilton", "Las Vegas", "\"Britain's Got Talent\"", "\"foreign\" Islamic extremist groups", "The Church of Christ, Scientist", "Ronald Reagan", "East Germany"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6548248626373626}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.16666666666666666, 1.0, 0.4, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.6666666666666666, 0.7692307692307693, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 0.8571428571428571, 1.0, 0.6666666666666666, 0.8571428571428571, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2793", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-4223", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-3488", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-3892", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3798", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5482", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8909", "mrqa_triviaqa-validation-2828", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-1618", "mrqa_searchqa-validation-1275"], "SR": 0.484375, "CSR": 0.5537109375, "EFR": 1.0, "Overall": 0.7094140624999999}, {"timecode": 32, "before_eval_results": {"predictions": ["Licensed Local Pastor", "a power outage", "13", "Hebrew", "Stately Homes & Gardens", "five", "Edith Cavell", "Cotentin", "De Lorean DMC-12", "Cuban Missile Crisis", "Action Comics", "Queen Elizabeth II", "The Merchant of Venice", "Northwestern University", "curling", "Cole Porter", "Colorado", "Google", "Aviva plc", "oil", "Project Gutenberg", "surf", "Dr John Sentamu", "Kiel Canal", "General Sir Herbert Kitchener", "Cevennes", "eggs", "Luigi Pirandello", "Sheffield United", "R. White's Lemonade", "Flatiron", "Eddy Shah", "Hugh Laurie", "a cappella", "Dutch", "American quintet", "Red squirrel", "Arabian Sea", "Francois Quesnay", "Model T", "Spice Girls", "Brian Blessed", "Michael Caine", "Sticky Wicket", "pig", "Bank of England", "Isaac Newton", "Pet Sounds", "a halfpenny", "Bangladesh", "St Clements", "Castor", "Nalini Negi", "last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "Ole Einar Bj\u00f8rndalen", "CommunityAmerica Ballpark", "University of Mississippi", "The Rebirth", "\"mastermind\" of the Rwandan genocide", "Pixar's \"Toy Story\"", "\"It is I, the chief executive officer, the one on the very top, should be responsible for this,\"", "Hamlet", "a nose", "kotleta po-kiyevski"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5972470238095238}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1910", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-6454", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-6905", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-373", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-907", "mrqa_naturalquestions-validation-6853", "mrqa_hotpotqa-validation-217", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-250", "mrqa_searchqa-validation-16717", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-12764"], "SR": 0.53125, "CSR": 0.553030303030303, "EFR": 1.0, "Overall": 0.7092779356060606}, {"timecode": 33, "before_eval_results": {"predictions": ["NASA Administrator Webb", "Duval County", "Atlantic", "Richard Branson", "electrical conductivity", "yorkshire", "yorkshire", "1720", "king Tutankhamun", "Morgan Spurlock", "the iris", "Massachusetts", "Andre Agassi", "JMW Turner", "manhattan", "Bruce", "nacre", "yellow", "tbilisi", "Mrs Merton", "r.A.P.", "Wyoming", "Catherine Cookson", "Hugh Quarshie", "Bud Flanagan", "Everyhit", "Alan Sugar", "9", "Henri Paul", "Red Sea", "Helen Gurley Brown", "Wash", "ship", "benfica", "Mark Carney", "Eva Marie", "dijon", "Utah", "Toy Story", "lord emsworth", "Italy", "lord Nelson", "George Osborne", "August 10, 1960", "Apollo", "Gentlemen Prefer Blondes", "Monopoly", "Ned Flanders", "Vincent Van Gogh", "tanzania", "proton", "Demi Moore", "28 July 1914", "yapping", "mid-2000s", "Ellie Kemper", "23 June 1912", "Oryzomyini", "Ryder Russell", "NATO's Membership Action Plan,", "The Washington Post", "manhattan", "1492", "Pearl Jam"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6265624999999999}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3929", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-3919", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-2042", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-5934", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1375", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-5950", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-8908", "mrqa_hotpotqa-validation-5041", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-12921"], "SR": 0.578125, "CSR": 0.5537683823529411, "EFR": 1.0, "Overall": 0.7094255514705882}, {"timecode": 34, "before_eval_results": {"predictions": ["international metropolitan region", "petroleum", "Crystal Pepsi", "nonesuch", "if", "if", "GIGO", "pawn", "house", "silk", "bamboo", "Arthur C. Clarke", "rice", "fox", "poor Richard's Almanack", "if", "scoop", "Led Zeppelin", "Alderney", "Charles Lindbergh", "river Phoenix", "smith", "if", "Krntnertor", "Jason", "The Curse of the Play", "the marine band", "AbeBooks.com Community Forum", "Florence Nightingale", "Profiles in Courage", "bogota", "keira Knightley", "Dorothy Zbornak", "smith", "phoenicia", "Vince Vaughan", "coal", "Jean Foucault", "Hanna Glawari", "humerus", "Harriet Tubman", "a horse", "Louisa May Alcott", "squires", "harry smith", "Margaret Atwood", "cheese", "Khartoum", "Joaquin Phoenix", "Winslow Homer", "Moby Dick", "The Hot Chick", "iOS", "on Mars Hill", "Aaron Harrison", "Gwyneth Paltrow", "gollum", "aromatherapy", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "140 million", "1923", "Saturday", "7-1", "South Carolina Republican Party Chairwoman Karen Floyd"], "metric_results": {"EM": 0.4375, "QA-F1": 0.47777777777777775}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-16300", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-1570", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-4738", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-4524", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-12487", "mrqa_searchqa-validation-15135", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-7837", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-12354", "mrqa_searchqa-validation-15325", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-16288", "mrqa_searchqa-validation-8940", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-1882", "mrqa_searchqa-validation-2976", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7832", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4753", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-30", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2996", "mrqa_newsqa-validation-4058"], "SR": 0.4375, "CSR": 0.5504464285714286, "EFR": 1.0, "Overall": 0.7087611607142856}, {"timecode": 35, "before_eval_results": {"predictions": ["arrow", "Chicago Bears", "Floridians", "green and yellow", "Galicia", "Regional Rural Bank", "M2M", "Ferdinand is an upcoming 2017 American 3D computer-animated comedy film produced by Blue Sky Studios and 20th Century Fox Animation", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies.", "Division of Cook", "July 16, 1971", "13 May 2018", "Kentucky River", "Barbara Niven", "Kramer\\'s caddy Stan", "Yellowcraigs", "Santiago del Estero Province", "a super-regional shopping mall", "Messiah Part II", "Abbey Road", "Mel Blanc", "Czech Kingdom (Czech: \"\u010cesk\u00e9 kr\u00e1lovstv\u00ed\"", "1986", "Lamar Wyatt", "Alfred Preis", "Terry Malloy", "her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine", "The interview", "various registries", "20th episode in the first season of the television series \"How I Met Your Mother\".", "William Shand Kydd", "January 15, 1975", "Chiwetel Umeadi Ejiofor, CBE", "Eden Valley Railway", "27 November 1956", "Paris", "The St Andrews Agreement", "Blackpool Football Club", "Victorian College of the Arts", "north", "Nick Cassavetes", "Cate Blanchett", "Linda Ronstadt", "January 28, 2016", "Hopi", "John Meston", "Romeo", "\u00c6thelred I of Northumbria", "the Albanian Coalation Perspect", "the University of Keele", "Battle of the Rosebud", "Jaffrey", "1998", "A. planci", "Sara Gilbert", "Elkie Brooks", "acrostic", "horizontal", "Asashoryu", "a communications breakdown", "1941", "Barrie", "blue", "Luxor Las Vegas"], "metric_results": {"EM": 0.5, "QA-F1": 0.5964784347596848}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2727272727272727, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.2222222222222222, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6153846153846153, 0.0, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5283", "mrqa_hotpotqa-validation-4577", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-3982", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-3644", "mrqa_naturalquestions-validation-6452", "mrqa_triviaqa-validation-1019", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-1457", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-15743"], "SR": 0.5, "CSR": 0.5490451388888888, "EFR": 1.0, "Overall": 0.7084809027777778}, {"timecode": 36, "before_eval_results": {"predictions": ["in an H+ or hydrogen ion gradient to generate ATP energy", "Caesars Palace Grand Prix", "A compact car", "Benjam\u00edn", "Koch Industries", "Enigma", "Flavivirus", "Julia Compton Moore", "Lord\\'s Resistance Army", "Workers' Party", "Talib Kweli", "1763\u20131791", "Bulgaria", "Swiss", "the remake", "Oldham County, Kentucky", "The Captain Matchbox Whoopee Band", "Jeff Schaffer", "Antigua & Barbuda", "Ghana", "Nikolai Alexandrovich Morozov", "Rabies", "Switzerland", "Tennessee", "Godiva Chocolatier", "October 21, 2016", "August 1973", "Lawrence of Arabia", "The Ansonia Hotel", "1937", "Government of Ireland", "Leona Lewis", "John Robert Cocker", "$7.3 billion", "Angus Brayshaw", "his most brilliant student.", "\"Black Abbots\"", "Sarah Kerrigan, the Queen of Blades", "German", "Katy Perry", "Bharat Ratna", "Wilderness Road", "12 mi east-southeast of Bridgeport", "\"Nebo Zovyot\"", "\"Orchard County\"", "The Kree", "more than 110 films", "shortest player ever to play in the National Basketball Association", "The authorship of Titus Andronicus", "Denmark", "Patricia Arquette", "James Corden", "`` Audrey II ''", "the end of the 2015 season", "red", "Buddha", "Jim Braddock", "\"He went there to receive this bullet.", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Brazilian supreme court judge", "the Yankees", "the Washington Redskins", "( Samuel Taylor) Coleridge", "Abel"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6219629329004329}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.4, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8903", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-4571", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-1835", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-667", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-1872", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-4569", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4184", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-11491", "mrqa_triviaqa-validation-768"], "SR": 0.53125, "CSR": 0.5485641891891893, "EFR": 1.0, "Overall": 0.7083847128378379}, {"timecode": 37, "before_eval_results": {"predictions": ["the plain moraine plateau", "Latin", "cauliflower", "Gorky", "James Bond", "money", "keeper of the Longstone (Fame Islands) lighthouse", "\"Carlos the Jackal\"", "Australia", "Annelies Marie Frank", "Belgium", "Sufjan Stevens", "the town and castle of Gibraltar", "\"The Benny Hill Show\"", "Roddy Doyle", "Kevin Spacey", "Alexandria", "the Republic of Chad", "1215", "the neck", "David Hockney", "Rudyard Kipling", "lactic acidosis", "the Central Criminal Courts", "the Netherlands", "fractal geometry", "Cosmos: A spacetime Odyssey", "the duck-billed platypus", "Aquaman", "Jean-Paul Sartre", "novel", "lBC", "the Esmeralda's Barn night  club", "malt", "Switzerland", "sheep", "a piano", "Lou Gehrig", "linda george", "a Movie", "Heston Blumenthal", "\"n\u00e9e Klein\"", "U2", "a peasant's wife", "Great strategic blunders", "Harper", "Eurythmics", "Canada", "Buster Edwards", "Inspector of Prisons", "Henley Royal Regatta", "Paul Lynde", "the Maryland Senate", "Cairo, Illinois", "Lazio", "Charles Ellis Schumer", "\" SKUM\"", "TNT", "the HSH Nordbank Arena", "October 2007", "\"Dora\" Salter", "Romania", "the Siberian Husky", "The pronghorn"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5154761904761905}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-953", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-665", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-1647", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-2340", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6211", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-4500", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-1536", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4496", "mrqa_naturalquestions-validation-8239", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-3529", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-731", "mrqa_searchqa-validation-10306", "mrqa_searchqa-validation-10797", "mrqa_hotpotqa-validation-2366"], "SR": 0.421875, "CSR": 0.5452302631578947, "EFR": 1.0, "Overall": 0.7077179276315789}, {"timecode": 38, "before_eval_results": {"predictions": ["rocketry and manned spaceflight", "Jewish", "tenor Peter Yarrow, baritone Noel Paul Stookey and alto Mary Travers.", "Washington", "Zack Snyder", "Mondays", "The Cosmopolitan", "Anna Clyne", "Meghan Markle", "terrorist activity", "Commissioner", "August 1973", "Burnley", "Teen Titan Go!", "Evey's mother", "Love and Theft", "1978", "SKUM", "Edmonton, Alberta.", "Seattle", "\"The School Boys\"", "Orchard Central", "The Kennedy Center", "commanders of the Great Army", "Environmental Protection Agency", "Humberside", "Diamond Rio", "The Tempest", "Northampton, England", "Mike Greenwell", "2017", "SAS Technical Services", "polka", "\"Irish Chekhov\"", "1860", "2006", "Ghanaian national team", "Coronation Street", "\"The Dragon\"", "Cold Spring Historic District", "Arctic fox", "Sophie Monk", "The Primettes", "Southern Progress Corporation", "Melbourne Storm", "twenty-eighth", "9 November 1967", "Retina display", "technical director", "Cincinnati metropolitan area", "Captain B.J. Hunnicutt", "19 June 2018", "naos", "31 - member", "kendo", "Jeffrey Archer", "illus curtis", "walk on ice in Alaska.", "Majid Movahedi,", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "Feb 2, 2016", "ninjutsu", "witch", "Mozart"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7374923687423687}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.6153846153846153, 0.0, 0.42857142857142855, 0.5, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.22222222222222224, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3812", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-668", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-2868", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-1300", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-5545", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-16163", "mrqa_searchqa-validation-3163"], "SR": 0.640625, "CSR": 0.547676282051282, "EFR": 1.0, "Overall": 0.7082071314102564}, {"timecode": 39, "before_eval_results": {"predictions": ["Much of the city's tax base dissipated", "Aly Raisman", "40,400 members", "Seoul, South Korea", "English", "banjo player", "Learjet", "Distinguished Service Cross", "Revolver", "Sam Raimi", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\" from DC Comics", "Appleby-in-Westmorland", "Wolfgang Amadeus Mozart", "Keith Crofford", "Mercer Bears", "Dame Eileen June Atkins, DBE", "August 20 or 21, 1745 \u2013 March 31, 1816", "\"Little Dixie\" area of western Missouri", "1983", "1905", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Loughborough University", "Los Angeles", "2.1 million members", "Sierra Nevada mountains", "Alfred Joel Horford Reynoso", "\"The Young Ones\"", "Ghanaian", "Tony Burke", "Alcorn State", "Wilderness Road", "A. E. Housman", "The Killer", "London", "4 km", "25 October 1921", "\"War & Peace\"", "2016 U.S. Senate election", "Tayeb Salih", "Mickey Gilley in Pasadena, Texas.", "his birth name", "Eddie Albert", "Akosua Busia", "Gian Carlo Menotti", "American astronaut and the first woman of Indian origin in space", "Target Corporation", "last Roman Catholic Archbishop of Canterbury", "sub-Saharan Africa", "66 km south of Newcastle & 93 km north of Sydney.", "\"cock of the game\"", "Aaliyah Dana Haughton", "Mad - Eye Moody", "A vanishing point", "16,801", "Lady Gaga", "married", "Georgetown", "The Palm Jumeirah", "consumer confidence", "Sobashima", "Saturday Night Live", "Fried Green Tomatoes", "Bon Jovi", "American teenage singer, Marcie Blane"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6592945249195249}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.2666666666666667, 0.6666666666666666, 0.888888888888889, 0.0, 0.5, 0.0, 1.0, 0.45454545454545453, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666665, 0.0, 0.5714285714285715, 0.0, 0.4, 0.8, 1.0, 0.18181818181818182, 0.0, 0.6666666666666666, 1.0, 0.30769230769230765, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-735", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-139", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-4770", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1074", "mrqa_newsqa-validation-101", "mrqa_searchqa-validation-2773", "mrqa_naturalquestions-validation-7367"], "SR": 0.484375, "CSR": 0.54609375, "EFR": 1.0, "Overall": 0.7078906249999999}, {"timecode": 40, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1611", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2331", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3188", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5538", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-5705", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-987", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4998", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-809", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12714", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5071", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9480", "mrqa_squad-validation-10044", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-10326", "mrqa_squad-validation-10425", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1231", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1472", "mrqa_squad-validation-1608", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2006", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2819", "mrqa_squad-validation-297", "mrqa_squad-validation-3001", "mrqa_squad-validation-3262", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3812", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4078", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4543", "mrqa_squad-validation-4611", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5079", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5355", "mrqa_squad-validation-5563", "mrqa_squad-validation-5597", "mrqa_squad-validation-5616", "mrqa_squad-validation-5881", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6223", "mrqa_squad-validation-6251", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-7952", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8199", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9768", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-244", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6607", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.84765625, "KG": 0.503125, "before_eval_results": {"predictions": ["gilt bronze", "\"The oceans are kind of the last frontier for use and development,\"", "President Obama", "they would not be making any further comments,", "Peshawar", "$250,000", "a nearby day care center whose children are predominantly African-American", "the actor who created one of British television's most surreal thrillers", "Shark River Park", "helicopters and unmanned aerial vehicles", "Mark Sanford", "between 1917 and 1924", "Frank Ricci", "Janet Napolitano", "Mahmoud Ahmadinejad", "\"utterly baseless\"", "Jacob", "Francisco X. Pacheco", "Microsoft", "Lousiana", "prostate cancer", "Eintracht Frankfurt", "Tsvangirai", "the FBI", "\"GoldenEye\"", "necropsy", "a dog", "the Kurdish Workers' Party, or Kurdish", "the London-born movie star", "for his role as Ralph Cifaretto on the HBO series \"The Sopranos\"", "illegal", "the federal government is asleep at the switch", "17", "a rally", "an empty water bottle", "Val d'Isere, France", "a head injury", "News of the World", "the Swat Valley", "200", "Manny Pacquiao", "1960", "Disney", "Mandi Hamlin", "environmental", "80", "EU naval force", "Angelo Nieve", "BC Place Stadium", "Gary Player", "200", "Adam", "Erika Mitchell Leonard", "1969", "V\u00e1clav Havel", "left", "may", "London", "703", "20 October 1980", "glaciers", "Richard M. Daley", "Cerberus", "1967"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5563864750445633}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.5, 0.0, 0.33333333333333337, 1.0, 0.0, 0.18181818181818182, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.18181818181818182, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0588235294117647, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1159", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-2427", "mrqa_searchqa-validation-2894"], "SR": 0.46875, "CSR": 0.5442073170731707, "EFR": 1.0, "Overall": 0.727435213414634}, {"timecode": 41, "before_eval_results": {"predictions": ["Business Connect", "to change the music on the CD player", "40-year-old", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "determining which Guant detainees should be tried by a U.S. military commision,", "five", "1-0", "Kenyan and Somali governments", "\"Pro-democracy leader and Nobel Peace Prize winner Aung San Suu Kyi has been confined in her home for 12 of the past 18 years.", "legitimacy of that race.", "\"The Tibetan spiritual leader, who fled to India in 1959 and established a government in exile there, visited the United States earlier this month.", "25 years in the fashion business for the New Yorker, as well as a quarter century as an advocate for social activism.", "Mashhad", "Islamabad", "North Korea's reclusive leader Kim Jong-IlThe missiles can travel about 3,000 kilometers (1,900 miles),", "pesos", "Steven Green", "millions of Americans gather around their Thanksgiving dinner to celebrate this most American of holidays,", "1000 square meters", "sail", "they'd get to bring a new puppy with them to the White House in January.", "27-year-old's", "Friday,", "Los Ticos", "Seasons of My Heart", "helping on the sandbags lines as the community raced to fill 1 million of them.", "$17,000", "opium", "for not doing more since taking office.\"", "10 years", "$8.8 million", "Noida,", "Lisa Brown", "at least two and a half hours.", "off the front pages for the first time in days.", "(George) Ncube, spokesman for the MDC splinter group,", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "London's Heathrow airport", "165-room", "Transportation Workers Union leaders", "state senators who will decide whether to remove him from office heard him loud and clear on FBI recordings of his phone calls.", "84-year-old", "(5 hours)", "the Southern Baptist Convention,", "Sen. Barack Obama", "a mastermind behind the September 11, 2001, terrorist attacks on the United States.", "actor and producer Anil Kapoor", "most devices carry few security risks.", "Marie-Therese Walter.", "A mother whose daughter and granddaughter attend Oprah Winfrey's school in South Africa considers the talk-show host heaven-sent,", "The eye of Hurricane Gustav", "Felix Baumgartner ( German : ( \u02c8fe\u02d0l\u026aks \u02c8ba\u028a\u032fm\u02cc\u0261a\u0250\u032ftn\u0250 )", "July 2012", "1273.6 cm", "(George) Shaw", "Dutch", "(13th cent.)", "Toronto", "1993", "Robert L. Stone", "Nike", "Barbara Bush", "the QWERTY", "a large or bulky person"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5522231001318656}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.29629629629629634, 1.0, 0.0, 0.09999999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3529411764705882, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.75, 0.16, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.10526315789473684, 0.6666666666666666, 0.5714285714285715, 1.0, 0.1111111111111111, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-3430", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-3800", "mrqa_newsqa-validation-2301", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-7458", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-2480", "mrqa_hotpotqa-validation-582", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-13411"], "SR": 0.453125, "CSR": 0.5420386904761905, "EFR": 1.0, "Overall": 0.7270014880952381}, {"timecode": 42, "before_eval_results": {"predictions": ["Wittenberg", "to implement the Prohibition Amendment by defining the process and procedures for banning alcoholic beverages, as well as their production and distribution.", "Bligh", "Parkinson's", "Blaketon", "Tallinn", "Moscow", "germany", "Portugal", "first among equals", "Nietzsche", "the moon", "Moldova", "Zak Starkey", "Craggy", "the Suez Canal", "otters", "hickory", "Port Talbot", "Rapa Nui", "Democrat President", "Charlie Cairoli", "Salvador Allende", "Mike Tyson", "J. M. W. Turner", "conductor", "Boyle\u2019s", "crystal ball", "a round, slightly tapered, ravenless fur hat", "Tony Blair", "Adolf Hitler", "Jamaica", "Lily", "hypertension", "1066", "crimean", "Jesse James", "Purple Heart Medal", "cephalus", "Jessica Simpson", "a bacterial genus", "the MacKenzie", "Robert Devereux", "NASCAR", "Canada", "Pennsylvania", "Louis Daguerre", "Argentina", "Kwame Nkrumah", "The Color Purple", "terrorism", "zinc", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Mariah Carey", "lifetime", "unidentified flying objects", "Chicago Bears", "a grizzly bear", "Turkey", "Pew Research Center", "inflammation, and as a steroid, prednisone is a potent anti-inflammatory.", "Ferrari", "crimean", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.5, "QA-F1": 0.5786073849396218}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false], "QA-F1": [0.5, 0.09523809523809523, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6060606060606061, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_squad-validation-2165", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-882", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-797", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-9911", "mrqa_naturalquestions-validation-4915"], "SR": 0.5, "CSR": 0.5410610465116279, "EFR": 0.96875, "Overall": 0.7205559593023256}, {"timecode": 43, "before_eval_results": {"predictions": ["John M. Grunsfeld", "dancing with the Stars", "psychotropic drugs", "opium", "1991-1993", "10 below in Chicago, Illlinois.", "Democrat", "test scores", "16", "iReporter Rany Freeman", "forgery and flying without a valid license,", "President Bush", "15-year-old", "fourteen", "upper respiratory infection,\"", "543", "Kevin Kuranyi", "Amy Bishop Anderson", "Susan Atkins", "Ameneh Bahrami", "Virgin America", "$1,500", "Al Nisr Al Saudi", "We Found Love", "his parents", "Ralph Lauren", "iWozniak", "hopes the journalists and the flight crew", "North Korea", "beetles", "Old Trafford", "Bronx County District Attorneys Office", "Arabic, French and English", "Britain.", "Arsene Wenger", "the FBI's Baltimore field office", "Michael Jackson", "all day starting at 10 a.m.,", "her mom,", "South African police", "a Korean-American missionary", "Palestinian Islamic Army,", "united states", "the man was dead,", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "consumer confidence", "Phil Spector", "the District of Columbia National Guard", "Australia and New Zealand", "Steven Gerrard", "opened considerably higher Tuesday", "ribbed vault", "Hermann Ebbinghaus", "statesmen", "lion", "myxoma virus", "Wisconsin", "Battle of Britain and the Battle of Malta", "Viacom Media Networks", "five", "carbon fiber", "Star Spangled Banner", "robert boyle", "i"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6565228174603175}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2065", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9516", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-2853", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-7435"], "SR": 0.5625, "CSR": 0.5415482954545454, "EFR": 1.0, "Overall": 0.7269034090909091}, {"timecode": 44, "before_eval_results": {"predictions": ["the Supreme Court of the United Kingdom", "Seal", "on to other parts of the brain.", "undertaker", "63 to 144 inches", "the Wye", "Zorro", "USS Thresher", "the Last Post", "Chongqing", "Daimler AG", "eagle", "Morgan Spurlock,", "Michael's eldest sister", "the different levels of importance of human psychological and physical needs.", "Prague", "Yellowstone National Park", "Watford Football Club", "Nevada", "Muslims", "milk and straight horns", "Rihanna", "Tintin", "Alexandrina", "of the four Majors at least four times", "Hector BERLIOZ", "Azerbaijan", "Ireland", "the Common Ash", "Madness", "Dalton", "Australia", "Elwyn Watkins", "bats", "the United States", "Penelope Keith", "Alexei Kosygin", "John Galsworthy", "Vinegar Joe", "James Van Allen,", "pangram", "the Mediterranean", "Steel Beads", "Nicaragua\u2019s", "Passepartout", "the first Duke of Wellington", "Lancashire", "Manet", "Kroc", "Thebes", "onda", "Xiu Li Dai and Yongge Dai", "the President", "David Ben - Gurion", "Chow Tai Fook Enterprises", "an organ accompanied by slow tempo drums and vocals.", "Point of Entry", "1973's \" Raw Power.\"", "journalists say government efforts to stem the flow of information are futile.", "anyone.", "Hungary", "China", "China", "Isabella (Belle) Baumfree"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6375868055555556}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-5745", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-7055", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-5556", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4657", "mrqa_naturalquestions-validation-2208", "mrqa_hotpotqa-validation-584", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3391"], "SR": 0.5625, "CSR": 0.5420138888888889, "EFR": 1.0, "Overall": 0.7269965277777779}, {"timecode": 45, "before_eval_results": {"predictions": ["trespassing at a nuclear-missile installation", "new york", "2", "skull", "new york", "new governor general David Johnston", "nicaragua", "Nuuk", "Manila", "pool", "China", "Graham Henry", "wool", "new york", "king edward vtoroy", "beans", "Leeds", "wood", "Elizabeth II", "a dog", "wool", "London Underground Piccadilly Line", "new york", "La Boh\u00e8me", "Nepal", "scurvy", "cutter\u2019s", "Indonesia", "purple coneflower", "the Variations", "keane", "gauteng", "crimson", "Pakistan", "Uranus", "come find yourself", "Abe Reles", "my favorite martian", "Niki Lauda", "petronas", "The Daily Mirror", "Eric Morley", "Radio waves", "york", "newison park", "football", "Reform Club", "William Shakespeare", "barbara", "Trimdon", "new york", "26,000 years", "US - grown fruit", "Orange Juice", "Fifteenth", "Socrates", "Joan Feynman", "Florida", "Martin Aloysius Culhane,", "400 years", "freelance", "mouth", "barbara dold", "will the release of the four men"], "metric_results": {"EM": 0.390625, "QA-F1": 0.44843750000000004}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-6714", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-7559", "mrqa_triviaqa-validation-2995", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-4038", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-5221", "mrqa_naturalquestions-validation-4437", "mrqa_naturalquestions-validation-10402", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-830", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-4100", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-11241", "mrqa_newsqa-validation-3806"], "SR": 0.390625, "CSR": 0.5387228260869565, "EFR": 1.0, "Overall": 0.7263383152173913}, {"timecode": 46, "before_eval_results": {"predictions": ["at Konwiktorska Street,", "a crust of mashed potato", "Lalo Schifrin", "on 16 November 2001", "Don McMillan", "7 correct numbers", "Billy Hill", "Paul Lynde", "halogenated paraffin hydrocarbons", "body - centered cubic ( BCC ) lattice", "May 2002", "2010", "virtual reality simulator", "beneath the liver", "1885", "pre-Columbian times", "2014", "Most days are sunny throughout the year", "caused by chlorine and bromine from manmade organohalogens", "Gamora", "1997 ( XXXII ), 1998 ( XXXIII ), 2015 ( 50 )", "in ingredients", "the homicidal thoughts of a troubled youth", "Mockingjay -- Part 1 ( 2014 )", "Ancient Greek terms", "October 22, 2017", "Amitabh Bachchan", "1998", "nine", "Jeff Bezos", "in response to a perceived harmful event, attack, or threat to survival", "cell - mediated, cytotoxic innate immunity", "via redox", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "produced with constant technology and resources per unit of time", "Lewis Carroll", "January 2004", "Charles Carroll", "Cetshwayo", "the early 1960s", "Sasha Banks", "Erica Rivera", "in New York", "November 1999", "in teaching elocution", "Chris Rea", "mashed potato", "disputes between two or more states", "three part", "Louisa Johnson", "The Mongol - led Yuan dynasty", "candy bar", "1948", "Ruth Rendell", "Hidden America with Jonah Ray", "the United Kingdom", "people working in film and the performing arts", "Jean Van de Velde", "between 1917 and 1924", "lawyers trying to save their client from the death penalty", "cops", "the Virgin Spring", "cookies", "uncle Juan Nepomuceno Guerra"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6125377025113279}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.2222222222222222, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.8, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7692307692307692, 1.0, 0.08695652173913042, 0.6666666666666666, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4615384615384615, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-1427", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-4196", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8372", "mrqa_naturalquestions-validation-6321", "mrqa_triviaqa-validation-7778", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-2156", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-7061", "mrqa_hotpotqa-validation-4241"], "SR": 0.4375, "CSR": 0.5365691489361701, "EFR": 0.9722222222222222, "Overall": 0.7203520242316784}, {"timecode": 47, "before_eval_results": {"predictions": ["Acadia National Park", "Earl Long", "Luxembourg", "pariah", "the Space Shuttle Challenger", "Mick Jagger", "Thomas G Nazareth", "Lapis Lazuli", "the Pentagon", "valley", "snails", "bamboo", "the Vietnam War", "Port Royal", "Minnie Pearl", "a sailor", "Ringling Bros.", "Diz 'N Bird", "nose", "Ernie Els", "Macedonia", "a period of pregnancy", "skin", "There Will Be Blood", "Herb Alpert", "Joe Biden", "the Rolling Stones", "Field Marshal Bernard Montgomery", "Zeus", "Fast Food Nation: The Dark Side of the All-American Meal", "the Tasmanian devil", "hatta al-nasr", "a terrarium", "cyclotron", "Priscilla", "a brothel", "Plaa de Catalunya", "Yellow Ribbon", "Indy 500", "The Hills", "alto", "porter", "state of the United States", "Strait of Gibraltar", "le bton rouge", "King Kamehameha", "menudo", "Alan Alda", "snorts", "a parkland", "sirloin", "a children's novel by American author Elizabeth George Speare", "Walter Pauk", "the lungs", "the American Civil War", "Jehovah's Witnesses", "the United States", "Rousillon Rupes", "Obafemi Martins", "\"Twice in a Lifetime\"", "two-state solution", "Republican", "a communications breakdown at a Federal Aviation Administration facility,", "The Tempest"], "metric_results": {"EM": 0.5, "QA-F1": 0.5543560606060606}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-847", "mrqa_searchqa-validation-8508", "mrqa_searchqa-validation-11698", "mrqa_searchqa-validation-9392", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-13199", "mrqa_searchqa-validation-4024", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-13048", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-4731", "mrqa_searchqa-validation-16787", "mrqa_searchqa-validation-6431", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4769", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-4236", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-3693", "mrqa_naturalquestions-validation-3893", "mrqa_triviaqa-validation-7062", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-904"], "SR": 0.5, "CSR": 0.5358072916666667, "EFR": 1.0, "Overall": 0.7257552083333334}, {"timecode": 48, "before_eval_results": {"predictions": ["photoelectric", "31", "Over", "king Gautamiputra Satakarni", "a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "the alpha efferent neurons", "bohrium", "London", "Soviet Union", "Dalveer Bhandari", "Bush", "the center of the Northern Hemisphere", "Exodus", "David Tennant", "political protest and mercantile protest", "the original timeline is eventually restored", "his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "A footling breech", "October 27, 1964", "four", "Jonathon Dutton", "1990", "sport utility vehicles", "Bob Dylan", "qualitative data, quantitative data or both", "Johannes Gutenberg", "to collect menstrual flow", "a ship is in active commission, with only the name used before or after a period of commission and for all vessels `` in service '' rather than commissioned status", "William the Conqueror", "Sir Ernest Rutherford", "Nicole Gale Anderson", "William Chatterton Dix", "April 12, 2017", "The Gupta Empire", "\u01c0xarra \u01c1ke", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "Gustav Bauer", "The external genitalia", "Robin Cousins", "the table of contents usually appears after the title page, copyright notices, and, in technical journals, the abstract", "the Arctic Ocean", "Ren\u00e9 Verdon", "revenge and karma", "the efferent nerves that directly innervate muscles", "1986", "1546", "robbery", "1928", "Bachendri Pal", "John Adams", "early 2014", "( Ninette) de Valois", "Jessica Smith", "Edward Elgar", "\"Big Mamie\"", "six", "\"First Family of Competitive Eating\"", "$2 billion", "the player", "on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "St. Valentine's Day Massacre", "(Liza) Minnelli", "Gabriel", "opposition parties"], "metric_results": {"EM": 0.5, "QA-F1": 0.5769531475059775}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true], "QA-F1": [0.4, 0.0, 0.0, 0.0, 0.9500000000000001, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.06896551724137932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6842105263157895, 0.125, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-4001", "mrqa_hotpotqa-validation-1210", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-1290", "mrqa_searchqa-validation-16493"], "SR": 0.5, "CSR": 0.5350765306122449, "EFR": 1.0, "Overall": 0.725609056122449}, {"timecode": 49, "before_eval_results": {"predictions": ["1912", "Standard Oil", "Kabuki", "These Boots Are Made For Walking", "pachycephalosaurs", "greece", "Nancy Lopez", "ozone", "Who's the Boss?", "Donnie Wahlberg", "Tasmania", "the Baltimore Orioles", "Tunisia", "Queen Mary 2", "Zionism", "Prague", "dressage", "Newton", "( Toby) Keith", "accordion", "black swan", "Edith Piaf", "the Stratosphere Tower", "parkinsonism", "Strings", "(William) Rehnquist", "Guinevere", "Department of Energy", "tangerine", "east of the Rhine", "Dead Ringers", "Johann Strauss II", "Solidarity", "(James) Carter", "Nick and Norah's Infinite Playlist", "Charles Lindbergh", "Lotus", "Disneyland Park", "a recipe", "Jack Nicklaus", "civil war", "a vacuum", "Teen Titans Go!", "helenuee", "Istanbul", "Mary Poppins", "St. Louis", "Amish-Mennonites", "the Air Force", "Levi's", "Badminton", "Canada", "Sunday", "autopistas", "Phar Lap", "a dove", "Lewis Carroll", "1983", "Excalibur Hotel and Casino", "Czech Kingdom", "Arsenal", "Abdullah Gul,", "humans", "Action Comics"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7489583333333333}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.8333333333333334, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-4796", "mrqa_searchqa-validation-3755", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-767", "mrqa_searchqa-validation-13380", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-1337", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-6473", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-16382", "mrqa_searchqa-validation-15313", "mrqa_naturalquestions-validation-8350"], "SR": 0.65625, "CSR": 0.5375, "EFR": 1.0, "Overall": 0.7260937500000001}, {"timecode": 50, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1338", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1604", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7716", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12265", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3025", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-3764", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8267", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9911", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3635", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.86328125, "KG": 0.496875, "before_eval_results": {"predictions": ["Aerosmith", "Willa Cather", "the General Assembly", "\"The Who\"", "Rendezvous with Rama", "Bismarck", "analog", "Antigua", "Luisa", "(Pierre) Renoir", "the polio vaccine", "THUMPER", "Chief Justice", "a bar exam", "The Brothers Karamazov", "a Smuckers", "a Spanish conquistador", "China", "grease", "Hollandaise", "Esau", "Dry Ice", "Martin Luther King III", "a \"combustion\"", "a catalyst", "Paris", "a Triceratops", "Uganda", "senators", "Sappho", "the Battle of Thermopylae", "the Maccabean", "the Continental navy", "Hamlet", "a Triceratops", "the river Ganga", "New Brunswick", "Copacabana", "Manilow", "We Own the Night", "Plonsk", "By the Sea", "Triceratops", "a coconut cream pie", "Memphis", "Thomas Mann", "Krackel", "a dog-eat-dog world", "Dmitri Mendeleev", "the Azkaban", "tea leaves", "thia Weil", "`` beloved ''", "the heads of federal executive departments who form the Cabinet of the United States", "Microsoft", "General John J. Pershing", "Charlie Brooks", "Taylor Swift", "September 23, 1935", "An invoice, bill or tab", "Martin \"Al\" Culhane,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Hyundai Steel", "following his impressive season"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5766090029761906}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25000000000000006, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0625, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15858", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-130", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-803", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-14038", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-14305", "mrqa_searchqa-validation-729", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-11693", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-14481", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-15320", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-16283", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-2780", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-998", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-6798", "mrqa_hotpotqa-validation-5801", "mrqa_newsqa-validation-762", "mrqa_newsqa-validation-1048", "mrqa_naturalquestions-validation-2976"], "SR": 0.515625, "CSR": 0.5370710784313726, "EFR": 1.0, "Overall": 0.7247579656862746}, {"timecode": 51, "before_eval_results": {"predictions": ["ross", "Wilkie", "Chief of Staff", "the Bible", "Taft", "the American Civil War", "the Nobel Prize", "Roussimoff", "ross", "Technetium", "Nazareth", "867-5309", "Miss Havisham", "Thailand", "Taft", "opal", "Taft", "dense", "air pressure", "echidna", "water", "porcelain", "Synchronicity", "Tim Duncan", "bees", "dark energy", "Reptiles", "Conakry", "William Herschel", "Taecilius Atticus", "Barbara Walters", "Jubal", "Perimeter", "Pumice", "watermelon", "Cole Porter", "ross", "Rotary", "drapery", "Cosmopolitan", "Madagascar", "C.G. Jung", "Lord Carnarvon", "Ontario", "Olympia", "Copernicus", "Lily Allen", "the Palladium", "the Black Death", "Google", "Defense", "they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "1898", "Brevet Colonel Robert E. Lee", "ross kemp", "Gilda", "the Buddha", "Lancia", "1963", "Brittany Snow", "Christopher Savoie", "authorizing killings and kidnappings by paramilitary death squads.", "\"Watchmen\"", "Cork"], "metric_results": {"EM": 0.546875, "QA-F1": 0.603431792237443}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9863013698630138, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-3888", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-1496", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-440", "mrqa_searchqa-validation-5000", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-9206", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-11468", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-4585", "mrqa_searchqa-validation-4971", "mrqa_naturalquestions-validation-7223", "mrqa_triviaqa-validation-7153", "mrqa_hotpotqa-validation-3399"], "SR": 0.546875, "CSR": 0.5372596153846154, "EFR": 1.0, "Overall": 0.7247956730769232}, {"timecode": 52, "before_eval_results": {"predictions": ["Deere", "(Ella) VICTORIA", "a electron", "the Missouri River", "brandy", "the typical... son", "GIGO", "(Ella) Rossini", "because she comes from sinners", "Rome", "the Isle of Wight", "Colorado Springs", "hay", "Possession", "(Ella) Waverley", "the contact lens", "Units", "the Buk missile system", "Vibe", "Pulp Fiction", "yelp", "Frederick Forsyth", "Independence Day", "Princess Leia", "Vietnam", "Vince Lombardi", "the global village", "Dubliners", "Sudan", "Kwanzaa", "Warren Buffett", "Charlie\\'s Angels", "President Lincoln", "imagism", "whimper", "a noun", "oscar wilde", "Taiwan", "Mickey Spillane", "Buzz Lightyear", "(Jack) Bauer", "Bingo", "kidney stones", "Necessity", "diamonds", "The Biggest Blowout", "Atlanta", "California (55), New York (31), Texas (34), Pennsylvania (21)", "glucose", "hours", "a gem", "usually in a way considered to be unfair", "Australia's Sir Donald Bradman", "The Caucasus Mountains", "Rambo", "Darby and Joan", "antelope", "Jung Yun-ho", "7.63\u00d725mm Mauser", "a united Ireland", "raping and murdering a woman in Missouri.\"", "troy Livesay", "damage from Hurricane Irene and Tropical Storm Lee in Bradford, Dauphin, Columbia, Wyoming and Luzerne counties.", "Estadio Victoria"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5946428571428571}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-3292", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-14452", "mrqa_searchqa-validation-10292", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-14015", "mrqa_searchqa-validation-9935", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-12531", "mrqa_searchqa-validation-13465", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5962", "mrqa_searchqa-validation-9744", "mrqa_searchqa-validation-4660", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-8817", "mrqa_searchqa-validation-14998", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9816", "mrqa_triviaqa-validation-6223", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3364"], "SR": 0.53125, "CSR": 0.5371462264150944, "EFR": 1.0, "Overall": 0.724772995283019}, {"timecode": 53, "before_eval_results": {"predictions": ["to protect and support crown - appointed colonial officials attempting to enforce unpopular Parliamentary legislation", "Debbie Gibson", "three", "February 28", "Ireland", "December 2, 1942", "an expression of unknown origin", "heart", "March 26, 1973", "Necator americanus", "June 11, 2002", "St Pancras International", "number of games where the player played, in whole or in part", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "Frank Langella", "Granada Studios in Manchester", "Human fertilization", "Aslan", "16 seasons", "Bill Russell", "Baylor", "Donald Trump", "vascular cambium", "Epidemiology", "1895", "the contestant", "between the Mediterranean Sea to the north and the Red Sea to a south, and is a land bridge between Asia and Africa", "a Border Collie", "Washington metropolitan area", "Julie Adams", "160km / hour between Delhi to Agra", "John Young", "Kevin Spacey", "novella", "uterus", "Gene MacLellan", "an American musical group founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O' Neal and Dougy Williams", "2010", "on location", "Frankie Muniz", "before the first letter of an interrogative sentence or clause to indicate that a question follows", "(Prince) Albert", "Bailey Graffman", "about the hardships of growing older and has no relationship to drug - taking", "2017", "1978", "a loanword of the Visigothic word guma `` man ''", "Rust", "birch", "John Brown", "brothers Henry, Jojo and Ringo Garza", "(Prince) Albert", "river Deben", "Roman history", "a vegetarian dish called Buddha\\'s delight", "Matthew Ryan Kemp", "pro-Confederate partisan rangers", "they", "30,000", "domestic disturbance calls to police since 2000 involving the Damas couple,", "( Pablo) Picasso", "(Scott) Peterson", "the Capitol", "thumb"], "metric_results": {"EM": 0.5, "QA-F1": 0.6138488604161354}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false], "QA-F1": [0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.06451612903225806, 0.7142857142857143, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.631578947368421, 0.0, 0.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-5363", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1009", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-4240", "mrqa_triviaqa-validation-6845", "mrqa_triviaqa-validation-4244", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-458", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3873", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-5691"], "SR": 0.5, "CSR": 0.5364583333333333, "EFR": 0.9375, "Overall": 0.7121354166666667}, {"timecode": 54, "before_eval_results": {"predictions": ["Norway", "Ecuador", "Home Improvement", "iron", "coho salmon", "Berlin", "Iago", "Fidel Castro", "Patrick Floyd Garrett", "Montana", "Harvard University", "(General) Custer", "an arboretum", "Marie Curie", "Abnormal Psychology", "love", "Lincoln", "the Italian flag", "Samuel Butler", "Kitty Kelley", "Abraham Lincoln", "teddy bears", "Crouching Tiger", "baseball", "upsilon", "banknotes", "arizonensis", "Jupiter", "conformation", "Ziegfeld Girl", "David Cassidy", "Volcanoes", "the Louvre", "Cyrillic", "royal", "oxygen", "a house of prayer", "Wessex", "a sled", "Aaron Copland", "red", "volts", "Act I of The Royal Ballet", "Nikola Tesla", "Lil Jon", "the diamond", "the plum tree", "Lizzie Borden", "Hockey", "Pop-Tarts", "bovine spongiform encephalopathy", "Henry Purcell", "Mankombu Sambasivan Swaminathan", "eight", "Ethiopia", "the cactus", "Argentina", "Real Madrid and the Spain national team", "1983", "Richard Street", "Ronaldinho", "\"Empire of the Sun,\"", "whether he should be charged with a crime,", "four"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6557291666666666}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-8125", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-8825", "mrqa_searchqa-validation-16878", "mrqa_searchqa-validation-10404", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-13693", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9692", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-4573", "mrqa_searchqa-validation-5952", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-14599", "mrqa_searchqa-validation-13757", "mrqa_hotpotqa-validation-4436", "mrqa_triviaqa-validation-261"], "SR": 0.546875, "CSR": 0.5366477272727272, "EFR": 1.0, "Overall": 0.7246732954545455}, {"timecode": 55, "before_eval_results": {"predictions": ["Santa Fe", "Beanie Babies", "kick drum", "chess", "cola", "Berlin", "Comedy Central", "swimming", "(B Boris) Godunov", "Romeo and Juliet", "Cerberus", "lice and roaches", "the Nile", "coins", "Trajan", "skating", "a submarine", "St. Augustine", "the Jornada del", "the Marshall Islands", "burnoose", "Tesla", "the Mekong", "the 36th", "Valentina Tereshkova", "Canada", "acrophobia", "Missouri", "ribonucleic acid", "Rubeus Hagrid", "southcentral", "Death of a Salesman", "Chocolate", "inshallah", "Saudi Arabia", "C.J. Parker", "Jenny Craig", "Idaho", "coppertone", "Edward VI", "the Empire State Building", "laugh", "Tennessee", "the Constitution", "Toronto", "University of Exeter", "\"Berenice, Queen of Egypt\"", "Lawrence of Arabia", "Andy Warhol", "creams", "Tara Reid", "Pac - 12 Conference Champions Stanford Cardinal", "near the 48th parallel north", "B.F. Skinner", "the Nokia tune", "General Paulus", "Cyprus", "2010", "Tufts University", "the Crips gang", "judge Ricardo Urbina", "he fears a desperate country with a potential power vacuum that could lash out.", "Monday.", "Lance Cpl. Cesar Laurean"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5362351190476191}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-13435", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15699", "mrqa_searchqa-validation-6014", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-3517", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9266", "mrqa_searchqa-validation-5151", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-10897", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-5672", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-438", "mrqa_hotpotqa-validation-5516", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2517"], "SR": 0.46875, "CSR": 0.5354352678571428, "EFR": 0.9705882352941176, "Overall": 0.7185484506302522}, {"timecode": 56, "before_eval_results": {"predictions": ["former English county of Humberside", "the Federal Bureau of Prisons", "\"Dumb and Dumber\"", "trans-Pacific", "John Hunt", "Walt Disney Productions", "Reinhard Heydrich", "British", "\"Sheen Michaels Entertainment\"", "\"S&M\"", "1770", "Sunflower County", "2005", "A Bug's Life", "U.S.", "a few", "the Qin dynasty", "Kentucky River", "fourth", "\"The Bob Edwards Show\" on Sirius XM Radio and \"Bob Edwards Weekend\" distributed by Public Radio International to more than 150 public radio stations", "The S7 series", "White Knights of the Ku Klux Klan", "Key West, Florida", "Charlie Puth", "Fort Albany", "Best Performance by an Actress in a Mini Series award", "Soviet Union", "the National Society of Daughters of the American Revolution (NSDAR)", "Martin Scorsese", "John Monash", "Protestant Christian", "Slugger Field", "Firestorm", "Agra", "50 million", "Henry II", "Scotty Grainger", "the improvisational style of Isadora Duncan", "An agricultural cooperative", "Kairi", "Kent, Washington", "Democratic Unionist Party (DUP)", "44", "Candice Susan Swanepoel", "that Fama and French's research is period dependent", "Mark \"Izzy\" Cole", "multiple awards", "McComb, Mississippi", "\"King of Cool\"", "1995", "\"Losing My Religion\"", "The Royalettes", "three", "1982", "Mazda", "'Q'", "Silverstone", "Britain's Got Talent", "Turkey", "Seoul", "Charleston", "emerald", "Don Juan", "Pandora"], "metric_results": {"EM": 0.5625, "QA-F1": 0.653622159090909}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 0.18181818181818182, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-557", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-983", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-314", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-4151", "mrqa_newsqa-validation-154", "mrqa_searchqa-validation-8091"], "SR": 0.5625, "CSR": 0.5359100877192983, "EFR": 1.0, "Overall": 0.7245257675438597}, {"timecode": 57, "before_eval_results": {"predictions": ["Italian architect and art theorist Leon Battista Alberti", "in a counter clockwise direction", "The Sixth Extinction II : Amor Fati", "December 2, 1942", "Ray Charles", "mid November", "start the UK Foundation Programme", "Daniel Suarez", "the population", "the Central and South regions, and by people of Mexican ancestry living in other places, especially the United States", "Stephen Graham", "Betty", "Dan Stevens", "9.7 m", "Guwahati", "Brobee", "sovereignty", "efferent nerves", "William Wyler", "Dragon Ball GT", "Nitty Gritty Dirt Band", "Donald Fauntleroy Duck", "2013", "the investment bank Friedman Billings Ramsey", "2018", "skeletal muscle", "Joe Spano", "Spanish moss", "Georges Auguste Escoffier", "Nodar Kumaritashvili", "October 29, 2015", "Madeline Reeves", "New England Patriots", "son of Bindusara", "Britain of Florida", "a charbagh", "an unknown recipient", "Andy Warhol", "Elected Emperor of the Romans", "Dalveer Bhandari", "Middle Eastern alchemy", "the division of Italy into independent states", "Nalini Negi", "Rightly Guided Caliphs", "the head of Lituya Bay in Alaska", "the cast", "17 -- 15", "The only person he tells is his best friend and drummer, Earl ( John Doe ), and that he is `` taking a walk, '' but does not say exactly where he is going or for how long", "A vanishing point", "The United States is a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "Emma Chambers", "Hans Lippershey", "Thermopylae", "August 6, 1845", "an album", "Spain", "Dr. Jennifer Arnold and husband Bill Klein,", "A family friend of a U.S. soldier", "patrolling the pavement", "Blue", "lump", "a knish", "CBS"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6849483543417367}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.4, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.4, 0.8, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.7843137254901961, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-31", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-419", "mrqa_searchqa-validation-416"], "SR": 0.578125, "CSR": 0.5366379310344828, "EFR": 0.9259259259259259, "Overall": 0.7098565213920818}, {"timecode": 58, "before_eval_results": {"predictions": ["in the five - year time jump for her brother's wedding to Serena van der Woodsen", "111", "every president since Woodrow Wilson, with the notable exception of Herbert Hoover", "Uralic", "22", "IBM", "the Representatives of the United States of America", "2018", "Jesus Christ", "September 2000", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "24", "the Coriolis force", "Hugh S. Johnson", "Paul Lynde", "Erica Rivera", "Malina Weissman", "the Northern Qi", "Bo\u00f6tes / bo\u028a\u02c8o\u028ati\u02d0z", "1969", "DeWayne Warren", "the nucleus", "1996", "Egypt", "statistical", "Tom Brady", "pilgrimages to Jerusalem", "1996", "Coconut Cove", "Curtis Armstrong", "Dolby Theatre in Hollywood, Los Angeles, California", "Category 4", "Rust", "Karen Gillan", "$19.8 trillion", "1,228 km / h ( 763 mph )", "Tommy Shaw", "warplanes", "Nigel Lythgoe, Mia Michaels, and Adam Shankman", "Central Germany", "Atlanta", "Ricky Nelson", "James Chadwick", "Welch, West Virginia", "Tristan Rogers", "15 February 1998", "Houston Astros", "Americans who served in the armed forces and as civilians during World War II", "it reaches to the south coast of eastern New Guinea, thereby including the Gulf of Papua", "middle of the 15th century", "Gladys Knight & the Pips", "Lake Nicaragua", "priests or the priesthood", "rue", "Delphi Lawrence", "Juan Manuel Mata Garc\u00eda", "Edward James Olmos", "vitamin injections that promise to improve health and beauty.", "Scotland", "military veterans", "a heart", "James Stewart", "Frank Sinatra", "Salisbury"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7013701923076923}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.3076923076923077, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.07999999999999999, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4134", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2503", "mrqa_searchqa-validation-8636", "mrqa_searchqa-validation-9866", "mrqa_hotpotqa-validation-3324"], "SR": 0.59375, "CSR": 0.5376059322033898, "EFR": 0.9615384615384616, "Overall": 0.7171726287483703}, {"timecode": 59, "before_eval_results": {"predictions": ["journalists, a seven-member Spanish flight crew and one Belgian", "at least 300", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "U.S. Supreme Court.", "an insect sting in August.", "Wednesday.", "Linda Hogan", "be silent.", "Crandon, Wisconsin,", "Turkey", "John Demjanjuk", "Somalia's piracy problem was fueled by environmental and political events", "eight", "Haiti.", "Missouri.", "\"peregruzka,\"", "Haiti.", "9", "many as 250,000", "Muhammad Ali,", "Former Mobile County Circuit Judge Herman Thomas", "more to stop the Afghan opium trade", "Nick Adenhart", "order", "his father", "\"disagreements\" with the Port Authority of New York and New Jersey,", "\"Operation Pipeline Express.\"", "death in the Holmby Hills, California, mansion he rented.", "Simon Cowell", "10-person", "April.", "heist follows the recent theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg, a spokesman for the Kunsthaus,", "promotes fuel economy and safety while boosted the economy", "gasoline", "to do jobs that Arizonans wouldn't do.", "a \"prostitute\"", "digging", "Tottenham", "could be secretly working on a nuclear weapon", "President Kennedy", "3-2", "\"The deceased appeared to have been there for some time.\"", "resonating with those tuning into programming aimed at and featuring the plus-sized.", "56,", "not the moptops", "15-month", "intravenously in operating rooms", "Summer", "give detainees greater latitude in selecting legal representation", "heavy turbulence", "Zac Efron", "drivers who were Daytona Pole Award winners, former Clash race winner, former Daytona 500 pole winners who competed full - time in 2017, and drivers who qualified for the 2017 Playoffs", "the spectroscopic notation for the associated atomic orbitals", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "show", "2", "John Buchan", "NCAA Division I Football Bowl Subdivision", "Kristoffer Kristofferson", "Ben Savage", "The Chronicles of Narnia", "Bering Strait", "Dame Ninette de Valois", "Burning Man"], "metric_results": {"EM": 0.421875, "QA-F1": 0.50910526191752}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.08, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.23529411764705885, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.10526315789473685, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5499999999999999, 1.0, 0.8205128205128205, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-920", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-318", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-3615", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4387", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-7393", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-3871", "mrqa_searchqa-validation-16736", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-13957"], "SR": 0.421875, "CSR": 0.5356770833333333, "EFR": 0.972972972972973, "Overall": 0.7190737612612613}, {"timecode": 60, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2665", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10461", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11693", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11890", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-11925", "mrqa_searchqa-validation-12105", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12441", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13915", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14450", "mrqa_searchqa-validation-14481", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16382", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-2634", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5612", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9852", "mrqa_searchqa-validation-9911", "mrqa_searchqa-validation-9935", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1876", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1145", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5886", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937"], "OKR": 0.828125, "KG": 0.4625, "before_eval_results": {"predictions": ["$7.8 million", "prostate cancer,", "the Ku Klux Klan", "Whitney Houston", "The attorney general will announce his decision early next week,", "South Africa", "consumer confidence", "Saturn", "Prague and tells of her love for the \"Golden City,\"", "35,000.", "Osama", "The EU naval force", "Police say he confessed to holding Elisabeth captive since 1984 and raping her repeatedly, fathering seven children with her.", "threatening messages", "in the west African nation", "misdemeanor", "firefighter", "$273 million", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Haiti", "air support.", "20", "$250,000 for Rivers' charity: God's Love We Deliver.", "April 2", "18 federal agents and two soldiers", "Blacks and Hispanics", "Australian officials", "the leader of a drug cartel that set off two grenades during a public celebration in September, killing eight people and wounding more than 100.", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "Rev. Alberto Cutie", "meter reader who led authorities last week to remains believed to be those of Caylee Anthony called police four months ago,", "the man facing up, with his arms out to the side.", "Garth Brooks", "Monday's", "Friday,", "10,000", "Ryan Adams.", "$1.5 million.", "three out of four", "relatives of the five suspects,", "his mood -- and interest in the ladies -- improved.", "581", "his health and about a comeback.", "up", "\"utterly baseless,\"", "businesses, the federal government will make at least 20 percent of its purchases from small- and medium-size companies,", "Caylee,", "0-0", "sole survivor of the crash that killed Princess Diana", "Lonnie", "the first", "worshippers of their sinfulness and mortality", "Tokyo for the 2020 Summer Olympics", "gives them access to US courts", "Doncaster Rovers", "Usain Bolt", "1973", "2,099", "PPG Paints Arena", "early 2017", "lactic acid", "The Greatest Show on Earth", "the republic", "Germany"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7422770503952569}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-56", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-2958", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7987", "mrqa_hotpotqa-validation-4619"], "SR": 0.671875, "CSR": 0.5379098360655737, "EFR": 1.0, "Overall": 0.7004725922131148}, {"timecode": 61, "before_eval_results": {"predictions": ["Windows Easy Transfer", "John Cooper Clarke", "Queen Victoria", "O'Meara", "Judi Dench", "to accomplish the objectives of the organization", "Omar Khayyam", "P.V. Sindhu", "1665", "Saturday", "1982", "Siddharth Arora / Vibhav Roy", "1948", "the process of mitosis", "Butter Island", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Pat McCormick", "10,605", "U.S. Electoral College", "Linda Davis", "Bob Sinclair & Eddie Amador", "the Yangtze River and in provinces in the south", "New York City", "July 21, 1861", "Nashville", "104 colonists and Discovery", "the cella", "about 375 miles ( 600 km ) south of Newfoundland", "April 12, 2017", "October 2012", "Dawn French, Timomatic and Geri Halliwell", "John Joseph Patrick Ryan", "49 cents", "counter clockwise", "Kit Harington", "Human anatomy", "above the light source and above the stage", "a divergent tectonic plate boundary", "Organisms in the domains of Archaea and Bacteria", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen", "Speaker of the House of Representatives", "1877", "18", "SURFACE VIEWA of ROOTS", "norm that sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "winter festivals", "a recognized group of people who jointly oversee the activities of an organization", "the 1820s", "The Royalettes", "Kate '' Mulgrew", "Fats Waller", "John Terry", "\"sun origin\", and it is often called the \"Land of the Rising Sun\".", "James Hogg", "Lawrence", "\"Realty Bites\"", "the \"Fuerza A\u00e9rea Argentina\"", "Athens,", "\"extremely weak\" and said he weighs barely 100 pounds in a court document filed this week,", "at the Verzasca hydro-electric dam in Switzerland", "chicken Little", "Saturn", "Russia", "Tchaikovsky\u2019s Eugene Onegin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.688759708120387}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4799999999999999, 1.0, 0.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.75, 0.9302325581395349, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-1915", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-9204", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-3163", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-1975", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3093", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-3859", "mrqa_triviaqa-validation-6243"], "SR": 0.5625, "CSR": 0.5383064516129032, "EFR": 0.9285714285714286, "Overall": 0.6862662010368663}, {"timecode": 62, "before_eval_results": {"predictions": ["1976", "Bacon", "from 1922 to 1991", "73", "Gibraltar", "1 January 1904", "Thebes", "Brooke Wexler", "March 29, 2018", "in the 1980s", "Alabama", "Evermoist", "in the mid - to late 1920s", "differential erosion", "Kanawha River", "Graham McTavish", "Thomas Alva Edison", "since been adopted by five other countries", "due to a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "Richard Masur", "Frankie Valli", "one", "JackScanlon", "Saturday", "sperm and ova", "sometime in 2018", "2015", "Sarah Josepha Hale", "Billy Gibbons", "Ledger", "The India and Pakistan Border", "in a 1945 NCAA game between Columbia and Fordham", "2017", "permanently absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Pink Floyd", "6 March 1983", "decades after its initial release, when it became a television staple during Christmas season in the late 1970s", "in 1986", "1939", "Himadri Station", "on a beach in Malibu, California", "clay", "in February 2017 in Japan and in March 2018 in North America and Europe", "FaZe Rug", "at the fictional elite conservative Vermont boarding school Welton Academy", "1973", "9 February 2018", "a two - layer coat which is close and dense with a thick undercoat", "94 by 50 feet", "Spanish Dominican Tom\u00e1s de Torquemada", "1918", "Kent", "Nicaraguan Sign Language", "Dusty Dvoracek", "South America", "Los Angeles", "has an inspiration: U.S. President Barack Obama.", "the foyer of the BBC building in Glasgow, Scotland", "has not been any major systemic problems.", "not to return.", "spring", "Shipwrecks", "Rev. Alberto Cutie"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5885874037114847}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 0.5, 0.9600000000000001, 0.9523809523809523, 0.0, 0.5, 0.2222222222222222, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 0.25, 1.0, 0.9411764705882353, 1.0, 1.0, 0.12500000000000003, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-3143", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-761", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-219", "mrqa_newsqa-validation-1330", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-2763"], "SR": 0.4375, "CSR": 0.5367063492063492, "EFR": 1.0, "Overall": 0.7002318948412698}, {"timecode": 63, "before_eval_results": {"predictions": ["phailand", "Billy Martin", "Nova Scotia", "chainmaille", "Strindberg's", "Mount Fuji", "Morocco", "Elia di_______\" Lammermoor", "Anne Rice", "(Henry) Ford", "Pop art", "embalming", "Portland", "Rihanna", "Dionysus", "a coral reef fish", "symbiosis", "Planets", "former Vice President", "Letra de Year 3000", "The Lost World", "Prince Edward Island", "New York Presbyterian Hospital", "El Mandeb Strait", "Red Heat", "Atlas Mountains", "kafkaesque", "Heather Mills", "2008", "Mount Vernon", "Mont Blanc On", "Rene Lacoste", "preemption", "the Nobel Prize", "summer", "ley spots", "Jawaharlal Nehru", "the last day", "The Agony and the Ecstasy", "a cat", "congruous", "Spain", "Toad in the Hole", "San Francisco", "A Brief History of Time", "a crossword", "Macy's", "Geoffrey Chaucer", "a midget who has escaped from prison", "Hillary's America", "Benazir Bhutto", "Pop", "Gibraltar", "Orographic lift", "Virginia Plain", "strawberry", "Venice", "Cameroon", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Stephen King", "Pat Quinn", "murder in the beating death of a company boss who fired them.", "\"Walk -- Don't Run\"", "Comeng and Clyde Engineering"], "metric_results": {"EM": 0.515625, "QA-F1": 0.569047619047619}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-11205", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-6802", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-5508", "mrqa_searchqa-validation-14989", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-16392", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-15031", "mrqa_searchqa-validation-4348", "mrqa_naturalquestions-validation-6298", "mrqa_hotpotqa-validation-3558", "mrqa_hotpotqa-validation-5688", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2308"], "SR": 0.515625, "CSR": 0.536376953125, "EFR": 0.967741935483871, "Overall": 0.6937144027217742}, {"timecode": 64, "before_eval_results": {"predictions": ["a Latino(a) would be of Latin America origin.", "Bonnie & Clyde", "Forrest Gump", "My Favorite Mistake", "The Crossing Guard", "Thomas Beekman", "I Have No Mouth, and I Must Scream", "Friday Night Lights", "contractions", "the skull", "China", "Florida State", "Russia", "Boston", "a Tibetan antelope", "The Godfather", "a bolt", "Australia", "Napalm", "Roald Dahl", "Mount Kenya", "John Lennon", "the Stamp Act", "Princeton University", "carbon dioxide", "The Battle of Thermopylae", "The Buenos Aires Herald", "Mulberry", "Romeo & Juliet", "Tucson", "Mad About You", "Wesley Clark", "cobalt", "Sing Sing", "salmon", "a falling star", "Herman Melville", "Abercrombie & Fitch", "Beatrix Potter", "The Romaunt of the Rose", "a cassowary", "the Gadsden Treaty", "the umbilical cord", "trees", "Sweden", "the House of Lords", "the Red Cross", "the Militia", "KC", "Ypres", "the Graceland", "David Tennant", "the southeastern coast of the Commonwealth of Virginia in the United States", "pulmonary heart disease", "8 gallons", "squash", "Facebook", "the Gospel Starlighter", "500-room", "Wal-Mart Canada Corp.", "Tuesday,", "Diversity,", "directly involved in an Internet broadband deal with a Chinese firm.", "gang rape"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6448373538011696}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.2222222222222222, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.9473684210526316, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11200", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-6468", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-16325", "mrqa_searchqa-validation-691", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-8741", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-2681", "mrqa_searchqa-validation-6512", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-12394", "mrqa_searchqa-validation-6678", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-11005", "mrqa_searchqa-validation-15724", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-14787", "mrqa_searchqa-validation-3727", "mrqa_naturalquestions-validation-5912", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-1283", "mrqa_hotpotqa-validation-2565"], "SR": 0.53125, "CSR": 0.536298076923077, "EFR": 1.0, "Overall": 0.7001502403846154}, {"timecode": 65, "before_eval_results": {"predictions": ["the Brown Mountain Lights", "3,384,569", "Vishal Bhardwaj", "around 169 CE", "Ed O'Neill", "Milwaukee Bucks", "138,535 people", "Dennis Hull, as well as painter Manley MacDonald.", "Max Martin, Savan Kotecha and Ilya Salmanzadeh", "a jersey", "\"Love Letter\"", "2014", "1968", "2005", "Stacey Kent", "Shenandoah National Park", "Regionalliga Nord", "Galo", "Samantha Spiro", "William Shakespeare", "over 1.6 million", "the Yule goat", "Sierra Leone", "West Tambaram", "Portal A Interactive", "Graduados", "Sada Thompson", "World Health Organization", "Chow Tai Fook Enterprises", "Michelle Anne Sinclair", "2011", "2012", "Haleiwa", "Lalit", "Kal Ho Naa Ho", "in Srinagar", "Ronald Wilson Reagan", "musicology", "left", "1835", "1926 Paris", "Erreway", "Forbes", "January 28, 2016", "69.7 million", "500-room", "Sochi, Russia", "2027 Fairmount Avenue", "WVNH", "Black Panther Party", "globetrotters", "in the United Kingdom ( with the exception of Scotland since August 1, 2016 )", "into the intermembrane space", "Butter Island off North Haven, Maine in the Penobscot Bay", "eye", "Leo Tolstoy", "gizzard", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "2,000 people,", "Asashoryu", "Joan of Arc", "Shirley Jackson", "Sue Miller", "(Dan) Parris,"], "metric_results": {"EM": 0.5, "QA-F1": 0.5894500968992248}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5581395348837209, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-134", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-180", "mrqa_triviaqa-validation-3498", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-245", "mrqa_searchqa-validation-11103", "mrqa_newsqa-validation-2296"], "SR": 0.5, "CSR": 0.5357481060606061, "EFR": 0.96875, "Overall": 0.6937902462121212}, {"timecode": 66, "before_eval_results": {"predictions": ["musician", "Captain Hans Geering", "50JJB Sports Fitness Clubs", "2015", "North America", "Seventeen", "Bhushan Patel", "Pamela Chopra", "Mark O'Connor", "Kinnairdy Castle", "South African", "Agent 99", "The 2008\u201309 UEFA Champions League", "National Hockey League", "science", "Parlophone Records", "a Soldier in Truck", "eight", "Cuban", "arts manager", "\"The Royal Family\".", "girls aged 11 to 18", "Jackie Harris", "water", "the National Basketball Development League", "Operation Overlord", "invoice", "[O.S. 20 October]", "1851", "during the first month of World War I", "12-year", "World War II", "Graham Payn", "Martin Truex Jr.", "twice", "Malayalam cinema", "47,818", "\"Every Rose Has Its Thorn\"", "13", "New England and New York", "1953", "German", "Scapegoat Mountain", "AC/DC", "Boston Celtics", "1912", "Dutch", "Bill Lewis", "the youngest publicly documented", "1968", "311", "Jason Lee", "Hans Zimmer", "slavery", "the Agulhas current", "The History Boys", "a tabby", "Mexico", "2,500", "'overcharged.'\"", "Melba", "Gary", "Henry Hudson", "the brain"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6552083333333334}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666665, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4061", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-2842", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-5251", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-7417", "mrqa_searchqa-validation-7226", "mrqa_naturalquestions-validation-3368"], "SR": 0.578125, "CSR": 0.5363805970149254, "EFR": 1.0, "Overall": 0.7001667444029851}, {"timecode": 67, "before_eval_results": {"predictions": ["classical", "Dr. Alberto Taquini", "democracy and personal freedom", "Wanda Chmielowska", "2015", "Fu\u00dfballklub Austria Wien", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Edward Albert Heimberger", "Squam Lake", "lambics", "Croatian", "the Harpe brothers", "Rihanna", "Marvel's Agent Carter", "Everton", "shorthand writing", "twelfth", "coal town in McDowell County, West Virginia, United States", "1975", "Dark Heresy", "Theodore Robert Cowell", "1943", "International Society for the Study of the Origin of Life", "Humvee", "Malta", "East Knoyle", "Philadelphia", "Maria Brink", "Jyothika Sadanah", "\"Sippin' on Some Syrup,\"", "24 January 76 \u2013 10 July 138", "Leonard Cohen", "General Theological Seminary", "BraveStarr", "25 million", "William Randolph Hearst", "Sunflower County", "848 km", "Ellesmere Port", "Homer Hickam, Jr.", "South America", "Montreal", "Eugene", "Chief of the Operations Staff of the Armed Forces High Command", "CBS News", "Philadelphia", "The Shirehorses", "October", "the best known globetrotters", "Chin Han", "John Schlesinger", "Pasek & Paul", "Diary of a Wimpy Kid", "Ed Sheeran", "Sarah Palin", "Ub Iwerks", "Changing Places", "onto the college campus.\"", "14 bodies", "one American diplomat to a \"prostitute\"", "drop-down list", "The Muppet Show", "Sulu", "missing"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6128787878787879}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.4, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-705", "mrqa_hotpotqa-validation-259", "mrqa_hotpotqa-validation-5082", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-5070", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5385", "mrqa_hotpotqa-validation-5270", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-6828", "mrqa_newsqa-validation-3337", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-5330", "mrqa_newsqa-validation-1398"], "SR": 0.515625, "CSR": 0.5360753676470589, "EFR": 1.0, "Overall": 0.7001056985294117}, {"timecode": 68, "before_eval_results": {"predictions": ["Stephen T. Kay", "Matt Stone", "Jay Gruden", "Wayman Tisdale", "Manchester Airport", "the Corps of Discovery", "stringed", "Fleetwood Mac", "County Louth", "Chelmsford", "2009", "Comedy Central", "five", "the Lazio region", "Mick Jackson", "The Livingston family", "U.S. saloon-keeper", "Fort Albany", "\"Kitty Hawk\"", "the Qin dynasty", "\"our greatest comedienne - Australia's Lucille Ball\".", "Charles de Gaulle Airport", "Francophone", "singer", "cricket fighting", "Jaguar Land Rover", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "Noel Gallagher", "1966", "\"Vogue\", \"W\" and \"Love\" magazines", "band director", "Ford Field in Detroit, Michigan", "required only men to register for the draft", "Mary O'Connell", "Bolton, England", "February 9, 1994", "Las Vegas", "Kongo", "Missouri River", "World War II", "Ector County", "Norse", "Mercedes-Benz Superdome in New Orleans, Louisiana", "1979", "a classroom specialist for the Peace Corps of America", "October 12, 1962", "rural", "Lombardy", "August 14, 1848", "Punjabi/Pashtun", "41st", "Shalmaneser V", "He chose to charter a plane to reach their next venue in Moorhead, Minnesota", "1948", "Mallard", "Sherlock Holmes", "Tokyo", "intention to set up headquarters in Dublin.", "on the Ohio River near Warsaw, Kentucky,", "Marxist guerrillas", "W. Somerset Maugham", "a spruce", "not a fast typist", "Thomas Jefferson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6785466269841269}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.5714285714285715, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 0.33333333333333337, 0.0, 0.888888888888889, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.5, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-5779", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-78", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-4720", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7939", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-2573", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-16102", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-1104"], "SR": 0.515625, "CSR": 0.5357789855072463, "EFR": 1.0, "Overall": 0.7000464221014492}, {"timecode": 69, "before_eval_results": {"predictions": ["November 1999", "the senior-most judge of the supreme court", "the Norman given name Robert", "March 16, 2018", "Irsay", "Germany", "relieves the driving motor from the load of holding the elevator cab", "federal", "Bongos", "December 2, 2013", "the 7th century", "it was first published on November 12, 1976", "2018", "Walter Pauk", "Walter Egan", "1975", "maquila", "1959", "Sarah Douglas", "Anjana Om Kashyap", "2016", "2003", "Isekai wa Sum\u0101tofon", "Representatives and Delegates", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "New Zealand", "a large roasted turkey", "currently a free agent", "Joe Pizzulo and Leeza Miller", "New York, New Jersey, Pennsylvania, Ohio, Indiana, Illinois, Iowa, Nebraska, Colorado, Wyoming, Utah, Nevada, and California", "the year 1 BC", "Nueva Vizcaya", "the mid-1970s", "Florida and into the town of Coconut Cove", "state sector", "tropical desert climate", "1988", "in the 1970s", "anembryonic gestation", "February 27, 2015", "John Hancock", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "elevation above sea level or above the tree line", "March 5, 2014", "FIGG Bridge Engineers", "Carol Ann Susi", "in the digestive systems of many organisms", "crown cutting of the fruit", "Chris Rea", "end of the 2015 season", "16 August 1975", "ovule", "Goliath", "Frankenstein", "Dutch", "Winecoff Hotel fire", "India Today", "Jet Republic", "Thousands", "Kabul", "Prison Break", "the Lone Ranger", "Ethiopia", "President Obama and Britain's Prince Charles"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5820509785353536}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.625, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.16666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5333333333333333, 0.4, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.26666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-3386", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-6372", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-2261", "mrqa_hotpotqa-validation-886", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-12778", "mrqa_newsqa-validation-2497"], "SR": 0.484375, "CSR": 0.5350446428571429, "EFR": 0.9393939393939394, "Overall": 0.6877783414502164}, {"timecode": 70, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4068", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-757", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8993", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10731", "mrqa_searchqa-validation-10872", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16049", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-226", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7278", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1330", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1949", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3687", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4301", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-562", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-6362", "mrqa_squad-validation-66", "mrqa_squad-validation-6962", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7693", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-855", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_squad-validation-9923", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4657", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-7726"], "OKR": 0.822265625, "KG": 0.5078125, "before_eval_results": {"predictions": ["aikido", "Peter Stuyvesant", "Cornell", "a cactus", "NASCAR", "Vivaldi", "8", "Grace Slick", "London", "Sweden", "Phil Lynott", "his devotion to a girl,", "Zachary Taylor", "Kempton", "Leonardo da Vinci", "(John) Millais", "Belfast", "chamboule-tout", "Fulham", "Kent", "bryophyta", "Margot Fonteyn", "Adonijah", "fondue", "glockenspiel", "(Amber) Lancaster", "William Shakespeare", "Mackinac Bridge", "a 965-foot ocean liner", "Lazing", "tea", "Joan Crawford", "blue", "Ptolemy", "1969", "the queen", "boxing", "Benjamin Disraeli", "Duke", "Babylon", "Nottingham", "(George) III", "25", "(S) Jimmy) Beck", "Antoine Lavoisier", "Australia", "Agenor", "X-Men Origins: Wolverine", "Jimmy Carter", "( Victoria) Coren", "King William IV", "December 15, 2017", "Charlotte Hegele", "Malibu Creek State Park", "Coal Miner's daughter", "National Association for the Advancement of Colored People", "gGmbH", "Fernando Gonzalez", "Alaska or Hawaii.", "\"falling space debris,\"", "jury", "oogenesis", "Brooke Shields", "Nepal"], "metric_results": {"EM": 0.546875, "QA-F1": 0.60625}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5144", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-4091", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-968", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6959", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-1468", "mrqa_triviaqa-validation-6308", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-468", "mrqa_hotpotqa-validation-1720", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-3347", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-14926"], "SR": 0.546875, "CSR": 0.5352112676056338, "EFR": 0.9655172413793104, "Overall": 0.7083488267969889}, {"timecode": 71, "before_eval_results": {"predictions": ["four", "Wyoming", "tobacco", "Oprah Winfrey", "Phil Spector", "Margaret Beckett", "Robin Hood Men in Tights", "Rapa Nui", "Fringillidae", "Greyfriars", "millais and director Michael Curtiz", "casselli", "baron Ritchie", "Harry Palmer", "millais kemilius Lepidus", "True History of the Kelly Gang", "yorkshire", "new zealand", "catherine kemullen", "Stockton-on-Trent", "a trumpet player", "$10", "king Midas", "sheep", "French Open", "a cactus", "a child", "a peach", "greece", "socket", "a bruise", "barber", "sienna", "alexandria", "federal district of Washington", "a man named Daedalus", "Tommy Roe", "a villa", "barleycorn", "Kopassus", "Uranus", "cressida", "exploits on the Island", "behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing, or engaging in sexual activity", "pascal", "brain", "ash", "17-day", "sisyphus", "Kathryn C. Taylor", "Hawaii", "usually in May", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "diurnal and insectivorous", "Derry City F.C.", "URO VAMTAC", "Christian", "Stop the War Coalition", "U.S. forces in the war zone", "she announced on her Twitter page Tuesday.", "lexicographer", "a heron", "Billy the Kid", "mayor of Seoul"], "metric_results": {"EM": 0.46875, "QA-F1": 0.545144092019092}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7878787878787877, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.30769230769230765, 1.0, 0.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5335", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-7364", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-207", "mrqa_triviaqa-validation-7241", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-4560", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-55", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-1222", "mrqa_hotpotqa-validation-1630", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2547", "mrqa_searchqa-validation-16275", "mrqa_newsqa-validation-3686"], "SR": 0.46875, "CSR": 0.5342881944444444, "EFR": 0.9411764705882353, "Overall": 0.7032960580065359}, {"timecode": 72, "before_eval_results": {"predictions": ["a chubby Boy Scout -- or \"Wilderness Explorer\"", "Colombia.", "U.S. and NATO forces.", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "they'd get to bring a new puppy with them to the White House in January.", "three", "Kurdistan Freedom Falcons,", "achievement gaps between racial groups", "Barbara Streisand", "U.S. President-elect Barack Obama", "other dancers.'\"When you have beautiful athletes, movie stars and TV personalities, and then you mix in a nerds, there's something a little strange and entertaining,\"", "travels four hours to reach a government-run health facility that provides her with free drug treatment.", "July 18, 1994,", "off the coast", "Pixar", "Friday", "state senators", "\"The Rosie Show,\"", "Gov. Arnold Schwarzenegger.", "South Africa", "Brazil", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds,", "Clifford Harris,", "Jeffrey Jamaleldine", "to sniff out cell phones.", "eight in 10", "five", "Karen Floyd", "around 8 p.m. local time Thursday", "15-year-old's", "150", "will lose millions of endorsements and a man who has destroyed the trust he built up with his fans.", "Ennis", "haitians", "start a dialogue of peace based on the conversations she had with Americans along the way.\"", "Daniel Radcliffe", "1 million", "55-year-old", "the Southeast,", "Aravane Rezai", "Russian bombers", "was planning to conduct attacks in Karachi, according to Karachi Police Chief Waseem Ahmad.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "Caylee Anthony's", "March 22,", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "Steve Young", "prisoners", "Woosuk Ken Choi,", "Apple employees", "In December 1971", "Norman Greenbaum", "over the next seven years", "kiki", "bitter", "surreal", "Matthew Ward Winer", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu", "Sun Woong", "darts", "Joe Louis", "16th", "octavian"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5717805693501916}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [0.10526315789473684, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0689655172413793, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.19354838709677416, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.1, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6363636363636364, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4104", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1433", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4180", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-857", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-3654", "mrqa_hotpotqa-validation-3757", "mrqa_searchqa-validation-2301", "mrqa_searchqa-validation-10852", "mrqa_hotpotqa-validation-1958"], "SR": 0.46875, "CSR": 0.533390410958904, "EFR": 1.0, "Overall": 0.7148812071917808}, {"timecode": 73, "before_eval_results": {"predictions": ["12", "Sodra nongovernmental organization,", "the United States", "not feel Misty Cummings has told them everything she knows.", "Mogadishu", "\"horrible crime that is designed to sabotage reconciliatory efforts by the Iraqi people, who, I am confident, will continue on the road of dialogue.\"", "more than 100", "\"The argument got heated, and Murray demanded he leave,\" the attorney general said.", "Scarlett Keeling", "took on water", "two", "Omar", "Dr. Cade", "a Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "Twitter", "preventing our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia.", "relatives of the five suspects,", "165-room", "U.S. Holocaust Memorial Museum,", "the simple puzzle video game,", "curfew in Jaipur", "40-year-old", "stand down.", "U.S. 93 in White Hills, Arizona, near Hoover Dam.", "different women coping with breast cancer in five vignettes.", "two years,", "269,000", "next week's elections in his favor,", "UK", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "because the Indians don't want to get involved in the armed struggle and refuse to reveal information on government troop actions.", "militants", "they'd get to bring a new puppy with them to the White House in January.", "as he tried to throw a petrol bomb at the officers,", "Austin, Texas,", "forgery and flying without a valid license,", "Kurt Cobain's", "ALS6,", "he was one of 10 gunmen who attacked several targets in Mumbai", "a rabbit hole,", "software magnate Larry Ellison,", "anesthetic", "10", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "six-year veteran of the museum's security staff,", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "on the bench", "12.3 million", "\"private client\" list,", "Seasons of My Heart", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "Justice A.K Mathur", "somatic cell nuclear transfer ( SCNT )", "1956", "The Parson Russell Terrier", "Brazil", "\"The English Patient''", "Don DeLillo", "10 Years", "\"Queen City\" of Maine", "Carl Sagan", "israel", "Copacabana", "seven"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6574981904366041}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8750000000000001, 1.0, 1.0, 0.0, 1.0, 0.9743589743589743, 0.08695652173913043, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.06896551724137931, 1.0, 0.888888888888889, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-534", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2397", "mrqa_naturalquestions-validation-5109", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-345", "mrqa_triviaqa-validation-6588", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-178", "mrqa_searchqa-validation-8780"], "SR": 0.546875, "CSR": 0.5335726351351351, "EFR": 0.9655172413793104, "Overall": 0.7080211003028891}, {"timecode": 74, "before_eval_results": {"predictions": ["brazil", "North Yorkshire", "Lou Gehrig", "Goat Island", "Loretta Lynn", "a bat", "email", "Andrew Lloyd Webber", "east of Eden", "Edward de Vere", "Mark Hamill", "Aslan", "kabaddi", "Merchant of Venice", "kvetch", "Coupe Van der Straeten Ponthoz", "Harold Wilson", "Bleak House", "Handley Page", "Tina Turner", "takifugu rubripes", "Sheffield United", "Capricorn", "President John F. Kennedy", "the bluebird", "Toy Story", "Tom Waits", "White", "Kiel Canal", "colombia", "Avro Lancaster", "Sarah Vaughan", "Abu Dhabi", "33 miles", "Emily Davison", "Marc Brunel", "Aberystwyth", "Oasis", "Peter Sellers", "the Indus Valley", "ghent", "an even break", "David Bowie", "Lorne Greene", "1655", "fusilli", "Thai", "Viola", "\u00e1stron", "october", "sewing machines", "Frederick County", "11 January 1923", "Roman Empire", "1994", "Sevens", "Tamara Ecclestone Rutland", "Kaka,", "himself made after his new boss, golfer Adam Scott, defeated Woods at the Bridgestone Invitational in Ohio in August.", "one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "noncommissioned officer", "William Henry Harrison", "carmen", "1982"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7302905701754385}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.10526315789473684, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-3406", "mrqa_triviaqa-validation-3882", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4672", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2719", "mrqa_searchqa-validation-15435"], "SR": 0.6875, "CSR": 0.535625, "EFR": 1.0, "Overall": 0.715328125}, {"timecode": 75, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "reached an agreement", "Dead Weather's \"Horehound\"", "two", "Oprah Winfrey's school", "twice.", "Haleigh Cummings,", "the eradication of the Zetas cartel", "Hamas,", "a mosque in Rawalpindi", "Rima Fakih", "against using injectable vitamin supplements because the quantities are not regulated.", "daniels", "Arthur E. Morgan III,", "tax incentives", "moved into her rental house", "\"Here Comes the Sun.\"", "22", "1969", "diabetes and hypertension,", "Tillakaratne Dilshan scored his sixth Test century", "Tuesday,", "100,000", "apartment building", "$17,000", "President Obama", "Caylee Anthony", "Glasgow office", "the journalists and the flight crew will be freed,", "AMD,", "returning combat veterans", "September 21.", "Thursday.", "246", "40", "stole", "maximum \u00a380,000", "elephants can roam freely, largely feed and shelter themselves and interact with others, often after years living alone in captivity,", "15", "Chamber of the Ten Maidens", "Arabic, French and English,", "$40", "many as 250,000", "state senators", "Israel and the United States", "Sheik Mohammed Ali al-Moayad", "in the 1950s,", "Orbiting Carbon Observatory,", "2.5 million", "5,600", "Yemen,", "Roger Staubach", "heroin", "contemporary Earth", "b\u00e9la Bart\u00f3k", "dontari Poe", "blackcurrant", "Premier League club Everton", "Captain Beefheart & His Magic Band", "Fat Man", "messenger", "the beaver", "john johnson", "R2-D2"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5785326363451364}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true], "QA-F1": [0.4444444444444445, 0.33333333333333337, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-10", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-3275", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-4422", "mrqa_naturalquestions-validation-2207", "mrqa_triviaqa-validation-7344", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-1555", "mrqa_hotpotqa-validation-5388", "mrqa_searchqa-validation-14660"], "SR": 0.484375, "CSR": 0.5349506578947368, "EFR": 0.9393939393939394, "Overall": 0.7030720444577352}, {"timecode": 76, "before_eval_results": {"predictions": ["Hawaii", "\"The Real Housewives of Atlanta\"", "commission, led by former U.S. Attorney Patrick Collins,", "The number of deaths linked to cantaloupes contaminated with the Listeria monocytogenes bacteria has risen to 28, the Centers for Disease Control and Prevention said Tuesday.", "said he was released Friday and taken to the Australian embassy in Bangkok, where he stayed until leaving for Australia at about midnight.", "Sheikh Sharif Sheikh Ahmed", "two", "club-themed", "ended his playing career at his original club of Argentinos Juniors in 2007 and has been coaching at Independiente.", "Nigeria", "Ameneh Bahrami", "the chief executive officer, the one on the very top,", "Daytime Emmy Lifetime Achievement Award.", "test-launched a rocket capable of carrying a satellite,", "said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "35,000.", "Roger Federer", "March 22,", "Venezuela", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "power-sharing talks", "\"Operation Crank Call,\"", "the body of the aircraft", "Rima Fakih", "more than 100", "Obama and McCain camps", "Ameneh Bahrami", "Sunday", "President Bill Clinton", "Haleigh Cummings,", "Transportation Security Administration", "pulling on the top-knot of an opponent,", "response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "J. Crew,", "was killed", "Kindle Fire", "Islamabad", "three", "Nigeria,", "Her husband and attorney, James Whitehouse,", "fill a million sandbags and place 700,000 around our city,\"", "a Ford F-150 work truck (a plain, regular-cab model),", "15-year-old's", "normal", "David Bowie,", "15,000", "looked depressed", "Franklin, Tennessee,", "said such joint exercises between nations are not unusual.", "Authorities in Fayetteville, North Carolina,", "Saturday.", "Lake Powell", "Lew Brown", "Rockwell", "lyndon", "Neighbours", "Pesach", "Walcha", "TNT Sameday", "Hern\u00e1n Crespo", "Scandinavia", "( Giuseppe) Verdi", "Serengeti", "Gaul"], "metric_results": {"EM": 0.59375, "QA-F1": 0.68482954937088}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5806451612903226, 1.0, 1.0, 0.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.9743589743589743, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.18181818181818185, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-2396", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-2519", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-648", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-5708", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-1821", "mrqa_triviaqa-validation-925"], "SR": 0.59375, "CSR": 0.5357142857142857, "EFR": 0.9230769230769231, "Overall": 0.6999613667582418}, {"timecode": 77, "before_eval_results": {"predictions": ["Sunday", "Charlotte Gainsbourg and Willem Dafoe", "Patrick McGoohan,", "flooding and debris", "Vicente Dale Coutinho, commander of Brazil's 4th Army, reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Woosuk Ken Choi,", "a head injury.", "1994,", "Math teacher Mawise Gumba", "his real name is Clifford Harris,", "43,000", "At least 15", "at least nine", "shows the world that you love the environment and hate using fuel,\"", "\"falling space debris,\"", "Transportation Security Administration", "$10 billion", "\"The Orchid Thief\"", "101", "his business dealings", "Jaime Andrade", "\"Vaughn,\" which is what co-workers called him,", "skyscrapers", "the United States", "financial gain,", "Gary Coleman is in critical condition in a Provo, Utah, hospital, a hospital spokesman said Thursday.", "trading goods and services without exchanging money", "Yang Hyong Sop, and Kim Kye Gwan,", "high tide -- expected to reach about 4 meters (13 feet) high", "July", "16", "Romney is the GOP candidate is best suited to defeat President Barack Obama in 2012.\"", "100 to 150", "reached an agreement late Thursday to form a government of national reconciliation.", "Zelaya and Roberto Micheletti,", "one of Africa's most stable nations.", "At the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "Inmates", "Cannes Film Festival,", "Mark Obama Ndesandjo", "in the Ronald Reagan UCLA Medical Center,", "one", "Afghan", "a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "Peru's ex-president", "\"Empire of the Sun,\"", "Basel", "\"@\"", "Jeffrey Jamaleldine", "Heshmatollah Attarzadeh", "At least 14", "Daryl Sabara", "in the eighth episode of Arrow's second season", "Gibraltar", "Barry White", "Thomas Jefferson", "Anita Brookner", "9", "British", "consulting", "flamboyant", "Orson Welles", "fence", "loyal"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6346383536967418}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.7894736842105263, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.25, 1.0, 0.16666666666666666, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.2857142857142857, 0.0, 0.5, 0.125, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3928", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-1497", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-2177", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-795", "mrqa_naturalquestions-validation-9330", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-3222"], "SR": 0.546875, "CSR": 0.5358573717948718, "EFR": 0.9655172413793104, "Overall": 0.7084780476348365}, {"timecode": 78, "before_eval_results": {"predictions": ["dress shop", "callable bonds", "British Columbia, Canada", "11 %", "September 14, 2008", "to prevent further offense by convincing the offender that their conduct was wrong", "Brad Johnson", "the closing of the atrioventricular valves and semilunar valves", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "a compiler", "Dalveer Bhandari", "bone marrow", "to collect menstrual flow", "Kansas City Chiefs", "the episode `` Killer Within ''", "Filipino", "the 15th century", "Wakanda", "Europe that created a concept of East and West originated in the Roman Empire", "Joel", "Everest creative Maganlal Daiya", "Gertrude Niesen", "Australia", "1983", "Dr. Addison Montgomery", "Set six months after Kratos killed his wife and child, he has been imprisoned by the three Furies for breaking his blood oath to Ares", "The photoelectric ( optical ) smoke detector", "the head", "New Orleans", "sedimentary rock", "$2 million", "Michael Buffer ( born November 2, 1944 ) is an American ring announcer for boxing and professional wrestling matches", "Eric Clapton", "115", "c. 3000 BC", "Vince Vaughan", "each team has either selected a player or traded its draft position", "1898", "a router", "his parents", "England", "Glenn Close", "Kenneth Cook", "Tim Russert", "1 - 2 spinal nerve segments above the point of entry", "a large, high - performance luxury coupe sold in very limited numbers", "displacement", "Second Continental Congress meeting at the Pennsylvania State House ( Independence Hall ) in Philadelphia", "the winter solstice", "phencyclidine and cocaine", "an integral membrane protein that builds up a proton gradient across a biological membrane", "Lewis Carroll", "neos", "The Apprentice", "A123 Systems", "41st President of the United States", "Henry Mills", "34", "Caylee Anthony,", "Michael Brewers,", "a Tattoo", "Tennessee Williams", "Nathaniel Hawthorne", "Friday,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6393870018870019}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9333333333333333, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.2857142857142857, 0.2857142857142857, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 0.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.18181818181818182, 1.0, 0.4, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-5536", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-1433", "mrqa_newsqa-validation-3438", "mrqa_searchqa-validation-16420"], "SR": 0.453125, "CSR": 0.5348101265822784, "EFR": 1.0, "Overall": 0.7151651503164557}, {"timecode": 79, "before_eval_results": {"predictions": ["Bury, Greater Manchester, England", "the Battle of the Rosebud", "Rabat", "Potomac River", "Hermione Baddeley", "Harmony Korine", "December 1993", "New Jersey", "rock and roll", "Red and Assiniboine Rivers", "King George IV and the Duke of Wellington", "June 24, 1935", "The Washington Post", "odd-eyed", "2001", "5AA", "1999", "tempo", "Presbyterian", "2002", "English", "Southaven", "Anheuser-Busch", "the University of Missouri-Kansas City in Kansas City, Missouri", "Francis the Talking Mule", "Leslie James \"Les\" Clark", "Giuseppe Verdi", "County Louth", "Gal Gadot", "Kurt Vonnegut", "top division", "\"Northern Ballads\"", "Mary Violet Leontyne Price", "Vincent Anthony Guaraldi", "2007", "Grave Digger", "Mulberry", "Isabella (Belle) Baumfree", "Jay Park", "Khalifa International Stadium", "Mel Blanc", "Centers for Medicare & Medicaid Services", "Pakistan", "\"God Wizard\"", "Steven Selling", "Scunthorpe", "Australian", "Argentinian", "Robert Norton Noyce", "throughout the 1970s and 1980s", "\"Personal History\",", "Timothy B. Schmit", "Bob Pettit", "the direction from which the wind is blowing", "France", "Shropshire", "Sophora microphylla", "Alfredo Astiz,", "\"They're exploring mathematics, composing music, learning new languages, designing animation, collecting data, collaborating with peers across borders and accessing learning tools (including textbooks) that they would never see otherwise.", "Kim", "Catherine", "Little Boy Blue", "Cheyenne", "March 27, 2017"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7385349025974026}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-598", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-3483", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-2518", "mrqa_naturalquestions-validation-601", "mrqa_triviaqa-validation-4205", "mrqa_newsqa-validation-433", "mrqa_naturalquestions-validation-5649"], "SR": 0.65625, "CSR": 0.536328125, "EFR": 1.0, "Overall": 0.71546875}, {"timecode": 80, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1288", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.82421875, "KG": 0.51640625, "before_eval_results": {"predictions": ["John Nash", "Two Greedy Italians", "Russ Conway", "the y unit", "Mel Brooks", "The Man Who Would Be Bond", "Edward Woodward", "Dutch", "Fiat SpA,", "three-stringed", "Bob Anderson", "Andre Agassi", "kelly", "India", "Pieve di San Pietro a Romena", "Mark Darcy", "a filly/mare", "the ex-gangland leader talks to Jason Bennetto about the twins' violent past", "Milan", "Steve Davis", "Bash Street", "Leonard Rossiter", "Robin Hood", "Me and My Girl", "Jehovah", "Augustus", "Shepherd Neame", "Titanic", "Peter Falk", "tax collector", "Daily Mail", "Mikhail Gorbachev", "Pocahontas", "Noah Beery, Jr.", "Argentina", "the spinal cord", "myxomatosis", "fruit with green, reddish-purple or blackish skin and rich yellowish pulp enclosing a single  large seed.", "World War I", "Captain America", "HARIBO", "chicken livers", "New Zealand", "Eva Braun", "the Arabian Sea", "Devon Loch", "Macau", "Bruce Willis", "Kwame Nkrumah", "The Fifth Amendment", "knotting in geometric patterns", "providing torque to all its wheels simultaneously", "pneumonoultramicroscopicsilicovolcanoconiosis", "Hercules", "Philip Livingston", "2010", "\"The Process\"", "Ameneh Bahrami", "Nearly eight in 10", "a tenement in the Mumbai suburb of Chembur,", "camels", "Shrek the Third", "Seoul", "Asia"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6875}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-3677", "mrqa_triviaqa-validation-2223", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-5713", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-1693", "mrqa_naturalquestions-validation-6334", "mrqa_searchqa-validation-457"], "SR": 0.65625, "CSR": 0.5378086419753086, "EFR": 0.9545454545454546, "Overall": 0.7080020693041525}, {"timecode": 81, "before_eval_results": {"predictions": ["London", "mountain-climbing", "NBA's 50th anniversary", "Roger Staubach", "World Health Organization", "Pulitzer Prize", "Argentine", "Pittsburgh Steelers", "Kim Jong-hyun", "Las Vegas", "Scandinavian design", "romantic comedy", "NBA 2K16", "tsung-Dao Lee", "December 19, 1967", "South African-born", "Ubbert Griffith", "1822", "1926 Paris", "Bulgarian", "Quahog, Rhode Island", "Operation Neptune", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Roslyn Castle", "The Battle of Dresden", "2015", "Violet", "Free Range Films", "Mondays", "Chrysler K platform", "Edinburgh", "Laurel, Mississippi", "Matt Flynn", "Camber Sands", "Jaguar Land Rover Limited", "base of support during the First Indochina War", "Norwood", "Shepardson Microsystems", "The Fault in Our Stars", "sixteen", "Crips", "Deftones", "Doctor of Philosophy", "Donald Richard \"Don\" DeLillo", "Hong Kong Mak\u00e9l\u00e9l\u00e9", "Blue Valley Northwest High School", "Magic Band", "the Salzburg Festival", "German princely Battenberg", "a German political and military leader as well as one of the most powerful figures in the Nazi Party (NSDAP) that ruled Germany from 1933 to 1945", "The Nassau Herald", "on the Fox Ranch in Malibu Creek State Park, northwest of Los Angeles", "Sufi verse Tum Ek Gorakh Dhanda Ho ( You are a puzzle )", "pineapple", "asia", "zenyJob", "gdansk Poland", "Steven Green", "Tiger Woods", "Umar Farouk AbdulMutallab", "Dr. No", "your shoulders", "Caonero II", "Colombia police have killed a drug trafficker who the government says is one of the most sought-after fugitive outside the country's rebel leaders."], "metric_results": {"EM": 0.5625, "QA-F1": 0.64786134004884}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.2, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-376", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2925", "mrqa_naturalquestions-validation-3587", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-5053", "mrqa_triviaqa-validation-3758", "mrqa_newsqa-validation-2810", "mrqa_newsqa-validation-1205", "mrqa_searchqa-validation-11199", "mrqa_searchqa-validation-5081", "mrqa_newsqa-validation-877"], "SR": 0.5625, "CSR": 0.538109756097561, "EFR": 0.9642857142857143, "Overall": 0.7100103440766551}, {"timecode": 82, "before_eval_results": {"predictions": ["Kenny Young", "40 million", "American Idol", "brother-in-law", "Terence Winter", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Daimler-Benz", "Edward Moore \"Ted\" Kennedy (February 22, 1932 \u2013 August 25, 2009)", "private", "Lee Seok-hoon", "Two Pi\u00f1a Coladas", "early traditions", "Most observers viewed the election as blatantly unfair", "Eucritta melanolimnetes", "Umberto II", "1866", "1860", "Attorney General and as Lord Chancellor of England", "Sexred", "British", "Westfield Tea Tree Plaza", "924", "1951", "Darci Kistler (born June 4, 1964)", "1966", "Potomac River", "Europe", "The Kingkiller Chronicle series", "Gateways", "England", "Black Panthers", "An extended play record", "\"127 Hours\" (2010)", "May 5 to July 8, 2014", "The Supremes", "Twelfth Night, or What You Will", "Wolf Creek", "Sky News", "High Court of Admiralty", "Chelsea Lately", "bobsledder", "Double Agent", "Oregon Ducks", "Dano-Norbert author Aksel Sandemose", "Sim Theme Park", "the Earth", "Eric Whitacre", "Sullivan University", "24 January 76 \u2013 10 July 138", "Marvel Comics", "Zambesi river", "along the Californian coast at The Inn at Newport Ranch", "1961 during the Cold War", "September 14, 2008", "Harry Truman", "The Kentucky Derby", "Gianni Versace", "London", "his former Boca Juniors teammate and national coach Diego Maradona,", "his club", "pink", "Beaker", "the Claddagh ring", "Piano Concerto No. 5 in E-flat major"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7479922161172161}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6399999999999999, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.9523809523809523, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-5066", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-4293", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-1777", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-4974", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-1828", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1461", "mrqa_searchqa-validation-11879", "mrqa_searchqa-validation-10992"], "SR": 0.609375, "CSR": 0.5389683734939759, "EFR": 1.0, "Overall": 0.7173249246987952}, {"timecode": 83, "before_eval_results": {"predictions": ["Thunder Road", "from the Anglo - Norman French waleis", "between the Eastern Ghats and the Bay of Bengal", "Times Square in New York City west to Lincoln Park in San Francisco", "Columbia University", "the oral mucosa ( a mucous membrane ) lining the mouth and also on the tongue and palates and mouth floor", "Eydie Gorm\u00e9", "Werner Ruchti", "Stephen Lang", "used their knowledge of Native American languages as a basis to transmit coded messages", "the Italian / Venetian John Cabot ( c. 1450 -- c. 1500 )", "Himadri Station", "1959", "Sun Tzu ( `` Master Sun '', also spelled Sunzi )", "their son Jack ( short for Jack - o - Lantern ) is born on Halloween 2023", "The Grasshopper Lies Heavy", "Pope Gregory I the Great", "1966", "December 20, 1951", "1978", "Warren Hastings", "The mixing of sea water and fresh water", "The caps at both poles consist primarily of water ice", "June 1992", "John Vincent Calipari ( born February 10, 1959 )", "1975", "Charles Darwin and Alfred Russel Wallace", "Left Behind", "1997", "Judith Cynthia Aline Keppel ( born 18 August 1942 )", "in the 1820s", "May 18, 2018", "Russia", "the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council ( UNSC ) : China ( formerly the Republic of China ), France, the United Kingdom, and the United States", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government", "Fusajiro Yamauchi", "Bart Cummings", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "pre-Christian festivals that were celebrated around the winter solstice", "Speaker of the House of Representatives", "the NFL", "Italy", "Detroit Red Wings", "practices in employment, housing, and other areas that adversely affect one group of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral", "Christopher Jones", "Blue with a harp of gold", "Florida and into the town of Coconut Cove", "S - shaped basin", "September 9, 2010", "no license or advanced training beyond just firearm familiarization ( for rentals )", "Lori Rom", "The savior of the human race", "Sinclair Lewis", "Akon", "Matilda the musical in London's West End", "Hawaii Five-0", "Gregg Harper", "23-year-old", "more than 200.", "Transportation Security Administration", "Vaio Z Canvas", "Newman", "Vogue", "East Knoyle"], "metric_results": {"EM": 0.546875, "QA-F1": 0.673065662285315}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.631578947368421, 1.0, 0.125, 1.0, 1.0, 1.0, 0.06666666666666667, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.3636363636363636, 1.0, 0.6, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5365853658536585, 0.16, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9666666666666666, 0.5, 0.25, 1.0, 0.5, 0.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-6308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8391", "mrqa_naturalquestions-validation-3968", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8617", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-1447", "mrqa_searchqa-validation-8173", "mrqa_searchqa-validation-6235"], "SR": 0.546875, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.7173437500000001}, {"timecode": 84, "before_eval_results": {"predictions": ["to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "A pulmonary artery", "Grand Inquisition", "Theodore Roosevelt", "the Western Bloc ( the United States, its NATO allies and others )", "Anna Faris", "the s - block", "late - night", "Emma Watson", "April 10, 2018", "the New York Yankees", "either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "in Armenia", "Ireland", "Saphira", "Pre-evaluation, strategic planning, operative planning, implementation", "Saint Peter", "active absorption", "1983", "restricted naturalization to `` free white persons '' of `` good moral character ''", "a heart rate that exceeds the normal resting rate", "Ptolemy", "XI", "the Battle of Antietam", "Bill Condon", "1939", "if the car is slowed initially by manual use of the automatic gear box", "Thomas Mundy Peterson", "Filipino Americans", "Glynis Johns", "the British Empire", "eusebeia", "Napoleon Bonaparte", "Lagaan", "A blighted ovum or anembryonic gestation", "September 19 - 22, 2017", "in the New Testament", "Kirsten Simone Vangsness", "asphyxia", "in 1902", "Camp Green Lake, Theodore `` Armpit '' Johnson", "A patent", "1999", "10,605", "the East Coast of the United States", "the Reverse - Flash", "March 1930", "John Donne", "Category 4", "season four", "6 January 793", "Rugby School", "gold", "Doctor John Dolittle, M.D.", "Premier League", "Christian Kern", "\"50 best cities to live in.\"", "regulators in the agency's Colorado office", "Cotto", "The tall 34-year-old, slouching exhausted in a Johannesburg church that has become a de facto transit camp,", "Rocky", "the Cumberland Gap", "salinity", "George Fox"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6081991792929292}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.5, 0.8, 0.3636363636363636, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.15, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.8333333333333333, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-1910", "mrqa_naturalquestions-validation-1688", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-4863", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3826", "mrqa_hotpotqa-validation-3900", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3511", "mrqa_newsqa-validation-2653"], "SR": 0.515625, "CSR": 0.5387867647058824, "EFR": 0.9354838709677419, "Overall": 0.7043853771347248}, {"timecode": 85, "before_eval_results": {"predictions": ["Chief Justice John Marshall", "The Curse of the Black Pearl", "Samuel de Champlain", "Louis XIV", "Lady Jane Grey", "the Barbary Coast", "Iceland", "the owl", "Richard Cory", "Volkswagen", "baldness", "Athens", "rum", "tea rose", "Aida", "give love a bad name", "the Banni grasslands", "Marie Antoinette", "rotunda", "the magnolia", "haryana", "bicentennial", "Julius Caesar", "auction", "the peace sign", "Michael Dell", "pizza", "Lusitania", "1972", "a hurricane", "the Amish", "the Rocky Mountains", "carbon", "His Dark Materials", "Boston", "Wu-Tang Clan", "king", "(Jose) Martin", "a whale", "Salt Lake City", "Luxembourg", "Texas", "Drag & Drop", "Monty Python", "oui ou non, je veux que vous alliez, qu'il aille", "Las Vegas", "Laura", "The New Yorker", "People of the Book", "a spoiled brat", "Tufts", "The results of the Avery -- MacLeod -- McCarty experiment", "Mike Higham", "the buttock", "Dan Gioia", "magnesium", "The Truman Show", "Robert Redford", "Great Lakes and Midwestern", "three", "three", "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "last March 3, 2008,", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6875}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-16592", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-12392", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-6995", "mrqa_searchqa-validation-1981", "mrqa_searchqa-validation-186", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-4152", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-11933", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-5727", "mrqa_triviaqa-validation-2130", "mrqa_newsqa-validation-3959", "mrqa_newsqa-validation-4210", "mrqa_newsqa-validation-2665"], "SR": 0.609375, "CSR": 0.5396075581395349, "EFR": 1.0, "Overall": 0.7174527616279069}, {"timecode": 86, "before_eval_results": {"predictions": ["the Granite State", "Bull", "Horse Feathers", "Bleak", "Chaillot", "Do the Right Thing", "coloring", "Asteroids", "a bad peace", "Yves Saint Laurent", "New Zealand", "United Kingdom", "the Lend-Lease Act", "Spanglish", "Monica Lewinsky", "Friday night", "Google", "Medusa", "the vest", "Prince", "a gulls", "Hammurabi", "Nixon", "snow", "(Erwin) Rommel", "sleep twitch or night start", "Ned", "the 747", "Terry Bradshaw", "Chris Evert", "Azerbaijan", "Mamma Mia!", "Fallingwater", "Alanis Morissette", "the colon", "a barrel", "Etna", "a law clerk", "Faneuil Hall", "Louisiana", "(George) Orwell", "tea", "Toro", "Stalin", "Metallica", "change horses", "Get Smart", "Lafayette", "Nick Carraway", "Captain Kangaroo", "Kosher", "1996", "Super Bowl XIX", "Gabrielle - Suzanne Barbot de Villeneuve", "paste", "nitrogen", "South Africa", "Cartoon Network Too", "Via Port Rotterdam", "Bruce Grobbelaar", "Tehran, Iran", "The Palm,", "35,000.", "Mashhad"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7401041666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-15017", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-7312", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-2504", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-16876", "mrqa_searchqa-validation-4740", "mrqa_searchqa-validation-13220", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-4307", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-1471", "mrqa_hotpotqa-validation-337", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-975"], "SR": 0.671875, "CSR": 0.5411278735632183, "EFR": 1.0, "Overall": 0.7177568247126438}, {"timecode": 87, "before_eval_results": {"predictions": ["Oblivion", "chiffon", "Corpus Christi", "President Grover Cleveland", "an eye", "the Federalist Papers", "Martin Luther King", "transitive & intransitive", "California", "the Central Pacific", "\"Bob ate the pie\"", "Tom Cruise", "Sicilian pizza", "a panda", "Risk", "brown rice", "Kansas State University", "scrabble", "1945", "Kentucky Bourbon Festival", "a stork", "Towers", "a Rodents", "anime", "Daisy Miller", "Icelandic", "Richard Crowe", "the Moon", "the Fortuner", "Stars and Stripes Forever", "One Hundred Years of Solitude", "lethal", "Henry Cavendish", "vanilla", "Terminus", "Italy", "Night of the Iguana", "Rhode Island", "Baseball", "Anne Rice", "the root", "the Bible", "1066", "Sir John Soane", "Little Yellow jacket", "the hip", "a hearse", "City Slickers", "Ned Kelly", "no worries", "a Doge", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "to the southeastern United States", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Italy", "Pakistan International Airlines", "Desert Quartet", "1998", "\"Peshwa\" (Prime Minister)", "Sleeping Beauty", "the Beatles", "Adidas", "President Bush", "Consumer Reports"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6946924603174603}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-203", "mrqa_searchqa-validation-15068", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-4561", "mrqa_searchqa-validation-2042", "mrqa_searchqa-validation-9515", "mrqa_searchqa-validation-5164", "mrqa_searchqa-validation-15006", "mrqa_searchqa-validation-11547", "mrqa_searchqa-validation-13444", "mrqa_searchqa-validation-11517", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-2206", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-8657", "mrqa_searchqa-validation-2267", "mrqa_searchqa-validation-727", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-13093", "mrqa_searchqa-validation-12454", "mrqa_searchqa-validation-16917", "mrqa_naturalquestions-validation-7133", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-4719", "mrqa_newsqa-validation-663"], "SR": 0.5625, "CSR": 0.5413707386363636, "EFR": 1.0, "Overall": 0.7178053977272727}, {"timecode": 88, "before_eval_results": {"predictions": ["Sputnik", "A Good Day to Die Hard", "The Kinks", "Gorbachev", "Jerez de la Frontera", "Buncefield Depot", "the royal court", "cable", "Westminster Abbey", "Cast", "the king of hearts", "Hawaii", "World War II", "aromatherapy", "Bill Wiki", "Downton Abbey", "Bobby Darin", "France", "Montmorency", "Kent", "Cliff Thorburn", "chamonix", "15", "cymbal", "violin", "Ireland", "Venus", "beetles", "paralysis", "eight", "Japanese silvergrass", "Swindon Town", "Billy Preston", "Happy Birthday to You", "Everton", "sash", "marc", "Staraya Russa", "Makepeace Thackeray", "Mud Rock", "Dumbo's best friend, mentor and the deuteragonist", "Jimmy Knapp", "Check Me", "4", "7,926 miles", "John Galliano", "Mangog", "seddon", "Chiricahua Apache", "Albert Reynolds", "Aug 24", "in the very late 1980s", "Taylor Michel Momsen", "Malayalam", "2008", "Western Canada", "Dutch", "the peace with Israel", "Jiverly Wong,", "28", "Eleanor \"Nell\" Lance", "Syrus \"the Syrian\"", "jam", "Mike Mills"], "metric_results": {"EM": 0.59375, "QA-F1": 0.675}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-5008", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-4236", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-4019", "mrqa_hotpotqa-validation-3566", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-9544", "mrqa_searchqa-validation-3464"], "SR": 0.59375, "CSR": 0.5419592696629214, "EFR": 0.9230769230769231, "Overall": 0.7025384885479689}, {"timecode": 89, "before_eval_results": {"predictions": ["18", "A third beluga whale belonging to the world's largest aquarium has died", "Los Ticos", "reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Alicia Keys", "Haiti,", "Rwanda", "dance", "repair", "Azzam the American,", "10", "fortune", "Hong Kong's Victoria Harbor", "helping to plan the September 11, 2001, terror attacks,", "to provide security as needed.", "malaysia", "some dental work done,", "hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "fill a million sandbags", "helping on the sandbag lines", "Immigration Minister Eric Besson", "since 1983", "133", "promotes fuel economy and safety while boosting the economy.", "planning processes are urgently needed", "Jewish", "12.3 million", "Ski Train", "two years", "Robert Park", "Piedad Cordoba,", "eight", "Itawamba County School District", "Frank Ricci", "1959", "$199", "ferry", "1983", "13", "The Valley Swim Club", "\"still trying to absorb the impact of this week's stunning events.\"", "Wellington, Florida,", "relatives of the five suspects,", "Sharon Bialek", "not", "robin hood", "five", "king Elizabeth", "the Stooges", "two", "Opryland", "Garfield Sobers", "R.E.M.", "San Jose, California", "John Donne", "127 Hours", "kiel Canal", "\"Slaughterhouse-Five\"", "1885", "Germanic", "hearsay", "buffoon", "17th century", "Esther"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6203215187590188}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.5454545454545454, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.9090909090909091, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-4138", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-268", "mrqa_naturalquestions-validation-878", "mrqa_hotpotqa-validation-4986"], "SR": 0.546875, "CSR": 0.5420138888888889, "EFR": 1.0, "Overall": 0.7179340277777777}, {"timecode": 90, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.857421875, "KG": 0.53125, "before_eval_results": {"predictions": ["More than 15,000", "Jezebel.com's Crap E-mail From A Dude", "Gov. Mark Sanford", "Russian air force,", "Chevron", "one", "serving its fast burgers in the Carrousel du Louvre,", "Melbourne.", "peanuts, fish, shellfish, peanuts, tree nuts, wheat and soy.", "opium", "Tuesday.", "former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "the fact that the teens were charged as adults.", "Wednesday,", "fastest circumnavigation of the globe in a powerboat", "order", "General Motors", "London, Ontario,", "\"Da Vinci Code,\"", "Nearly eight in 10", "Golden Globe", "no motive has been determined for the killing,", "Opry Mills,", "striker", "Jan Brewer.", "oceans", "Columbia, Missouri.", "speaking out about a cause someone feels passionate about.", "anti- strikers by a ratio of 9 to 1 on Tuesday.", "Leo Frank,", "Ralph Lauren,", "the American Civil Liberties Union", "Islamabad", "evokes childhood memories in this four-line ode to Mom.", "September,", "on an eight-day trip through Greece, the birthplace of the Olympics,", "\"Avatar,\"", "Frank Ricci,", "U.S. Navy", "the Russian air force,", "we agreed to co-chair the Genocide Prevention Task Force,", "she wonders if part of the appeal of plus-sized", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "ClimateCare,", "Jaipur", "Venezuela", "likening one American diplomat to a \"prostitute\"", "meeting with the president to discuss her son.", "toffelmakaren.", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "buckling under pressure from the ruling party.", "16 seasons", "12.9 - kilometre ( 8 mi )", "1939", "Afghanistan", "Agnolo Bronzino", "61", "Nicolas Winding Refn", "Father Dougal McGuire", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "alfalfa", "Thomas Jefferson", "The Exorcist", "Adolphe Adam"], "metric_results": {"EM": 0.625, "QA-F1": 0.6951745718050065}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.18181818181818182, 1.0, 0.4, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.26086956521739124, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.06060606060606061, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1212121212121212, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1368", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-2983", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-3262", "mrqa_searchqa-validation-1104"], "SR": 0.625, "CSR": 0.5429258241758241, "EFR": 0.9166666666666666, "Overall": 0.7145747481684982}, {"timecode": 91, "before_eval_results": {"predictions": ["Coca-Cola.", "Linus van Pelt", "chestnut", "colorless", "Mark Twain", "Oslo", "Humphrey Bogart", "Hawaii", "glockenspiel", "George Orwell.", "Goldtrail", "The Archers", "Ben Franklin", "Jack Nicholson", "photography", "liqueur", "Taiwan", "Willem de Zwijger", "Oliver Stone", "Neil Armstrong and Edwin \" Buzz\" Aldrin", "Oregon", "king", "Nikola Tesla", "Thomas De Quincey", "Susie Dent", "Pancho Villa", "the Crusades", "Ivan Owen", "1925", "copper", "Pickwick", "Bluebell Girls", "Columbus", "switzerland", "Ann Darrow", "blue", "Flying Pickets", "St Moritz", "switzerland", "Vietnam", "1985", "Bogota", "the United States", "James Murdoch", "Crystal Palace", "Belfast", "You see I've forgotten, if they're green or they're blue", "Thermopylae", "Elton John", "Seattle", "Marshalsea", "card verification value", "\u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "Tandi", "94", "August 6, 1845 - October 6, 1931", "Ted", "24", "Tuesday", "opium trade", "Ham", "resuscitation", "Hudson", "Subway"], "metric_results": {"EM": 0.625, "QA-F1": 0.6915134803921569}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true], "QA-F1": [0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1, 0.8235294117647058, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-3978", "mrqa_triviaqa-validation-4026", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-6962", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-6887", "mrqa_hotpotqa-validation-5628", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-2175", "mrqa_searchqa-validation-13767", "mrqa_searchqa-validation-8329"], "SR": 0.625, "CSR": 0.5438179347826086, "EFR": 1.0, "Overall": 0.7314198369565217}, {"timecode": 92, "before_eval_results": {"predictions": ["orangutans", "lowestoft", "jesse", "new zealand", "net Worth", "Ernest Hemingway", "Charles II", "the Godfather of Italian cooking", "the Hubble Space Telescope", "France", "greece", "a window", "dennis smith", "a coffee house", "the little dog laugh'd to see such Craft,", "baseball cards", "austria", "netherlands", "Neighbours", "kursk", "freaks, artists and entertainment junkies", "six", "blind Beggar", "Mark Darcy", "bacofoil", "homo sapiens", "alistair Darling", "Yuri Andropov", "surrey", "Theo Walcott", "anabaptists", "british airways", "santa santa", "Oregon", "petula Clark", "surrey colquhoun", "a dice game", "Saturn", "Sinclair Lewis", "the Fleet River", "Unseen", "james chadwick", "LDV", "1879", "the weasel", "table tennis", "bison", "alberich", "geomorphology", "mars", "Tina Turner", "com TLD", "abdicated", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Guangzhou", "29, 1985", "Jane Mayer", "a storm,", "a member of the band for more than 40 years", "American", "avanti", "Coors Field", "the Chrysler Building", "the USS \"Enterprise\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.5523003472222222}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.22222222222222218, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4018", "mrqa_triviaqa-validation-5157", "mrqa_triviaqa-validation-1795", "mrqa_triviaqa-validation-654", "mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-6329", "mrqa_triviaqa-validation-904", "mrqa_triviaqa-validation-3598", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2308", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-7470", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-2717", "mrqa_newsqa-validation-3990", "mrqa_searchqa-validation-6690"], "SR": 0.515625, "CSR": 0.5435147849462365, "EFR": 0.967741935483871, "Overall": 0.7249075940860215}, {"timecode": 93, "before_eval_results": {"predictions": ["Robert Barnett,", "a public housing project,", "a hospital in Amstetten,", "tibet", "h Zimbabwean government", "a one-shot victory in the Bob Hope Classic", "the administration's progress, while we also encourage him to 'evolve faster' on supporting full marriage equality,\"", "was in fine condition at takeoff, and said Schrenker is \"an accomplished pilot\" who owns \"a couple of airplanes\"", "Susan Atkins,", "80,", "she was crying when she was talking about her daughters.", "Iraqi", "Passers-by", "subscribers to a daily publication which is the primary service of Stratfor,\"", "Oprah Winfrey.", "Former Mobile County Circuit Judge Herman Thomas", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "heavy flannel or wool", "Diego Milito's", "of-the-moment style.", "snowstorm", "the defense of the interests of Argentineans, not just about sovereignty,\"", "the wars in Iraq and Afghanistan", "two", "finance", "Indonesia", "Charlie Daniels Band,", "three out of four", "trading goods and services without exchanging money.", "gasoline", "cambodia", "South Africa", "Harrison Ford", "Jacob,", "Cash for Clunkers", "100 meter", "Shenzhen in southern China.", "fluoroquinolone", "Chinese and international laws", "he had struggled to adapt to the different culture and religious life in Sudan.", "the charter", "27,", "five", "Secretary of State", "the ACLU", "it was unjustifiable", "his father", "The Sopranos.", "finance", "infrastructure.", "managing his time.", "merengue and bachata music, both of which are the most popular forms of music in the country", "Georgia", "Washington Redskins", "jonathan smith", "bill bryson", "argument", "1960s", "al Ariel ram\u00edrez", "antisemitism", "the Yangtze River", "oui", "the Andes Mountains", "season"], "metric_results": {"EM": 0.5, "QA-F1": 0.5999771985407066}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.8571428571428571, 0.5, 0.0, 0.0, 0.8, 0.5, 0.2580645161290323, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3233", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-3547", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-4073", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-2837", "mrqa_triviaqa-validation-4841", "mrqa_triviaqa-validation-3004", "mrqa_hotpotqa-validation-316", "mrqa_hotpotqa-validation-3981", "mrqa_hotpotqa-validation-1657", "mrqa_searchqa-validation-13597", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-6906", "mrqa_triviaqa-validation-2535"], "SR": 0.5, "CSR": 0.5430518617021276, "EFR": 1.0, "Overall": 0.7312666223404255}, {"timecode": 94, "before_eval_results": {"predictions": ["Larry King", "Immigration Minister Eric Besson", "$500,000", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "bankruptcy", "At least 38", "laid 11 healthy eggs and, this week, all 11 of them hatched", "Eleven", "41,", "McDonald's", "And he won it after facing various challenges and turning them to his advantage.", "10.1,\"", "Democrats and Republicans", "Zac Efron", "The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "Sylt", "the National Restaurant Association", "Congress", "Cash for Clunkers", "11th year in a row.", "2,000", "\"face of the peace initiative has been attacked,\"", "after Wood went missing off Catalina Island,", "grossed $55.7 million during its first frame,", "Tulsa, Oklahoma.", "almost 100", "Janet Napolitano", "56,", "There's no chance", "Barack Obama,", "everyone can use solar and renewable energy at home everyday,\"", "seven years ago", "30,000", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "$14.1 million.", "the Movement for Democratic Change,", "cartel from the state of Veracruz, Mexico,", "Orbiting Carbon Observatory,", "the IAAF", "Pakistan", "Kim Clijsters", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Daryeel Bulasho Guud", "onto the college campus.", "two remaining missing men.", "Opry Mills,", "Caylee Anthony", "in the neighboring country of Djibouti,", "18", "school,", "Hayden", "A firm, flexible cup - shaped device worn inside the vagina", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "Governor Hutchinson", "greece", "Muhammad Ali", "Ub Iwerks", "Croatan, Nantahala, and Uwharrie", "4,613", "black Sabbath", "raytheon", "the Lion King", "Xurbia Endless", "Johnny Got His Gun"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6269163333938226}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5581395348837208, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.24489795918367346, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-955", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2341", "mrqa_naturalquestions-validation-2406", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-644", "mrqa_triviaqa-validation-2079", "mrqa_hotpotqa-validation-3036", "mrqa_searchqa-validation-9921"], "SR": 0.5625, "CSR": 0.5432565789473685, "EFR": 1.0, "Overall": 0.7313075657894738}, {"timecode": 95, "before_eval_results": {"predictions": ["Ford", "peptide", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "around 3,000 - 5,000 program - erase cycles, but some flash drives have single - level cell ( SLC ) based memory that is good for around 100,000 writes", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "October 6, 2017", "autopistas", "northwest of Bemis Heights", "Fusajiro Yamauchi", "1854", "Tim McGraw and Kenny Chesney", "Qutab - ud - din Aibak", "the Devastator", "Abraham Gottlob Werner", "lacteal", "Fix You", "In some cases, there was a transitional stage where toilets were built into the house but accessible only from the outside", "Britain", "the Origination Clause of the United States Constitution", "architecture", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "the arms of Ireland", "pop ballad", "2010", "12 - 10", "Annette Strean", "Charles Path\u00e9", "eukaryotic", "during World War II", "Bactrian", "2003", "Thomas Hobbes in his Leviathan, though with a somewhat different meaning ( similar to the meaning used by the British associationists )", "fall of 2015", "Rodney Crowell", "Spanish moss", "the inmates have been detained indefinitely without trial and several detainees have alleged torture", "to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "961", "Lightning Thief", "cutting", "The Cornett family", "Allison Janney", "around 2011", "Patrick Swayze", "McFerrin, Robin Williams, and Bill Irwin", "Ann Gillespie", "Lula's", "2017", "March 15, 1945", "an Aldabra giant tortoise", "Oliver Goldsmith", "Brussels", "Iron Age", "Seventeen", "Cartoon Network", "What You Will", "Kurt Cobain", "Madonna", "Brian Smith.", "Shakespeare", "Bob Hope", "three", "$40 and a loaf of bread."], "metric_results": {"EM": 0.59375, "QA-F1": 0.7117060090591879}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.2758620689655173, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8181818181818181, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.45454545454545453, 0.5, 1.0, 1.0, 0.0, 0.8125000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-3309", "mrqa_newsqa-validation-1963", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-7262"], "SR": 0.59375, "CSR": 0.5437825520833333, "EFR": 0.9615384615384616, "Overall": 0.7237204527243589}, {"timecode": 96, "before_eval_results": {"predictions": ["the game", "stood 6 feet 6 inches,", "Another high tide -- expected to reach about 4 meters (13 feet) high", "April 22,", "\"Hawaii Five-O\"", "a number of calls, and those calls were intriguing, and we're chasing those down now,\"", "30", "U.S. senators", "The son of Gabon's former president", "chairman of the House Budget Committee,", "is the U.N. nuclear watchdog agency's strongest warning yet that Iran could be at work on building a nuclear weapon.", "Abu Sayyaf,", "Islamabad,", "the run-of-the-mill", "nine newly-purchased bicycles at the scene,", "hot and humid", "a hospital", "in Fayetteville, North Carolina,", "\"We're not going to forget you in Washington, D.C.", "Ewan McGregor", "September,", "anything could have stopped Robert Hawkins from going on a murderous rampage at an Omaha, Nebraska, shopping mall on Wednesday.", "genocide, crimes against humanity, and war crimes.", "any person who has been abused by any priest of the Diocese of Cloyne during my time as bishop or at any time,\"", "Barney Stinson,", "19-year-old", "\"The Cycle of Life,\"", "collaborating with the Colombian government,", "the U.S. Holocaust Memorial Museum,", "sportswear,", "fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives.", "Sunday's", "Unseeded Frenchwoman Aravane Rezai", "researchers", "the Army,", "30-minute", "Grease.", "in good spirits, especially comforted to be receiving care from talented doctors in a world-class hospital named in honor of her late husband,\"", "gunned down four Lakewood, Washington, police officers Sunday.", "the home,", "students often know ahead of time when and where violence will flare up on campus.", "between 1917 and 1924", "some work rule issues.", "Kerstin Fritzl,", "salary", "\"CNN Heroes: An All-Star Tribute\"", "clean up Washington State's decommissioned Hanford nuclear site,", "over 1,000 pounds", "3,000 kilometers (1,900 miles)", "UH-60 Blackhawk helicopters", "Marc by Marc Jacobs", "Midazolam", "the Coppolas and, technically, the Farrow / Previn / Allens", "To capitalize on her publicity", "the Church", "Ivor Novello", "pollen", "\u00c6thelwald Moll", "The Bears", "Nikolai Trubetzkoy", "indian alphabet", "Charles Dana Gibson", "the Constitution", "a snail"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5663532684273611}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.19999999999999998, 0.25, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.9600000000000001, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.5454545454545455, 1.0, 0.0, 0.967741935483871, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.2857142857142857, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-1918", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2130", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-3543", "mrqa_searchqa-validation-114"], "SR": 0.421875, "CSR": 0.5425257731958764, "EFR": 1.0, "Overall": 0.7311614046391753}, {"timecode": 97, "before_eval_results": {"predictions": ["Ferrari", "Andy Murray", "Six", "girls around 11 or 12.", "David Bowie,", "1957,", "behind the counter.", "\"We want to reset our relationship and so we will do it together.'\"", "heavy brush,", "1959.", "Phoenix, Arizona,", "\"project work\"", "Buddhism", "40", "\"He is telling me to regain the trust of those customers who are driving our vehicles.\"", "one of Africa's most stable nations.", "Molotov cocktails, rocks and glass.", "Alfredo Astiz,", "the former World Trade Center's \"archaeological heart,\"", "the northeastern Iranian city of Mashhad", "nearly $162 billion in war funding", "$106.5 million", "Obama administration has not yet articulated a Sudan policy approach,", "Matthew Fisher", "autonomy,", "Crandon, Wisconsin,", "government efforts at control and censorship remain rife across the Middle East and North Africa,", "Guinea, Myanmar, Sudan and Venezuela.", "Brazil,", "Uzbekistan.", "45 minutes, five days a week.", "urged NATO to take a more active role in countering the spread of the narcotics trade,", "took on water", "Four bodies", "July", "Visitors aren't allowed", "an auxiliary lock", "Diego Milito's", "\"We Found Love\" in a muddy barley field owned by farmer Alan Graham outside Bangor,", "Robert Mugabe", "gang rape", "\"wacko.\"", "threatening messages", "Mexico", "more than 1.2 million people.", "Sunday.", "tennis", "Osama bin Laden's sons", "Chesley \"Sully\" Sullenberger", "Gary Player,", "Karen Floyd", "Asuka", "Authority", "Prince William, Duke of Cambridge", "the Astor family", "Gryffendor", "Rowan Atkinson", "841", "Adelaide", "gender queer", "Windsor Castle", "Tad Hamilton", "Israel", "Bill Irwin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6720366082038876}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.11764705882352941, 1.0, 1.0, 1.0, 0.8, 0.888888888888889, 0.1818181818181818, 0.0, 0.0, 0.0, 1.0, 0.0, 0.35294117647058826, 0.0, 0.0, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1362", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-1775", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-2337", "mrqa_triviaqa-validation-5311", "mrqa_triviaqa-validation-5472", "mrqa_hotpotqa-validation-2217"], "SR": 0.5625, "CSR": 0.5427295918367347, "EFR": 0.9285714285714286, "Overall": 0.7169164540816327}, {"timecode": 98, "before_eval_results": {"predictions": ["Cambodian officials", "monarchy.", "\"pleased\"", "AbdulMutallab", "in a stream in Shark River Park in Monmouth County", "consumer confidence", "anesthetic", "Madonna", "Egyptian striker", "Iran's", "18", "2050,", "U.S. State Department and British Foreign Office", "Janet Napolitano", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "break up ice jams.", "likely to top $60 million by the time the Presidents Day holiday weekend is over.", "unable to pass significant restrictions on war funding", "Islamabad", "the 3rd District of Utah.", "36", "Nineteen", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "nearly $2 billion", "near Garacad, Somalia,", "Al-Shabaab,", "February 12", "Kenneth Cole", "T.I.", "heavy turbulence", "over Saturday's killing of a 15-year-old boy", "to sniff out cell phones.", "in California, Texas and Florida,", "Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "Ozzy Osbourne", "Revolutionary Armed Forces of Colombia,", "suicides", "Jaime Andrade", "Aung San Suu Kyi", "$81,8709", "Patrick McGoohan,", "\"GoldenEye\"", "J.Crew,", "Dublin.", "1995", "Chester Arthur Stiles,", "hopes the journalists and the flight crew will be freed,", "a skilled hacker", "$7.8 million", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "remains committed to British sovereignty", "Lee Thompson Young", "September 2017", "France", "Monopoly", "carpe diem", "hydrocephalus", "India Today", "Eliot Cutler", "7 January 1936", "turlogh Dubh O'Brien", "Istanbul", "Washington Redskins", "1942"], "metric_results": {"EM": 0.59375, "QA-F1": 0.684461564350187}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.4444444444444445, 0.9565217391304348, 0.0, 0.375, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 0.21052631578947364, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-3888", "mrqa_naturalquestions-validation-8858", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-6475"], "SR": 0.59375, "CSR": 0.5432449494949495, "EFR": 1.0, "Overall": 0.7313052398989899}, {"timecode": 99, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2871", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1961", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-739", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-2079", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4698", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-5339", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-904"], "OKR": 0.818359375, "KG": 0.4953125, "before_eval_results": {"predictions": ["bacteria", "1648 - 51", "The final venues were confirmed, along with the tournament's schedule, on 2 May 2013", "Darren McGavin", "writ of certiorari", "Emily Osment", "pour point of a liquid", "Madison", "May 29, 2018", "Dan Stevens", "in Ephesus in AD 95 -- 110", "Bulgaria", "lithium iron phosphate", "1986", "2018", "The symbol", "2010", "The 14th game of this series", "Clarence Anglin", "Ariana Clarice Richards", "James Intveld", "Darlene Cates", "one season", "mashed potato", "2018", "in the dress shop", "Eric Clapton", "Seton Hall Pirates men's basketball team", "July 2014", "Universal Pictures and Focus Features", "Cyanea capillata", "enabled business applications to be developed with Flash", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "Lee County, Florida", "digestive systems of many organisms", "Theodosius I", "Christopher Allen Lloyd", "reared", "Mark Jackson", "Fall 1998", "TLC", "Inequality of opportunity", "In 1929", "Abid Ali Neemuchwala", "Bhupendranath Dutt", "May 19, 2017", "the people of France", "two to three barrel vaults", "on its headwaters in the Brazilian state of Mato Grosso", "Gustav Bauer", "either in front or on top of the brainstem", "Leicestershire", "\"The curse is come upon me,\"", "3", "Tim Kaine", "Soviet Union", "Heinkel Flugzeugwerke", "suicide bombing", "Zac Efron", "Aryan Airlines Flight 1625", "Ocean's Twelve", "Trajan", "Anne Rice", "Wilkie Collins"], "metric_results": {"EM": 0.59375, "QA-F1": 0.715802937554332}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5882352941176471, 0.0, 1.0, 0.6666666666666666, 0.761904761904762, 0.5, 0.8, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5517241379310346, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-5094", "mrqa_triviaqa-validation-4314", "mrqa_hotpotqa-validation-5509", "mrqa_newsqa-validation-846"], "SR": 0.59375, "CSR": 0.54375, "EFR": 0.9230769230769231, "Overall": 0.6982872596153846}]}