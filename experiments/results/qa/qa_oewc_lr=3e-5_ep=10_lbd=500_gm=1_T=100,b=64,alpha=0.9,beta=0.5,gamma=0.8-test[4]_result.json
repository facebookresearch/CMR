{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=500_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=500.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=500_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]_result.json', stream_id=4, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4140, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["fall of 1937", "The Skirmish of the Brick Church", "beliefs of Sunni Islamic thinkers", "\"The Lodger\"", "Londonistan", "a high-level marketing manager", "Houston, Texas", "cone-shaped", "San Francisco Bay Area's Levi's Stadium", "Ren\u00e9 Lalique", "absolution", "$105 billion", "ABC Cable News", "trial division", "their belief in the validity of the social contract", "Hyde Park", "four years", "Grey Street", "most of the items in the collection, unless those were newly accessioned into the collection", "literacy and numeracy", "Luther", "prime elements", "the Aveo", "one week", "Steymann v Staatssecretaris van Justitie", "1937", "The governments of the United States, Britain, Germany and France", "mother-of-pearl", "cholera", "Tower District", "ring theory", "Euclid's fundamental theorem of arithmetic", "Tony Hawk", "Beyonc\u00e9", "The Book of Discipline", "USSR", "Schmalkaldic League", "2006", "70%", "Einstein", "Genghis Khan", "four half-courses per term", "2011", "Brownlee", "Tracy Wolfson", "the wisdom and prudence of certain decisions of procurement", "1971", "the Uighurs surrendered to the Mongols first", "cnidarians", "CBS", "842 pounds", "two of Tesla's uncles", "up to \u00a332,583", "the City council", "three", "shopping", "4 weeks", "propulsion, electrical power and life support", "William Smilie", "George Westinghouse", "1279", "complexity classes", "\"everything that smacks of sacrifice\"", "a system to function"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7404040404040404}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9630", "mrqa_squad-validation-7687", "mrqa_squad-validation-4836", "mrqa_squad-validation-131", "mrqa_squad-validation-2297", "mrqa_squad-validation-5505", "mrqa_squad-validation-1802", "mrqa_squad-validation-9136", "mrqa_squad-validation-9061", "mrqa_squad-validation-116", "mrqa_squad-validation-5877", "mrqa_squad-validation-6294", "mrqa_squad-validation-7214", "mrqa_squad-validation-3699", "mrqa_squad-validation-8247", "mrqa_squad-validation-4419", "mrqa_squad-validation-553", "mrqa_squad-validation-3811", "mrqa_squad-validation-2092"], "SR": 0.703125, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 1, "before_eval_results": {"predictions": ["the Pulfrich effect", "semi-legal", "Westinghouse Electric", "Pax Mongolica", "The date of 2035", "internal strife", "the Marburg Colloquy", "Northumbria University", "non-cryogenic", "the defense and justification of empire-building", "the Carm Michael numbers", "1999", "Scorpion", "October 16, 2012", "a commune", "the metal locking screw on the camera lens", "Eldon Square Shopping Centre", "type III secretion system", "$680 billion", "296", "New Collegiate Division", "four", "18 million volumes", "15,100", "Warner Bros. Presents", "V\u03b39/V\u03b42 T cells", "1985", "the Augustinian friars", "third", "2012", "gold", "force model that is independent of any macroscale position vector", "378", "tourism", "the Jews", "all", "many celebrated seasons", "Charles-Fer Ferdinand University", "The Nationals", "a computational problem where a single output (of a total function) is expected for every input", "Katharina von Bora", "1888", "the middle of the continent", "Schmalkaldic League", "4:51", "Knaurs Lexikon", "constant pressure", "detective shows", "the southern and central parts of France", "Maria Fold and thrust Belt", "making it seem like climate change is more serious by overstating the impact", "1945", "1876", "Elway", "spring of 1349", "Extreme Makeover: Home Edition", "the Wesleyan Holiness Consortium", "it has settled as one of the pillars of history", "The four Railroads are fairly lucrative properties", "The Sphinx would devour anyone who could not answer her riddle", "The Smashing Pumpkins are an American alternative rock band from Chicago, Illinois, formed", "the 107th justice to serve on the United States Supreme Court", "32-year-long investigation into the enigmatic hijacker", "If the citizen's heart was heavier than a feather they would face torment in a lake of fire"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8365891167494787}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.2666666666666667, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 0.18181818181818182, 0.3076923076923077, 0.0, 0.0, 0.2105263157894737]}}, "before_error_ids": ["mrqa_squad-validation-8546", "mrqa_squad-validation-9024", "mrqa_squad-validation-10466", "mrqa_squad-validation-1030", "mrqa_squad-validation-1189", "mrqa_squad-validation-1600", "mrqa_squad-validation-4287", "mrqa_squad-validation-2166", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-1274", "mrqa_hotpotqa-validation-3713"], "SR": 0.78125, "CSR": 0.7421875, "EFR": 0.9285714285714286, "Overall": 0.8353794642857143}, {"timecode": 2, "before_eval_results": {"predictions": ["Solim\u00f5es Basin", "the seal of the Federal Communications Commission", "Islamism", "the E. W. Scripps Company", "Grand Canal d'Alsace", "food security", "exothermic", "Newcastle Diamonds", "the wisdom and prudence of certain decisions of procurement", "D\u00fcrer", "Grover Cleveland", "Erg\u00e4nzungsschulen", "concrete", "to promote advanced research and education networking in the United States", "The Newlywed Game", "the German-Swiss border", "the \"blurring of theological and confessional differences in the interests of unity.\"", "microbes", "300", "the electrostatic force", "Jim Nantz and Phil Simms", "1530", "from 12:00 to 6:00 p.m. Eastern Time", "employ consultant pharmacists and/or provide consulting services", "the murder of Christ", "1708", "the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "the Religious Coalition for Reproductive Choice", "The Arrow", "intractable problems", "yellow fever", "silver and inlaid with gold", "Richard Wilkinson and Kate Pickett", "elsewhere in the Northern United Kingdom", "president and CEO", "Von Miller", "the weak force", "a diverse phylum of bacteria capable of carrying out photosynthesis", "vaccination", "the plague theory", "the loss of soil fertility and weed invasion", "the fact (Fermat's little theorem)", "11th", "the most popular show", "Robert of Jumi\u00e8ges", "student populations", "German", "Persia", "superheaters", "two", "Johann von Staupitz", "lectures", "Short Short", "What a wonderful World", "the White House", "Dugout canoe", "Ganges", "the Heritage 1981 brand", "Britney Spears", "the title My Fair Lady", "Don Bradman", "Ford Motor Co.", "Fidenza", "Charles Scribner's"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7465411324786324}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5521", "mrqa_squad-validation-4847", "mrqa_squad-validation-597", "mrqa_squad-validation-5828", "mrqa_squad-validation-10460", "mrqa_squad-validation-8777", "mrqa_squad-validation-4974", "mrqa_squad-validation-9023", "mrqa_squad-validation-1188", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-9859", "mrqa_searchqa-validation-6857", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-3869"], "SR": 0.6875, "CSR": 0.7239583333333333, "EFR": 1.0, "Overall": 0.8619791666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["Winter Film Capital of the World", "clear boundaries", "collenchyma tissue", "24 March 1879", "the Convention", "constant factors and smaller terms", "1996", "CBS", "genetic branches", "on a religious basis", "14,000", "Killer T cells", "about 11 million", "third", "The Earth's mantle", "expansions", "Necessity-based", "glaucophyte chloroplasts", "artisans and farmers", "inverted repeat", "pharmacists", "September 2007", "a declining state of mind", "G", "civil disobedience", "Sociologist", "World News Tonight", "the carriage of their respective basic channels", "cytotoxic", "Liao, Jin, and Song", "Tyneside Classical", "nine", "18 million", "four", "Christian Whiton", "his mother", "Johann Gerhard", "Korean", "The Time of the Doctor", "7 January 1900", "90\u00b0", "the Natives of these localities are very badly disposed towards the French, and are entirely devoted to the English", "stolen", "Centrum", "$200,000", "4,686", "economic", "1950s", "Karl Marx", "President of the United States of America", "Disneyland", "the king", "South Africa", "fibre optics", "Lawrence Brooks", "a cone-shaped utensil", "Yasser Arafat", "Pigeon", "voodoo", "a dowry", "Macduff", "Jean Halliwell", "Prince James, Duke of York and of Albany ( later King James II & VII )", "Charles Perrault"], "metric_results": {"EM": 0.640625, "QA-F1": 0.702951388888889}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1947", "mrqa_squad-validation-1714", "mrqa_squad-validation-9597", "mrqa_squad-validation-10107", "mrqa_squad-validation-8703", "mrqa_squad-validation-6409", "mrqa_squad-validation-3958", "mrqa_squad-validation-6670", "mrqa_squad-validation-6884", "mrqa_squad-validation-7083", "mrqa_squad-validation-2406", "mrqa_squad-validation-10186", "mrqa_squad-validation-1509", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-15033", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-495"], "SR": 0.640625, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 4, "before_eval_results": {"predictions": ["Mars", "education", "Kingdom of Prussia", "May 18, 1756", "odd prime", "economic growth by collecting resources from colonies, in combination with assuming political control by military and political means", "quantum electrodynamics", "topographic", "Hassan al Banna", "regional burden sharing", "Indianapolis Colts", "Pole Mokotowskie", "a school or other place of formal education", "Francis Blackburne", "black earth", "photolysis of ozone by light of short wavelength", "smart ticketing", "State Route 99", "F and \u2212F are equal in magnitude and opposite in direction", "free", "Air", "1,548", "whether the bill is within the legislative competence of the Parliament", "two", "Galileo Galilei", "Catholic", "patient care rounds drug product selection", "vicious and destructive", "greater scarcity", "The Eleventh Doctor", "86", "Arizona Cardinals", "Not designed to fly through the Earth's atmosphere or return to Earth", "2015", "one hunting excursion", "Deacons", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "socialist realism", "July 23, 1963", "1162", "a method which pre- allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "Barbara Walters", "as soon as 2050", "Red River", "the hundreds", "London and Buenos Aires", "Sub-Saharan Africa", "D, E or F", "the Carrousel du Louvre", "$1.5 million", "3 to 17", "Isabella", "Obama", "World Wide Village", "Preah Vihear temple", "Ralph Lauren", "Noriko Savoie", "T.I.", "(Zed)", "the Kenyan and Somali governments issued a joint communique declaring Al-Shabaab \"a common enemy to both countries.\"", "battles, political intrigue, and the characters", "oldpatricktoe-end", "Mulberry", "Shaft"], "metric_results": {"EM": 0.6875, "QA-F1": 0.751541832010582}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29629629629629634, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9926", "mrqa_squad-validation-8634", "mrqa_squad-validation-1891", "mrqa_squad-validation-10333", "mrqa_squad-validation-3706", "mrqa_squad-validation-2564", "mrqa_squad-validation-4746", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-2234", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-712", "mrqa_searchqa-validation-8929"], "SR": 0.6875, "CSR": 0.7, "EFR": 0.9, "Overall": 0.8}, {"timecode": 5, "before_eval_results": {"predictions": ["visitation of the Electorate of Saxony", "United States", "11", "May", "to employ limited coercion in order to get their issue onto the table", "1798", "3D printing technology", "Waterlogged", "Tim Allen", "wealth and income", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "40", "LeGrande", "filaments", "Joanna Lumley", "Energiprojekt AB in Sweden", "DuMont Television Network", "1913", "Egyptian Islamic Jihad organization", "consumer prices", "petroleum", "1870", "27.7 million tons", "the remainder of the British Isles", "Jean Auguste Dominique Ingres,", "Eliot Ness", "to obey the temporal authorities", "Edgar", "experience and extra responsibilities", "chameleon circuit", "All-Channel Receiver Act", "secular powers", "at the opposite end from the mouth", "areas controlled by Russia in 1914", "kinescope", "University Athletic Association (UAA)", "three", "Chebyshev", "reality television", "Disco", "Christopher Lloyd Smalling", "Mary Harron", "Polk", "Minette Walters", "1983", "79 AD", "1993 to 2001", "Major League Soccer", "1669", "University of Vienna", "\"lo Stivale\" (the Boot)", "Richa Sharma", "Centennial Olympic Stadium", "Violet", "Vernier, Switzerland", "October 21, 2016", "an Indian cricketer and former captain of the Indian cricket team", "international association football competitions", "The conversation", "9 February 2018", "Canada", "teenage", "an increase in dew point", "Donna Mills"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8171875000000001}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-3345", "mrqa_squad-validation-5519", "mrqa_squad-validation-2322", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4614", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-2982", "mrqa_searchqa-validation-4118"], "SR": 0.765625, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 6, "before_eval_results": {"predictions": ["woodblocks", "General Hospital", "the dot", "Denver Broncos", "129 MSPs", "Lenin", "completed (or local) fields", "\"push\" motivations", "alone", "stem cells", "John Pell, Lord of Pelham Manor", "quickly", "induction motor", "Holy War", "pressure terms", "ten million", "Tommy Lee Jones", "nine", "13.34% (116.7 sq mi or 302 km2)", "kilopond", "water level", "1981", "rules that conflict with morality", "early as the sixteenth century", "R\u00fcdesheim", "an epidemiological account of the plague", "time or space", "Jacksonville Consolidation", "Aristotle", "1724", "mid-Cambrian period", "Canada", "Stanford University", "Reuben Townroe", "small forward", "Alamo Bowl", "The King of Chutzpah", "Charles Russell", "German", "Minette Walters (born 26 September 1949)", "St. Patrick's Day in 1988", "Dulwich", "Michael Sheen", "Hungary", "ITV", "Ella Fitzgerald", "\"Confessions of a Teenage Drama Queen\"", "EBSCO Information Services", "Dutch", "Marc Bolan", "\"The Nightmares Before Christmas\" (1992)", "John Mills", "1992", "Saint-Domingue", "Airline Deregulation Act", "\"Kill Your Darlings\"", "University of Kansas", "The Land of Enchantment", "2001", "AnCIENT Seven Wonders of The World", "illegal", "Billy Bob Thornton (born August 4, 1955)", "Charles Martel", "The Blues Brothers"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7227306547619047}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5774", "mrqa_squad-validation-9430", "mrqa_squad-validation-7614", "mrqa_squad-validation-2567", "mrqa_squad-validation-5303", "mrqa_squad-validation-7476", "mrqa_squad-validation-9718", "mrqa_squad-validation-9098", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-4387", "mrqa_triviaqa-validation-2856", "mrqa_newsqa-validation-696", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-12796"], "SR": 0.65625, "CSR": 0.703125, "EFR": 0.9545454545454546, "Overall": 0.8288352272727273}, {"timecode": 7, "before_eval_results": {"predictions": ["Germany", "1985", "William Iron Arm", "Muhammad Abd al-Salaam Farag", "comedies", "lack of understanding", "British East Africa", "the Atlantic", "Jingshi Dadian", "economic instability", "Jean- Marc Bosman", "Ismailiyah", "Wiesner", "polynomial time", "quarterback", "ten times their own weight", "primes", "light", "success", "TGIF", "often married outside their immediate French communities", "George Westinghouse", "color confinement", "certification", "Alberto Calder\u00f3n", "Mercury", "Marconi successfully transmitted the letter S from England to Newfoundland", "private", "Cadeby", "Ten", "vice president", "Steven Gerrard", "the international community", "Hine's school", "Obama", "Silvio Berlusconi", "London Heathrow's Terminal 5", "at the House of Blues", "football", "sharia law", "Chandler Keys", "Sonia Sotomayor", "Kurdish militant group", "pilot", "Wednesday", "$50", "back at work", "flooding", "1971", "composer", "50,000", "to best your own fuel economy achievements", "Bhola district", "SSM Cardinal Glennon Children's Medical Center", "military", "he has no plans to fritter his cash away", "Secretary of State Hillary Clinton", "7th century", "Wigan", "2004", "the Mormon Tabernacle Choir", "Hagai Amir", "The Little Foxes", "Lord Tennyson"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5463834972394754}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.08695652173913045, 0.05714285714285715, 0.33333333333333337, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6034", "mrqa_squad-validation-6925", "mrqa_squad-validation-8339", "mrqa_squad-validation-4289", "mrqa_squad-validation-4692", "mrqa_squad-validation-9614", "mrqa_squad-validation-2160", "mrqa_squad-validation-8771", "mrqa_squad-validation-3069", "mrqa_squad-validation-10445", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_naturalquestions-validation-8664", "mrqa_hotpotqa-validation-2330", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-1341"], "SR": 0.484375, "CSR": 0.67578125, "EFR": 0.9696969696969697, "Overall": 0.8227391098484849}, {"timecode": 8, "before_eval_results": {"predictions": ["2 July 1505,", "beginning in early September and ending in mid-May", "high risk preparations and some other compounding functions", "Non Governmental and Intergovernmental Organizations", "chief electrician position", "1671", "United States", "12 January 1943", "his own men", "Sonia Shankman Orthogenic School", "Innate immune systems", "the manufacturing sector", "Wardenclyffe Tower project", "horizontal compression", "Edgar Scherick", "Imperial", "Moscone Center", "Great Yuan", "Since the 1980s", "oxygen-16", "Lessing", "one advanced lay servant course", "the Tyne Tunnel", "Threatening government officials", "ca. 22,000\u201314,000 yr BP,", "two", "Lek", "New England Patriots", "work rule issues", "28 of those now on hardcourt surfaces.", "July 4.", "the Catholic League", "more than two years,", "Newark's Liberty International Airport,", "We Found Love", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "a kidney transplant", "foster national reconciliation between religious and ethnic groups", "Addis Ababa,", "Christian bookstores across the country that carry the publication.", "military trials", "to launch a group that will serve as an alternative to the Organization of American States.", "to pay him a monthly allowance", "Tutsi and Hutu rivalry", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Draquila -- Italy Trembles.", "hooked up with Mildred, a younger woman of about 80, in March.", "his former Boca Juniors teammate and national coach Diego Maradona,", "17 Again", "five", "75", "Kgalema Motlanthe,", "preventing our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia.", "about 3,000 kilometers (1,900 miles)", "two courses", "NATO fighters", "Austin, Texas,", "two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "the community", "Anah\u00ed", "Romanian Communist leader, Nicolae Ceausescu,", "Nikkei 225 Stock Average", "Surrey", "David Bowie"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6015660396824801}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1818181818181818, 0.8, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333336, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.058823529411764705, 0.2608695652173913, 0.1818181818181818, 0.28571428571428575, 0.06451612903225806, 0.5, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8526", "mrqa_squad-validation-1279", "mrqa_squad-validation-3113", "mrqa_squad-validation-589", "mrqa_squad-validation-1570", "mrqa_squad-validation-6128", "mrqa_squad-validation-7377", "mrqa_squad-validation-8084", "mrqa_squad-validation-2405", "mrqa_squad-validation-10083", "mrqa_squad-validation-5357", "mrqa_squad-validation-6671", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-490", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-93", "mrqa_searchqa-validation-8602", "mrqa_triviaqa-validation-1"], "SR": 0.46875, "CSR": 0.6527777777777778, "EFR": 1.0, "Overall": 0.8263888888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["Private Education Student Financial Assistance", "philanthropy", "in an unmarked grave", "Daily Mail", "heard her songs; he followed the fishermen and captured the mermaid.", "action-reaction", "Metropolitan Statistical Areas", "orogenic wedges", "eleven", "Grissom, White, and Chaffee", "Amtrak San Joaquins", "two", "1", "Barbara Walters", "temperate", "girls", "by citizens", "by up to 3 pence in the pound", "Lessing", "the courts of member states and the Court of Justice of the European Union", "1887", "new laws or amendments to existing laws", "487", "the Presiding Officer", "expansion", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "in the axial skeleton", "Thebes", "moral", "The Maidstone Studios in Maidstone, Kent", "Russian name for several breeds of dogs similar to the husky", "National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1986", "Charles Darwin", "the Germanic god Wodan, who was associated with the pagan midwinter event of Yule and led the Wild Hunt, a ghostly procession through the sky.", "in Poems : Series 1", "1927", "merengue", "lamina dura", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "Proposition 103", "1997 and the Middle East in 2000", "at the intersection of Mud Mountain Road and Highway 410", "United States", "Tom Brady", "2017", "Duck", "three levels", "Sylvester Stallone", "every 23 hours", "never made", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "1963", "costume party", "altitude", "1 point", "sow", "in feces or vomit", "John Joseph Travolta", "Allies of World War I", "U.S. 93", "have a smile on her face when her kids were around.", "Danny Williams", "Utah"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6599670203699809}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428572, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 0.631578947368421, 1.0, 1.0, 0.09523809523809523, 0.3076923076923077, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.2857142857142857, 0.13333333333333333, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5937", "mrqa_squad-validation-805", "mrqa_squad-validation-3922", "mrqa_squad-validation-9641", "mrqa_squad-validation-2404", "mrqa_squad-validation-9452", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-1173", "mrqa_triviaqa-validation-2329", "mrqa_hotpotqa-validation-3886", "mrqa_newsqa-validation-4179", "mrqa_searchqa-validation-10449"], "SR": 0.515625, "CSR": 0.6390625, "EFR": 0.9354838709677419, "Overall": 0.7872731854838709}, {"timecode": 10, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1735", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1877", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3994", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6984", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-4000", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-69", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16779", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2470", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-350", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-9187", "mrqa_squad-validation-10015", "mrqa_squad-validation-10052", "mrqa_squad-validation-10068", "mrqa_squad-validation-1008", "mrqa_squad-validation-10083", "mrqa_squad-validation-10103", "mrqa_squad-validation-10107", "mrqa_squad-validation-10116", "mrqa_squad-validation-10125", "mrqa_squad-validation-10186", "mrqa_squad-validation-10210", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-1030", "mrqa_squad-validation-10308", "mrqa_squad-validation-10333", "mrqa_squad-validation-10333", "mrqa_squad-validation-10344", "mrqa_squad-validation-10367", "mrqa_squad-validation-10374", "mrqa_squad-validation-104", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10466", "mrqa_squad-validation-10493", "mrqa_squad-validation-1051", "mrqa_squad-validation-1052", "mrqa_squad-validation-1068", "mrqa_squad-validation-1113", "mrqa_squad-validation-116", "mrqa_squad-validation-1165", "mrqa_squad-validation-1178", "mrqa_squad-validation-1188", "mrqa_squad-validation-1193", "mrqa_squad-validation-1200", "mrqa_squad-validation-1207", "mrqa_squad-validation-1211", "mrqa_squad-validation-1257", "mrqa_squad-validation-1269", "mrqa_squad-validation-1279", "mrqa_squad-validation-131", "mrqa_squad-validation-1330", "mrqa_squad-validation-1348", "mrqa_squad-validation-1368", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1509", "mrqa_squad-validation-1527", "mrqa_squad-validation-1536", "mrqa_squad-validation-1541", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1714", "mrqa_squad-validation-1769", "mrqa_squad-validation-1802", "mrqa_squad-validation-1891", "mrqa_squad-validation-1947", "mrqa_squad-validation-1967", "mrqa_squad-validation-2030", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2166", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-2297", "mrqa_squad-validation-2331", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2405", "mrqa_squad-validation-2409", "mrqa_squad-validation-2438", "mrqa_squad-validation-25", "mrqa_squad-validation-2554", "mrqa_squad-validation-2559", "mrqa_squad-validation-2564", "mrqa_squad-validation-2567", "mrqa_squad-validation-2576", "mrqa_squad-validation-2579", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2717", "mrqa_squad-validation-2778", "mrqa_squad-validation-2822", "mrqa_squad-validation-2827", "mrqa_squad-validation-2870", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-3050", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-313", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3261", "mrqa_squad-validation-3269", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3388", "mrqa_squad-validation-3445", "mrqa_squad-validation-3492", "mrqa_squad-validation-3603", "mrqa_squad-validation-3617", "mrqa_squad-validation-365", "mrqa_squad-validation-3699", "mrqa_squad-validation-3759", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3815", "mrqa_squad-validation-3833", "mrqa_squad-validation-3837", "mrqa_squad-validation-3844", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3922", "mrqa_squad-validation-3938", "mrqa_squad-validation-3958", "mrqa_squad-validation-3976", "mrqa_squad-validation-4030", "mrqa_squad-validation-4086", "mrqa_squad-validation-4191", "mrqa_squad-validation-4231", "mrqa_squad-validation-4232", "mrqa_squad-validation-4248", "mrqa_squad-validation-4269", "mrqa_squad-validation-43", "mrqa_squad-validation-4419", "mrqa_squad-validation-4480", "mrqa_squad-validation-4491", "mrqa_squad-validation-4560", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4746", "mrqa_squad-validation-475", "mrqa_squad-validation-4765", "mrqa_squad-validation-4836", "mrqa_squad-validation-4847", "mrqa_squad-validation-4896", "mrqa_squad-validation-4935", "mrqa_squad-validation-5009", "mrqa_squad-validation-5075", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5164", "mrqa_squad-validation-5180", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5221", "mrqa_squad-validation-5272", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5357", "mrqa_squad-validation-5363", "mrqa_squad-validation-5424", "mrqa_squad-validation-5451", "mrqa_squad-validation-5455", "mrqa_squad-validation-5471", "mrqa_squad-validation-5505", "mrqa_squad-validation-5519", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5541", "mrqa_squad-validation-5616", "mrqa_squad-validation-5651", "mrqa_squad-validation-5670", "mrqa_squad-validation-5774", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-583", "mrqa_squad-validation-5840", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-5877", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5908", "mrqa_squad-validation-5937", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-5971", "mrqa_squad-validation-5976", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6048", "mrqa_squad-validation-6083", "mrqa_squad-validation-6098", "mrqa_squad-validation-6098", "mrqa_squad-validation-6128", "mrqa_squad-validation-6158", "mrqa_squad-validation-618", "mrqa_squad-validation-6238", "mrqa_squad-validation-6294", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6381", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6506", "mrqa_squad-validation-6527", "mrqa_squad-validation-6530", "mrqa_squad-validation-6569", "mrqa_squad-validation-6580", "mrqa_squad-validation-6605", "mrqa_squad-validation-6670", "mrqa_squad-validation-6681", "mrqa_squad-validation-6707", "mrqa_squad-validation-6754", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-69", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-6996", "mrqa_squad-validation-7002", "mrqa_squad-validation-7020", "mrqa_squad-validation-7022", "mrqa_squad-validation-7034", "mrqa_squad-validation-7080", "mrqa_squad-validation-7083", "mrqa_squad-validation-7092", "mrqa_squad-validation-7094", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7303", "mrqa_squad-validation-7304", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7420", "mrqa_squad-validation-7476", "mrqa_squad-validation-7502", "mrqa_squad-validation-7614", "mrqa_squad-validation-7687", "mrqa_squad-validation-7690", "mrqa_squad-validation-7704", "mrqa_squad-validation-775", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-7886", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7981", "mrqa_squad-validation-805", "mrqa_squad-validation-8052", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8197", "mrqa_squad-validation-8247", "mrqa_squad-validation-829", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8364", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8416", "mrqa_squad-validation-8479", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8526", "mrqa_squad-validation-8546", "mrqa_squad-validation-8580", "mrqa_squad-validation-8600", "mrqa_squad-validation-863", "mrqa_squad-validation-8680", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8777", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8953", "mrqa_squad-validation-8957", "mrqa_squad-validation-8965", "mrqa_squad-validation-9002", "mrqa_squad-validation-9012", "mrqa_squad-validation-902", "mrqa_squad-validation-9023", "mrqa_squad-validation-9024", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9136", "mrqa_squad-validation-9141", "mrqa_squad-validation-9208", "mrqa_squad-validation-9254", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9337", "mrqa_squad-validation-9411", "mrqa_squad-validation-9430", "mrqa_squad-validation-9452", "mrqa_squad-validation-9457", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9527", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9614", "mrqa_squad-validation-9615", "mrqa_squad-validation-9624", "mrqa_squad-validation-9635", "mrqa_squad-validation-9641", "mrqa_squad-validation-9665", "mrqa_squad-validation-9718", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_squad-validation-9845", "mrqa_squad-validation-985", "mrqa_squad-validation-9926", "mrqa_squad-validation-9940", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.900390625, "KG": 0.4046875, "before_eval_results": {"predictions": ["the Miller\u2013Urey experiment", "five", "Mughal emperors", "a series of strikes by coal miners and railroad workers over the winter of 1973\u201374 became a major factor in the change of government.", "\" Informal\" and \"informal\" imperialism", "HO", "increased settlement and deforestation", "Afranji, meaning \"Franks.\"", "Abercynon in south Wales", "drinking water", "permafrost", "10 to 15 million people", "Cabot Science Library, Lamont Library, and Widener Library", "the Henry Cole wing", "the mayor (the President of Warsaw), who may sign them into law. If the mayor vetoes a bill, the Council has 30 days to override the veto by a two-thirds majority vote.", "The time and space hierarchy theorems", "The innate immune system", "Cow Counties", "11", "Royal Institute of British Architects", "the 6th century", "The owner", "United Parcel Service, Inc.", "Ernie", "Sapporo", "the Rosetta Stone", "m\u0101-jongg", "The Tonight Ensemble", "the right hand side of the second line of letters, the semi colon key is swapped for the M key.", "mikado", "William Boyd", "Hilary Mantel's Wolf Hall", "Sports Stack Exchange", "the gums", "the Irishman, born in Kilkenny", "Richmond, Va.", "Wawrinka", "Humphrey Bogart", "Forrest Gump", "\" animal", "Auric Goldfinger", "The Poseidon Adventure", "five", "Mary Poppins", "Neil Armstrong", "the centre bull", "the Metropolitan Borough of Oldham, in Greater Manchester, England", "British Defence Secretary", "Old Ironsides", "Bullnose", "blue", "the skull", "mhoiz", "gold", "Brainy", "New Zealand", "Paige O'Hara", "Chris Martin", "Ishtar Gate", "neo-Nazi", "several weeks", "an animal tranquilizer, can put users in a dazed stupor for about two hours, doctors said.", "Robin Givens", "the mouth"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5974338909414341}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6206896551724138, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3722", "mrqa_squad-validation-9808", "mrqa_squad-validation-1134", "mrqa_squad-validation-6223", "mrqa_squad-validation-962", "mrqa_squad-validation-944", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5918", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-1905", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-865", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-1579", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-515", "mrqa_newsqa-validation-265", "mrqa_searchqa-validation-2154"], "SR": 0.53125, "CSR": 0.6292613636363636, "EFR": 1.0, "Overall": 0.7388210227272728}, {"timecode": 11, "before_eval_results": {"predictions": ["\"Old Briton\"", "Tiffany & Co.", "1985", "y. p. orientalis", "4k + 3", "Samuel Reshevsky", "mujahideen Muslim Afghanistan", "July 1977", "coal", "programmes", "Stanford Stadium", "friction", "his work was published first", "monatomic", "MetroCentre", "SAP Center", "composite", "the courts of member states", "German-language publications", "27", "6", "summer", "groin vault", "Konakuppakatil Gopinathan Balakrishnan", "Burj Khalifa", "March 14, 1942", "book and architecture", "Bart Howard", "along with a cover slip or cover glass", "nihonium", "Britney Spears", "2005", "1612", "February 29", "transmission", "216", "Dr. Addison Montgomery", "old English pyrige", "food and clothing", "Kansas City Chiefs", "the Indians", "Jennifer Parker", "fascia surrounding skeletal muscle", "the amount of surface", "October 2004", "Babe Ruth", "george", "Jaydev Shah", "Jodie Foster", "13,000 astronomical units", "1978", "wisdom", "Thomas Chisholm", "Rocinante", "before the first letter of an interrogative sentence", "Wikia", "The Daily Mirror", "an invoice, bill or tab", "41 kilometres north-west of the Sydney central business district in the local government area of City of Blacktown", "1,500", "the underprivileged", "george onegin", "george", "The Time Machine"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6193576388888888}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10142", "mrqa_squad-validation-4963", "mrqa_squad-validation-9024", "mrqa_squad-validation-3947", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-4549", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-3602", "mrqa_triviaqa-validation-412", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-2913", "mrqa_newsqa-validation-3686", "mrqa_searchqa-validation-14088", "mrqa_searchqa-validation-2383"], "SR": 0.546875, "CSR": 0.6223958333333333, "EFR": 0.9655172413793104, "Overall": 0.7305513649425287}, {"timecode": 12, "before_eval_results": {"predictions": ["2014", "Muslim state", "Frederick William", "herbal remedies", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "96.26%", "19th", "Raoul Pierre Pictet", "$45,000", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "n < p < 2n \u2212 2", "illegal acts", "A", "European Court of Justice", "chlorophyll a and phycobilins", "German-language publications", "1992", "25 percent", "in the stems and roots of certain vascular plants", "Valens", "1976", "was an incident on March 5, 1770, in which British Army soldiers shot and killed people while under attack by a mob", "Judiththia Aline Keppel", "Ted '' Levine", "gregorito", "noble gas", "an object that moves around an external axis", "Jaydev Shah", "Polly Walker", "Raya Yarbrough", "Gilbert Grape", "2014", "the evolution of light hair", "March 27, 2017", "a truck owned by his brother", "28 July 1914", "Jean F Kernel", "Haiti", "1948", "Sauron", "Chandan Shetty", "Bill Henderson", "Liam Cunningham", "Nancy Jean Cartwright", "Max", "Jonathan Goldstein", "a premalignant flat ( or sessile ) lesion", "on the medulla oblongata", "Humpty Dumpty and Kitty Softpaws", "Ren\u00e9 Descartes", "Clarence Anglin", "Jason Lee", "October 1941", "2013", "Antarctica", "Bath and Wells", "John Nash", "Alistair Grant", "1989", "nearly $2 billion", "Tutsis the privileged ethnicity, thus giving them better opportunities.", "gregoria", "Brownsville", "Ralph Lauren"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6693070818070819}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.11111111111111112, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.07692307692307691, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.6, 0.4, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8206", "mrqa_squad-validation-8412", "mrqa_squad-validation-6655", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3611", "mrqa_triviaqa-validation-6797", "mrqa_hotpotqa-validation-2319", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-3744"], "SR": 0.5625, "CSR": 0.6177884615384616, "EFR": 1.0, "Overall": 0.7365264423076924}, {"timecode": 13, "before_eval_results": {"predictions": ["in the early 1990s", "the 17th century", "Four thousand", "NP complete problems", "the seal of the Federal Communications Commission", "landlords found new residents willing to pay higher market rate for housing", "a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "1206", "2011", "in his lab and elsewhere", "their bright colors sometimes override the chlorophyll green", "Radio Corporation of America (RCA)", "up to three-fourths of the population of the Iranian Plateau", "partial funding", "Open Door Policy", "1538", "a surgical resident at Seattle Grace Hospital", "a minimum adequate diet", "Tony Almeida", "Herbert Hoover", "Wenceslas", "coal", "Union Pacific & the Central Pacific", "gyp Hill Premiere", "the skulls of \"Peking Man,\" or Homo erectus,", "Latin", "War of the Worlds", "Luxor Governorate", "to keep your nose to the grindstone", "the Osmonds", "a stuffed animal", "butterflies", "an oval shape, either almost touching the hips, or at navel level, or raised above the dancer's head", "'Star Wars: Han Solo' Comic Book Series", "Calypso", "Richard I", "A Million Little pieces", "\"Give me liberty, or give me death!", "to bring comfort to people.", "the Billy Goats Gruff", "palindrome", "bacon strips", "Saturn", "the Urals", "Amsterdam", "Loon Mountain", "the Valley Isle", "Richard Nixon", "Little Mermaid", "an offensive formation with two tight ends,", "George Carlin", "Che Guevara", "a raven", "Kurdish", "Nicholas Sparks", "Lizzy Greene", "Bart\u00f3k", "Spike McPike", "English Electric Canberra", "the era of Texan history between 1821 and 1836, when it was part of Mexico.", "Krishna Rajaram", "the Lindsey oil refinery in eastern England.", "a month of training", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah, overloading that facility."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6210776458019105}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false], "QA-F1": [0.8, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.4444444444444444, 0.0, 0.1904761904761905]}}, "before_error_ids": ["mrqa_squad-validation-4094", "mrqa_squad-validation-1804", "mrqa_squad-validation-7578", "mrqa_squad-validation-9575", "mrqa_squad-validation-8229", "mrqa_squad-validation-6250", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-16654", "mrqa_searchqa-validation-5858", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-6353", "mrqa_searchqa-validation-9558", "mrqa_triviaqa-validation-5059", "mrqa_hotpotqa-validation-1876", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-909"], "SR": 0.515625, "CSR": 0.6104910714285714, "EFR": 1.0, "Overall": 0.7350669642857143}, {"timecode": 14, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "J. S. Bach", "Denver", "from January 1964, until it achieved the first manned landing in July 1969,", "unicellular organisms", "The Quasiturbine", "Dave Logan", "Roger NFL", "a cascade method", "December 2014", "Ladner", "the Ming dynasty", "Newton", "15,100", "Ronnie Hillman", "Masha Skorobogatov", "Daniel A. Dailey", "on the urinary floor", "31 December 1600", "National Park Service's Shenandoah National Park", "1945", "Milira", "in vitro", "The Walking Dead", "ceramists or potters", "State Bar of Arizona", "Tbilisi", "a candidate state", "Yondu Udonta", "Sanskrit text Natya Shastra", "Rocinante", "one", "John Roberts", "Ray Charles", "British and French Canadian fur traders", "routing table", "the source of the donor organ", "Bobby Darin", "a 420A motor with an upgrade to a T3 turbo and front mount intercooler", "Lana Del Rey", "Kim Basinger", "smoke detector", "4,840", "Dasharatha", "September 19", "President pro tempore", "February 16, 2016", "Wisconsin", "Merry Clayton", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "Florida", "a salvation story", "Nepal", "Austria - Hungary", "Word Options", "Australia", "June 2, 2008", "Reverend Lovejoy", "Arthur E. Morgan III", "a Coptic family", "Australia", "Oxfam", "a German immigrant", "Chief Oshkosh"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5720486111111112}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-32", "mrqa_squad-validation-6453", "mrqa_squad-validation-690", "mrqa_squad-validation-80", "mrqa_squad-validation-8062", "mrqa_squad-validation-362", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-9517", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-1161", "mrqa_triviaqa-validation-5650", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-913", "mrqa_searchqa-validation-9115"], "SR": 0.46875, "CSR": 0.6010416666666667, "EFR": 1.0, "Overall": 0.7331770833333333}, {"timecode": 15, "before_eval_results": {"predictions": ["chest pains", "thermodynamic", "The Lone Ranger", "broken arm", "Arthur Woolf", "a noble man who respected people's dignity and lives", "Manning", "Warren Buffett", "Malkin Athletic Center", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "planktonic", "Lucas Horenbout", "a 3\u20130 lead", "Spektor", "writ of certiorari", "the right of the dinner plate", "Joanne Wheatley", "flawed democracy", "Southwest Florida International Airport ( RSW )", "In 1987", "The temporal lobes", "Missi Hale", "their son", "a Norwegian town circa 1879", "the Dutch United Provinces", "Television demonstrations", "57 days", "1937", "KU", "the gastrocnemius", "great king", "Christmas Day", "1961", "1 atm pressure", "Qutab Minar", "Isaiah Amir Mustafa", "dry lake beds northeast of Los Angeles", "The photoelectric ( optical ) smoke detector", "New York University", "the Carnaval de Qu\u00e9bec", "the ball is fed into the gap between the two forward packs", "microscope's stage", "9 February 2018", "31 October 1972", "Jesse McCartney", "4.25 inches", "Lady Gaga", "Director of National Intelligence", "April 1979", "Mainland Greece", "no license or advanced training beyond just firearm familiarization ( for rentals )", "the Royal Air Force ( RAF )", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "a single peptide bond or one amino acid with two peptide bonds", "5,874 miles", "Alberich", "Katharine Juliet Ross", "\"The Brothers\"", "about 100 light bulbs", "58 people", "the ulna", "\"to compare\"", "Monday night", "a fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5538779958991097}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.6976744186046512, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-3190", "mrqa_squad-validation-845", "mrqa_squad-validation-9399", "mrqa_squad-validation-4636", "mrqa_squad-validation-800", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-4064", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-2131", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-1447", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-2439", "mrqa_searchqa-validation-7968", "mrqa_searchqa-validation-10092", "mrqa_newsqa-validation-85"], "SR": 0.421875, "CSR": 0.58984375, "EFR": 0.9459459459459459, "Overall": 0.7201266891891892}, {"timecode": 16, "before_eval_results": {"predictions": ["427", "Michael Mullett", "impact process effects", "since 2001", "double or triple non-French linguistic origins", "German", "a course of study, lesson plan, or a practical skill", "Super Bowl XXXIII", "a protest that blocked heavy traffic passing over the A13, Brenner Autobahn, en route to Italy.", "unequal", "charging their students tuition fees", "every four years", "John Gatsby", "the swallows", "crawdads", "Lewis", "a parabhu", "\"Raging Bull\"", "Logan International Airport", "a drink of water", "etching", "Canada", "an asylum", "4.0", "Judy Garland", "Yod", "airplanes", "Detroit", "bay", "John", "Puccini", "a parodramatic songs.", "carbon dioxide", "San Francisco", "the Venus landing", "Saturn", "nickel", "the ocean", "Lake Baikal", "Brazil", "the L'Abime bridge", "Laos", "Chang Apana", "the Jeckle", "Erma Bombeck", "Clinton", "Jio's HoF", "John the Baptist", "Iran", "a serve", "Touch of Evil", "Billy Idol", "Nightingale", "the double man", "Daryl Sabara", "holiday", "William Shakespeare", "Nick Berry", "a split 7\"", "North Dakota", "Turkey", "sumo wrestling", "more than 26,000", "Fat Man"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5492703111946533}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5263157894736842, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4267", "mrqa_squad-validation-4065", "mrqa_squad-validation-1844", "mrqa_squad-validation-4332", "mrqa_squad-validation-6983", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-16782", "mrqa_searchqa-validation-3214", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-9973", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-15522", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-519", "mrqa_naturalquestions-validation-94", "mrqa_hotpotqa-validation-1701", "mrqa_newsqa-validation-1120", "mrqa_hotpotqa-validation-5388"], "SR": 0.484375, "CSR": 0.5836397058823529, "EFR": 1.0, "Overall": 0.7296966911764706}, {"timecode": 17, "before_eval_results": {"predictions": ["forceful taking of property", "Robert Iger", "Central business districts", "the variety of occupations necessary to sustain the community as distinct from the indigenous population", "prime number theorem", "a nonphotosynthetic eukaryote engulfed a chloroplast-containing alga", "Santa Clara", "Trevathan", "soap opera General Hospital", "being drafted into the Austro-Hungarian Army in Smiljan", "an innate force of impetus", "a statue", "Titanic", "Queen Hatshepsut", "Heroes", "Prince of Denmarke", "\"It's a dog eat dog world, Woody, and I'm wearing Milk Bone underwear", "Sir Anthony Eden", "Defending Your Life", "a person", "Jalisco state", "Santa Fe", "the Space Coast Convention Center", "Muddy Waters", "the First Telegraphic Message", "endorsement requests", "Manfred von Richthofen", "cowboys", "Michael Collins", "John J. Pershing", "a balloon", "La Crosse", "the Jesuit", "Javier", "William O'Rourke", "an anatomical animation", "Susan Lahrman", "the University of Massachusetts Amherst", "60 beats per minute", "a poacher", "Lignite", "the heart", "rabbit", "raising livestock", "The Call of the Wild", "Vladimir Putin", "Hillary Clinton", "Nikola Tesla", "a gingerbread", "Elza", "a trestus", "a light-gathering mirror", "David Tyree", "Quantitative psychological research", "The federal government", "the Bahamas", "Doncaster Rovers", "the City of Onkaparinga", "A simple iron boar crest", "Monday night", "Immigration Minister Eric Besson", "needle - like teeth", "3", "relieve families who had difficulty finding jobs during the Great Depression in the United States"], "metric_results": {"EM": 0.421875, "QA-F1": 0.553587962962963}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.962962962962963]}}, "before_error_ids": ["mrqa_squad-validation-3106", "mrqa_squad-validation-8638", "mrqa_squad-validation-5938", "mrqa_squad-validation-1232", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-7483", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-8777", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-15293", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-9231", "mrqa_naturalquestions-validation-222", "mrqa_triviaqa-validation-2095", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-9856"], "SR": 0.421875, "CSR": 0.5746527777777778, "EFR": 1.0, "Overall": 0.7278993055555556}, {"timecode": 18, "before_eval_results": {"predictions": ["308", "Trajan's Column", "the solution", "the Danube", "conduct inquiries and scrutinise legislation", "Ralph Woodward", "major cities", "19th Century", "high density", "the Ten Commandments", "amyotrophic lateral sclerosis (ALS)", "Jefferson Memorial", "Magic Johnson", "between 7,500 and 40,000", "The Soloist", "2012 Olympic bronze medalist", "Odawa", "feats of exploration", "Abdul Razzak Yaqoob", "Norwegian", "1868", "James FitzJames, 1st Duke of Berwick", "Hopeless Records", "Lorne Michaels", "the National Basketball Association (NBA)", "Ang Lee", "Lake County, Illinois", "1971", "former Chelsea and Middlesbrough striker Jimmy Floyd Hasselbaink", "Arkansas", "2009", "The God of Small Things", "Sakura Uzumaki", "the Cumberland Plateau", "March 30, 2025", "Miller Brewing", "Mike Greenwell", "Daimler-Benz", "1994", "John of Gaunt", "classical", "1,521", "a palace", "one", "fantasy role-playing game", "Leofric", "Australian Electoral Division", "Argentinian", "the International Hotel", "American black bear", "Mot\u00f6rhead", "Richa Sharma", "An impresario", "Lake Michigan", "Tulsa", "homeless", "Joy Gruttmann", "Hungary", "five", "Maryland", "OK", "Ulysses S. Grant", "\"The Maracot Deep\"", "kidnapping"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6632947781385281}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.18181818181818182, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4297", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2241", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-4001", "mrqa_newsqa-validation-4077", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-10375"], "SR": 0.578125, "CSR": 0.5748355263157895, "EFR": 1.0, "Overall": 0.7279358552631578}, {"timecode": 19, "before_eval_results": {"predictions": ["toward the end of his life", "motivated students", "native tribes", "Cuba", "Dai \u00d6n Ulus, also rendered as Ikh Yuan \u00dcls or Yekhe Yuan Ulus", "Parliamentary time", "Private Bill Committees", "photolysis of ozone", "Eliot Ness", "Joseph Stalin", "Bobby Eli", "the Eurasian Plate", "Anglican", "10.5 %", "living - donor", "the South Pacific Ocean", "silk floss", "Tom Waits", "Cherbourg in France", "Olympic - class ocean liners", "T.J. Miller", "scoring", "1995", "Paradise, Nevada", "Mike Nesmith", "the country club pool", "Office of Inspector General", "Skat", "1878", "Michael Jackson", "Scott Bakula", "October 2008", "The Parable of the Prodigal Son ( also known as the Two Brothers, Lost Son, Loving Father, or Lovesick Father )", "the theory of T\u0101\u1e47\u1e0dava dance ( Shiva )", "Heather Stebbins", "Spanish surname", "supervillains who pose catastrophic challenges to the world", "1999", "Geraldine Margaret Agnew - Somerville", "Joe Lawrence", "October 2, 2017", "Stephen Curry of Davidson", "December 15, 2016", "a star", "heavy tank", "The Stanley Hotel", "Marty Robbins", "Albert Einstein", "early 2017", "Ed Sheeran", "state ownership of the means of production", "HTTP / 1.1", "A substitute", "Midsomer Murders", "Hawaii", "Hopeless Records", "death", "Seasons of My Heart", "Linda Hogan", "To Build a Fire", "an Emergency Tracheotomy", "business", "Vancouver", "Hudson River"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5638174019607843}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-3628", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-6584", "mrqa_hotpotqa-validation-4897", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-10347"], "SR": 0.46875, "CSR": 0.56953125, "EFR": 1.0, "Overall": 0.7268749999999999}, {"timecode": 20, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2467", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2025", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-3628", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-16723", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6061", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-6969", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_squad-validation-10052", "mrqa_squad-validation-10107", "mrqa_squad-validation-10125", "mrqa_squad-validation-10149", "mrqa_squad-validation-10186", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10333", "mrqa_squad-validation-10341", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10445", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-116", "mrqa_squad-validation-1193", "mrqa_squad-validation-1257", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1684", "mrqa_squad-validation-1754", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2092", "mrqa_squad-validation-2166", "mrqa_squad-validation-2288", "mrqa_squad-validation-2302", "mrqa_squad-validation-232", "mrqa_squad-validation-2322", "mrqa_squad-validation-2324", "mrqa_squad-validation-2344", "mrqa_squad-validation-2406", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2559", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2737", "mrqa_squad-validation-2778", "mrqa_squad-validation-2827", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-32", "mrqa_squad-validation-3217", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3506", "mrqa_squad-validation-3617", "mrqa_squad-validation-362", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3923", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-401", "mrqa_squad-validation-4086", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4248", "mrqa_squad-validation-4287", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4836", "mrqa_squad-validation-4974", "mrqa_squad-validation-5012", "mrqa_squad-validation-5088", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5379", "mrqa_squad-validation-5451", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5950", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6069", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6250", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6605", "mrqa_squad-validation-6671", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6843", "mrqa_squad-validation-6846", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7476", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-789", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-800", "mrqa_squad-validation-805", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8193", "mrqa_squad-validation-8197", "mrqa_squad-validation-8307", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-845", "mrqa_squad-validation-852", "mrqa_squad-validation-8580", "mrqa_squad-validation-8696", "mrqa_squad-validation-8771", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8798", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8935", "mrqa_squad-validation-8953", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9141", "mrqa_squad-validation-9254", "mrqa_squad-validation-9270", "mrqa_squad-validation-929", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9457", "mrqa_squad-validation-9479", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9630", "mrqa_squad-validation-964", "mrqa_squad-validation-9718", "mrqa_squad-validation-9766", "mrqa_squad-validation-9768", "mrqa_squad-validation-985", "mrqa_squad-validation-9968", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1751", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.873046875, "KG": 0.45, "before_eval_results": {"predictions": ["Museum of the Moving Image in London", "Jean Fran\u00e7ois de Troy", "the Privy Council", "During the Second World War", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "an assembly center", "Leonardo da Vinci", "end of 1350", "two Nobel Peace Prizes", "as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "June 12, 2017", "Prussian", "London Heathrow", "27 November 1956", "Dan Conner", "supernatural psychological horror film", "Billy Joel", "The Times Higher Education Guide", "The conversation", "Al D'Amato", "nuclear weapons", "Lush Ltd.", "Port Macquarie", "non-alcoholic recipe", "Big & Rich", "Brea, California", "World War II", "Brazilian Jiu-Jitsu", "Biola University", "Revolution Studios", "February 22, 1968", "Erreway", "1997", "Kim Bauer", "the Joint Chiefs of Staff", "Iron & Wine", "1955", "1993", "near Philip Billard Municipal Airport", "Rank Organisation", "John Boyd Dunlop", "Iran", "The Cherokee\u2013American wars", "a creek", "Edward Trowbridge Collins Sr.", "Province of New York", "Archie Andrews", "January 28, 2016", "NBA Finals Most Valuable Player Award", "Steve Martin", "Israeli Declaration of Independence in 1948", "Sleepy Hollow", "Mumbai", "1996", "Ithaca", "The National Council for the Unmarried Mother and her Child", "Doubting Castle", "Noida", "genocide", "Frank Hamer", "Howard Hughes Jr.", "DQ", "AC/DC", "the Iberian Peninsula"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7013888888888888}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7674", "mrqa_squad-validation-5489", "mrqa_squad-validation-8189", "mrqa_squad-validation-5061", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2669", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-5727", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-5418", "mrqa_newsqa-validation-3563", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11847", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-12943"], "SR": 0.609375, "CSR": 0.5714285714285714, "EFR": 1.0, "Overall": 0.7249888392857142}, {"timecode": 21, "before_eval_results": {"predictions": ["Several thousand", "Paris", "Islamic Republic", "different subject specialists each session during the week", "Sonia Shankman Orthogenic School", "inverse proportionality of acceleration", "alternating current", "1002", "Guardians of the Galaxy Vol.  2", "1987", "arts manager", "software programmer", "11 (or virtual channel 6 via PSIP)", "their unusual behavior", "bassline", "Ireland", "Valley Falls", "U\u00ed \u00cdmair", "Ry\u016bkyuan people", "Dallas", "a leg injury", "Hirsch index rating", "neo-Nazi", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "2008", "Michael Crawford", "Isobel", "Nicholas \" Nick\" Offerman", "American", "the Dominican Republic", "29,000", "Wes Unseld", "Mr. Church", "1990", "Ronald Wilson Reagan", "Raymond Albert Romano", "The Dressmaker", "Kennedy Road", "30.9%", "Peter Yarrow", "\"From Here to Eternity\"", "the Cincinnati Reds", "\"The Royal Family\"", "Backstreet Boys", "Lynn Minmei", "Margaret Willoughby", "Derek Jacobi", "February 14, 1859", "Catwoman", "The Hindu Group", "Detroit Lions and the Los Angeles Rams", "BBC Formula One coverage", "1886", "Phillip Schofield and Christine Bleakley", "USS Chesapeake", "Jupiter", "a German supermodel", "CNN", "Somali", "Walking on Sunshine", "Joe Jackson", "Michael Arrington", "Krishna Rajaram", "at least 12 months"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6342261904761906}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1894", "mrqa_squad-validation-10424", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-4843", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2473", "mrqa_naturalquestions-validation-1786", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3190", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-6538", "mrqa_searchqa-validation-939", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-274"], "SR": 0.53125, "CSR": 0.5696022727272727, "EFR": 1.0, "Overall": 0.7246235795454545}, {"timecode": 22, "before_eval_results": {"predictions": ["1,230 kilometres (764 miles)", "anti-colonial movements", "The Middle and Modern Family", "An increase in imported cars", "immunomodulators", "inequality", "Duval County", "Univision", "Pieter van Musschenbroek", "\"Beauty and the Beast\"", "every aspect of public and private life", "Missouri Tigers", "Acela Express", "German", "Cartoon Network", "Aamir Khan", "trans-Pacific flight", "Kristina Ceyton and Kristian Moliere", "Ginger Rogers", "Dan Castellaneta", "from 1995 to 2012", "Saint Petersburg Conservatory", "Harlequin", "one", "Colonel", "Ars Nova Theater", "Donna Paige Helmintoller", "Detroit, Michigan", "CBS News", "1838", "Assistant Director Neil J. Welch", "near Philip Billard Municipal Airport", "ZZ Top", "2", "The Handmaid's Tale", "Rick and Morty", "second largest", "Band-e Amir National Park", "Melbourne", "13 May 1619", "247,597", "jurisdiction", "Yoruba", "Madonna Louise Ciccone", "Telugu", "1800000 sqft", "Malayalam cinema", "New York State Route 907E", "\"The Remains of the Day\"", "\"former child actor\"", "October 21, 2016", "Taeko Ikeda", "Claire Rhiannon Holt", "acronym", "Thomas Chisholm", "Brazil", "Superman", "Wales", "since 1983", "three", "threatening messages", "New Zealand", "\"Hey Ya!\"", "SS Mayaguez"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7896949404761905}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9302", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-1384", "mrqa_triviaqa-validation-5792", "mrqa_newsqa-validation-377", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-6136"], "SR": 0.703125, "CSR": 0.5754076086956521, "EFR": 1.0, "Overall": 0.7257846467391305}, {"timecode": 23, "before_eval_results": {"predictions": ["28.5\u00b0E", "Robert Guiscard, a Hauteville, and his younger brother Roger the Great Count", "seven", "BBC Dead Ringers series", "Mnemiopsis", "Dr. George E. Mueller", "salmon", "chile", "heating", "Gerald Robert Newhart", "Constellations", "Gerald Gerald", "Gerald Gerald", "Gerald Dostoyevsky", "The Plaza Hotel", "Stephen Hawking", "Rodeo", "Hawaii", "a Gerald Gerald Dictionary", "Gerald Coleman", "Gerald Sharpe", "an M1 Abrams' main gun", "Earth", "a bottle", "Gerald IV", "Lon Foucault", "Gerald Schwarzenegger", "Gerald E. Neugebauer", "College of William and Mary", "Who's Afraid of Virginia Woolf", "Gerald", "jamais", "Barnard College", "Caracalla", "Gerald Goodstein", "Gerald", "Delaware", "a stone", "George W. Bush", "a rat", "James Cook", "Bosom Buddies", "Gerald Cantor Foundation", "Gerald Retriever", "Walter Crawford Kelly, Jr.", "Edith Wharton", "Rapa Nui National Park", "Nike+ iPod", "gravity", "an oblate spheroid.", "Gerald Diamond", "a Gerald's Chemistry Set", "Bay of Montevideo", "Peach State", "1820s", "Gerald ibn Ezra", "QWERTY keyboard", "Gerald", "Las Vegas Boulevard", "Slaughterhouse-Five", "Food and Agriculture Organization", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "an enormous step forward in Dana Gas' strategy across the Middle East, North Africa and South Asia.", "2050"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4749255952380952}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1045", "mrqa_squad-validation-7993", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-4758", "mrqa_searchqa-validation-15407", "mrqa_searchqa-validation-5554", "mrqa_searchqa-validation-12149", "mrqa_searchqa-validation-13231", "mrqa_searchqa-validation-9639", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-14893", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-4374", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-7572", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-15964", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-3087", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-1639"], "SR": 0.359375, "CSR": 0.56640625, "EFR": 1.0, "Overall": 0.723984375}, {"timecode": 24, "before_eval_results": {"predictions": ["2 million", "Catholic", "waxy cuticle", "theta intermediary form", "subsequent long-run economic growth", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law,\"", "two-state solution", "WFTV", "\"an Afghan patriot\"", "Juliet", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "eight", "World Wide Village", "American", "Indian Ocean", "at least 25 dead", "18", "Sunday", "a rally at the State House next week", "Herman Cain", "humans", "Friday", "Saluhallen", "murder", "the Swat Valley", "two years,", "228", "Keating Holland", "serving its fast burgers in the Carrousel du Louvre, an underground shopping mall which lies under the main entrance of the museum and which still contains an ancient wall that was discovered during construction works.", "Australian Open", "iTunes", "Nazi Germany", "\"I loved the convenience [of the train],'", "30 years ago", "five", "July", "\"Oprah: A Biography,\"", "100 percent", "schools", "militant group Abu Sayyaf,", "The Palm Jumeirah", "there's no chance of it being open on time.", "Egypt", "Jeddah, Saudi Arabia", "poor families", "Russia", "2011", "use of torture and indefinite detention", "skull", "Iran", "a review of state government practices completed in 100 days.\"", "stripper pole photos", "Luigi Fagioli", "the governor of West Virginia", "Massachusetts", "phi", "the natural world", "The Lion King", "7pm", "1966", "Wendell Erdman Berry", "Twelfth Night", "Shabbat", "yellow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5928489547732969}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.10526315789473685, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.4210526315789474, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6435", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1419", "mrqa_naturalquestions-validation-6952", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-173", "mrqa_searchqa-validation-2076"], "SR": 0.515625, "CSR": 0.5643750000000001, "EFR": 0.967741935483871, "Overall": 0.7171265120967742}, {"timecode": 25, "before_eval_results": {"predictions": ["primes", "UNESCO's World Heritage list", "lower incomes,", "Wankel engine", "five or more", "China and Japan.", "Peshawar", "backbreaking labor,", "Oregon", "Democrats and Republicans are saying Meehan shouldn't be using a 9/11 image to make a political point.", "Fullerton, California,", "an unprecedented wave of buying amid the elections.", "The minister later apologized, telling CNN his comments had been taken out of context.", "Carl Froch", "10 below in Chicago, Illlinois.", "considers the talk-show host heaven-sent,", "system must recommend a national policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies.", "a rifle and began firing.", "The pilot, whose name has not yet been released,", "a humanitarian mission.", "U.S. troops working in support of Iraqi soldiers killed two snipers, two other men firing rocket-propelled grenades and \"multiple others from a nearby building where soldiers were taking RPG and machine gun fire,\"", "air support.", "Marie-Therese Walter.", "neither Sudanese nor orphans,", "\"it should stay that way.\"", "housing, business and infrastructure repairs,", "Swansea", "U.S. Court of Appeals for the District of Columbia.", "Michael Jackson", "Kurdistan Freedom Falcons,", "insurgent small arms fire.", "quality of teaching and learning in American schools", "a lump in Henry's nether regions", "Doral", "glamour and hedonism", "\"I am sick of life", "a peace sign.", "the Southeast,", "Diego Milito", "a series of bear attacks in recent months in the United States.", "The FARC", "Florida", "Christopher Savoie", "rebels", "three Ghanaians, two Liberians and a Togo national", "\"How I Met Your Mother,\"", "the British capital's other two airports, Stansted and Gatwick,", "Mafia", "Stratfor", "The BBC", "CBS, CNN, Fox and The Associated Press.", "Ashley \"A.J.\" Jewell,", "The stratum lucidum", "Gettysburg College", "John Cabot", "Celsius", "75", "supreme religious leader of all the subordinate priests", "Charles Quinton Murphy", "Rio Gavin Ferdinand", "more than 110", "Athens", "The subjects for his columns come from the popular \"On language\" column in", "artesian"], "metric_results": {"EM": 0.359375, "QA-F1": 0.48230517695131664}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.23529411764705882, 0.5, 0.0, 1.0, 0.0, 0.2666666666666667, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.36363636363636365, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.5, 0.0, 1.0, 0.3333333333333333, 0.8, 0.15384615384615385, 0.0, 0.0, 0.375, 0.28571428571428575, 1.0, 1.0, 0.5, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-561", "mrqa_naturalquestions-validation-8585", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-1390", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-4004", "mrqa_searchqa-validation-1916", "mrqa_searchqa-validation-16961"], "SR": 0.359375, "CSR": 0.5564903846153846, "EFR": 1.0, "Overall": 0.7220012019230769}, {"timecode": 26, "before_eval_results": {"predictions": ["enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible.", "Katy\u0144 Museum", "two", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship.", "Martin O'Neill", "1822", "Stephanie Plum", "Dominican", "Evgeni Platov", "Knoxville, Tennessee", "Enkare Nairobi", "Field Marshal Stapleton Cotton,", "Shari Shattuck", "Si Da Ming Bu", "The Blue Album", "Apsley George Benet Cherry-Garrard", "A basilica", "\"the ultimate guide to music and more\"", "The Odawa", "Columbus Crew SC", "Yoo Seung-ho", "1989 until 1994", "\"To Know Him Is to Love Him\"", "Wildhorn", "pornographicstar", "Montreal", "Domingo \"Sam\" Samudio", "John Nicholas Galleher", "San Francisco 49ers", "Prince of Cambodia Norodom Sihanouk", "Duke University", "Double Crossed", "1963", "Donald Sutherland", "provides its services in the Japanese market.", "Security Management", "14,372", "musician", "Cersei Jaimeister", "Fort Worth", "Dutch", "shorthand writing", "Operation Gladio", "\"The Young Ones\"", "Tabasco", "Sunday, November 2, 2003", "1853", "1.5 million", "Neymar", "Plymouth Regional High School", "\"Tainted Love\"", "Dissection", "Cecil Lockhart", "MercyMe", "Five years later", "a ride cymbal", "Al Jazeera", "Anabaptists and the non-sectarians", "38 feet", "ties to paramilitary groups,", "Shenzhen", "Brigham Young", "\"Jeopardy\"", "GNMA"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5744887672811061}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6426", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-34", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3889", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-2131", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-5740", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-3954", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-8085", "mrqa_searchqa-validation-10970"], "SR": 0.46875, "CSR": 0.5532407407407407, "EFR": 1.0, "Overall": 0.7213512731481482}, {"timecode": 27, "before_eval_results": {"predictions": ["In low-light conditions", "1550", "60 days", "$60,000 in cash and stock", "four", "Steve Goodman", "Jonathan Goldstein", "Cristeta Comerford", "Secretary of Homeland Security", "1996", "Tristan Rogers", "Mase Dinehart", "counter clockwise direction", "Paul Hogan", "tennis", "Matt Monro", "subtractive notation", "Fa Ze Rug", "8ft", "Robert Irsay", "to manage the characteristics of the beer's head", "a moral tale", "2019", "Niall Matter", "Arctic Ocean", "earliest known official or large - scale celebration of Pi Day was organized by Larry Shaw at the San Francisco Exploratorium", "is then literally hung around the mariner's neck by the crew to symbolize his guilt in killing the bird", "1", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "December 1, 2017", "James Watson and Francis Crick", "Sara Gilbert", "23 cities", "the courts", "John Travolta", "Djokovic", "Rigg", "Ant & Dec", "Nepal", "Joanne Wheatley", "UNESCO / ILO", "September 24, 2012", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "summer", "between the Eastern Ghats and the Bay of Bengal", "Daniel Suarez", "1 - 2 spinal nerve segments", "16 August 1975", "useless, time - wasting", "Kyla Pratt", "Lori McKenna", "Beorn", "UN Supreme Commander Gen. Douglas MacArthur", "Daily Mail", "Scotland", "Forbes", "2006", "Fife", "on China, Taiwan, Hong Kong and Mongolia,", "Russia and the United States", "one American diplomat to a \"prostitute\"", "unassisted triple play", "Ben Kingsley", "New Mexico"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6771233974358974}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 0.0, 0.125, 0.0, 0.08333333333333333, 1.0, 0.09090909090909093, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.625, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.4, 0.26666666666666666, 0.4, 0.9090909090909091, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1313", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7208", "mrqa_triviaqa-validation-3649", "mrqa_triviaqa-validation-6967", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-506", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-2350", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-12174"], "SR": 0.546875, "CSR": 0.5530133928571428, "EFR": 0.896551724137931, "Overall": 0.7006161483990148}, {"timecode": 28, "before_eval_results": {"predictions": ["Elisabeth Sladen", "1206", "Genghis Khan", "on Monday to meet with the Shanghai mayor and hold a town hall-style meeting with \"future Chinese leaders\" before heading to Beijing to meet his host, Chinese President Hu Jintao.", "24", "Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "a delegation of American Muslim and Christian leaders", "September 21.", "1,500", "Romney for his \"solid credentials,\"", "269,000", "Eden Park", "The Ski Train is a 68-year-old local favorite that shuttles about 750 people between Denver and Winter Park.", "a sailboat matching the description of the missing 38-foot boat was found overturned about 5:15 p.m. Saturday,", "facing up, with his arms out to the side. He is wearing socks but no shoes.", "to stop rocket fire on its southern cities and towns.", "for shoes that are also worn by dogs who walk on ice in Alaska.", "for him to order the pop star's estate totaling more than $86,000 a month,", "\"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "Haitians", "President Obama", "a Chevron oil station in the Niger Delta region", "9,500 energy-efficient light-emitting diodes that will illuminate the ball drop from a flagpole atop the One Times Square building at midnight.", "aren't allowed onto the property to view the elephants, and only a handful of media members are able to visit each year, in an effort to make the animals' lives as natural as possible.", "to secure more funds", "to fire a missile toward Hawaii", "Karl Eikenberry", "defaulted on the mortgage and the house fell into foreclosure.", "A member of the group dubbed the \"Jena 6\"", "among a growing number of state governments going after them.", "His father Surinder and elder brother Boney", "Illlinois.", "raised its alert level,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Nazi Germany", "doupled with government-supported prevention efforts and aggressive public awareness campaigns, the so-called Brazilian response has been hailed as a model for developing countries.", "gun", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "don't believe the U.S. assertion that the system is needed to guard against imminent threats from Iran or North Korea.", "David McKenzie", "citizenship because he was depriving his wife of the liberty to come and go with her face uncovered,", "a steep embankment in the Angeles National Forest", "through a facility in Salt Lake City, Utah,", "it is very easy for comments to be taken out of context and create unnecessary drama -- especially between us women,\"", "remains unknown, and that lack of knowledge has led to the use of a variety of treatments, including fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives.", "will be inducted into the Baseball Hall of Fame in July.", "Damon Bankston", "along the equator between South America and Africa.", "robert Williams", "an empty tub, his face blue and purple and a chain around his neck,", "in a tenement in the Mumbai suburb of Chembur,", "California, Texas and Florida,", "Lady Olenna Tyrell", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "the port of Veracruz", "The Duchess of Devonshire", "Bahrain", "Tokyo Metropolitan Assembly,", "Danny Lebern Glover", "William Shakespeare", "British", "douglas macarthur", "douglas macarthur", "Turtle Wax"], "metric_results": {"EM": 0.234375, "QA-F1": 0.39768759894742395}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0689655172413793, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.2222222222222222, 0.13333333333333333, 0.0, 0.0, 0.13333333333333333, 0.0, 0.07692307692307691, 1.0, 1.0, 0.26666666666666666, 0.2857142857142857, 0.125, 0.6666666666666666, 0.7499999999999999, 0.6666666666666666, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.75, 0.4615384615384615, 1.0, 0.34285714285714286, 1.0, 0.0909090909090909, 0.08333333333333333, 1.0, 0.1111111111111111, 0.923076923076923, 1.0, 0.05555555555555556, 0.13333333333333333, 0.0, 0.0, 0.8750000000000001, 0.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.2857142857142857, 0.0, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-2338", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-7484", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-1993", "mrqa_hotpotqa-validation-1922", "mrqa_searchqa-validation-9212", "mrqa_searchqa-validation-6288"], "SR": 0.234375, "CSR": 0.5420258620689655, "EFR": 1.0, "Overall": 0.7191082974137931}, {"timecode": 29, "before_eval_results": {"predictions": ["Disney\u2013ABC Domestic Television", "2011", "Soviet", "yaltrexone", "sense of smell", "Brigit Forsyth", "Florence", "Wrigley's Spearmint", "Oprah Winfrey", "2004", "a Great Dane", "Director General of the Security Service", "(Maiguetia) airport", "Rock Follies of '77", "Do I Hear a Waltz", "Nobel Prize in Literature", "Celtic", "Northwestern University", "the best value diamond for your money", "The Star Spangled Banner", "(Adjective)", "Ibrox Stadium", "New York City", "Stirling Moss", "Micael Caine", "Llyn Padarn", "Little Dorrit", "Tacitus", "apples", "Chekhov", "Chris Evans", "John Keats", "Declaration of Independence", "lome", "a condor", "Belgium", "Pilgrim's Progress", "Plato", "Fulham Football Club", "clay", "Australia", "Alaska", "Cain", "Jerez", "Michelin Man", "Tesco", "Deep Throat", "a bear suit", "oofl86zCJ andsigRd-DUO1ZsVcuIJ7qeDxrUp6oUe8&hl", "Pygmalion", "Watford Football Club", "Trainspotting", "English", "eliminate or reduce the trade barriers among all countries in the Americas", "Vicente Fox", "Holy Land", "1919", "August 24, 1983", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.\"", "International Polo Club", "killed at least 63 people and wounded more than 200.", "Sappho", "Washington, DC", "Steely Dan"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6520856973642299}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.4, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.4, 0.8695652173913044, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.4615384615384615, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7651", "mrqa_triviaqa-validation-2203", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-4186", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-566", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-5844", "mrqa_triviaqa-validation-855", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-4547", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1612", "mrqa_searchqa-validation-8687"], "SR": 0.53125, "CSR": 0.5416666666666667, "EFR": 0.9666666666666667, "Overall": 0.7123697916666667}, {"timecode": 30, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-3249", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3655", "mrqa_hotpotqa-validation-3701", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-505", "mrqa_hotpotqa-validation-5213", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5645", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-988", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4308", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-919", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-14268", "mrqa_searchqa-validation-14735", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-1664", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_squad-validation-10052", "mrqa_squad-validation-10125", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10308", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-1159", "mrqa_squad-validation-1193", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-1368", "mrqa_squad-validation-1503", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2166", "mrqa_squad-validation-2324", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2778", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-3259", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3831", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3916", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-4065", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4191", "mrqa_squad-validation-4248", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4746", "mrqa_squad-validation-4836", "mrqa_squad-validation-5009", "mrqa_squad-validation-5088", "mrqa_squad-validation-5108", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5180", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5521", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5964", "mrqa_squad-validation-6001", "mrqa_squad-validation-6069", "mrqa_squad-validation-6082", "mrqa_squad-validation-6158", "mrqa_squad-validation-6256", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6592", "mrqa_squad-validation-6605", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-709", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7476", "mrqa_squad-validation-7485", "mrqa_squad-validation-7502", "mrqa_squad-validation-7578", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-8159", "mrqa_squad-validation-8213", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8580", "mrqa_squad-validation-8681", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8935", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9141", "mrqa_squad-validation-9270", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9510", "mrqa_squad-validation-9569", "mrqa_squad-validation-964", "mrqa_squad-validation-9759", "mrqa_squad-validation-9766", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1329", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-576", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-996"], "OKR": 0.857421875, "KG": 0.428125, "before_eval_results": {"predictions": ["1947", "Madame de Pompadour", "$125 per month", "helen shap", "Newbury", "doubting man", "a cornet", "doubting man", "Lisieux", "the Astor family", "diolchwch", "canned Heat", "John Huston", "midsomer Murders", "Nick Nack", "cabot", "defence of science", "Patrick Kielty", "watcher", "d'Ivoire", "Mexico", "doubting castle", "\"The French Connection\"", "Albert Finney", "carrefour", "Ken Russell", "in the garden of Gethsemane", "John Galliano", "Swallow Sidecar Company", "Bob Balaban", "Mickey Mouse", "UVB", "Plato", "Bugsy Malone", "1812", "\"white man's blues\"", "basil", "mEXICO", "weir", "the Scillies", "fondue", "the AllStars", "pennsylvania", "Copenhagen", "hokkaido", "mambo", "George W. Bush", "jockey", "dmitchell", "Zachary Taylor", "USS Constitution", "to make wrinkles in one's face", "Total Drama World Tour", "the king's army", "Real Madrid", "two", "Balvenie Castle", "January 18, 1977", "threatening messages", "London", "fight outside of an Atlanta strip club", "lemur", "Communist Party", "doubting"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5324118589743589}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.30769230769230765, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5491", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2299", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5311", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-6074", "mrqa_triviaqa-validation-3052", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-7608", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-1468", "mrqa_triviaqa-validation-3517", "mrqa_naturalquestions-validation-6353", "mrqa_hotpotqa-validation-1351", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-85", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-3674"], "SR": 0.484375, "CSR": 0.5398185483870968, "EFR": 1.0, "Overall": 0.7107762096774194}, {"timecode": 31, "before_eval_results": {"predictions": ["nine", "the Lower Rhine", "Pet Shop Boys", "rugby", "geth", "London", "glenn baryshiro", "a doodle", "daniel mitchell", "I'm Sorry, I'll Read That Again", "Jean-Paul Gaultier", "javelin throw", "a medium-sized cat", "birchen", "French", "the Vatican", "the gluteus maximus", "isambard", "Turandot", "daniel edmonson", "daniel shannon", "Pablo Picasso", "apple", "a cello", "the Spanish", "daniel", "durness", "brazils", "the Soviet government", "finland", "sense of taste", "chile", "a joabberwocky", "b", "tongru", "lance", "Paul Merton", "Anna Roosevelt", "Paris", "an extremely sick discus fish,", "finado Tuerto, Argentina", "Charlie Chaplin", "The Perfect Storm", "a crosse or lacrosse stick", "willow", "the Queen of England", "newbury", "Abraham's", "a vice-admiral", "1936", "geth", "daniel Gabriel Rossetti", "Rachel Kelly Tucker", "the BBC", "pilgrimages to Jerusalem", "a farmers' co-op", "Tim Allen", "Anatoly Lunacharsky", "an empty water bottle down the touchline following a disallowed goal for Arsenal.", "Japan's", "The Rev. Alberto Cutie", "WTO", "glarlon Brando", "sons"], "metric_results": {"EM": 0.265625, "QA-F1": 0.3723157051282051}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.5, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.4615384615384615, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-4054", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4765", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-1633", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-5259", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-5549", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-6730", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5728", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-103", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-12801", "mrqa_searchqa-validation-2441"], "SR": 0.265625, "CSR": 0.53125, "EFR": 0.9787234042553191, "Overall": 0.7048071808510639}, {"timecode": 32, "before_eval_results": {"predictions": ["TeacherspayTeachers.com", "provisional elder", "yarn", "sandhurst", "Mujib", "b\u00e9la Bart\u00f3k", "Denver's", "astronaut", "Salt Lake City", "four feet", "teacher", "brazil", "Canada", "Peter Nichols", "American Family Publishers", "seven", "Microsoft", "Brigit Forsyth", "Celsius", "Poincar\u00e9 conjecture", "argentina", "leicestershire", "Boris Johnson", "HMS Conqueror", "Tamar", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "golf ball", "inner ear", "Gloucestershire", "Norway", "gymnastics", "george cukor", "Isaac", "bluebell", "sweden", "Prokofiev", "Cyclone", "Dan Brown", "horses", "Newcastle United", "Thank you", "magnetism", "second year", "William Neil Connor", "riyadh", "charles chaplin", "horse-racing", "\"Slow\"", "spain", "dragon", "peregrines", "1938", "Anirudh Sinha", "Alan Menken", "five", "The Walking Dead", "1979", "1999", "capital murder and three counts of attempted murder", "Apple employees", "mental health treatment", "Cromwell's", "Brain Teaser", "Athol Fugard"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6302083333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10088", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-2608", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-1113", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-7671", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-6815", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-1889", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-6553"], "SR": 0.578125, "CSR": 0.5326704545454546, "EFR": 1.0, "Overall": 0.7093465909090909}, {"timecode": 33, "before_eval_results": {"predictions": ["1,100", "since 2001", "horseshoes", "Barry Briggs", "table tennis", "captain Archibald Haddock", "bart\u00f3k", "14-time major champion, and 4-time finalist", "Harold Shipman", "The Assembly", "Michael hordern", "rogers", "yeast", "Michael Anthony Holding", "(359-299 Ma)", "Thank you", "nipples", "Queen Mary", "lolita", "muscle tissue", "Surrealism", "shinto", "sewing machines", "Morgan Spurlock", "john Buchan", "seals", "workington", "an omnibus anti-sided bill that is expected to shutter all but a handful of abortion providers,", "shorthand", "Altamont Speedway", "fourteen", "jack Sprat", "The Dunkirk evacuation,", "the Allstars", "marc", "Praseodymium", "brawn", "queen Victoria", "an antipasto, sometimes herbs, and may at times be topped with onion, cheese and meat.", "chairman", "Wicked Witch", "Rita Hayworth", "the Observer", "Isaac Newton", "Turnbull & Asser, Hawes & Curtis, Thomas Pink, Harvie & Hudson,", "bullfighting", "Arthur C. Clarke", "Marc Warren", "entropy", "Chad", "Earring", "taggart", "on the microscope's stage", "the three men's individual events, as well as participating in the gold - medal winning relay team", "Norway", "Virgin", "the Philadelphia Naval Shipyard", "Afro-Caribbean", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "cross-country skiers", "discussed ways their countries could work together to overthrow the socialist government of Chile and a discussion of collusion with the United States.\"", "a ladder", "Hill Street Blues", "white"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6589342948717949}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.13333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5384615384615384, 1.0, 0.2285714285714286, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-7142", "mrqa_triviaqa-validation-368", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-7043", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-3060", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-1369", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1538", "mrqa_hotpotqa-validation-1813", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-11071"], "SR": 0.578125, "CSR": 0.5340073529411764, "EFR": 0.9259259259259259, "Overall": 0.6947991557734204}, {"timecode": 34, "before_eval_results": {"predictions": ["second Gleichschaltung", "two of Tesla's uncles", "Renault", "kinks", "Massachusetts", "lyonesse", "three", "Bono", "ink", "Toy Story", "house sparrow", "The Lion", "intestines", "Independence Day", "charles Brooker", "Oxford", "number 13", "Glasgow", "Stephen Hawking", "Florence", "Wat Tyler", "Steve Davis", "the events of 16 September 1992", "vomiting", "Ennio Morricone", "NBA", "enid blyton", "1749", "horseradish", "china", "1066", "alan", "$2", "Glasgow", "Dian Fossey", "port", "checkers", "Norman Mailer", "an action figure", "jura", "george costanza", "chatsworth", "obelix", "quant", "Scooby Doo", "Pennine Way", "john esmonde", "lorne Greene", "ren\u00e9 descartley", "frans hals", "charles", "Rita Hayworth", "the Church of England", "Matt Monro", "William Chatterton Dix", "Belarus", "\"Big Fucking German\"", "Welterweight", "on, wax off", "The EU naval force", "Miami Beach, Florida,", "upperglacial", "39 Steps", "Armageddon"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6216299019607843}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-1015", "mrqa_triviaqa-validation-4981", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-3840", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-661", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-1891", "mrqa_newsqa-validation-4", "mrqa_searchqa-validation-14545"], "SR": 0.59375, "CSR": 0.5357142857142857, "EFR": 0.9615384615384616, "Overall": 0.7022630494505495}, {"timecode": 35, "before_eval_results": {"predictions": ["Tracy Wolfson", "2013", "Russ Conway", "green", "london", "York", "Austria", "a bra", "john Mellencamp", "jons arthur", "cuthbert", "king arthur", "soybeans", "annie leibovitz", "Pinot Noir", "seal", "london", "king kung fu grip", "dennis whittington", "peacock", "barbara", "ishmael", "smell", "Brad Pitt", "Eleanor Rigby", "The Simpsons", "one Direction", "yellows", "Cornell University", "kung fu grip", "kung fu grip", "Portugal", "Follicle-stimulating hormone", "paramitas", "dennis taylor", "barbara", "france", "london", "racecar", "Costa Concordia", "barbara", "dennis taylor", "london", "true", "horseshoe", "king andrew", "wales", "oil capital of Europe", "orange", "jays", "yasser Arafat", "Black Sea", "Gary Grimes", "49 cents", "1", "Francisco Antonio Zea", "Wings of Desire", "actor and former fashion model", "Airbus A330-200", "an account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "an FAA-certified physician every year; those over 40,", "Roland Garros", "cyrano de Bergerac", "economies and societies"], "metric_results": {"EM": 0.453125, "QA-F1": 0.49642857142857144}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-584", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-1323", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-4807", "mrqa_triviaqa-validation-168", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-6664", "mrqa_triviaqa-validation-3593", "mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-5128", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-276", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-11091"], "SR": 0.453125, "CSR": 0.5334201388888888, "EFR": 1.0, "Overall": 0.7094965277777778}, {"timecode": 36, "before_eval_results": {"predictions": ["independent schools", "cymbal", "Djibouti and Yemen", "nubia", "Barcelona", "Salma Hayek", "Caernarfon", "nodel", "glaciers", "Delaware", "rodents", "crow", "Flintstones", "the Great Chicago Fire", "gelatine", "Ecuador", "Cyprus", "an orphanage", "homeless charity", "Dublin", "Sin City", "walker", "doesn't include additional costs such as insurance or business rates", "Google", "lulu the elephant", "pembrokeshire Coast National Park", "Tripoli", "jimmy Burns", "dreamgirls", "Opus Dei", "mexico", "education", "The Press Gang", "Liberator", "Farlake", "a goat", "Dubai", "Sydney", "orange", "Lehman Bros International", "married their husbands", "Ordovices", "Robert Devereux,", "dombey and Son", "mexico", "pascal", "John Galsworthy", "Boris Becker", "Julius No", "Amsterdam", "michael boyd", "24", "Ford", "between 27 July and 7 August 2022", "94", "Kang and Kodos", "La Liga", "environmental videos", "five minutes before commandos descended from ropes that dangled from helicopters,", "st. Louis,", "milk", "jeconiah", "goodson", "richmond"], "metric_results": {"EM": 0.5, "QA-F1": 0.5501302083333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.5, 1.0, 0.0, 0.625, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-275", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2281", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7186", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-7594", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-562", "mrqa_naturalquestions-validation-5647", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1292", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-12212"], "SR": 0.5, "CSR": 0.5325168918918919, "EFR": 0.96875, "Overall": 0.7030658783783784}, {"timecode": 37, "before_eval_results": {"predictions": ["uncivilized", "ryan de niro", "8", "m\u00e1laga", "julius hefner", "Tiananmen Square", "noises off", "joke machine", "Till Death Us Do Part", "javier Bardem", "1720", "Australia", "dysmenorrhea", "The Hague", "pomegranate", "kung fu grip", "julay vtoroy", "Sally Ride", "g\u00e9rard Depardieu", "jimmy Carter", "pitlochry", "herpes zoster", "small faces", "zoom", "Angela dothea Kasner", "green gables", "lung cancer", "Herbert Henry Asquith,", "magnator", "2 1/2", "Aslan", "jethro", "Vancouver Island", "purdy", "blues Brothers", "six", "sash", "mary Trepanier", "Stockholm", "st Helens", "beta", "jack ryan", "Basil Fawlty Towers", "blue", "mary west", "Boreas", "heavyweight champion", "violins and cellos", "Gaga", "Burgundy", "colleen McCullough", "retinal ganglion cell axons and glial cells", "2026", "Kevin Zegers", "50th anniversary of the founding of the National Basketball Association (NBA)", "Cherokee National Holiday", "empire", "the 1800s and the era of Mark Twain,", "emily and Jonas", "the man facing up, with his arms out to the side.", "muskrat", "Santo Versace", "trisha yearwood", "B.J. Thomas"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5100285947712418}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.35294117647058826, 0.4, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-7331", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5616", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-4129", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-856", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-3899", "mrqa_triviaqa-validation-6605", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-6638", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-1958", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-12111", "mrqa_searchqa-validation-15107"], "SR": 0.390625, "CSR": 0.528782894736842, "EFR": 1.0, "Overall": 0.7085690789473684}, {"timecode": 38, "before_eval_results": {"predictions": ["71", "(Cecil) Rhodes", "constant", "theology", "a triangle", "longlegs", "howdy", "Audley", "root beer", "honey Nut Cheerios", "Venus", "Berry", "christian", "Secretary of State", "density", "Vladimir Putin", "Ocean's Eleven", "Barack Obama", "Stockholm", "little toaster", "crescent rolls", "macau", "light speed", "Tel Aviv", "Dan Marino", "Munich", "viola", "Luxembourg", "Alice", "suntory", "Tower of London", "you Bet Your Life", "deuteronomy", "greece", "bix Beiderbecke", "yellow", "the Crimean War", "opium", "Northanger Abbey", "concave", "the Caspian Sea", "champagne", "Scooby-Doo", "Dean Cain", "hypnotic", "South Dakota", "constantine greece", "48", "shell companies", "orson welles", "roxy theatre", "The border between the Cocos Plate and North American Plate", "Erica Rivera", "Julia Roberts", "wuthering Heights", "Craggy Island", "greece", "Dallas/Fort Worth", "Belgian", "Esteban Ocon", "Roberto Micheletti", "greece", "Twelve", "Oahu"], "metric_results": {"EM": 0.5, "QA-F1": 0.5723958333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-3544", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-15961", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-13762", "mrqa_searchqa-validation-16868", "mrqa_searchqa-validation-10062", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-844", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-5544", "mrqa_searchqa-validation-9662", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-11364", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-6285", "mrqa_hotpotqa-validation-4023", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-857", "mrqa_hotpotqa-validation-4625"], "SR": 0.5, "CSR": 0.5280448717948718, "EFR": 1.0, "Overall": 0.7084214743589744}, {"timecode": 39, "before_eval_results": {"predictions": ["inequality in wealth and income", "Princess Aisha bint Hussein", "House of Fraser", "Westchester County", "City Mazda Stadium", "841", "American", "Lonestar", "Kaep", "armed", "Southern Rhodesia", "Alonso L\u00f3pez", "made into a TV series for the BBC channel CBeebies", "Voni Morrison", "The Galleria Vittorio Emanuele II", "Coalwood", "Anne Mae Lee", "neuro-orthopaedic", "the City of Westminster", "British", "Albert", "6,241", "Dan Bilzerian", "STS-51-L.", "a valuation method", "Crackle", "Kristy Lee Cook", "Perth", "Love Streams", "1935", "5.3 million", "Manhattan", "Dara Torres", "The dyers of Lincoln", "actress", "a role-playing game or wargame campaign", "May 5, 2015", "Red and Assiniboine Rivers", "Ephedrine", "Neymar", "The Jefferson Memorial", "Strange Interlude", "Bothtec", "2,099", "a body of water", "35,402", "2004 Paris Motor Show", "1996", "33", "1999", "Axl Rose", "5", "John McConnell", "George Harrison", "Bruno Mars", "Wyre", "Azzurri", "Heshmatollah Attarzadeh", "a one-shot victory in the Bob Hope Classic", "700", "The Untouchables", "Monopoly", "a cookie jar", "2002"], "metric_results": {"EM": 0.625, "QA-F1": 0.7144717261904763}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.8, 0.5, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-3495", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2707", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-943", "mrqa_triviaqa-validation-1097", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1230"], "SR": 0.625, "CSR": 0.53046875, "EFR": 1.0, "Overall": 0.70890625}, {"timecode": 40, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1616", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-612", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12704", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-2178", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7865", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9870", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10149", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1844", "mrqa_squad-validation-1967", "mrqa_squad-validation-2049", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3428", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3815", "mrqa_squad-validation-3836", "mrqa_squad-validation-3837", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4135", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-503", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5338", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-5859", "mrqa_squad-validation-5893", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6435", "mrqa_squad-validation-6506", "mrqa_squad-validation-6671", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7002", "mrqa_squad-validation-7193", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7704", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8084", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8935", "mrqa_squad-validation-902", "mrqa_squad-validation-9254", "mrqa_squad-validation-9300", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9479", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3640", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3874", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4527", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-6762", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7400", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-996"], "OKR": 0.84765625, "KG": 0.47890625, "before_eval_results": {"predictions": ["2006", "Ghana's Asamoah Gyan", "Dublin", "Ribhu Dasgupta", "Lord Byron", "\"Catch Me If You Can\"", "Irish", "sulfur mustard H or HD blister gas", "12\u201318", "Mike Biden", "Jeffrey William Van Gundy", "People!", "Ballarat Bitter", "Bank of China Building", "(January 11, 1881 \u2013 March 24, 1938)", "26,000", "TransAd Adelaide", "Gillian Leigh Anderson", "Steve Carell", "December 16, 1959", "Lauren Alaina", "Louisiana Tech University", "1943", "Fountains of Wayne", "Saint Paul, Minnesota", "organist Cristoforo Benvenuti", "(91.1 FM)", "October 20, 2017", "Zimbabwe", "Iowa State", "Barbara Niven", "Friedrich Nietzsche", "Rabat", "Straits of Gibraltar", "American burlesque", "Russell T Davies", "Jay Schottenstein", "600", "(born April 30, 1982),", "Martin \"Marty\" McCann", "Nine Inch Nails", "Labour Party", "Orlando\u2013Kissimmee\u2013Sanford, Florida Metropolitan Statistical Area", "Prussia", "Boston", "Bambi, a Life in the Woods", "from 1993 to 1996", "Watertown, New York", "green and yellow", "ice hockey", "1995 teen drama \"Kids\"", "3 September", "the Gaget, Gauthier & Co. workshop", "200 to 500 mg", "Paul Gauguin", "Ynys M\u00f4n", "Saturday Night Live", "an antihistamine and an epinephrine auto-injector", "Chrysler Corporation", "outside his house in Najaf's Adala neighborhood", "a Panic's in thy breastie", "Shauna", "a person holding dumbbells in his/her arms", "hemoglobin"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5243799603174604}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 0.5, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-5509", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-507", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-883", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2527", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-325", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-4639", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-2482", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-8555", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-5417", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-1606", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-10053"], "SR": 0.390625, "CSR": 0.5270579268292683, "EFR": 1.0, "Overall": 0.7164272103658538}, {"timecode": 41, "before_eval_results": {"predictions": ["his sons and grandsons", "memory", "antonyms", "a fisheye lens", "berenstain Bears", "Parris Island", "bagel", "Elizabeth Taylor", "Molly Ringwald", "Rolling Stones", "White blood cells", "Al Capone", "Matsu", "Stardust", "operas", "The Grasshopper and the Ants", "Quinn", "practice", "a grizzly bear", "Kareem Abdul-Jabbar", "the Police", "Bravo", "Henry Clay Frick", "Mikhail Gorbachev", "a ghost", "mayor", "Amateur Radio Club", "a pagoda", "Godefroy", "Margaret Tobin Brown", "Franklin D. Roosevelt", "Aries", "the Golden Fleece", "Auguste Deter", "Chuck Yeager", "Barney Stinson", "\"PANT\"s", "a flushing toilet", "Vermont", "vilebrequin", "a tank", "zenith", "the Whig Party", "Vietnam", "Ectoplasm", "Alberto Fujimori", "Old North Church", "binocular", "honey bunch", "Legally Blonde", "Scorpius", "the Rashidun Caliphs", "Kaley Christine Cuoco", "on a membrane", "Salix", "Pearl Slaghoople", "The Book of Proverbs", "Crown Holdings Incorporated", "New Orleans Saints", "the Fundamentalist Church of Jesus Christ of Latter Day Saints", "is the owner of EMS, which provides janitors to businesses around the country.", "Liverpool Street Station", "because the Indians don't want to get involved in the armed struggle.", "crossword"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6090909090909091}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-13266", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-9985", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-6579", "mrqa_searchqa-validation-9793", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-16343", "mrqa_searchqa-validation-7876", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-1035", "mrqa_triviaqa-validation-987"], "SR": 0.546875, "CSR": 0.5275297619047619, "EFR": 0.9655172413793104, "Overall": 0.7096250256568145}, {"timecode": 42, "before_eval_results": {"predictions": ["to Westminster", "Ratatouille", "Ecclesiastes", "Catherine de Medici", "Bohemia", "Ecuador", "Microsoft", "Katharine Hepburn", "binocular", "the forest", "London", "Little Boy Blue", "cotton", "Philip II", "Seinfeld", "(John) Jones", "the sound barrier", "Spider-Man", "Camelot", "Hudson Bay", "Hamlet", "a statement", "Patrick\\'s", "frosted", "King George III", "Saul", "Duckworth", "(Joseph) Conrad", "Rastafari", "Beverly Cleary", "Neapolitan", "\"Hound Dog\"", "Spain", "angels", "Pisa", "\"Joliet\"", "Bangkok", "Tracy Ullman", "Russia", "Burt Lancaster", "diagonals", "the Chinese Communist Party", "a sacristy", "Israel", "(William) Shakespeare", "Alabama", "Making the Band 3", "Martinique", "deodorant", "The Pathfinder", "the Lion King", "Florida", "Norman", "through the next episode, `` Seeing Red ''", "dragonfly", "\"Rarely is the question asked, is our children learning?\"", "Firethorn", "alt-right", "Australian", "the Hawaii House of Representatives", "reached an agreement late Thursday to form a government of national reconciliation.", "paid tribute to pop legend Michael Jackson,", "a Daytime Emmy Lifetime Achievement Award", "Australia"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5491815476190476}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714285, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9392", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-1830", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-11412", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8318", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-5271", "mrqa_searchqa-validation-1179", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-8651", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-8416", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10320", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-755", "mrqa_newsqa-validation-1351"], "SR": 0.453125, "CSR": 0.5257994186046512, "EFR": 1.0, "Overall": 0.7161755087209303}, {"timecode": 43, "before_eval_results": {"predictions": ["jellyfish", "enforcing racially separated educational facilities", "beef", "February", "Monk's Caf\u00e9", "Christopher Lloyd", "Morgan Freeman", "1910", "over 74", "John Vincent Calipari", "Palmer Williams Jr.", "2019", "James Madison", "voted in favour of the bill with more than 50 % of the total members of a house,", "1917", "Vikare", "Brad Johnson", "Red Sea", "1982", "1956", "Hagrid", "Monk's", "entering Arkansas, turning south near Fulton, Arkansas, and flowing into Louisiana, where it flows into the Atchafalaya River", "Pete Seeger", "low mood", "Sam", "the closing of the atrioventricular valves and semilunar valves", "Kevin Sumlin", "Twisty the Clown", "a child with Treacher Collins syndrome trying to fit in", "two", "gastrocnemius", "all transmissions are in clear text, and usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "The main character Roy Eberhardt moves to Florida and into the town of Coconut Cove, where his classmate Dana Matherson starts bullying him", "senior enlisted sailor", "March 9, 2018", "741 weeks", "Jikji", "cadmium", "1961", "2 September 1990", "White oak", "cephalopods", "1939", "March 26, 1973", "Stephen Foster", "Charles Carson", "the Bee Gees", "the Brazilian state of Mato Grosso", "The White House Executive Chef", "3.9 and 5.5 mmol / L ( 70 to 100 mg / dL )", "Mike Danger", "conductor", "a game of bridge", "Manchester United", "Skipton Castle", "Duncan Kenworthy", "Elena Kagan", "a cardio", "Sadat\\'s assassination was recently revisited by his daughter, Roqaya al-Sadat,", "Che Guevara", "pinnipeds", "Heather Locklear", "The Arizona Health Care Cost Containment System"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5153039638237007}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.16666666666666669, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.19047619047619047, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.21052631578947367, 0.0, 0.9333333333333333, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.14814814814814814, 0.5, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.0, 0.5, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-2630", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-946", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-912", "mrqa_searchqa-validation-8799", "mrqa_hotpotqa-validation-1803"], "SR": 0.40625, "CSR": 0.5230823863636364, "EFR": 0.9473684210526315, "Overall": 0.7051057864832535}, {"timecode": 44, "before_eval_results": {"predictions": ["Confucianism", "April 2011", "James Intveld", "frontal lobe", "Haliaeetus ( sea eagles )", "black and yellow", "a low concentration in pigmentation", "Carol Ann Susi", "osseo - cartilaginous", "a premalignant flat ( or sessile ) lesion of the colon", "Arnold Schoenberg", "a jazz funeral without a body", "asexually", "mid November", "Deposition", "Dimitar Berbatov and Carlos Tevez", "George Strait", "boiling water reactor", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "October 2012", "201", "Rafael Nadal", "Bangladesh -- India border", "Melissa Disney", "in the Hebrew Bible, in the books of Exodus and Deuteronomy", "Gene MacLellan", "the fingers on either side of the mouth", "William Shakespeare's As You Like It", "Frozen carbon dioxide accumulates as a comparatively thin layer about one metre thick on the north cap in the northern winter only", "Michael Schumacher", "on the microscope's stage", "deceased - donor ( formerly known as cadaveric )", "North Atlantic Ocean", "John Hancock", "silk floss tree", "around 100,000 writes", "Triple threat", "Clarence Darrow", "alpha efferent neurons", "in teaching elocution", "the optic disc to the optic chiasma", "Butter Island off North Haven, Maine in the Penobscot Bay", "a combination of genetics and the male hormone dihydrotestosterone", "British Columbia, Canada", "saliva", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Frankie Valli", "908 mbar", "1939", "Pyeongchang County, Gangwon Province, South Korea", "Utah, Arizona, Wyoming, and Oroville, California", "Kuala Lampur", "Lidice", "Augustus", "Ars Nova Theater", "French", "1902", "Najaf.", "a monthly allowance,", "(3 degrees Fahrenheit),", "silverware", "a Caesar salad", "Possession is nine-tenths of the law", "last summer."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5870633706571207}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.25, 1.0, 0.3333333333333333, 1.0, 0.19999999999999998, 1.0, 0.0, 0.7692307692307692, 0.2727272727272727, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.3333333333333333, 0.4615384615384615, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 0.28571428571428575, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-9196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-8391", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-1662", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-2863", "mrqa_newsqa-validation-1076", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-6194"], "SR": 0.46875, "CSR": 0.521875, "EFR": 0.9411764705882353, "Overall": 0.703625919117647}, {"timecode": 45, "before_eval_results": {"predictions": ["the adaptive immune system", "Andaman and Nicobar Islands", "the engineer \u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "a mid-size four - wheel drive luxury Mercedes -Benz GL - Class", "Hermann M\u00fcller", "The ratio of the length s of the arc", "Lake Wales", "an unmasked and redeemed Anakin Skywalker", "Megan Park", "Michael Jeter as Smokey and Steamer", "Tulsa, Oklahoma", "Broken Hill and Sydney", "John Goodman", "the right side of the heart to the lungs", "during the period of rest ( day )", "Oklahoma", "11 January 1923", "southwestern Colorado and northwestern New Mexico", "Ann Gillespie", "New Zealand", "Master Christopher Jones", "during the united monarchy of Israel and Judah", "Claudia Grace Wells", "Jerry Leiber and Mike Stoller", "1995 Toyota Supra", "Natural - language processing", "Sir Alex Ferguson", "around 1872", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "2011", "September 1973", "Cairo, Illinois", "comic", "Abanindranath Tagore CIE", "Coldplay", "the retina of mammalian eyes", "Empiricism", "more than 160 high - rises, 42 of which stand taller than 400 feet ( 122 m )", "Lana Del Rey", "Mutt Lange", "a total of six degrees of freedom", "December 1886", "Ludacris", "A costume", "approximately $ 18 ) in the Philippines or $60 abroad", "Frankie Muniz", "Freddie Highmore", "the nerves and ganglia outside the brain and spinal cord", "Andy Cole", "1966", "Scar's henchmen", "a visual defect in which colorless objects appear to be tinged with color", "Perth", "raspberries", "England", "two", "6,241", "Roy Foster", "share personal information.", "Stephen Tyrone Johns", "Around the", "flatware", "Sir Francis Drake", "Charles"], "metric_results": {"EM": 0.5, "QA-F1": 0.643409802003552}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.8, 0.7777777777777778, 0.3636363636363636, 0.0, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.8571428571428571, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3018", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5041", "mrqa_hotpotqa-validation-5438", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2549", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-2528"], "SR": 0.5, "CSR": 0.5213994565217391, "EFR": 0.96875, "Overall": 0.7090455163043479}, {"timecode": 46, "before_eval_results": {"predictions": ["The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316,", "2.5 %", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "Cliff Richard", "McKim Marriott", "The British Indian Association", "foreign investors", "Redenbacher family", "British and French Canadian", "a line of committed and effective Sultans", "Jules Shear", "the 10th anniversary of the 2002 World Summit on Sustainable Development ( WSSD ) in Johannesburg", "at Tandi, in Lahaul", "H.L.A. Hart", "Janie Crawford", "West Norse sailors", "the EPs Sounds of the Season : The Taylor Swift Holiday Collection and Beautiful Eyes", "2012", "in the front of the body", "Dottie West", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a symbol of the resurrection of Christ", "it failed to enforce its rule, and its vast territory was divided into several successor polities", "Buffalo Lookout", "Aristotle", "March 4, 1789", "John Donne", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Cristeta Comerford 2005 -- present", "Paspahegh Indians", "6 - 7 % average GDP growth annually", "Arnold Schoenberg", "Identification of alternative plans / policies", "The Outback", "quartz", "Wisconsin", "85 %", "Long Island", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law", "seven", "NFL commissioner Roger Goodell", "the eighth episode in the ninth season of the American animated television series South Park", "if he was not named Romeo he would still be handsome and be Juliet's love", "a certified question or proposition of law from one of the United States Courts of Appeals", "gathering money from the public", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "2018", "a contestant makes a thirty - second call to one of a number of friends ( who provide their phone numbers in advance ) and reads them the question and answer choices", "San Jose, California", "Nicklaus", "Indo - Pacific", "who's the cat that won't cop out when there's danger all about", "Denise van Outen", "West Virginia", "Syracuse", "Girls' Generation", "Manchester, England", "Authorities in Fayetteville, North Carolina,", "Nearly eight in 10 say things are going badly in the country,", "If a security officer were to pull a gun on an armed individual in a mall, it could result in \"the gunfight at the 'OK corral,'", "Jericho", "James Garfield Davis", "Catherine of Aragon", "Ashley \"A.J. Jewell,"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5666807017340325}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true], "QA-F1": [0.2222222222222222, 0.0, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5283018867924527, 0.5263157894736842, 0.72, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7837837837837839, 1.0, 0.3333333333333333, 0.5882352941176471, 0.0, 0.11764705882352941, 0.47058823529411764, 0.4615384615384615, 1.0, 0.9090909090909091, 0.0, 1.0, 0.05714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.32, 0.0, 1.0, 0.4, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-259", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-171", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-3226", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-2578", "mrqa_triviaqa-validation-3771", "mrqa_hotpotqa-validation-4117", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-16615", "mrqa_searchqa-validation-5939"], "SR": 0.390625, "CSR": 0.5186170212765957, "EFR": 0.9487179487179487, "Overall": 0.7044826189989088}, {"timecode": 47, "before_eval_results": {"predictions": ["mumps", "The Last King of Scotland", "Kazakhstan", "Dickens", "the pulmonary pleura", "Knutsford", "Burma", "Jim Broadbent", "a falcon", "South Park", "Shylock", "Canada", "Phil Spector", "the Champagne Cosmopolitan Cocktail", "Tiny Tim", "the Dada movement", "bemidji", "Roddy Doyle", "geography", "Operation Frequent Wind", "Berlin", "Charlie Chan", "Wanderers", "Pinwright's Progress", "a winter fur hat", "Lady Gaga", "a waterfowl", "Christian Wulff", "the Kinks Are the Village Green Preservation Society", "the Queen of Comedy", "Debbie Rowe", "Sir Herbert Kitchener", "a centaur", "iodine deficiency", "36", "Margaret Beckett", "James Hogg", "Welsh", "george Bernard Shaw", "Table Tennis", "shaftton pie", "the Florida Current", "Fleet Street", "in the United Kingdom between London and Brighton, East Sussex, England", "Gandalf", "1930", "Motown", "Canadian", "Pope Benedict XVI", "a dove", "Cable", "a rearrangement of chromosomal material between chromosome 21 and another chromosome", "the American Civil War", "$315,600", "Amy Poehler", "Eric Allan Kramer", "Koninklijke Ahold N.V.", "via YouTube days after 35 bodies were found in two trucks during rush hour in the city of Boca del Rio.", "the Dutch patent office", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "sweden", "Nancy Drew", "schizophrenia", "shaft"], "metric_results": {"EM": 0.53125, "QA-F1": 0.593547077922078}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.09523809523809523, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-2077", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-1527", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-6090", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-961", "mrqa_triviaqa-validation-304", "mrqa_naturalquestions-validation-10537", "mrqa_hotpotqa-validation-4091", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-3960", "mrqa_searchqa-validation-14439"], "SR": 0.53125, "CSR": 0.5188802083333333, "EFR": 1.0, "Overall": 0.7147916666666666}, {"timecode": 48, "before_eval_results": {"predictions": ["biological taxonomy", "eight", "April 1st", "June 1992", "won", "William Lanteau", "Kimberlin Brown", "March 31, 2017", "Henri Marie de Toulouse", "New York City", "George Strait", "John Adams", "a major fall in stock prices", "On the west", "Charles Path\u00e9", "Phillip Paley", "statute", "18", "Game 1", "Thomas Jefferson", "Abraham Gottlob Werner", "IETF protocols", "the 18th century", "Lesley Gore", "sometime between 124 and 800 CE", "Plank", "mongrel female", "Teri Hatcher", "John Quincy Adams", "August 1991", "Uralic", "dromedary", "Bhupendranath Dutt", "2011", "a drug which under some conditions behaves as an agonist ( a substance that fully activates the receptor that it binds to )", "Bill Russell", "Battle of Antietam", "pickup trucks", "Hunter Tylo", "Buffalo Bill", "the 2nd century", "James Rodr\u00edguez", "around 10 : 30am", "Jack Barry", "White Sox", "45 %", "to condense the steam coming out of the cylinders or turbines", "Bill Russell", "1984", "Identification of alternative plans / policies", "the longest rotation period ( 243 days )", "the Northern line", "cilla Black", "canada de Reims", "yuriko Yoshitaka", "Taoiseach", "Los Angeles Dance Theater", "Kurdistan Workers' Party,", "japan tukel", "don Draper", "typewriter", "calico", "lute", "Gary Player"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6696330318986569}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.12500000000000003, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16, 1.0, 0.4799999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6923076923076924, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.5, 0.0, 0.5, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-9749", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-183", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-4655", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-1200", "mrqa_searchqa-validation-8622"], "SR": 0.578125, "CSR": 0.5200892857142857, "EFR": 1.0, "Overall": 0.7150334821428571}, {"timecode": 49, "before_eval_results": {"predictions": ["Napoleon", "iron", "Patrick Warburton", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "pneumonoultramicroscopicsilicovolcanoconiosis", "Joseph M. Scriven", "Andrew Garfield", "July 4, 1776", "Keith Thibodeaux", "Jesus Christ", "Charles Path\u00e9", "eleven", "President alone, and the latter grants judicial power solely to the federal judiciary", "Johannes Gutenberg", "O'Meara", "published in the United States by Melvil Dewey in 1876", "Marley & Me is a 2008 American comedy - drama film about the titular dog, Marley", "fourth season", "four seasons and 67 episodes", "it was a Confederate victory, followed by a disorganized retreat of the Union forces", "the probability of rejecting the null hypothesis given that it is true", "slavery", "Lou Stallman", "United States", "The Outback", "its vast territory was divided into several successor polities", "Louis XV", "Super Bowl XXXIX in Jacksonville", "its genome", "Beorn", "power", "nationalists of the Union proclaimed loyalty to the U.S. Constitution", "in response to the Weimar Republic's failure to continue its reparation payments in the aftermath of World War I", "Indirect rule", "Zachary John Quinto", "the governor of West Virginia, who is elected to a four - year term at the same time as presidential elections", "Wednesday, September 21, 2016, on NBC and finished on Wednesday, May 24, 2017, with a two - hour season finale", "ninth", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "milling process removes material by performing many separate, small cuts", "in North America", "1939", "1992", "Millerlite", "Felix Baumgartner", "Donald Fauntleroy Duck", "c. 3000 BC", "Bart Howard", "Paris", "1966", "the vascular cambium", "child of the 1980\u2019s", "Leeds", "the Netherlands", "Coleman Hawkins", "Zero Mostel", "Ellesmere Port, United Kingdom", "the Genocide Prevention Task Force", "Boundary County, Idaho,", "U.S. Naval Forces Central Command,", "titan", "Treasure Island", "Pablo Picasso", "sailor"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5986053598584199}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9473684210526316, 0.2857142857142857, 0.5, 0.33333333333333337, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.38095238095238093, 0.2857142857142857, 0.6666666666666666, 0.8837209302325582, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-1290", "mrqa_naturalquestions-validation-6888", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6021", "mrqa_triviaqa-validation-460", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2428", "mrqa_searchqa-validation-9956", "mrqa_searchqa-validation-15480", "mrqa_newsqa-validation-1446"], "SR": 0.484375, "CSR": 0.5193749999999999, "EFR": 0.9696969696969697, "Overall": 0.708830018939394}, {"timecode": 50, "UKR": 0.689453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1373", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10692", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5387", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8709", "mrqa_naturalquestions-validation-8819", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5744", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99"], "OKR": 0.857421875, "KG": 0.5, "before_eval_results": {"predictions": ["Pelias", "Trainspotting", "rockin' at the Hops", "oakum", "spark plugs", "Concorde", "Al Jazeera", "French", "hoggery", "squid", "1925", "Goldfinger", "Midway", "Flower", "Gerald R. Ford", "Dengue fever", "Japan", "Ted Turner", "bleeding", "Cowslip", "Mount Everest", "Strangeways", "Carthage", "Wensum", "Robben Island", "Germany", "Taekwondo", "feet", "apple", "sixth Wimbledon championship", "Nelson Mandela", "George Orwell", "Andrew Jackson", "Muriel Spark", "Table Tennis", "Entwistle Reservoir", "a DeLorean", "six", "Perseus", "Yakutat", "the United Nations of Football", "syndicate", "muscle", "transuranic elements", "John Buchan", "Tesco", "Lolita", "jukebox", "the Indus Valley", "duck", "Pickwick", "Nancy Jean Cartwright", "Watson and Crick", "Authority", "his superhero roles", "German", "Che Guevara", "Robert Barnett", "five", "Jet Republic", "the Lexus 460", "John Lennon", "Murder by Death", "Republicans"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7338541666666667}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-139", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-6476", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-204", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-3982", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-112", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-881", "mrqa_searchqa-validation-5409"], "SR": 0.6875, "CSR": 0.522671568627451, "EFR": 0.95, "Overall": 0.7039093137254901}, {"timecode": 51, "before_eval_results": {"predictions": ["Flatbush", "Australian", "\"Traumnovelle\" (\"Dream Story\")", "Denmark", "Bad Meets Evil", "Bellagio and The Mirage", "George Lawrence Mikan, Jr.", "more than 20", "Guthred", "The New Yorker", "Edward R. Rooney", "St. Louis Cardinals", "NXT Tag Team Championship", "Lee Byung-hun", "as many as 16 universities in the eastern half of the United States from 1979 to 2013", "February 1", "capital crimes or capital offences", "\"Rainy Days and Mondays\"", "March", "Chuck Noll", "cate Blanchett", "Oregon", "Atlas ICBM", "Democratic", "Kim So-hyun", "Rolling Stones", "22,500 acres", "Trey Parker", "Kew", "new York", "twelfth", "Wembley Stadium", "Shameless", "Brigadier General Raden Panji", "skiing and mountaineering", "Indian", "comedian Chris Hardwick", "cruiserweight", "five books", "Leofric", "Sasquatch", "March 17, 2015", "Yubin, Yeeun", "5249", "fourth", "three", "28 November 1973", "\"O\", \"La Nouba\", \"Myst\u00e8re\", \"Alegr\u00eda\", and \"Quidam\"", "Londonderry", "Santiago Herrera", "jewelry designer", "Steve Russell", "an old pronunciation of Gaultier or Walter", "studying All My Sons by Arthur Miller, a play about a man whose choice to send out faulty airplane parts for the good of his business and family caused the death of twenty one pilots during World War II", "Pegasus", "sunday", "Basildon-born Perry", "15-year-old's", "July 23.", "northern Baghdad", "yermo", "circumference", "T.S. Eliot", "althea Gibson"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5058035714285714}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false], "QA-F1": [0.25, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-3270", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-3712", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-1886", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-7352", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-4557", "mrqa_newsqa-validation-1779", "mrqa_searchqa-validation-6192", "mrqa_searchqa-validation-834", "mrqa_triviaqa-validation-4216"], "SR": 0.421875, "CSR": 0.5207331730769231, "EFR": 1.0, "Overall": 0.7135216346153846}, {"timecode": 52, "before_eval_results": {"predictions": ["Frank Ocean", "Selden, New York", "2010", "Ryukyuan people", "Robert L. Stone", "Spanish", "The King of Hollywood", "five times", "1968", "Charles Eug\u00e8ne Jules Marie Nungesser, MC", "Kim Yoon-seok and Ha Jung-woo", "Jennifer Grey", "1978", "M2M", "Mark Neveldine and Brian Taylor", "#Gun", "Beauty and the Beast", "\"Odorama\"", "The 8th Habit", "Larnelle Harris", "Total Nonstop Action Wrestling", "Lambic", "Bit Instant", "Tom Jones", "Charles Guiteau", "Secrets and Lies", "Hard rock", "Ludwig van Beethoven", "Peter Kay\\'s Car Share", "Orfeo ed Euridice", "Dirt track racing", "Frederick Barbarossa", "Karakalpak", "Walldorf, Baden-W\u00fcrttemberg", "Jack Ryan", "Campbellsville", "Shinjuku Eastside Square Building", "1933", "Delphi Lawrence", "Philadelphia", "December 13, 2015", "American financier who was Chairman and Chief Executive Officer of E. F. Hutton & Co. from 1970 to 1987", "Paradise, Nevada", "Russell T Davies", "four", "Marlborough, New Hampshire", "Alan Young", "2018\u201319 UEFA Europa League group stage", "Argentinian", "76,416", "Burning Man", "the president", "Ra\u00fal Eduardo Esparza", "October 30, 2017", "Live and Let Die", "The Village Vomit", "Olympus Mons", "Reggae legend Lucky Dube,", "destroyed and his business is shattered,", "the Internet", "a hockey mask", "Yes", "East Germany", "heart"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7034007352941176}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.11764705882352942, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4800", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-1263", "mrqa_naturalquestions-validation-321", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-7133", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-2853", "mrqa_searchqa-validation-15932", "mrqa_triviaqa-validation-3362"], "SR": 0.609375, "CSR": 0.5224056603773585, "EFR": 1.0, "Overall": 0.7138561320754717}, {"timecode": 53, "before_eval_results": {"predictions": ["badminton", "Swansea", "in the Outer Hebrides", "a muezzin", "nippon Sangyo", "tempera", "James Hogg", "Sarajevo", "Darby and Joan", "the Vietnam War", "Stanley Kubrick's Full Metal jacket", "Blur", "chicken", "Nelson Mandela", "Major General Charles Gordon", "white", "a dress", "grizzly bear", "bukwus", "a school of the Salesians of Buenos Aires", "rowing", "the end of March 1939", "Nowhere Boy", "President Gerald Ford", "Gorbachev", "Popeye", "John Key", "charlie brooker", "Northwestern University", "the Gulf of Mexico", "a lion", "Aceso", "dynamite", "Take That", "Jean Alexander", "Norman Brookes", "David Hockney", "La Toya Jackson", "Jimmy Carter", "Greek Home Management", "chile", "Edinburgh", "Today", "carrie", "Bolton", "Norwegian", "Super Bowl Sunday", "carpere", "the USSR\u2019s", "a teen", "a bear market", "Augustus Waters", "1967", "silk, hair / fur ( including wool )", "Jamaica", "Peter Kay\\'s Car Share", "Miller Brewing", "the country\\'s longest-serving ruler.", "more than two years,", "auction off one of the earliest versions of the Declaration of Independence,", "the tartan of the Argyle clan", "a cause, principle, or system of beliefs", "a ringmaster", "Unseeded Frenchwoman"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5334276149654439}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7368421052631577, 0.4, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-5884", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-7162", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-2385", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-498", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-5344", "mrqa_searchqa-validation-4662", "mrqa_newsqa-validation-3285"], "SR": 0.4375, "CSR": 0.5208333333333333, "EFR": 0.9444444444444444, "Overall": 0.7024305555555556}, {"timecode": 54, "before_eval_results": {"predictions": ["jerry lewis", "The Great Gatsby", "ford administration", "japan", "the First World War", "new york", "prince andrew", "duchess", "putin", "ford administration", "Appalachians", "Skylab", "jederson poulson", "Lady Antonia Margaret hartman", "shoe", "black Tuesday", "bores", "her Majesty The Queen", "alpha Orionis", "dicken\\'s Dream", "Swansea City", "krypton", "silurian", "meatloaf", "congregational", "j.M.W.", "The Lone Gunmen", "jewelled Easter eggs", "davy jesus", "at the north-west corner of the central business district", "wonderwall", "basketball", "carburetor", "Norway", "wheels", "germany", "Jean- Martin Charcot", "winged horse", "michael chaplin", "bathe", "sir peggotty", "Scotland", "germany", "arthur c. Clarke", "Buzz Aldrin", "power outage", "Napoleon Bonaparte", "fingers", "Blenheim Palace", "Rihanna", "kyshtym nuclear energy complex", "28 July 1914 to 11 November 1918", "Fix You", "the spectroscopic notation for the associated atomic orbitals", "Copa Airlines", "My Beautiful Dark Twisted Fantasy", "Sharman Joshi", "in hip and beauty.", "debris", "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry", "typhoid fever", "France", "the Edict of Nantes", "in Austin and Pflugerville"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5367931547619047}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-1536", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2312", "mrqa_triviaqa-validation-4349", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-5635", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5896", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-4795", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-7712", "mrqa_hotpotqa-validation-303", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-4060", "mrqa_searchqa-validation-5331", "mrqa_naturalquestions-validation-3995"], "SR": 0.46875, "CSR": 0.5198863636363636, "EFR": 0.9705882352941176, "Overall": 0.7074699197860962}, {"timecode": 55, "before_eval_results": {"predictions": ["aviva plc", "Venezuela", "Mozart", "anacapri", "2001: A Space Odyssey", "Catherine Cookson", "almonds", "ford administration", "Geneva", "wesley shatner", "peter paul Rubens", "Arabian Gulf", "river Rhine", "ascot", "seine", "muppets From Space", "sheryl Crow", "winnie Mae", "spartile", "come quietly", "children of Israel", "graphite", "Narragansett Bay", "Moby Dick", "The Scream", "gingerbread", "Cream of Manchester", "king john of England", "wellbeing", "raspberries", "france", "surfer", "oakum", "blancmange", "rochdale", "france penhaligon", "Black September", "510", "Germany", "shoe", "hydrogen", "fre Enterprise", "Professor Brian Cox", "Meow Mix", "7", "kidneys", "Bolivia", "jewish community", "Jordan", "Hans Lippershey", "india", "Christina Giles", "statistical advantage for the casino that is built into the game", "one towing a trailer, such that it resembles the acute angle of a folding pocket knife", "verve", "Japan", "the last living pilot of the X-15 program", "touma", "Madeleine K. Albright", "three empty vodka bottles,", "walrus", "curtis", "Liam Neeson", "a centaur"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5969381313131312}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.1818181818181818, 0.4, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4616", "mrqa_triviaqa-validation-5454", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1324", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-6866", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3275", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-3897", "mrqa_triviaqa-validation-2442", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6964", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5510", "mrqa_hotpotqa-validation-752", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3067", "mrqa_searchqa-validation-2027"], "SR": 0.546875, "CSR": 0.5203683035714286, "EFR": 1.0, "Overall": 0.7134486607142858}, {"timecode": 56, "before_eval_results": {"predictions": ["sarah rowe", "Judy Garland", "william hartnell", "friedrich Nietzsche", "Ben Affleck", "jamaican", "magical Mystery Tour", "Rio de Janeiro", "Cyclopes", "purple", "1929", "Oklahoma", "florence", "robbie coltrane", "georgia coltrane", "Antoine Lavoisier", "30th anniversary", "merkat", "tara", "Henri Rousseau", "united states", "united states", "sorrentian", "robbie coltrane", "kingdom plantae", "sweeny teddington bear", "tarn", "the A38", "tidal Bay", "robbie coltrane", "alastair Cook", "persian National Oil Company Inc.", "cribbage", "1960s", "north york", "LMFAO", "Emma Chambers", "kinks", "Tony Blackburn", "spain", "rebecca", "united states", "Pink Floyd", "robbie coltrane", "Spider-Man", "sorcer", "American Revolutionary War", "Tokyo", "hyphenated", "mono", "Augustus Caesar", "the south coast of eastern New Guinea", "Lady Gaga", "revenge and karma", "\"Secrets and Lies\"", "October 3, 2017", "Morris Barney Dalitz", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Unseeded Frenchwoman Aravane Rezai", "Kingman Regional Medical Center", "altrane", "the fairway", "Irish Republican Army", "2018"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5671875}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-188", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-4757", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-2743", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-3716", "mrqa_naturalquestions-validation-4719", "mrqa_hotpotqa-validation-4161", "mrqa_newsqa-validation-3287", "mrqa_searchqa-validation-16213", "mrqa_searchqa-validation-11582"], "SR": 0.546875, "CSR": 0.5208333333333333, "EFR": 1.0, "Overall": 0.7135416666666666}, {"timecode": 57, "before_eval_results": {"predictions": ["george tector Gorch", "Illinois", "Edward hopper", "robocop", "Pope Alexander VI", "Quentin Blake", "bazaar", "mardi gras", "new york", "Hamlet", "Jose Antonio Reyes", "george p Pearson", "finsbury Park", "hobbits", "Jordan", "Tangled", "So Far Away", "the United States", "crossword puzzle", "dennis taylor", "bofrot", "Robin Ellis", "sea shells", "dennis cooke", "War and Peace", "paphos", "three", "east of Eden", "de quincey", "zaragoza", "Debbie Abrahams", "brazil", "george vii", "Belgium", "A Christmas Carol", "bridge", "elliptical", "Koblenz", "george mktgpartners Canada", "blood", "zips", "isar", "Roman history", "mj\u00f6llnir", "Admiral Vernon", "florence", "woodstock", "crow", "nijinsky", "p Preston", "Sven Goran Eriksson", "the most senior position in the Bank of England", "B.J. Thomas", "65,535 bytes", "Prince Aimone of Savoy", "San Antonio", "Skogsr\u00e5", "the Obama administration", "Republicans", "some of the Awa", "Manhattan", "Patrick Henry", "Parkinson\\'s", "Eagles"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6112132352941178}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.11764705882352941, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1493", "mrqa_triviaqa-validation-3686", "mrqa_triviaqa-validation-4243", "mrqa_triviaqa-validation-910", "mrqa_triviaqa-validation-6237", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-1563", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-6016", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7378", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-4973", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-1787", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-2399", "mrqa_searchqa-validation-384"], "SR": 0.578125, "CSR": 0.5218211206896552, "EFR": 1.0, "Overall": 0.713739224137931}, {"timecode": 58, "before_eval_results": {"predictions": ["the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "10th Cavalry Regiment", "Hong Kong Mak\u00e9l\u00e9l\u00e9", "John Robert Cocker", "Connie Smith", "mountaineer", "\"Lonely\"", "Garrett Morris", "October 5, 1937", "18 January 1669", "Dizzy Dean", "Target Corporation", "British Labour Party", "Bandai", "Bill Ponsford", "Ward Bond", "Code#02Pretty pretty", "every Rose has its Thorn", "Cleveland Browns", "Jacking", "1901", "my Beautiful Dark Twisted Fantasy", "Broadcasting House in London", "20", "Amway", "Congo River", "Minneapolis", "Alemannic German", "illnesses", "XVideos", "1967", "1967", "pinball machine", "Lawrence of Arabia", "The Fault in Our Stars", "Gareth Jones", "head of the Cabinet of Bluhme I", "J35", "Scotty Grainger", "balloons Street, Manchester", "Somerset County, Pennsylvania", "Italy", "Psych", "Gateways", "Isfahan", "Veneto", "Empire Falls", "Robert FitzRoy", "Vernon L. Smith", "Dan Rowan", "Bohemia", "March 18, 2005", "1978", "Austria", "Switzerland", "the Treaty of Waitangi", "1930-1939", "a U.S. military helicopter", "African National Congress Deputy President Kgalema Motlanthe,", "new DNA evidence", "blue whale", "Christopher Darden", "books that are no longer being published", "bullfight"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6460069444444444}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.5, 0.5, 1.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.8, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-10135", "mrqa_triviaqa-validation-5517", "mrqa_newsqa-validation-1382", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-1481", "mrqa_triviaqa-validation-6175"], "SR": 0.546875, "CSR": 0.5222457627118644, "EFR": 1.0, "Overall": 0.7138241525423729}, {"timecode": 59, "before_eval_results": {"predictions": ["the first integrated circuit", "Oracle Corporation", "Levittown", "the Teenage Mutant Ninja Turtles", "seven", "Ashanti Region", "1934", "Cheshire County", "1980", "cricket fighting", "Dachshunds", "wild boar, and red, fallow and roe deer", "Duncan Kenworthy", "Potsdam", "the Netherlands", "the Continental Army", "various deities, beings, and heroes", "Henry Lau", "1", "Russian Empire", "The Catholic Church in Ireland", "people working in film and the performing arts", "Lykan HyperSport", "1989", "Gareth Barry", "1999", "Warner Animation Group", "Margarine Unie", "David Naughton", "A123 Systems, LLC", "Ian Fleming", "Golden Valley, Minnesota", "7 members", "an anvil", "50 Greatest Players in National Basketball Association History", "James G. Kiernan", "\"Dizzy\" Dean", "Magnus Carlsen", "BBC Focus is a British monthly magazine about science and technology published in Bristol, UK by Immediate Media Company", "Towards the Sun", "1958", "World War II", "Jenn Brown", "\"Glee\"", "Purdue University", "Indianapolis", "AC/DC are an Australian hard rock band, formed in Sydney in 1973 by brothers Malcolm and Angus Young", "Bury, Greater Manchester, England", "\"Agent Vinod\"", "Marxist and a Leninist", "George Timothy Clooney", "Laura Jane Haddock", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "1979", "camelopardalis", "Ghee is a hard fat that is obtained by heating butter made from the milk of a cow or a buffalo", "Wagner", "to step up.\"", "former Pakistani Prime Minister Benazir Bhutto,", "Dr. Christina Romete,", "\"Home on the Range,\"", "one meter", "Donnie Wahlberg", "1918"], "metric_results": {"EM": 0.59375, "QA-F1": 0.675505713649096}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false], "QA-F1": [0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0909090909090909, 0.0, 0.0, 0.1111111111111111, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-2810", "mrqa_hotpotqa-validation-1055", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-5966", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-4253", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-3068", "mrqa_searchqa-validation-12933", "mrqa_newsqa-validation-2789"], "SR": 0.59375, "CSR": 0.5234375, "EFR": 1.0, "Overall": 0.7140625}, {"timecode": 60, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4534", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4728", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-637", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-768", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-921", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-10651", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4312", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2415", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1403", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1768", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3432", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5228", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6536", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7606", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-997"], "OKR": 0.841796875, "KG": 0.50546875, "before_eval_results": {"predictions": ["September, 2004", "five minutes before commandos descended", "Arsene Wenger", "carving in the middle of our Mountain View, California, campus.\"", "having a devastating impact on the city's population causing enormous suffering and massive displacement.", "They are co-chair of the Genocide Prevention Task Force.", "allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring,", "4.6 million people", "sports cars", "Vicente Carrillo Leyva,", "discovery of \"a whole new treasure collection of fossils\"", "Communist Party of Nepal (Unified Marxist-Leninist)", "581 points", "Molotov cocktails, rocks and glass.", "1994,", "10 years in prison", "the Gulf", "25 percent", "Orbiting Carbon Observatory,", "then-Sen. Obama", "Claude Monet", "4,000", "apartment building", "Pennsylvania", "Former Mobile County Circuit Judge Herman Thomas", "Daytime Emmy Lifetime Achievement Award", "South Africa", "Barack Obama,", "dual nationality", "Knox's parents", "Cash for Clunkers", "will not answer questions.", "Jen and husband Bill Klein,", "prostate cancer,", "Zimbabwe", "Britain.", "thunderstorms.", "10 percent", "eight or nine young girls, some younger then 18,", "Jaipur", "cancer", "Mrs. Graham,", "salutes the \"People of Palestine\" and calls on them to fight back against Israel in Gaza.", "forgery and flying without a valid license,", "GeorgeWashington", "Sean,", "three out of four questioned say that things are going well for them personally.", "poems", "third", "environmental efforts", "South Africa", "2009", "William Jennings Bryan", "The neck", "Nicholas Garland", "Mozambique Channel", "Ede & Ravenscroft", "Adelaide", "punk rock", "Great Northern Railway", "Otis Elevator", "Iberian Peninsula", "George Balanchine", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.6426405885780886}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.1818181818181818, 0.26666666666666666, 0.0, 0.16, 0.8, 0.5, 1.0, 0.7142857142857143, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-56", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-2468", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-15121"], "SR": 0.515625, "CSR": 0.5233094262295082, "EFR": 1.0, "Overall": 0.7155212602459016}, {"timecode": 61, "before_eval_results": {"predictions": ["Donald Duck", "Iran's parliament speaker", "Department of Homeland Security", "18", "china", "Obama", "Casalesi Camorra clan", "managing his time.", "his club", "\"We are a nation of Christians and Muslims, Jews and Hindus -- and nonbelievers.\"", "$199", "collaborating with the Colombian government,", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "1,500", "Jada,", "200", "Karen Floyd", "Space shuttle Discovery,", "Brazil", "EU naval force", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "Harrison Ford", "Sunday", "28", "The Falklands, known as Las Malvinas", "New York City Mayor Michael Bloomberg", "Department of Homeland Security Secretary Janet Napolitano", "two", "Walking on ice in Alaska.", "30-minute", "338", "UNICEF", "eight", "Daniel Radcliffe", "Department of Homeland Security", "Portuguese water dog", "Al Nisr Al Saudi", "lightning strikes", "if he did cheat on you (and you didn't cheat back),", "Afghan lawmakers", "New York Philharmonic Orchestra", "Colombia", "247", "nearly 100", "1616.", "to sniff out cell phones.", "Casey Anthony,", "people are going to be focused now that we've [get] alternative views on how to move our country forward.", "2005", "root out terrorists within its borders.", "a point for Bayern Munich as the German Bundesliga leaders were held to a 1-1 draw by Cologne on Saturday.", "March 31 to April 8, 2018", "Baseball Writers'Association of America ( or BBWAA )", "\" Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "St Paul's Cathedral", "22", "Rome", "University of Vienna", "Dutch", "Naomi Campbell", "Earhart", "Thunder", "\"Mad Men\"", "the courts"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5380898805893968}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8823529411764706, 1.0, 0.0, 1.0, 0.33333333333333337, 0.2857142857142857, 0.4444444444444445, 1.0, 0.12500000000000003, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.07142857142857142, 0.0, 1.0, 0.1111111111111111, 0.5, 0.5882352941176471, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-417", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-2325", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2145", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4915", "mrqa_triviaqa-validation-1058", "mrqa_hotpotqa-validation-3500", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7617"], "SR": 0.4375, "CSR": 0.5219254032258065, "EFR": 0.9722222222222222, "Overall": 0.7096889000896057}, {"timecode": 62, "before_eval_results": {"predictions": ["Golden Valley, Minnesota", "an Emmy and four Academy Awards for Best Original Song", "small forward", "Molly Hatchet", "Araminta Ross", "Mach number", "eight", "August 17, 2017", "Al Capone", "Atomic", "St Augustine's Abbey", "Vilyam \"Willie\" Genrikhovich Fisher", "minister", "Carl Michael Edwards", "\"the most influential private citizen in the America of his day\"", "dance", "Standard Oil", "over 1.6 million passengers", "British Labour Party", "September 8, 2017", "Obafemi Akinwunmi Martins", "Charles Edward Stuart", "HackThis Site", "Steve Carell", "Saint Motel", "Melissa Rauch", "Flyweight", "Levon Helm", "Jean Acker", "the attack on Pearl Harbor", "Fountains of Wayne", "Nick Offerman", "Sam Raimi", "SAS", "Double Crossed", "Edmonton, Alberta", "8,211", "KXII", "Wikimedia Foundation", "Greek-American", "Mika H\u00e4kkinen", "Debbie Isitt", "Los Angeles", "1999", "Outside (magazine)", "United Nations Global Ambassador for the Food and Agriculture Organization", "Wojtek", "Edward I", "Los Angeles", "Cold Spring", "New York City", "Speaker of the House of Representatives", "13", "Shakespearean actresses and car salespeople", "an arrowhead", "Corin Redgrave", "Aaron", "UNICEF", "Bob Bogle,", "she was humiliated by last month's incident, in which she was forced to painful remove the piercings behind a curtain as she heard snickers from male TSA officers nearby.", "In the Heat of the Night", "Mead", "David", "6"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7597269917582418}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6153846153846153, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-5444", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-821", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-2993", "mrqa_hotpotqa-validation-298", "mrqa_naturalquestions-validation-839", "mrqa_triviaqa-validation-6039", "mrqa_newsqa-validation-390"], "SR": 0.65625, "CSR": 0.5240575396825398, "EFR": 1.0, "Overall": 0.7156708829365079}, {"timecode": 63, "before_eval_results": {"predictions": ["African National Congress", "Ronald Cummings", "five", "Bob Bogle", "Bob Bogle", "radical Muslim sheikh called for the creation of an Islamic emirate in Gaza,", "suppress the memories and to live as normal a life as possible;", "Caster Semenya", "Kandi Burruss,", "the BBC's central London offices", "Kgalema Motlanthe,", "as he tried to throw a petrol bomb at the officers,", "Karl Eikenberry", "left Brooklyn, New York, for Miami Beach, Florida,", "North Korea", "Elena Kagan", "Harrison Ford", "Christmas parade", "through a facility in Salt Lake City, Utah,", "Nearly eight in 10", "your ex's loved ones ask why", "2-1", "racial intolerance.", "acid", "1 km tall.", "part of the proceeds", "bicycles", "landed in Cameroon,", "Akshay Kumar", "August 19, 2007.", "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.", "should have met with the Dalai Lama.", "it does not grant full health-care coverage,", "of the test results", "know what's important in life,", "dancing with the Stars", "Brown-Waite", "are \"totaled,\"", "40 militants and six Pakistan soldiers dead,", "strife in Somalia,", "protest child trafficking and shout anti-French slogans", "colonel", "September 6, 1918,", "in the mouth.", "cancer", "Susan Atkins,", "137", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "for pulling on the top-knot of an opponent,", "at the University of Alabama in Huntsville,", "former U.S. secretary of state.", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "the temperature at which it becomes semi solid and loses its flow characteristics", "a paint consisting of pigment and glue size commonly used in the United States as poster paint", "gold hallmarks", "Quentin Tarantino", "uric Goldfinger", "Thomas Mawson", "Charles Ellis Schumer", "Peel Holdings", "Java", "contrite", "zodiac", "Harriet M. Welsch"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5659870465791518}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 0.0, 0.0, 1.0, 0.8205128205128205, 0.0, 0.13333333333333333, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-2287", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-10403", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-6865", "mrqa_hotpotqa-validation-3529", "mrqa_searchqa-validation-5471"], "SR": 0.453125, "CSR": 0.52294921875, "EFR": 1.0, "Overall": 0.71544921875}, {"timecode": 64, "before_eval_results": {"predictions": ["Abbot and Costello", "Great British Bake Off", "Gary Havelock", "actor", "Fiji", "Natty Bumppo", "Derek Jacobi, John Hurt and George Baker", "Guinea-Bissau, Senegal, Mali, C\u00f4te d'Ivoire", "Turnbull & Asser, Hawes & Curtis, Thomas Pink,", "Jon Stewart", "\"Barefoot Bandit\"", "ytterby", "fox-like", "lithium", "Boston Braves", "yokai", "Bette Davis", "1825", "argentina", "Lisieux", "Ascot", "morry morry", "Charlie Cairoli", "gavaskar", "The Blue Danube", "florida", "William Caxton", "Neil Armstrong", "a cocktail", "a brownish-black fossil fuel", "Dutch", "the Reform Club", "unite", "Saint Cecilia", "the Netherlands", "The Wizard", "pistil", "The World as Will", "Thomas Cranmer", "the Mad Hatter", "Nick Clegg", "Virginia", "Leonard Nimoy", "the largest buttock muscle", "Nikola Tesla", "Adrian Edmondson", "Turkey", "the innermost digit of the forelimb", "bavy", "J. S. Bach", "Bachelor of Science", "the lamina dura", "July 2014", "the French CYCLADES project directed by Louis Pouzin", "Massachusetts", "Princess Jessica", "supply chain management", "Dr. Jennifer Arnold and husband Bill Klein,", "2nd Lt. John Auer,", "that Birnbaum had resigned \"on her own terms and own volition.\"", "the Arctic", "Private Benjamin", "the Rhine & the Main", "Amber Heard"], "metric_results": {"EM": 0.46875, "QA-F1": 0.4835069444444444}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-1020", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-6566", "mrqa_triviaqa-validation-5536", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-1949", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5009", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-1171", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-1001", "mrqa_newsqa-validation-1862", "mrqa_searchqa-validation-7466", "mrqa_hotpotqa-validation-652"], "SR": 0.46875, "CSR": 0.5221153846153845, "EFR": 1.0, "Overall": 0.7152824519230768}, {"timecode": 65, "before_eval_results": {"predictions": ["hemlock", "ewan gordon mcgregor", "robert", "eyes", "spain", "david hockney", "sierra leone", "Preston", "sandown castle", "chile", "Maine", "Coalbrookdale", "borgia", "Periodic Table", "anton Ashe", "bread", "japan", "jakarta", "spike milligan", "Mitsubishi and Nakajima", "the Panama Canal", "1960", "crawford", "apples", "lug", "paul crawford", "Hamelin", "Harold II", "Kuwait", "leicestershire", "sprint", "green", "stanley grapewin (Grandpa Joad)", "Coldplay", "pamphlets, posters, ballads", "Rugrats", "chief of the Exchequer", "crawford", "fat", "Austria", "\"Inspector\"", "crawford", "\"Tom and Jerry\"", "Markus Aemilius Lepidus", "secretary", "candelabrum", "dennis antonovo", "Nikita Khrushchev", "blue ivy", "high fructose corn Syrup (HFCS)", "molecular structure of nucleic acids", "Schwarzenegger", "Wakanda", "the lamina dura", "William Henry Cosby Jr.", "Robert L. Stone", "Haitian Revolution", "sierra leone", "to launch a group that will serve as an alternative to the Organization of American States.", "64,", "kolkata", "crawford", "Anaheim", "Old English pyrige ( pear tree )"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5690508540372671}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 0.34782608695652173, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-1899", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-1228", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-3962", "mrqa_triviaqa-validation-4255", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6015", "mrqa_hotpotqa-validation-4757", "mrqa_newsqa-validation-3226", "mrqa_newsqa-validation-2224", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-16126", "mrqa_searchqa-validation-884"], "SR": 0.515625, "CSR": 0.5220170454545454, "EFR": 0.967741935483871, "Overall": 0.7088111711876832}, {"timecode": 66, "before_eval_results": {"predictions": ["Air NEXUS card", "severn taylor", "Buddhism", "Andrew Jackson", "The Bad Beginning", "rooney mary simon", "Red sea", "red", "unhulled seeds", "grizzly bear", "antonio crooisier", "Swiss", "The Pilgrim's Progress", "duke of Suffolk", "terence Edward \" Terry\" Hall", "butyl acetate", "San Francisco", "Paris", "sewing machine", "rosione", "rosagan", "anophthalmia", "robert gentlyfus", "dolly", "peter Principle", "video", "Frank McCourt", "Little jack Horner", "mark-girl", "blancmengier", "mark", "Louis XVIII", "stand-up", "\"Good Morning to All\"", "1992", "Pride & Prejudice", "william golding", "mark", "chronicles of narnia", "Mr. Brainwash", "calypso", "\"round-eyed\"", "Sch\u00e4dellehre", "mary Tudor", "stingy", "arthur", "driver", "Joan Rivers", "Mr. Humphries", "katherine mansfield", "heineken", "1987", "1996", "Ella Mitchell", "\"Wicked Twister\"", "Lerotholi Polytechnic Football Club", "3730 km", "mild to moderate depression", "Saturday", "\"green-card warriors\"", "Bering Sea", "Charlottetown", "Agatha Christie", "Yoko Ono Lennon, Olivia Harrison and Ringo Starr"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5143229166666666}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-3268", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-5164", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-3718", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6164", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-5391", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-1067", "mrqa_hotpotqa-validation-758", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-140", "mrqa_newsqa-validation-2128"], "SR": 0.46875, "CSR": 0.5212220149253731, "EFR": 0.9705882352941176, "Overall": 0.7092214250438981}, {"timecode": 67, "before_eval_results": {"predictions": ["Yuri Andropov", "michelle mcdonald", "jennifer mcern", "Camino Franc\u00e9s", "fox", "linda coren", "\"sound and light\"", "coffee", "tomato", "british", "wrought iron", "st Columba", "peter", "1215", "1937", "fifth", "michelle foot", "george IV", "british", "Venice", "adnams", "boston", "nikkei", "Nutbush", "robert wieck", "jape", "NASCAR", "Jordan", "linda evans", "llanfairpwllgwyngyll", "chicken Marengo", "darshaan", "o", "Nicaragua", "The Female Brain", "par", "Oklahoma", "Jason Bourne", "Venus", "aluminium", "carbohydrates", "antelope", "Nevada", "SW19", "Mizrahi Jews", "Eva Marie", "bobby", "b Burma", "wales", "london", "anquaman", "Justin Bieber", "2017 season", "eleven", "CD Castell\u00f3n", "East Knoyle", "Jan Kazimierz", "2.5 million copies,", "Vivek Wadhwa,", "Wednesday.", "parody", "Delta", "Lake Victoria", "nuclear warheads"], "metric_results": {"EM": 0.5, "QA-F1": 0.5505208333333333}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-565", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-6399", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-2021", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-7545", "mrqa_triviaqa-validation-1943", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-7710", "mrqa_hotpotqa-validation-5588", "mrqa_searchqa-validation-11044", "mrqa_newsqa-validation-1443"], "SR": 0.5, "CSR": 0.5209099264705883, "EFR": 0.90625, "Overall": 0.6962913602941176}, {"timecode": 68, "before_eval_results": {"predictions": ["fort boyard", "Richard Seddon", "16", "farmer phil archers", "stanolas cephalonia", "top cat", "fotheringhay castle", "tungsten", "New Zealand", "fenn street school", "stanley", "South Pacific", "klaine", "mozart", "\u03b8\u03ac\u03bb\u03b5\u03b9\u03b1", "paddy mcinness", "woodstock", "stan boyard", "chicago", "jack", "dog sport", "alfresco", "Sarajevo", "Hokkaido", "Norman Mailer", "david boyard", "florence", "apple", "braille", "PC", "st james", "george w", "Switzerland", "mozans", "pressure", "the Tower of London", "daniel ostroff", "peter boyard", "dr ichak adizes", "1936", "honda", "st jones", "Dunfermline", "cribbage", "midtown", "the Archive of American Folk Song", "stave", "osmium", "pear", "white Star Line", "elton john", "peptide bond", "William the Conqueror", "castle", "Gregory Carlton \" Greg\" Anthony", "\"Pete and Gladys\"", "Lowe's Companies, Inc.", "India", "Cash for Clunkers", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Nassau", "michelle jarre", "degauss", "10 Years"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6377604166666666}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-4762", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-3851", "mrqa_triviaqa-validation-5077", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-1862", "mrqa_triviaqa-validation-5611", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-7144", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-2760", "mrqa_naturalquestions-validation-3016", "mrqa_hotpotqa-validation-3119", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-3627", "mrqa_searchqa-validation-8095"], "SR": 0.546875, "CSR": 0.521286231884058, "EFR": 1.0, "Overall": 0.7151166213768116}, {"timecode": 69, "before_eval_results": {"predictions": ["victoria plum Brit", "Ronald Searle", "Steve Davis", "Loki Laufeyi Larson", "Avengers", "snakes", "insulin", "Lilac", "ryan stone", "laryngeal prominence", "Andes", "banshee", "Honolulu, Hawaii", "high-elevation", "heraldry", "good life", "jones", "british", "Sherlock Holmes", "Ida Tacke", "Dudley Do- right", "vindaloo", "Botswana", "Hep Stars", "mark Twain", "Holly Johnson", "steaks", "khaki uniforms", "birmingham", "joseph stillwell", "dunfermline athletic", "4", "joseph caiaphas", "penrhyn", "new South Wales", "African violet", "ourselves alone", "James Dean", "Eva Herzigov\u00e1", "drizzle", "chiropractic", "The Wicker Man", "stieg Larsson", "james woods", "bucatini", "berry university", "christopher berners- leonsis", "Croatia", "cete", "greyfriars", "Mr. chips", "John Locke", "the end of the 2015 season", "Matt Monro", "comic", "Disha Patani", "USS \"Enterprise\"", "Ben Freeth", "Kenneth Cole", "Donald Duck", "lunar eclipse", "Cher", "Black Sea", "Crank Yankers"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5885416666666666}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2531", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-4485", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1322", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-6456", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-7614", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-110", "mrqa_searchqa-validation-9522", "mrqa_searchqa-validation-14235"], "SR": 0.515625, "CSR": 0.5212053571428572, "EFR": 0.967741935483871, "Overall": 0.7086488335253456}, {"timecode": 70, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3054", "mrqa_hotpotqa-validation-306", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-975", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1461", "mrqa_squad-validation-147", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2564", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3473", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3923", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5884", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6670", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-6981", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7083", "mrqa_squad-validation-7094", "mrqa_squad-validation-7339", "mrqa_squad-validation-78", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-9002", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9344", "mrqa_squad-validation-9411", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1514", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5117", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6285", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.837890625, "KG": 0.49140625, "before_eval_results": {"predictions": ["the Philippines", "silurian", "ricky gervais", "big eggo", "quila", "Wimbledon", "perry pear", "lyon", "gold", "Tina Turner", "Sparks", "nissan", "washing", "ireland", "arvo p\u00e4rt", "Eric Coates", "st Pancras", "beer", "Toronto", "cevennes", "lady Gaga", "phil Glenister", "bcc", "1979", "Donald Trump", "1 tenth", "Tomorrow Never Dies", "tea", "stan getz", "Melbourne", "bullfighting", "Autobahn", "Kiss Me, Kate", "watches", "Hindenburg", "Andre Agassi", "sandra", "Tangled", "spanish", "Morrissey", "red stockings", "ooperatiou", "smallpox", "steerpike", "london", "violin", "nipples", "Sir Winston Churchill's", "Temple of Artemis", "acetophenone", "Achille Lauro", "Frank Langella", "anion", "a woman who had a sexual relationship with Paul", "Hilo", "Objectivism", "16,116", "CEO", "\u00a320 million ($41.1 million) fortune", "Eleven", "Harold M. Ickes", "bone marrow", "Smilla's Sense of Snow", "France's"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5647277661064426}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8235294117647058, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-4689", "mrqa_triviaqa-validation-5348", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-6736", "mrqa_triviaqa-validation-7655", "mrqa_triviaqa-validation-6183", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-5521", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-2633", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-2648", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-1591", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-2372", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2351", "mrqa_hotpotqa-validation-4382", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-772", "mrqa_searchqa-validation-15744", "mrqa_searchqa-validation-11038", "mrqa_newsqa-validation-299"], "SR": 0.484375, "CSR": 0.5206866197183099, "EFR": 0.9696969696969697, "Overall": 0.7061235928830559}, {"timecode": 71, "before_eval_results": {"predictions": ["31536000 seconds", "suez Canal", "robert boyle", "southampton", "Paris", "john poulson", "breadfruit", "1963", "r Richard Strauss", "spain", "sandi Tok svig", "lite le Vau", "robert my Davies", "bette davis", "Edinburgh", "ireland", "Arabah", "sandie shaw", "Ut\u00f8ya island", "lesley Garrett", "thomas hardin", "b\u00e4umer", "36 gallons", "sandstone", "Bristol Aeroplane", "charlie Harper", "anita Brookner", "a keyhole", "endometriosis", "philip yordan", "typographer", "eight", "Pizza Express", "Lilo & Stitch", "Hugh Quarshie", "billie holiday", "khadi boli", "st leger", "Eric Morley", "sandstone trail", "spain", "daniel day-Lewis", "steam engines", "ireland", "antelope", "relativistic mass", "james", "muskets", "bajan", "copper", "hoagland", "Jonny Buckland", "Stanley Tucci", "an African - American woman in her early forties", "teen volleyball", "Milk Barn Animation", "nursery rhyme", "Russian air force,", "President Obama and Britain's Prince Charles", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "Disko Troop", "15", "kayak", "Wisconsin"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4166666666666667}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.4, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.26666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-64", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-7290", "mrqa_triviaqa-validation-7061", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-4467", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-4048", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-4979", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-2128", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7629", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5018", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-10194", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5346", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1051", "mrqa_searchqa-validation-14649", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-3830"], "SR": 0.3125, "CSR": 0.5177951388888888, "EFR": 1.0, "Overall": 0.7116059027777777}, {"timecode": 72, "before_eval_results": {"predictions": ["Cambridge", "eyes", "Poland", "Washington, D.C.", "apple", "high jump", "a horizontal desire", "Hungary", "port Talbot", "pantagruel", "activewear", "halloween", "Sydney", "charlie chaplin", "smell", "russell", "judy holliday", "cupressaceae", "michael connelly", "Yellowstone", "blue ivy", "prince and philip russell", "Israel", "Wanderers", "1943", "Elizabeth Taylor", "daimler", "daniel", "Austria", "james hargreaves", "lady antonia fraser", "peter stuyvesant", "south africa", "dirk bikembergs", "bermany", "mark Twain", "surfer", "ever decreasing circles", "quito", "Sensurround", "russellberger", "sandown", "goat", "lady", "germany", "bb", "poenicus", "Mental Floss", "Kajagoogoo", "Carly Simon", "Doncaster Airport", "March 12, 2013", "a wood block struck by a rubber mallet, drench in studio reverb", "intermembrane space", "Ronnie Schell", "La Familia Michoacana", "Starlite", "the neighboring country of Djibouti,", "The cervical cancer vaccine,", "along the equator between South America and Africa.", "Warsaw", "City Slickers", "ex-wife", "Richa Sharma"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6409722222222223}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-1196", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-7446", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4215", "mrqa_triviaqa-validation-249", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-5471", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-180", "mrqa_hotpotqa-validation-1782", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2784", "mrqa_searchqa-validation-1212"], "SR": 0.59375, "CSR": 0.5188356164383562, "EFR": 1.0, "Overall": 0.7118139982876712}, {"timecode": 73, "before_eval_results": {"predictions": ["flesh and the Devil", "c\u00e9vennes", "Lilo and Stitch", "Tacitus", "George Best", "loki", "bagram collection point", "pink", "charlie chaplin", "ostrich", "ireland", "Louren\u00e7o marques", "silk", "swaziland", "cartoons", "Dracula", "jaws", "Dodo", "Imola", "album", "brazil", "Thailand", "United States", "worcester", "the Poincar\u00e9 conjecture", "Superman", "wales", "christian", "Madison Square Garden", "The Equals", "baffin", "Woodstock", "molybdenum", "permian", "Hungary", "Apollon", "Matterhorn", "gold hallmarks", "tide-wise", "genesis", "trumpet", "South Carolina", "Ourselves alone", "james chadwick", "coffee house", "Apocalypse Now", "fasting", "boris Becker", "althorp", "Pyrenees", "Noah", "Richmond, BC", "October 1, 2015", "Flamborough Head", "20 June 1990", "Jesper Myrfors", "Helsinki, Finland", "Rod Blagojevich,", "Mary Procidano,", "opium", "the War of the Spanish Succession", "Minnesota", "quid", "Ugly Betty"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7859375}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-80", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-1967", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-1499", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-3373", "mrqa_hotpotqa-validation-413", "mrqa_newsqa-validation-3632", "mrqa_searchqa-validation-7502"], "SR": 0.734375, "CSR": 0.5217483108108107, "EFR": 1.0, "Overall": 0.7123965371621621}, {"timecode": 74, "before_eval_results": {"predictions": ["jamaican", "steptoe and son", "jordan Baptiste le Roy", "rudolph", "assault on Precinct 13", "Compundyne", "Samson", "copenhagen", "selenium", "north of Cuba, northeast of Costa Rica, north of Panama and northwest of Jamaica", "bathtub curve", "john", "Japanese silvergrass", "macbeth", "Eton College", "geomagnetic field", "Diego Garcia", "lighthouse", "ritchie allsup", "russellius", "phobos", "Sphinx of Giza", "grande jordan", "william Morris", "pennsylvania state university", "Father Brown", "Henry Ford", "embraer EMB 120 Brasilia", "dihydrogen monoxide", "marius wilson", "russet", "alison Krauss", "raspberries", "4", "neurons", "Poland", "banjo", "cricket", "time bandits", "The Hague", "one foot in the grave", "philip abram", "copper", "E. T. A. Hoffmann", "speed camera", "self-actualized,", "blue", "passport", "florence", "fancy dress shop", "jabba", "in Yemen's Sufi monasteries", "1937", "turkey", "business", "boxer", "The Handmaid's Tale", "Stanford University", "Sgt. Barbara Jones", "because the Indians were gathering information about the rebels to give to the Colombian military.", "George F. Babbitt", "Emanuel Swedenborg", "nod", "Thorleif Haug"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5411458333333332}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-4919", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-1912", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-4433", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-3070", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-4603", "mrqa_triviaqa-validation-53", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-957", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-2830", "mrqa_hotpotqa-validation-2388", "mrqa_newsqa-validation-2236", "mrqa_searchqa-validation-13441", "mrqa_naturalquestions-validation-2509"], "SR": 0.4375, "CSR": 0.520625, "EFR": 0.9722222222222222, "Overall": 0.7066163194444444}, {"timecode": 75, "before_eval_results": {"predictions": ["rugby union", "hyperbole", "North by Northwest", "Danelaw", "Mahatma Gandhi", "for gallantry", "filibustering expeditions", "colette", "willow", "eurozone", "Separate Tables", "Percy Spencer", "Ulysses S. Grant", "1929", "aviva plc", "Antarctica", "hurt locker", "douglas mac MacArthur", "caribule", "zager and Evans", "c\u00e9vennes", "Genesis", "Jimmy Hoffa", "volleyball", "JeSuis Charlie", "gumm sisters", "dark blood", "lowestoft", "washington", "creation of man as taught in the Bible, and to teach instead that man has descended from a lower order of animals.", "lulu", "erinyes", "faggots", "Godwin Austen", "Angus deayton", "david bowie", "Chester Racecourse", "tchaikovsky", "faversham", "Jimmy Knapp", "Perseus", "germany", "anton\u00e9 de force", "butcher", "charlie habert", "priesthood", "violins", "charlie taylor", "eucalyptus", "1883", "harry of free enterprise", "3000 BC", "A lacteal", "Renishaw Hall", "The Jefferson Memorial", "96", "New Orleans, Louisiana", "JBS Swift Beef Company,", "Silicon Valley.", "10-person", "to kill Czar Alexander III", "beta blockers", "Yoko", "an oracle"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7448279867777151}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.75, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5856", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3649", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-378", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-1786", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-5109", "mrqa_naturalquestions-validation-10408", "mrqa_hotpotqa-validation-1123", "mrqa_newsqa-validation-1820", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-4936"], "SR": 0.65625, "CSR": 0.5224095394736843, "EFR": 0.8636363636363636, "Overall": 0.6852560556220095}, {"timecode": 76, "before_eval_results": {"predictions": ["Ginsburg", "propeller", "Joe Louis", "George Clooney", "Wyeth", "the Louvre", "feminist", "potatoes", "Wallace and Gromit", "anorthos", "Mozambique", "the Blue Nile", "troy", "\"Timber!\"", "teeth", "car leasing", "coconut", "Imaginext King Leonidis", "the Tsardom of Russia", "Profumo", "Finland", "Making the Band", "Abraham Lincoln", "Colorado", "stiaroscuro", "onomatopoeia", "the Library of Congress", "New Guinea", "stenka Razin", "Georgetown University", "kidney maladies", "maine", "Colin Colin Kaepernick", "Madison County", "Kennebunkport", "A Room with a View", "an eye", "Africa", "Ingnue", "Notre-Dame de Paris", "NIH dual review", "brownie brown", "paul mccartney", "the Iberian Peninsula", "bionic", "jeopardy", "baccarat", "Drums Along the Mohawk", "Wallis Warfield Simpson", "grapevine", "Coco Chanel", "December 14, 2017", "Virginia Dare", "related to the Common Germanic word guma ( Old English guma `` man '', Middle English gome )", "jesus", "the Panama Canal", "Will Smith", "1950", "\"Loch Lomond\"", "An aircraft", "Russia and China", "Alwin Landry's supply vessel Damon Bankston", "The Obama administration", "funchal"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6438988095238095}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.4, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-11059", "mrqa_searchqa-validation-5372", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-737", "mrqa_searchqa-validation-5637", "mrqa_searchqa-validation-9899", "mrqa_searchqa-validation-4056", "mrqa_searchqa-validation-6024", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-13006", "mrqa_searchqa-validation-12225", "mrqa_searchqa-validation-635", "mrqa_searchqa-validation-5273", "mrqa_searchqa-validation-15174", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-3019", "mrqa_triviaqa-validation-5219", "mrqa_hotpotqa-validation-2065"], "SR": 0.546875, "CSR": 0.5227272727272727, "EFR": 0.9655172413793104, "Overall": 0.7056957778213165}, {"timecode": 77, "before_eval_results": {"predictions": ["(Elvis) Presley", "Bob Fosse", "S", "Mexico", "Miles Davis", "volleyball", "Cuba", "Edwin Hubble", "Einstein", "Lhasa", "the Census Bureau", "New Kids on the Block", "Manila Bay", "Lady Chatterley\\'s lover", "molasses", "The Hard Knock Life", "a crumpet", "(Douglas) MacArthur", "Fred Thompson", "Sappho", "the Netherlands", "Texas", "(Victor) Hugo", "The Hippocratic Oath", "the Taliban", "Solidarity", "Kookaburra", "the Hastings", "(Tom) Tom", "Craftsman", "Shift", "W.H. Auden", "Chuck Berry", "Lou Gehrig", "the diaphragm", "the Marquis de Sade", "Louis Comfort Tiffany", "a tornado", "the joker", "New Zealand", "a glove", "Jutland", "Kindergarten", "tentacles", "Titanic", "San Francisco", "Gulliver's Travels", "a carriage", "Billy Bathgate", "Richmond", "steel", "the pop duo Boy Meets Girl", "Cheryl Campbell", "Ohio newspaper", "Joe Brown", "Funchel", "Virgil", "Timothy Dowling", "WANH", "near Bear Creek.", "FBI Special Agent Daniel Cain,", "A staff sergeant in the U.S. Air Force,", "\"procedure on her heart,\"", "55th district"], "metric_results": {"EM": 0.625, "QA-F1": 0.6739583333333332}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-7796", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-1687", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-1963", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14160", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-11728", "mrqa_searchqa-validation-3821", "mrqa_searchqa-validation-388", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-12470", "mrqa_searchqa-validation-10670", "mrqa_naturalquestions-validation-8528", "mrqa_naturalquestions-validation-6665", "mrqa_triviaqa-validation-1610", "mrqa_hotpotqa-validation-5500", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2547", "mrqa_hotpotqa-validation-5006"], "SR": 0.625, "CSR": 0.5240384615384616, "EFR": 1.0, "Overall": 0.7128545673076923}, {"timecode": 78, "before_eval_results": {"predictions": ["Flint, Michigan.", "a president who understands the world today, the future we seek and the change we need.", "about 5:20 p.m. at Terminal C", "Former Mobile County Circuit Judge Herman Thomas", "\"Top Gun\"", "Daniel Radcliffe", "a Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "Tennessee", "Intensifying", "Violentulation and asphyxiation and had two broken bones in his neck,", "Dubai", "gun", "Gov. Bobby Jindal", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "bankruptcy", "repression and dire economic circumstances.", "African National Congress", "1.2 million people.", "Haiti.", "police dogs", "Zuma", "dummies", "US Airways Flight 1549", "the estate with its 18th-century sights, sounds, and scents.", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "an auxiliary lock", "The Rosie Show", "Between 1,000 and 2,000 requests", "Tuesday,", "the Arctic north of Murmansk down to the southern climes of Sochi", "6-2 6-1", "fear of discomfort", "Diego Milito's", "\"Three Little Beers,\"", "romantic", "Biden", "military commissions", "Friday.", "a judge to order the pop star's estate to pay him a monthly allowance,", "consumer confidence", "Afghanistan and India", "Hugo Chavez", "austria", "want to protect ocean ecology, address climate change and promote sustainable ocean economies.", "bronze", "nine", "JBS Swift Beef Company, of Greeley, Colorado,", "as many as 250,000 unprotected civilians", "state senators", "Friday,", "Araceli Valencia,", "veil", "the chief lawyer of the United States government", "China in American colonies", "Arkansas", "Adam Smith", "sea level", "Manchester Victoria station in air rights space", "communist", "1896", "the ceiling", "Jonathan Swift", "John Molson", "sir Adrian Boult"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5528740376468493}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false], "QA-F1": [0.0, 0.9565217391304348, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.2222222222222222, 0.5217391304347826, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.3870967741935484, 0.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-4205", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-2026", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-642", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-7615", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-375", "mrqa_searchqa-validation-3681", "mrqa_searchqa-validation-15735", "mrqa_triviaqa-validation-5099"], "SR": 0.421875, "CSR": 0.5227452531645569, "EFR": 0.9459459459459459, "Overall": 0.7017851148221006}, {"timecode": 79, "before_eval_results": {"predictions": ["an Italian and six Africans", "Daniel Radcliffe", "the remaining rebel strongholds in the north of Sri Lanka,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Samoa", "BET", "$3 billion,", "Adam Yahiye Gadahn,", "mental health and recovery.", "75.", "co-wrote", "2005.", "12", "Lana Clarkson", "Iran", "attempted burgl stemming from a fatal encounter with police officer Daniel Enchautegui.", "70,000", "severe flooding", "56,", "frozen world located in the Gaslight Theater.", "AbdulMutallab", "anesthetic and sedative.", "Saturday's Hungarian Grand Prix.", "Sub-Saharan Africa", "Manny Pacquiao", "Aung San Suu Kyi", "Aniston, Demi Moore and Alicia Keys", "modern and classic designs", "Michael Schumacher", "Austin Wuennenberg", "1983", "the U.S. Holocaust Memorial Museum,", "the college campus.", "Elena Kagan", "\"fusion teams,\"", "misdemeanor", "Thursday", "the man facing up, with his arms out to the side.", "golf", "Israel handed the United Nations Friday a report", "prostate cancer,", "Iraqi and U.S. soldiers were attacked", "$1.45 billion", "people look at the content of the speech, not just the delivery.", "\"Iron Eyes Cody\"", "for strategy, plans and policy on the Army staff.", "walk", "\"wow.\"", "regulators in the agency's Colorado office", "Chancellor Angela Merkel", "\"Nothing But Love\"", "appearances", "Audrey II", "access to US courts", "the troposphere", "Arthur Ashe", "Hippo", "2004", "Tim \"Ripper\" Owens", "Brad Silberling", "Mother Vineyard", "Mars", "the Capitol", "Out - With"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7497217908902691}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2835", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-1706", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-4441", "mrqa_searchqa-validation-15009", "mrqa_naturalquestions-validation-582"], "SR": 0.671875, "CSR": 0.524609375, "EFR": 1.0, "Overall": 0.7129687499999999}, {"timecode": 80, "UKR": 0.669921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-512", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7035", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-984"], "OKR": 0.826171875, "KG": 0.51796875, "before_eval_results": {"predictions": ["Tyler \"Ty\" Mendoza", "\"Sara Crewe: or, What Happened at Miss Minchin's\"", "four", "professional footballer", "five", "Julie Taymor", "Greg Anthony", "Drifting (motorsport)", "Logar", "Rebirth", "President's Volunteer Service Award", "Harpe brothers", "Bedknobs and Broomsticks", "Dou Gray Scott, Jessica De Gouw and Martin McCann", "Yubin, Yeeun", "Herbert Ross", "Elena Verdugo", "melodic hard rock", "9 February 1971", "Chancellor of Austria", "Taylor Swift", "SARS", "son", "1345 to 1377", "Austro-Hungarian Army", "Noel", "India Today", "North Dakota", "2006", "\"Histoires ou contes du temps pass\u00e9\"", "Bernice Pauahi Bishop", "mixed martial arts", "Yarrow and Stookey", "Mathieu Kassovitz", "Dame Eileen June Atkins", "Summerlin, Nevada", "Jean- Marc Vall\u00e9e", "Klasky Csupo", "post\u2013World War II", "Prussia", "Newfoundland and Labrador", "Tom Kartsotis", "Knowlton", "shock cavalry", "Manhattan", "Professor Frederick Lindemann,", "dementia", "The 5 foot 9 inch tall twins", "seven", "January 2001", "\"Nina\"", "5 - 7", "40 %", "May 2017", "trumpet", "Top Cat", "G\u00e9rard Depardieu", "is a businessman, team owner, radio-show host and author.", "dozens", "14", "Rhizo", "Emperor Maximillian", "Lake Michigan", "The Full Monty"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7144108495670995}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.4, 1.0, 0.25, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-46", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3650", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-399", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-4131", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-190", "mrqa_newsqa-validation-781", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-6921"], "SR": 0.578125, "CSR": 0.5252700617283951, "EFR": 1.0, "Overall": 0.7078665123456791}, {"timecode": 81, "before_eval_results": {"predictions": ["No views", "the White Shadow", "Hungary", "HIV", "Nepal", "the second", "Sanjaya", "Fauvism", "Dresden", "Turkish", "the Shirley Temple Story", "flavor Flav", "Mike Nichols", "backcountry", "blue blood", "acetylene", "32", "Harriet the Spy", "Red Grange", "Cosmopolitan", "Amsterdam", "Grover Cleveland", "Clyde", "James Naismith", "Harold Godwinson", "North Carolina", "Job", "1969", "Take Me", "pickles", "Stand by Me", "Lead", "Nokia", "Malamud", "Cyprus", "the foot-marker", "Neil Diamond", "Munich", "Babe Ruth", "wildebeest", "Sicilian", "Pirates of the Caribbean", "tuna", "Arts and Crafts", "lm", "Subclue", "Uvula", "Biloxi", "Treasure Island", "Robots", "a hope chest", "comprehend and formulate language", "Robber baron", "NFL owners", "Jane Seymour", "Guy", "George Bernard Shaw", "Michael Greif", "Tom Ewell", "Wolfgang Amadeus Mozart", "House and Senate Republicans", "seven", "16", "is now a dad."], "metric_results": {"EM": 0.703125, "QA-F1": 0.7427083333333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-6682", "mrqa_searchqa-validation-6307", "mrqa_searchqa-validation-7539", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-14093", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-1314", "mrqa_searchqa-validation-14091", "mrqa_searchqa-validation-7545", "mrqa_searchqa-validation-15450", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-7174", "mrqa_hotpotqa-validation-2337", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4028"], "SR": 0.703125, "CSR": 0.5274390243902439, "EFR": 1.0, "Overall": 0.7083003048780487}, {"timecode": 82, "before_eval_results": {"predictions": ["Jaws 2", "the femur", "Ovid", "explosive test site at the New Mexico Institute of Mining & Technology", "a knight", "Tudor", "Australia", "sugar", "sheep", "Washington, D.C.", "lily", "Hammurabi", "the Isle of Wight", "gung ho", "Dale Earnhardt", "Johns Hopkins", "Saudi Arabia", "Lindsay Davenport", "North Africa", "the bass drum", "Stephen Hawking", "James Madison", "X-Ray", "Disturbia", "Michael Moore", "The Indianapolis 500", "I, Daniel Blake", "tapping", "an anchor", "Johannesburg", "carbon", "the Philistines", "Deep brain stimulation", "Louis Chevy", "Morocco", "Pe peanut Chocolate Candies", "Bosch", "Neil Diamond", "Cardinal Richelieu", "Malaysia", "a bionic limb", "Hamlet", "Lance Armstrong", "chicken Fried Steak", "Edith Wharton", "the Berlin Wall", "Uranus", "George Costanza", "a telephone operator", "a bonnet", "Henry Moore", "summer", "the 12 - year Great Depression", "Elena Anaya", "Brisbane Road", "Vinegar Joe", "austria", "Girl Meets World", "three", "\"Pour le M\u00e9rite\"", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony", "New York-based Human Rights Watch", "used cars", "Don Draper"], "metric_results": {"EM": 0.578125, "QA-F1": 0.666232638888889}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-734", "mrqa_searchqa-validation-7268", "mrqa_searchqa-validation-802", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-15533", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-14165", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-1283", "mrqa_searchqa-validation-13445", "mrqa_searchqa-validation-10309", "mrqa_searchqa-validation-14350", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-319", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8837", "mrqa_triviaqa-validation-5807", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2964"], "SR": 0.578125, "CSR": 0.5280496987951807, "EFR": 1.0, "Overall": 0.7084224397590362}, {"timecode": 83, "before_eval_results": {"predictions": ["Atlanta", "Dmitri Mendeleev", "calligraphy", "Duke Ellington", "Maria Sharapova", "Chile", "glow", "John Waters", "Aristophanes", "the Clean Air Act", "theist", "Yahoo", "Thurman Munson", "a barrel", "Chippewa", "Rooster Cogburn", "15", "Richard Burton", "gears", "Meringue pie Shell", "The Dying Swan", "the Big Bang", "winter", "Alyssa Milano", "Tahiti", "Herbert Hoover", "Keith Urban", "an isosceles triangle", "Nick Naylor", "Neil Armstrong", "the Netherlands", "Kelly Clarkson", "Michael Douglas", "an aquiline nose", "Troy weight", "Neil Simon", "the Candidate", "trespass", "Ronald Reagan", "Patrick Henry", "the incandescent light bulb", "war", "a cello", "an ostrich", "I love rock and roll", "the American Mind", "America", "Ziploc", "Hannibal", "Anne Wiggins Brown", "Beethoven", "Gene Barry", "Tachycardia", "American comedy - drama film directed by Fred Schepisi", "the American Civil War", "Mexico", "Renard", "India Today", "Distinguished Service Cross", "Genderqueer", "rape and murdering a woman in Missouri.", "Jeanne Tripplehorn", "the supreme court,", "New York Islanders"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7666666666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8000000000000002, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4451", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-7651", "mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-9041", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-15147", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16490", "mrqa_searchqa-validation-8851", "mrqa_searchqa-validation-9870", "mrqa_searchqa-validation-3237", "mrqa_searchqa-validation-6308", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-7158", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-435", "mrqa_newsqa-validation-3882"], "SR": 0.671875, "CSR": 0.5297619047619048, "EFR": 0.9523809523809523, "Overall": 0.6992410714285715}, {"timecode": 84, "before_eval_results": {"predictions": ["Hoffmann", "Ford", "Enzyme", "Grover\\'s Corner", "\"Tooth Decay\"", "topaz", "Universal City", "surrender", "Kathleen Winsor", "subtraction", "Harpy", "Macon, Georgia", "the count", "fur", "Titan", "the crossword clue", "quick picks", "Fulgencio Batista", "Yakutat Bay", "macram", "Toy Story", "the Black Maria", "the Ark of the Covenant", "the French Legion of Honour", "Granite", "Alan Shepard", "Klondike", "a dove", "the Jet Propulsion Laboratory", "Francis Scott Key", "Eminem", "Tarzan", "Diebold", "cheese", "Puncak Jaya", "Queen Latifah", "the Liberty Bell", "anchovies", "a saint", "Clarence Thomas", "the day of Mars", "nacreous", "the whimper", "Prison Break", "Iberian Peninsula", "the ceiling", "the kart", "Kilimanjaro", "the koala", "the Great Circus Parade", "Extradition", "1979", "Josh Flitter", "Allan Holdsworth", "pasta carbonara", "his finger", "robert schumann", "Pacific Place", "1941", "hump and Hampton\\'s line", "help the convicts find calmness in a prison culture fertile with violence and chaos.", "Polo", "Friday,", "2009"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5608382936507936}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.888888888888889, 0.2, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-10313", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-3217", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-14803", "mrqa_searchqa-validation-12869", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-12518", "mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-828", "mrqa_searchqa-validation-4630", "mrqa_searchqa-validation-8767", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-12943", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-7275", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-10541", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-5264", "mrqa_triviaqa-validation-7214", "mrqa_triviaqa-validation-7611", "mrqa_hotpotqa-validation-3149", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-1008", "mrqa_naturalquestions-validation-1856"], "SR": 0.46875, "CSR": 0.5290441176470588, "EFR": 0.9705882352941176, "Overall": 0.7027389705882353}, {"timecode": 85, "before_eval_results": {"predictions": ["15", "cancer", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "Picasso's muse and mistress, Marie-Therese Walter.", "a motor scooter", "Pixar's \"Toy Story\"", "supermodel", "Graeme Smith", "Missouri", "AbdulMutallab", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "between 5 and 10 knots an hour.", "At least 13", "detention of Immigration and Customs Enforcement accuse the agency in a lawsuit of forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "8 p.m.", "acid attack by a spurned suitor.", "Bowie", "\"The Kirchners have been weakened by this latest economic crisis,\" said Robert Pastor, who was a Latin America national security adviser for former President Carter.", "a number of calls,", "summer", "Tuesday afternoon.", "bowel syndrome,", "Molotov cocktails, rocks and glass.", "Sen. Evan Bayh of Indiana and Virginia Gov. Tim Kaine", "last April,", "2008", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "three", "the L'Aquila earthquake,", "88", "Cash for Clunkers", "$50 less", "cancerous tumor.", "American", "CNN's \"Piers Morgan Tonight\"", "several weeks,", "$1.4 million,", "next year", "the death of", "the 11th year in a row.", "explore the world on smaller scales than any human invention has explored before.", "Saturn owners", "dead", "11 healthy eggs", "Alfredo Astiz,", "drug cartels", "some of the Awa", "two", "Sabina Guzzanti", "400", "Robert Gates", "the 50 states of the United States of America", "Hanna Alstr\u00f6m", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "161-C-1", "Gaston Leroux", "Volkswagen", "\u00c6thelred I", "Robert \"Bobby\" Germaine, Sr.", "white and orange", "epiphyte", "Re- Animator", "a stride", "Girls' Generation"], "metric_results": {"EM": 0.5, "QA-F1": 0.6627319677871149}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.8, 0.11764705882352942, 1.0, 0.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.057142857142857134, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.9523809523809523, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3186", "mrqa_naturalquestions-validation-3918", "mrqa_triviaqa-validation-164", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3537", "mrqa_searchqa-validation-9547"], "SR": 0.5, "CSR": 0.5287063953488372, "EFR": 1.0, "Overall": 0.7085537790697675}, {"timecode": 86, "before_eval_results": {"predictions": ["cement", "the Cayman Islands", "orsche", "coax", "haiku", "waive", "China", "loverly", "economics", "Graceland", "funnel", "Beverly Hills", "coffee", "a live young chicken", "automobiles", "Isaac Newton", "Billy Budd", "Macbeth", "Communist", "Gene Krupa", "diamonds", "Cain", "Smashing Pumpkins", "krulle", "I", "\"Ma\" Barker", "Northanger Abbey", "Wyatt Earp", "Star Trek", "Mensa", "febreze", "a portrait", "MEN & WOMEN of LETTERS", "Philip Seymour Hoffman", "opni", "Wayne Gretzky", "the molar mass", "Michael Irvin", "Gap", "salt", "the Tower of London", "Arbor Day", "Westinghouse Electric Company", "a salad Dressing bottle", "The Fugitive", "Sisyphus", "Java", "Ponce de Leon", "bioluminescence", "Rococo", "the First Barbary War", "Pakistan", "961", "American country music artists Reba McEntire and Linda Davis", "dodo", "Northumberland", "Louis XVI", "July 16, 1971", "pastels and oil painting", "9", "monarchy's", "Africa", "Trump and Joan Rivers", "Jean-Claude Van Damme"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7205255681818181}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false], "QA-F1": [0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-8424", "mrqa_searchqa-validation-1300", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-6460", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-13389", "mrqa_searchqa-validation-14681", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-6245", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-13204", "mrqa_searchqa-validation-2161", "mrqa_searchqa-validation-5867", "mrqa_naturalquestions-validation-3672", "mrqa_hotpotqa-validation-680", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1586", "mrqa_hotpotqa-validation-4514"], "SR": 0.65625, "CSR": 0.5301724137931034, "EFR": 1.0, "Overall": 0.7088469827586207}, {"timecode": 87, "before_eval_results": {"predictions": ["Washington, Jay and Franklin", "no more than 4.25 inches ( 108 mm )", "internal reproductive anatomy", "the Mandate of Heaven", "eleven", "the fourth ventricle", "Cody Fern", "five", "in 2007 and 2008", "New York City", "Schadenfreude", "Indian Standard Time", "Midnight Mass in Rome", "Johannes Gutenberg", "The 256 - acre ( 1.04 km ; 0.400 sq mi ) campus", "Rocky Dzidzornu", "Jennifer Grey", "Experimental neuropsychology", "on the slopes of Mt. Hood in Oregon", "April 3, 1973", "Alan Tudyk as King Candy, the ruler of Sugar Rush", "administrative supervision", "1997", "Emma Watson", "near Flamborough Head", "Ki Toy Johnson", "the President of India", "John J. Flanagan", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "the language of the House bill, then agrees to the bill as amended", "Session Initiation Protocol", "ulcerative colitis", "presbyters", "Tessa Peake - Jones", "Katherine Allentuck", "September 19 - 22, 2017", "the 2001 -- 2002 season", "1773", "Randy VanWarmer", "senators", "Teri Garr", "imports from other countries through methods such as tariffs on imported goods, import quotas, and a variety of other government regulations", "13", "10.5 %", "Gene MacLellan", "from shore to shore", "Brad Dourif", "during a game in 1993", "Sanchez Navarro", "in the late 1970s", "23 September 1889", "My Fair Lady", "Armageddon", "Vietnam", "\u00c6thelstan", "Eugene", "\"Highwayman\"", "Israel", "Former Mobile County Circuit Judge Herman Thomas", "the abduction of minors.", "Antnio Guterres", "Erin Go Bragh", "Art Garfunkel", "three"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7375108284151919}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [0.4210526315789474, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8750000000000001, 0.5, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9189189189189189, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0, 1.0, 0.46153846153846156, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4571", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-4707", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3080", "mrqa_searchqa-validation-3012"], "SR": 0.65625, "CSR": 0.5316051136363636, "EFR": 0.9090909090909091, "Overall": 0.6909517045454545}, {"timecode": 88, "before_eval_results": {"predictions": ["2013", "the Triple Alliance of Germany, Austria - Hungary, and Italy", "John von Neumann", "Joanne Wheatley", "Dominic West, Walton Goggins, Daniel Wu, and Kristin Scott Thomas", "The island is a land mass ( smaller than a continent ) that is surrounded by water", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Mickey Rourke", "4.37 light - years ( 1.34 pc )", "eight hours ( UTC \u2212 08 : 00 )", "Jack Lord", "12 to 36 months old", "1988", "Malayalam", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law", "the Old Testament", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "Krypton", "Shawn Wayans", "a crust of mash potato", "a central place in Christian eschatology", "a hydrolysis reaction", "the employer", "publishing", "1973", "fresh nuclear fuel", "Nancy Jean Cartwright", "Germany", "total cost ( TC )", "Justin Timberlake", "1978", "geologist Charles Lyell", "1956", "minor key", "Thomas Middleditch", "Taittiriya Samhita", "Fix You", "2011", "during initial entry training", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Andy Kim", "2003", "Kirsten Simone Vangsness", "Ludacris", "light utility", "tissues in the vicinity of the nose", "Yuzuru Hanyu", "Felicity Huffman", "the RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "1974", "National Industrial Recovery Act ( NIRA )", "Mercury", "Bob Marley & the Wailers", "Quick Drying Woodstain", "actress and model", "between the 8th and 16th centuries", "Blue Origin", "2,700-acre", "to provide security as needed.", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "Akron", "words", "the Cherokee", "a hotly contested presidential challenge from MDC leader Morgan Tsvangirai in 2002 amid widespread accusations of vote rigging."], "metric_results": {"EM": 0.46875, "QA-F1": 0.6060360848896269}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.48275862068965514, 1.0, 0.7499999999999999, 0.6, 0.0, 0.5714285714285715, 1.0, 1.0, 0.7837837837837839, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.25, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 0.888888888888889, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5217391304347826, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2272", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-2908", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-5026", "mrqa_hotpotqa-validation-4797", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-129", "mrqa_searchqa-validation-11445", "mrqa_newsqa-validation-1133"], "SR": 0.46875, "CSR": 0.5308988764044944, "EFR": 0.9705882352941176, "Overall": 0.7031099223397225}, {"timecode": 89, "before_eval_results": {"predictions": ["Paul Lynde", "the naos", "in England", "420", "the fourth ventricle", "Emmett Lathrop `` Doc '' Brown", "Steve Russell", "if the occurrence of one does not affect the probability of occurrence of the other", "full '' sexual intercourse", "the Archies", "the government - owned Panama Canal Authority", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "Native American nation", "Jay Baruchel", "The Ed, Edd n Eddy animated television series", "the 1890s", "Thomas Hobbes in his Leviathan", "in the red bone marrow of large bones", "Sarah Silverman", "Janie Crawford", "in 1958", "3", "supervillains who pose catastrophic challenges to the world", "in 1932", "March 15, 1945", "reproductive", "the NFL", "living and organic material", "1975", "the British colonists", "March 1995", "Austin, Texas", "Muhammad", "July 21, 1861", "the Deathly Hallows", "the 1984 Summer Olympics in Los Angeles", "trunk code", "David Joseph Madden", "December 12, 2017", "8 December 1985", "British Indian Association", "Internal epithelia", "on the microscope's stage", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "The uvea", "he was unable to wrestling", "in 1971", "Moton Field, the Tuskegee Army Air Field", "1995 Mitsubishi Eclipse", "Beijing", "off - road vehicles", "Nowhere Boy", "The Cavern Club", "kidney", "Luigi Segre", "Ronald Lyle \" Ron\" Goldman", "Adrian Lyne", "543", "eight", "stay on track and get me through prison,\"", "a tank", "the Potomac", "Horn", "Basketball"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7651862026862026}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.4444444444444444, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-699", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-10250", "mrqa_triviaqa-validation-519", "mrqa_hotpotqa-validation-2410", "mrqa_newsqa-validation-2027", "mrqa_searchqa-validation-2590"], "SR": 0.65625, "CSR": 0.5322916666666666, "EFR": 0.9545454545454546, "Overall": 0.7001799242424243}, {"timecode": 90, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3812", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8093", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.814453125, "KG": 0.490625, "before_eval_results": {"predictions": ["Eriksson", "Tiananmen", "Colonel Sebastian charlie", "December 7", "frauds", "Chrysler", "nirvana", "Real Madrid", "york", "Norman Hartnell", "A Beautiful Mind", "Red Admiral", "The Meadows", "23.5\u00b0", "Verona", "Ishmael", "Let It Snow!", "Macbeth", "throw", "physics", "Poland", "Rapa Nui", "Peter Sellers", "fever", "milton", "Kuiper", "1955", "china", "wimpole", "fishes", "Independence Day", "French", "keane", "Nicolas Sarkozy", "The Princess bride", "mercury gilding", "Jack Ruby", "fat", "website", "Helen Gurley Brown", "australia", "Groucho", "Exile", "1664", "Shanghai", "Stieg Larsson", "five", "Manitoba", "Priam", "denise van Outen", "an argument", "775", "nasal septum", "September 1493", "Dealey", "four months in jail", "25 December 2009", "L'Aquila", "Janet and La Toya", "that he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "2C", "rain", "I Will Remember You", "Schwangau"], "metric_results": {"EM": 0.625, "QA-F1": 0.6885416666666666}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-3199", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-7092", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-4760", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-3004", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-5474", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-4546", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-3057"], "SR": 0.625, "CSR": 0.5333104395604396, "EFR": 0.875, "Overall": 0.683302712912088}, {"timecode": 91, "before_eval_results": {"predictions": ["The little dog", "lusitania", "The Times of India", "japan", "blind side", "korea", "Imola", "fifty-six", "Herald of Free Enterprise", "bridge", "norway", "le Leicester", "dr taylor", "ormolu", "yellow", "Burkina Faso", "mortadella", "Wembley", "phil archer", "The Telegraph", "palladium", "leander", "seussian Landsturm", "aluminium oxide", "dorset", "1803", "benito m Mussolini", "florence", "wildeve", "eye", "Donald Trump", "ruritania", "scarlet", "nelson", "lily alan", "germany", "Blofeld", "mozart", "flippers", "Cardiff", "one", "alberta", "de goya", "brawn", "zipporah", "carousel", "michael hordern", "Mary Poppins", "The Quatermass Experiment", "Benjamin Disraeli", "a cappella", "radioisotope thermoelectric generator", "cases that have not been considered by a lower court may be heard by the Supreme Court in the first instance", "June 2010", "Cielos del Sur S.A.", "is a tragedy", "Eastern College Athletic Conference", "one evening last week", "The Cycle of Life", "July 23.", "a tartar sauce", "Enron", "Mercury", "May 2010"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5760416666666667}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-684", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-3194", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-7358", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-409", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-1023", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-4865", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-2585", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2013", "mrqa_naturalquestions-validation-5300"], "SR": 0.484375, "CSR": 0.5327785326086957, "EFR": 1.0, "Overall": 0.7081963315217392}, {"timecode": 92, "before_eval_results": {"predictions": ["maarten tromp", "indonesia", "Brooklyn", "Arizona Diamondbacks", "Dan Dare", "rudolph", "fat", "Singapore", "bird", "stanie mary", "Hebrew", "heisenberg", "carlsberg", "Cumberland", "Billy Connolly", "spain", "Kiel Canal", "australia", "faggots", "mary square garden", "Jeffery deaver", "John Flamsteed", "pangram", "croquet", "kinks", "Spearchucker", "reservoirs", "Botticelli", "giraffe", "king urien", "Donatello", "South-West Africa", "blackfriars", "Patsy Cline", "pet Sounds by The Beach Boys", "riddick", "haute couture", "archer", "villefranche", "pashana Bedhi", "kiki", "sir john Major", "sheep", "Siberia", "Astronaut", "poirot", "three", "pariolis", "Harley", "barra Streisand", "kipps: The Story of a Simple Soul", "in the 2001 -- 2002 season", "the NFL", "Identity Theory", "Melbourne Storm", "A Rush of Blood to the Head", "1993", "1994", "Derek Mears", "11", "a truss", "Lake Titicaca", "lonsdale", "in an unauthorized party disclosed personally identifiable information and related credit card data, including information on 4,000 credit cards and the company's \"private client\" list, had been released."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6832589285714286}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-4506", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-606", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-3842", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-525", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-1601", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-1507", "mrqa_newsqa-validation-406", "mrqa_searchqa-validation-4717", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-6744", "mrqa_newsqa-validation-3021"], "SR": 0.578125, "CSR": 0.533266129032258, "EFR": 0.9259259259259259, "Overall": 0.6934790359916368}, {"timecode": 93, "before_eval_results": {"predictions": ["star", "jabbah", "spain", "japan", "james Garner", "denise richards", "flower", "kerry kitten", "dennis bannatt", "sinus node", "red", "darts checkout table", "Dutch", "republic of indonesia", "spanish", "giovanni bologna", "jeremy b Belushi", "gluteal region", "Majorca (Menorca)", "12", "carry on Cleo", "arthur borodin", "h Hector BERLIOZ", "king arthur", "spain", "st aidan", "john Virgo", "Richard John Seddon", "mole", "h Hercules", "Prince Andrew", "duke of parma", "cryonic suspension", "human", "puffer fish", "sodor", "madison purdie", "willie nelson", "giambologna", "frank whittle", "giambologna", "jupiter", "travolta\u2019s", "germany", "edward oscar", "Austria", "amessex county cricket club", "Kaiser Chiefs", "philistines", "nicolas cage", "philip catelinet", "an active supporter of the League of Nations", "April 1917", "Saint Alphonsa", "Takura Tendayi", "Whitesnake", "310", "Robert Barnett,", "voice-assistant software", "The switch had been scheduled for February 17,", "an orchid", "in the 1960s this U.N. organization raised the temples of Abu Simbel", "lira", "Joanna Moskawa"], "metric_results": {"EM": 0.5, "QA-F1": 0.5491950757575756}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2634", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-4429", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6771", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-4211", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-3276", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-1424", "mrqa_searchqa-validation-14194"], "SR": 0.5, "CSR": 0.5329122340425532, "EFR": 1.0, "Overall": 0.7082230718085106}, {"timecode": 94, "before_eval_results": {"predictions": ["a quarter tone", "Frida Khalo", "The Kite Runner", "Pope John Paul II", "Louisa May Alcott", "Rock Island", "Turandot", "the Bolsheviks", "cloning", "Signs", "Edward", "forgery", "the Police", "a carrots anlam", "Manhattan", "\"Rehab\"", "a ballpoint pen", "tap", "Ernie Banks", "Christopher Columbus", "Olivia Newton-John", "the Great White Way", "shrewd", "v.C. Andrews", "Peter Shaffer", "the Congo River", "(William) Harvey", "reptiles", "a gizzard", "Bangkok", "Reform", "Catwoman", "bats", "(Giacomo) Puccini", "Omaha", "the Monitor", "carbonate", "silver", "Gone Before", "the Takana", "the Silk Road", "dreams", "Google", "Jack Ruby", "take Me Out", "(William) Shakespeare", "a palace", "type O", "in late 2015", "green", "Engelbert Humperdinck", "1940", "three", "When Calls the Heart is a Canadian - American television drama series", "the Velvet Revolution", "taka", "Hampton Court Palace", "Marvel Comics", "three", "Donald Wayne Johnson", "Marcell Jansen", "At least 88 people had been hurt,", "African-Americans", "rocket"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7408854166666666}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-10265", "mrqa_searchqa-validation-5390", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-14368", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-14428", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-11073", "mrqa_naturalquestions-validation-4594", "mrqa_hotpotqa-validation-4578", "mrqa_newsqa-validation-2068"], "SR": 0.71875, "CSR": 0.5348684210526315, "EFR": 1.0, "Overall": 0.7086143092105263}, {"timecode": 95, "before_eval_results": {"predictions": ["Clifford Roberts", "a cobras", "New York", "Mexico", "Peter Paul Rubens", "Colonel (Tom) Parker", "Belgium", "Hakuin", "cholesterol", "his only begotten Son", "Dolphins", "a radar-guided surface-to-air missile", "a utensil", "Mar 11, 2010", "Northern Exposure", "Pocahontas", "Easy Rider", "the East River", "milk", "Ned Kelly", "Batavia", "the Cherokee Nation", "Jim Bunning", "a brood", "November 22, 1963", "Arby\\'s", "Einstein", "a bacterio", "(Victor) Hugo", "fudge", "grievous bodily harm", "a cattle prod", "Robert Fulton", "stimulation", "an egg", "Ken Russell", "The Crucible", "the United Healthcare Workers East", "the zenith", "apogee", "Calais", "the semaphore", "a sharp", "the Coors Field", "Edgar Rice Burroughs", "the Union", "foragers", "the former president of Liberia", "2W", "a monkey", "Kansas City", "1979", "The Hunger Games : Mockingjay -- Part 1 ( 2014 )", "Massachusetts", "Illinois", "South Africa", "Kenobi", "George Adamski", "five", "Marine Corps", "two remaining crew members", "the FDA's order to put the \"black box\" warning on Cipro and other fluoroquinolones, and also to warn doctors.", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "black"], "metric_results": {"EM": 0.546875, "QA-F1": 0.606650641025641}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.6923076923076924, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3709", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-4716", "mrqa_searchqa-validation-8919", "mrqa_searchqa-validation-16678", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9198", "mrqa_searchqa-validation-3964", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-7194", "mrqa_triviaqa-validation-3778", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3198"], "SR": 0.546875, "CSR": 0.5349934895833333, "EFR": 0.9655172413793104, "Overall": 0.7017427711925287}, {"timecode": 96, "before_eval_results": {"predictions": ["Flickr", "Van Allen", "air", "Leontyne Price", "a damselflies", "Charles I", "Casey Kasem", "San Juan", "a sheep", "The witches of Eastwick", "Joseph Smith", "the Black Death", "May", "aOreo", "Cyrano de Bergerac", "Alaska", "birds", "the European Union", "Verdi", "Matt Lauer", "the Kremlin", "Muqtada al-Sadr", "a Leopard", "heracles", "a bylaws", "England", "Austin", "the Sacred Cod", "Agatha Christie", "a new age", "Esther", "a Cat Scratch Fever", "New Kids on the Block", "the Tigris river", "country", "The Crucible", "Abraham Lincoln", "center of gravity", "Moriarty", "Simon Cowell", "potassium", "Lenin", "a fruitcake", "nests", "The Firebird", "Kansas", "a radical", "Air France", "Louis Brandeis", "a tooth", "David", "the 1979 -- 80 season", "Dollree Mapp", "a quarterback", "Dick Whittington", "Leeds", "gloster", "1828", "Elijah Wood", "Dizzy Dean", "The Ski Train", "Israel", "a signal", "skirts"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7359374999999999}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-2946", "mrqa_searchqa-validation-13498", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-14947", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-12287", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-6912", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-3093", "mrqa_hotpotqa-validation-4621", "mrqa_newsqa-validation-1176"], "SR": 0.640625, "CSR": 0.5360824742268041, "EFR": 1.0, "Overall": 0.7088571198453608}, {"timecode": 97, "before_eval_results": {"predictions": ["Louis", "a hoo-hoo", "Fear of Flying", "the War Admiral", "Abraham Lincoln", "the B horizon", "Donald Duck", "Czechoslovakia", "Roussimoff", "Maria Callas", "Buddhism", "Theodore Roosevelt", "a deluge", "Cold Mountain", "the Grand Guignol", "knead", "A Night at the Roxbury", "King Henry II", "the Claddagh Ring", "Keith Richards", "the Hydra", "(Barbara) King", "the Bronx Zoo", "Marcia Clark", "the Lincoln Tunnel", "the albatrosses", "Bob Fosse", "Dictum", "Georgia", "(Richard) Nixon", "Madame Tussaud", "Cloverfield", "Shakespeare", "Mother Jones", "Dr. King", "Hatfield and McCoy", "Walter Scott", "Pig Latin", "the Nile", "air traffic control", "mutton", "Latin", "1975", "Patrick Ewing", "Prague", "Genesis of Germs", "New Orleans", "Parody", "broccoli", "arteries", "Carol Burnett", "copper ( Cu )", "a legal case in certain legal systems", "9 February 2018", "18", "Robert Maxwell", "blue", "A Song of Ice and Fire", "British Labour Party", "Balvenie Castle", "Animal Planet", "St. Louis, Missouri.", "\"the strawberry,\"", "ITV"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6864583333333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-15628", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-15943", "mrqa_searchqa-validation-16528", "mrqa_searchqa-validation-11879", "mrqa_searchqa-validation-8337", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-4255", "mrqa_searchqa-validation-16590", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-8347", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-2490", "mrqa_triviaqa-validation-1589"], "SR": 0.59375, "CSR": 0.5366709183673469, "EFR": 1.0, "Overall": 0.7089748086734694}, {"timecode": 98, "before_eval_results": {"predictions": ["Franklin, Indiana", "Nelson", "47", "nathan Rothschild", "1988", "5 February 1976", "Boston Celtics", "Hermione Baddeley", "45,698", "Andries Jonker", "Ashanti Region of Ghana", "Groupe PSA", "Texas Tech University", "brigadier general", "Omega SA", "South Australia", "\"Apatosaurus\"", "Resorts World Genting", "the Beatles", "British", "1950", "six", "Future", "London", "Cuyler Reynolds", "New York City", "Toxics Release Inventory", "Figaro", "South African", "\"Apprendi v. New Jersey\"", "Bambi: Eine Lebensgeschichte aus dem Walde", "50 million", "Whoopi Goldberg", "Transporter 3", "Disco", "Duke Frederick", "Afghanistan", "thriller", "Antonio Lippi", "March 30, 2025", "English", "Azeroth", "Isabella II", "McG", "Vitor Belfort", "11 November 1821", "villanelle", "Emilia-Romagna", "Who\\'s That Girl", "Mickey's Christmas Carol", "Cristiano Ronaldo", "Virginia Dare", "David Tennant", "are said to be unattainable", "Simeon Williamson", "Baffin", "bobby riggs", "Alwin Landry's", "World leaders", "the Sri Lankan", "a hoist", "Bananas", "Tallahassee", "Brooke Wexler"], "metric_results": {"EM": 0.625, "QA-F1": 0.7161458333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5774", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-847", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-3420", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-1102"], "SR": 0.625, "CSR": 0.5375631313131313, "EFR": 1.0, "Overall": 0.7091532512626263}, {"timecode": 99, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5687", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-1037", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-1440", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15285", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15680", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3057", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1734", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2791", "mrqa_triviaqa-validation-2844", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.822265625, "KG": 0.4890625, "before_eval_results": {"predictions": ["Robert Arthur Mould", "(23 February 1922, Katowice \u2013 24 September 1999, New York City)", "Antonio Lippi", "1993", "Allies of World War I", "Logan International Airport", "1979", "an Academy Award in the category Best Sound", "The Suite Life of Zack & Cody", "Switzerland", "September 29, 2017", "Stern-Plaza", "Pakistan", "science fiction drama", "Marigold Newey", "Darkroom", "1972", "Royce da 5'9\" (Bad) and Eminem (Evil)", "evangelical Christian periodical", "Lionel Eugene Hollins", "water", "Harlem neighborhood", "Bardot", "Love Actually", "the Commanding General of the United States", "Fredric John Warburg", "five", "20 March to 1 May 2003", "1993", "imp My Ride", "1886", "the Swiss Confederation", "Syracuse", "Godspell", "1755", "The Big Bang Theory", "1966", "John Paul \"Johnny\" Herbert", "Annales de chimie et de physique", "British", "punk rock", "Atlantic Ocean", "Adelaide Laetitia \" Addie\" Miethke", "Red", "Theodore Robert Cowell", "Matthew Ryan Kemp", "Epic Records", "Roslyn Castle", "2.1 million members", "Rothschild banking dynasty", "Corendon Airlines", "Claudia Grace Wells", "Hugo Weaving", "Freedom Day", "Misery", "the Discovery", "geometry", "Adidas", "whether to close some entrances, bring in additional officers, and make security more visible.", "the 3rd Platoon, A Company, 2nd Light armored Reconnaissance Battalion", "Lebanon's", "the cherry almond", "the Queen\\'s Day", "bullnose"], "metric_results": {"EM": 0.625, "QA-F1": 0.7214285714285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-5238", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-1711", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-1822", "mrqa_hotpotqa-validation-735", "mrqa_naturalquestions-validation-9150", "mrqa_triviaqa-validation-1697", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1859", "mrqa_searchqa-validation-16845", "mrqa_searchqa-validation-14467", "mrqa_searchqa-validation-14664"], "SR": 0.625, "CSR": 0.5384375, "EFR": 1.0, "Overall": 0.7047187500000001}]}