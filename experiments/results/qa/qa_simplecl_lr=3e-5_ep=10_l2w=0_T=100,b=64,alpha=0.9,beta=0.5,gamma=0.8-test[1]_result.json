{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4140, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["April 20", "Paul Whiteman", "2005", "Islamism", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "the center of the curving path", "eight", "28", "global", "Fresno", "Amazonia: Man and Culture in a Counterfeit Paradise", "88%", "broken wing and leg", "Emmy Awards", "silt up the lake", "The TEU specifically excludes certain regions, for example the Faroe Islands, from the jurisdiction of European Union law", "legitimate medical purpose", "week 7", "Kromme Rijn", "Robert Boyle", "five", "Paul Samuelson", "The First Doctor encounters himself in the story The Space Museum", "until the age of 16", "80%", "Five", "threatened \"Old Briton\" with severe consequences", "less than 200,000", "Anheuser-Busch InBev", "Charles River", "Veni redemptor gentium", "northwestern Canada", "intracellular pathogenesis", "1998", "seven months old", "The Service Module was discarded", "July", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "Article 17(3)", "Doctor Who \u2013 The Ultimate Adventure", "Eric Roberts", "electricity could be used to locate submarines", "1910\u20131940", "the owner", "2.666 million", "September 1969", "Apollo Program Director", "dreams", "Charles I", "more than 1,100 tree species", "complete the modules to earn Chartered Teacher Status", "true larvae", "apicomplexan-related diseases", "Rev. Paul T. Stallsworth", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "2009", "Daniel Diermeier", "PNU and ODM camps", "136", "12 to 15 million", "flax", "February 1, 2016", "2001", "lipophilic alkaloid toxins"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8867121848739495}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6614", "mrqa_squad-validation-4170", "mrqa_squad-validation-6426", "mrqa_squad-validation-8037", "mrqa_squad-validation-7774", "mrqa_squad-validation-3885", "mrqa_squad-validation-1492", "mrqa_squad-validation-5788", "mrqa_squad-validation-4343"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["Carolina Panthers", "1530", "Gallifrey", "ca. 2 million", "intractable problems", "effective planning", "pyrenoid and thylakoids", "Canterbury", "1873", "A bridge", "clergyman", "Commission v Italy", "sports tourism", "G", "eating both fish larvae and small crustaceans", "slow to complete division", "Astra's", "T. T. Tsui Gallery", "1905", "theatres", "those who proceed to secondary school or vocational training", "1521", "Search the Collections", "CD8", "3 January 1521", "a bill", "the University of Aberdeen", "2014", "Missy", "US$10 a week", "Super Bowl City", "Dudley Simpson", "esoteric", "temperature and light", "4000 years", "new entrance building", "Levi's Stadium", "tutor", "electricity", "2007", "Los Angeles", "zeta function", "adviser", "over $40 million", "Sunday Service of the Methodists in North America", "Lead fusible plugs", "occupational stress", "Synthetic aperture radar", "European Parliament and the Council of the European Union", "the coronation of Queen Elizabeth II", "O(n2)", "the Main Quadrangles", "35", "quadratic time", "antisemitic", "over-fishing and long-term environmental changes", "the Onon River and the Burkhan Khaldun mountain", "Hassan al-Turabi", "robbery", "Arlen Specter", "it is Oprah's daughters", "Venus Williams", "prisoners", "a globose pome"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7448529411764706}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8691", "mrqa_squad-validation-3958", "mrqa_squad-validation-4648", "mrqa_squad-validation-8864", "mrqa_squad-validation-9454", "mrqa_squad-validation-7818", "mrqa_squad-validation-1272", "mrqa_squad-validation-2122", "mrqa_squad-validation-1530", "mrqa_squad-validation-9977", "mrqa_squad-validation-1818", "mrqa_squad-validation-2525", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-808", "mrqa_triviaqa-validation-5325"], "SR": 0.71875, "CSR": 0.7890625, "EFR": 1.0, "Overall": 0.89453125}, {"timecode": 2, "before_eval_results": {"predictions": ["it developed into a major part of the Internet backbone", "The best-known legend", "Egyptians", "quantity surveyor", "\"missile gap\"", "Tanaghrisson", "1852", "1564", "Concentrated O2", "adenosine triphosphate", "300 men", "11.5 inches (292.1 mm)", "1964", "infected corpses", "CD4", "the Lutheran and Reformed states in Germany and Scandinavia", "Chinggis Khaan International Airport", "modern buildings", "the warmest months from May through September, while the driest months are from November through April", "NBC", "Three", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Gymnosperms", "to punish Christians by God", "Word and Image department", "Nafzger", "MPEG-4", "BSkyB", "chromalveolates", "embroidery", "three", "26", "Duran Duran", "The Mallee and upper Wimmera are Victoria's warmest regions with hot winds blowing from nearby semi-deserts", "long distance services", "chloroplasts of C4 plants", "the violence that subsequently engulfed the country", "primality", "divergent boundaries", "the Chancel Chapel", "Kurt H. Debus", "the construction of military roads", "In bays where they occur in very high numbers", "Dallas", "Ted Heath", "late 14th-century", "high-voltage", "contract", "Arabic numerals", "pamphlets on Islam", "draftsman", "1993\u201394", "$10 billion", "France", "20", "Iran", "the Koreans edge into second place in Asian qualifying Group 2 to finish ahead of Saudi Arabia on goal difference and seal their place in the finals", "the results by a chaplain about 1:45 p.m., per jail policy", "56", "school", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League with a 3-2 victory", "AbdulMutallab, accused of trying to detonate an explosive device in his underwear", "coca wine", "Dissection"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7705798796791443}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.23529411764705882, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0909090909090909, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4847", "mrqa_squad-validation-3088", "mrqa_squad-validation-3480", "mrqa_squad-validation-8905", "mrqa_squad-validation-3270", "mrqa_squad-validation-7211", "mrqa_squad-validation-1909", "mrqa_squad-validation-2565", "mrqa_squad-validation-9379", "mrqa_squad-validation-2906", "mrqa_squad-validation-2899", "mrqa_squad-validation-4322", "mrqa_squad-validation-2291", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1201"], "SR": 0.703125, "CSR": 0.7604166666666666, "EFR": 1.0, "Overall": 0.8802083333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["\"vanguard of change and Islamic reform\"", "less than a year", "dampening the fire", "187 feet (57 m)", "Zagreus", "Swynnerton Plan", "extra-legal", "Harvey Martin", "December 1895", "lion, leopard, buffalo, rhinoceros, and elephant", "Establishing \"natural borders\"", "drama series", "17 years", "sold", "income inequality", "pastor", "1534", "mesoglea", "20%", "Isaac Newton", "Miocene", "\"citizenship", "13", "force model", "Super Bowl City", "three", "Georgia", "Fort Caroline", "Horace Walpole", "Afrikaans", "adaptive immune system", "24", "applications such as on-line betting, financial applications", "United Kingdom", "issues related to the substance of the statement", "2007", "Luther's anti-Jewish works", "short-tempered and even harsher", "any member of the Scottish Government", "two catechisms", "orientalism and tropicality", "John Dobson", "Super Bowl 50", "Beryl", "prima scriptura", "the Mongol Empire", "\"LOVE Radio", "\u201cLady\u201d or a \u201cWoman\u201d", "The Armoury Inn", "\"The Mullen \u00ef\u00bf\u00bd\"", "cutis anserina", "Bogota", "Artemis", "malaxis paludosa", "2.8 magnitude", "pilot, author and broadcaster", "\"Cup of tea!\"", "South Africa", "\"Crying\"", "Dorset", "The Daily Mirror", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Paul Lynde", "adenosine diphosphate"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7475160256410256}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-10466", "mrqa_squad-validation-3139", "mrqa_squad-validation-2486", "mrqa_squad-validation-433", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-5104"], "SR": 0.71875, "CSR": 0.75, "EFR": 0.8888888888888888, "Overall": 0.8194444444444444}, {"timecode": 4, "before_eval_results": {"predictions": ["four days", "the International Fr\u00e9d\u00e9ric Chopin Piano Competition", "Nuda", "Erg\u00e4nzungsschulen", "21", "The Christmas Invasion", "vertebrates", "Miocene", "Robert Underwood Johnson", "Cuba", "the Horniman Museum", "the ability to pursue valued goals", "University of Erfurt", "curved", "Johann Sebastian Bach", "1524\u201325", "in compensation for Spain's loss to Britain of Florida (Spain had ceded this to Britain in exchange for the return of Havana, Cuba)", "by using net wealth (adding up assets and subtracting debts)", "2009", "British", "2005", "Germany and Austria", "self-starting", "two tumen", "international metropolitan region", "Henry Laurens", "projects sponsored by the National Science Foundation (NSF)", "Go-Ahead", "entertainers", "second-largest", "biologist", "upper sixth", "arrested", "Pakistan", "1185", "Central business districts", "Hisao Yamada", "the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "cornwall", "organisms", "co-NP", "between 1.4 and 5.8 \u00b0C above 1990 levels", "cornwall", "cornwall", "conce conce conce", "cornwall", "cornwall", "rob Cantrell", "lotharingen", "ag", "in the car with her mother, Mrs. Har-", "cornwall", "conce conceals", "cornwall", "cornane", "robin", "agdel", "chihiro", "Gavin MacLeod", "cornwall", "Skat", "agotiable", "Christopher Nolan", "gun"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6118665099268548}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060606060606060615, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-680", "mrqa_squad-validation-10145", "mrqa_squad-validation-7554", "mrqa_squad-validation-6046", "mrqa_squad-validation-4848", "mrqa_squad-validation-5374", "mrqa_squad-validation-8372", "mrqa_squad-validation-6279", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-9913", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-4510", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-2062", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-13775", "mrqa_triviaqa-validation-1816"], "SR": 0.59375, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 5, "before_eval_results": {"predictions": ["the wedding banquet", "respiration", "a deficit", "$216,000", "City of Malindi", "Apollo Applications Program", "trial division", "the mid-sixties", "An attorney", "eight", "The Book of Discipline", "Olivier Messiaen", "1759-60", "The Rankine cycle", "deadly explosives", "first half of the eighteenth century", "5,560", "Tricia Marwick", "the main contractor", "\"ctenes", "third most abundant chemical element", "an adjustable spring-loaded valve", "agriculture", "metamorphosed", "David Suzuki", "if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "June 1979", "4.95 mL", "December 12", "second half of the 20th Century", "Ten", "the edge railed rack and pinion Middleton Railway", "John Elway", "1598", "The Eleventh Doctor", "Tugh Temur", "\"War of Currents", "dampening the fire", "five", "the Dalai Lama", "\"This highly honed skills are used daily in conflict zones such as Iraq and Afghanistan.\"", "a multibillion dollar arms deal", "the Dalai Lama", "gas-down on the ground", "2-0", "Chesley \"Sully\" Sullenberger", "Stanford", "issued his first military orders", "July", "women", "McChrystal", "the Bronx", "Beijing", "Fernando Gonzalez", "sportswear", "Jennifer Arnold and husband Bill Klein", "suspend all aid operations", "the sins of the members of the church", "humans", "a simple majority vote", "George Best", "November 6, 2018", "Mahler Symphonies", "March 19, 2017"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8278128338675215}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3559", "mrqa_squad-validation-3555", "mrqa_squad-validation-3179", "mrqa_squad-validation-383", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-539", "mrqa_naturalquestions-validation-8633", "mrqa_searchqa-validation-6817"], "SR": 0.765625, "CSR": 0.7265625, "EFR": 0.9333333333333333, "Overall": 0.8299479166666667}, {"timecode": 6, "before_eval_results": {"predictions": ["John Mayow", "he was profoundly influenced by a math teacher Martin Sekuli\u0107.:32", "1 July 1851", "1562", "32.9%", "the architect's client and the main contractor", "2009", "Katy\u0144 Museum", "Brandon Marshall", "silicates", "methotrexate or azathioprine", "over 100,000", "3.55 inches", "Bukhara", "Beyonc\u00e9", "its safaris", "Huntington Boulevard", "Northern Pride Festival", "eight", "Henry Laurens", "Roone Arledge", "the Florida legislature", "autoimmune", "Diarmaid MacCulloch", "permafrost", "University Athletic Association", "idolatry", "a model for the practice of clerical marriage, allowing Protestant clergy to marry.", "a suite of network protocols", "German creedal hymn", "21 to 11", "eight", "\"Guilt implies wrong-doing.", "1130", "15", "the 13 stripes represent the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "2002", "The North Pole is the northernmost point on the Earth, lying diametrically opposite the South Pole.", "fovea centralis", "God is also sometimes spelled Molech, Milcom, or Malcam.", "Will", "MGM Resorts International", "Bobby Darin", "Gene MacLellan", "Matt Monro", "Chilka Lake is a brackish water lake along the eastern coastal plain. It lies in the state of Odisha and stretches to the south of the Mahanadi Delta.", "having moved to the division in 2013 after spending their first 51 seasons in the National League ( NL ).", "Wisconsin", "Jonathan Breck", "Neil Young", "cutting surfaces", "hot enough that light in the form of either glowing or a flame is produced.", "de jure racial segregation was ruled a violation of the Equal Protection Clause of the Fourteenth Amendment of the United States Constitution.", "a humid subtropical climate, with hot summers and mild winters. On average, between 40 inches ( 1,000 mm )", "Sir Alex Ferguson", "Bachendri Pal", "It has been a part of the Winter Dew Tour, the World Skiing Invitational, and the inaugural 2012 Winter Youth Olympics.", "1979", "Manet", "Justin Spitzer", "Dr. Paul Appelbaum,", "Resting on or touching the ground or bottom", "a lung disease that causes coughing, wheezing, shortness of breath, and other symptoms.", "Jim Inhofe"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6809524293301867}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5333333333333333, 0.6153846153846153, 1.0, 0.5, 1.0, 0.0, 1.0, 0.15384615384615385, 0.9189189189189189, 0.5, 0.125, 0.8, 0.1111111111111111, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.72, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1166", "mrqa_squad-validation-8400", "mrqa_squad-validation-6223", "mrqa_squad-validation-2118", "mrqa_squad-validation-4673", "mrqa_squad-validation-978", "mrqa_squad-validation-6913", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-4865", "mrqa_newsqa-validation-130", "mrqa_searchqa-validation-10794", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-9370"], "SR": 0.59375, "CSR": 0.7075892857142857, "EFR": 0.9615384615384616, "Overall": 0.8345638736263736}, {"timecode": 7, "before_eval_results": {"predictions": ["The Hoppings", "Chicago", "some scholars, notably Ratchnevsky, have commented on the possibility that Jochi was secretly poisoned by an order from Genghis Khan.", "Inherited wealth", "\"There is a world of difference between his belief in salvation and a racial ideology.", "the StubHub Center", "WMO Executive Council and UNEP Governing Council", "trial division", "1999", "Gian Lorenzo Bernini", "densely occupied", "dendritic cells, keratinocytes and macrophages", "Newcastle Mela", "ambiguity", "1665", "The Day of the Doctor", "punts", "1.6 kilometres", "\"winds up\" the debate by speaking after all other participants.", "2010", "around 100,000", "thermal expansion", "John Sutcliffe", "tight end Owen Daniels", "incitement to terrorism", "Brookhaven", "A construction project", "Roy", "17 February 1546", "If (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people),", "September 30, 1960", "Brian Steele", "the status line", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "callable bonds", "left coronary artery", "nine", "Havana Harbor", "three", "1932", "Walter Mondale", "Mitch Murray", "Marie Fredriksson", "December 1, 2009", "virtual reality simulator", "Noah Schnapp", "Antonio Banderas", "a pair of compasses", "Kanawha River", "Julie Stichbury", "in consistency and content", "William J. Bell", "Spanish explorers", "September 2017", "Krypton", "1990", "Russia", "a member of the subfamily Satyrinae in the family Nymphalidae", "Murcia", "India", "Franz Xaver Mozart", "Armin Meiwes", "Mot\u00f6rhead", "President Bill Clinton"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6757240774216664}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.08, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.8372093023255813, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-6294", "mrqa_squad-validation-2608", "mrqa_squad-validation-8910", "mrqa_squad-validation-5189", "mrqa_squad-validation-6402", "mrqa_squad-validation-9465", "mrqa_squad-validation-4385", "mrqa_squad-validation-6205", "mrqa_squad-validation-797", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3182", "mrqa_triviaqa-validation-2674", "mrqa_searchqa-validation-5845", "mrqa_hotpotqa-validation-3034"], "SR": 0.59375, "CSR": 0.693359375, "EFR": 1.0, "Overall": 0.8466796875}, {"timecode": 8, "before_eval_results": {"predictions": ["Prague", "humidity", "Manning", "Sports Night", "2nd century BCE", "taxation", "punish", "limit the supply of workers", "the Ilkhanate", "a second Gleichschaltung", "1864", "NL and NC", "fifty", "Albert Einstein", "discarded", "state or government schools", "ash tree", "Commission v France", "purposed to remove random noise and camera shake without destroying historical legitimacy", "1689", "adaptive immune system", "case law by the Court of Justice", "Von Miller", "mad scientist", "Dendritic cells", "Allston Science Complex", "writing a five volume book in his native Greek \u03a0\u03b5\u03c1\u03af \u03cd\u03bb\u03b7\u03c2 \u03b9\u03b1\u03c4\u03c1\u03b9\u03ba\u03ae\u03c2", "wars", "in soils", "solve South Africa's'' ethnic problems", "Hirschman", "retirement", "Sergeant Himmelstoss", "four", "bohrium", "a major fall in stock prices", "Buddhism", "March 31", "Blind carbon copy to tertiary recipients who receive the message", "start fires, hunt, and bury their dead", "Ray Charles", "a play about a man whose choice to send out faulty airplane parts for the good of his business and family", "omitted and an additional panel stating the type of hazard ahead", "Valene Kane", "continues the pre-existing appropriations at the same levels as the previous fiscal year", "Arnold Schoenberg", "Tiffany Adams Coyne", "Trace Adkins", "Evermoist", "Notts County", "July 23, 2016", "Auburn Tigers", "Kate '' Mulgrew", "1943", "Rumplestiltskin", "Kryptonite", "convert single - stranded genomic RNA into double - stranded cDNA", "Santiago", "Japan", "Lance Cpl. Maria Lauterbach", "The Demon Barber of Fleet Street", "californium", "Stradivirius", "an isosceles triangle"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6369664156416721}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9166666666666666, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8421052631578948, 1.0, 0.0, 0.0, 1.0, 0.10526315789473684, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 1.0, 0.5, 0.6, 1.0, 0.6792452830188679, 0.0, 0.0, 0.6896551724137931, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.4864864864864865, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7164", "mrqa_squad-validation-7409", "mrqa_squad-validation-4634", "mrqa_squad-validation-4029", "mrqa_squad-validation-3113", "mrqa_squad-validation-6678", "mrqa_squad-validation-3946", "mrqa_squad-validation-6310", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-2351", "mrqa_newsqa-validation-2518", "mrqa_searchqa-validation-10924", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-2789"], "SR": 0.53125, "CSR": 0.6753472222222222, "EFR": 0.9333333333333333, "Overall": 0.8043402777777777}, {"timecode": 9, "before_eval_results": {"predictions": ["gold", "War of Currents", "113", "Joseph Priestley", "United Kingdom, Australia, Canada and the United States", "well before Braddock's departure for North America.", "Philip Webb and William Morris", "forming a 'A National Gallery of British Art',", "Henry's", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party, while Scotland itself elected relatively few Conservative MPs", "exceeds any given number", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)", "Channel Islands", "an additional warming of the Earth's surface", "they were nomads", "private southern Chinese manufacturers and merchants", "Owen Jones", "the Eleventh Doctor", "the BBC National Orchestra of Wales", "bilaterians", "Chinese autocratic-bureaucratic system", "August 1992", "NBC", "the region was superior to that of the British, since Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.", "New South Wales", "November 17, 2017", "Daren Maxwell Kagasoff ( KA - guh - sawf ) ( born September 16, 1987 )", "the Royal Air Force ( RAF ) defended the United Kingdom ( UK ) against large - scale attacks by Nazi Germany's air force, the Luftwaffe", "Sunni Muslim family", "2,050 metres ( 6,730 ft )", "to form a higher alkane", "Paradise, Nevada", "James Watson and Francis Crick", "September 29, 2017", "11 p.m. to 3 a.m", "Kingdom of Strathclyde was originally a part of the Hen Ogledd, its people speaking a Brythonic language distinct from Scottish Gaelic and the English derived from Lothian", "204,408", "Lake Michigan", "the body is said to orbit", "3.016 049 281 99 ( 23 ) u )", "inversely proportional to the wave frequency, so gamma rays have very short wavelengths that are fractions of the size of atoms, whereas wavelengths on the opposite end of the spectrum can be as long as the universe", "Martin Alarcon", "roofs of the choir side - aisles at Durham Cathedral", "the president", "different parts of the globe", "Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences", "florida it is illegal to sell alcohol before 1 pm on any sunday", "4,840", "Napoleon", "spinal nerve roots arise from the cord as they get closer to the head", "T'Pau", "Johnny Depp", "Julia Ormond", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "George Strait", "Jack Osbourne and Cheryl Burke", "the fallopian tube", "study insects and their relationship to humans, other organisms, and the environment", "1994", "Fred Derry", "Hawaii", "Hakeemullah Mehsud", "Bob Dylan", "a mythical half-human and half-eagle creature"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5968329560723862}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7647058823529412, 1.0, 1.0, 0.4615384615384615, 0.19999999999999998, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.08, 0.5, 0.5, 0.33333333333333337, 1.0, 0.18518518518518515, 0.0, 0.11764705882352941, 1.0, 0.16666666666666666, 0.07692307692307693, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7888", "mrqa_squad-validation-3451", "mrqa_squad-validation-3585", "mrqa_squad-validation-9334", "mrqa_squad-validation-7771", "mrqa_squad-validation-8410", "mrqa_squad-validation-5733", "mrqa_squad-validation-10232", "mrqa_naturalquestions-validation-2672", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-3868", "mrqa_hotpotqa-validation-1398", "mrqa_newsqa-validation-3354", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-16618"], "SR": 0.484375, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 10, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-940", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8859", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-996", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-299", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-13775", "mrqa_searchqa-validation-16618", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10035", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10145", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10301", "mrqa_squad-validation-10359", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-10415", "mrqa_squad-validation-10466", "mrqa_squad-validation-1082", "mrqa_squad-validation-1109", "mrqa_squad-validation-1131", "mrqa_squad-validation-1151", "mrqa_squad-validation-1166", "mrqa_squad-validation-1180", "mrqa_squad-validation-1180", "mrqa_squad-validation-1195", "mrqa_squad-validation-121", "mrqa_squad-validation-1217", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1395", "mrqa_squad-validation-1468", "mrqa_squad-validation-1488", "mrqa_squad-validation-1492", "mrqa_squad-validation-1621", "mrqa_squad-validation-1630", "mrqa_squad-validation-1645", "mrqa_squad-validation-170", "mrqa_squad-validation-1719", "mrqa_squad-validation-1732", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1962", "mrqa_squad-validation-1971", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2082", "mrqa_squad-validation-2122", "mrqa_squad-validation-2159", "mrqa_squad-validation-217", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2361", "mrqa_squad-validation-2412", "mrqa_squad-validation-2416", "mrqa_squad-validation-2418", "mrqa_squad-validation-2457", "mrqa_squad-validation-2486", "mrqa_squad-validation-2548", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2608", "mrqa_squad-validation-2633", "mrqa_squad-validation-2667", "mrqa_squad-validation-2678", "mrqa_squad-validation-2843", "mrqa_squad-validation-2861", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2899", "mrqa_squad-validation-291", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-2985", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3088", "mrqa_squad-validation-3104", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3119", "mrqa_squad-validation-3162", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3209", "mrqa_squad-validation-3215", "mrqa_squad-validation-3241", "mrqa_squad-validation-3307", "mrqa_squad-validation-3355", "mrqa_squad-validation-3362", "mrqa_squad-validation-3407", "mrqa_squad-validation-3411", "mrqa_squad-validation-3413", "mrqa_squad-validation-3451", "mrqa_squad-validation-348", "mrqa_squad-validation-349", "mrqa_squad-validation-3522", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3559", "mrqa_squad-validation-3569", "mrqa_squad-validation-3585", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3822", "mrqa_squad-validation-383", "mrqa_squad-validation-3830", "mrqa_squad-validation-3841", "mrqa_squad-validation-3855", "mrqa_squad-validation-3882", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-3946", "mrqa_squad-validation-4008", "mrqa_squad-validation-4028", "mrqa_squad-validation-4046", "mrqa_squad-validation-4121", "mrqa_squad-validation-4172", "mrqa_squad-validation-423", "mrqa_squad-validation-4296", "mrqa_squad-validation-433", "mrqa_squad-validation-4331", "mrqa_squad-validation-4376", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4418", "mrqa_squad-validation-4430", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-4463", "mrqa_squad-validation-4495", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4648", "mrqa_squad-validation-4673", "mrqa_squad-validation-4704", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-477", "mrqa_squad-validation-4772", "mrqa_squad-validation-4803", "mrqa_squad-validation-4807", "mrqa_squad-validation-4841", "mrqa_squad-validation-4848", "mrqa_squad-validation-4936", "mrqa_squad-validation-4983", "mrqa_squad-validation-5023", "mrqa_squad-validation-5063", "mrqa_squad-validation-5083", "mrqa_squad-validation-513", "mrqa_squad-validation-513", "mrqa_squad-validation-5136", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5247", "mrqa_squad-validation-5250", "mrqa_squad-validation-5254", "mrqa_squad-validation-5265", "mrqa_squad-validation-5295", "mrqa_squad-validation-5307", "mrqa_squad-validation-5318", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5418", "mrqa_squad-validation-5445", "mrqa_squad-validation-5485", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5563", "mrqa_squad-validation-5564", "mrqa_squad-validation-5566", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5608", "mrqa_squad-validation-5653", "mrqa_squad-validation-5664", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5766", "mrqa_squad-validation-5782", "mrqa_squad-validation-5788", "mrqa_squad-validation-5821", "mrqa_squad-validation-5843", "mrqa_squad-validation-5852", "mrqa_squad-validation-5951", "mrqa_squad-validation-6046", "mrqa_squad-validation-6049", "mrqa_squad-validation-6067", "mrqa_squad-validation-6073", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6294", "mrqa_squad-validation-6310", "mrqa_squad-validation-638", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-653", "mrqa_squad-validation-6531", "mrqa_squad-validation-6535", "mrqa_squad-validation-6548", "mrqa_squad-validation-6567", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-6678", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6840", "mrqa_squad-validation-6841", "mrqa_squad-validation-6868", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6917", "mrqa_squad-validation-6959", "mrqa_squad-validation-6962", "mrqa_squad-validation-6981", "mrqa_squad-validation-703", "mrqa_squad-validation-7030", "mrqa_squad-validation-7142", "mrqa_squad-validation-7175", "mrqa_squad-validation-7211", "mrqa_squad-validation-7252", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-743", "mrqa_squad-validation-7435", "mrqa_squad-validation-7456", "mrqa_squad-validation-7554", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7632", "mrqa_squad-validation-7633", "mrqa_squad-validation-7678", "mrqa_squad-validation-7699", "mrqa_squad-validation-7704", "mrqa_squad-validation-7709", "mrqa_squad-validation-7716", "mrqa_squad-validation-7747", "mrqa_squad-validation-7766", "mrqa_squad-validation-7771", "mrqa_squad-validation-7774", "mrqa_squad-validation-7775", "mrqa_squad-validation-7800", "mrqa_squad-validation-7818", "mrqa_squad-validation-7831", "mrqa_squad-validation-7846", "mrqa_squad-validation-7863", "mrqa_squad-validation-7888", "mrqa_squad-validation-7899", "mrqa_squad-validation-795", "mrqa_squad-validation-7965", "mrqa_squad-validation-797", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8071", "mrqa_squad-validation-8163", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8365", "mrqa_squad-validation-8372", "mrqa_squad-validation-8410", "mrqa_squad-validation-8414", "mrqa_squad-validation-8441", "mrqa_squad-validation-8486", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8589", "mrqa_squad-validation-859", "mrqa_squad-validation-8598", "mrqa_squad-validation-8600", "mrqa_squad-validation-8666", "mrqa_squad-validation-8670", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8907", "mrqa_squad-validation-9020", "mrqa_squad-validation-9030", "mrqa_squad-validation-9050", "mrqa_squad-validation-9116", "mrqa_squad-validation-9121", "mrqa_squad-validation-9151", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9379", "mrqa_squad-validation-9401", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9454", "mrqa_squad-validation-9465", "mrqa_squad-validation-9484", "mrqa_squad-validation-9540", "mrqa_squad-validation-9590", "mrqa_squad-validation-9608", "mrqa_squad-validation-9689", "mrqa_squad-validation-9738", "mrqa_squad-validation-976", "mrqa_squad-validation-9760", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9954", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2579", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6711"], "OKR": 0.875, "KG": 0.4171875, "before_eval_results": {"predictions": ["2009", "Huntington Boulevard", "since 1979", "William Hartnell and Patrick Troughton", "Distributed Adaptive Message Block Switching", "1331", "Confucianism", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "military and economic", "a military coup d'\u00e9tat", "the A1", "proteolysis", "a form of starch", "Eisleben", "break off the cathode, pass out of the tube, and physically strike him", "late night talk shows", "13\u20137", "around half", "the comprehensive institutions of the Great Yuan", "a D loop mechanism", "Chuck Howley", "sea level change", "Annette", "Tim McGraw and Kenny Chesney", "a man who could assume the form of a great black bear", "Casino promotions such as complimentary matchplay vouchers or 2 : 1 blackjack payouts allow the player to acquire an advantage without deviating from basic strategy", "sperm plasma then fuses with the egg's plasma membrane, the sperm head disconnects from its flagellum and the egg travels down the Fallopian tube to reach the uterus", "appointed to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "a sociological perspective which developed around the middle of the twentieth century and that continues to be influential in some areas of the discipline", "6 January 793", "a historical street in downtown Cebu City that is often called the oldest and the shortest national road in the Philippines", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Jane Fonda", "Robert Irsay", "Edgar Lungu", "UNESCO / ILO Recommendation concerning the Status of Teachers", "five", "Achal Kumar Jyoti", "the President", "Homer Banks, Carl Hampton and Raymond Jackson", "The management team", "British R&B girl group Eternal", "Detroit Red Wings", "When the others arrive", "six", "Yuzuru Hanyu", "a receptor or enzyme is distinct from the active site", "geologist James Hutton", "British - American rock band Fleetwood Mac", "31 October 1972", "December 2, 1942", "September 8, 2017", "Hollywood Masonic Temple", "Dolph Lundgren", "the Gaget, Gauthier & Co. workshop", "Renhe Sports Management Ltd", "Tangled", "a set of standards for smartphones and similar devices to establish radio communication with each other by touching them together or bringing them into close proximity, usually no more than a few centimetres", "Kenneth Hood \"Buddy\" MacKay Jr.", "a hard rock/blues rock band, they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\".", "Scotland", "she was a young skater and desperately wanted to make her mother proud.", "prohibiting the expansion of slavery into a territory where slave status was favored", "Peyton Place"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5119100951628304}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.782608695652174, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.07692307692307693, 0.0, 0.9444444444444444, 0.09523809523809523, 0.0, 0.10526315789473684, 0.375, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 1.0, 0.0625, 0.0, 0.0, 1.0, 0.21428571428571427, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_squad-validation-4849", "mrqa_squad-validation-9831", "mrqa_squad-validation-5357", "mrqa_squad-validation-1388", "mrqa_squad-validation-434", "mrqa_squad-validation-625", "mrqa_squad-validation-8747", "mrqa_squad-validation-8530", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-2277", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3223", "mrqa_searchqa-validation-15757"], "SR": 0.421875, "CSR": 0.6349431818181819, "EFR": 1.0, "Overall": 0.7365980113636363}, {"timecode": 11, "before_eval_results": {"predictions": ["The availability of the Bible in vernacular languages", "detention", "Western Xia", "voted by qualified majority to approve changes", "carbon related emissions", "co-chair", "Katharina", "three", "August 10, 1948", "the holy catholic (or universal) church", "30\u201360%", "Louis Paul Cailletet", "Class II MHC molecules", "crust and lithosphere", "orange", "electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons", "stroke", "13 June 1525", "the type of reduction being used", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "Peter Zephania", "1957", "Spanish explorers", "Western Australia", "Beaverton to be destroyed", "April 10, 2018", "October 1, 2015", "Games played", "cat in the hat", "nobiliary particle indicating a noble patrilineality or as a simple preposition that approximately means of or from in the case of commoners", "Tom Brady", "state", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Kida", "1960", "Bengal tiger", "Part 2", "a barrier that runs across a river or stream to control the flow of water", "De Wayne Warren", "triacylglycerol", "1971", "autu", "Tom Brady", "Margaery Tyrell", "Norway and at the Isle of Sheppey in England", "Lake Powell", "Clarence L. Tinker", "Bed and breakfast   Botel   Boutique hotel   Bunkhouse", "Trace Adkins", "Garbi\u00f1e Muguruza", "Keith Richards", "Hercules", "June 12, 2018", "Bart Howard", "111", "Leonard Bernstein", "The Hague", "historic buildings, arts, and published works", "Lisa", "Arnoldo Rueda Medina", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "George C. Wallace", "Florida", "CNN"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6311052630397638}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9743589743589743, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 0.4, 0.0, 0.5, 0.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.4615384615384615, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4074", "mrqa_squad-validation-8581", "mrqa_squad-validation-3474", "mrqa_squad-validation-4952", "mrqa_squad-validation-10483", "mrqa_squad-validation-3385", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-386", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1549", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-13057"], "SR": 0.53125, "CSR": 0.6263020833333333, "EFR": 1.0, "Overall": 0.7348697916666665}, {"timecode": 12, "before_eval_results": {"predictions": ["savanna or desert", "Downtown San Bernardino", "Allan Bloom, ''The Good War\" author Studs Terkel, American writer, essayist, filmmaker, teacher, and political activist Susan Sontag, analytic philosopher and Stanford University Professor of Comparative Literature Richard Rorty", "three", "Graz, Austria", "Sky Digital", "until 1796", "Galileo Galilei and Sir Isaac Newton", "Gary Kubiak", "soy farmers", "The Sinclair Broadcast Group", "mercuric oxide (HgO)", "Guglielmo Marconi", "Luther's education", "wages and profits", "southern Europe", "the judicial branch", "biostratigraphers", "as a track on MercyMe's 1999 album The Worship Project, which was released on an independent record label", "Miami Heat of the National Basketball Association ( NBA )", "the nasal septum", "a warrior, Mage, or rogue coming from an elven, human, or Dwarven background", "Bob Dylan", "1799", "interphase", "$315,600", "a tree species ( that generally grows in the elevation range of 3,000 to 4,200 metres ( 9,800 to 13,800 ft ) in the Himalayas )", "Roger Federer", "James Corden", "Pasek & Paul", "October 2", "1956", "Richard T. Jones", "Gwendoline Christie", "Orangeville, Ontario, Canada", "Cameron Fraser ( who disappeared with \u00a3 60,000 of her savings )", "RAM", "Afghanistan", "as the official residence of the President of the Russian Federation", "Walter Brennan", "James Rodr\u00edguez", "8ft", "various submucosal membrane sites of the body", "1985, 2016, 2018", "the United States", "April 17, 1982", "Acid rain", "July 1, 2005", "a stem", "Thespis", "Ra\u00fal Eduardo Esparza", "By functions", "Rhodes", "a constitutional monarchy in which the power of the Emperor is limited and is relegated primarily to ceremonial duties", "the Vital Records Office of the states, capital district, territories and former territories", "false", "the forefoot", "southwestern", "Planet Terror", "between South America and Africa", "one", "Meander", "fractional dimensions", "the ruble"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6428866481937603}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.4, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 0.41379310344827586, 0.5, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.625, 0.0, 0.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2644", "mrqa_squad-validation-8032", "mrqa_squad-validation-10341", "mrqa_squad-validation-9895", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2644", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6998", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-559", "mrqa_newsqa-validation-2782", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-10635"], "SR": 0.5625, "CSR": 0.6213942307692308, "EFR": 0.9285714285714286, "Overall": 0.7196025068681319}, {"timecode": 13, "before_eval_results": {"predictions": ["return home", "socialist realism", "to spearhead the regeneration of the North-East", "sleep deprivation", "July 1977", "Anderson", "quantum electrodynamics (or QED)", "provisional elder/deacon", "antigenic variation", "kinematic measurements", "worker, capitalist/business owner, landlord", "the communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Licensed Local Pastor", "to lure small enemy groups away from the larger group and defended position for ambush and counterattack", "both Kenia and Kegnia", "24 March 1879", "Rob Van Winkle", "electrons", "Edie Falco", "Leonard Bernstein", "Empire of the Sun", "James Knox Polk", "Nicaragua", "Afghanistan", "volatile country of Korijan", "the Aladdin Hotel", "Uncle Henry", "Lady and the Tramp", "volatile", "China", "solmn", "the University of Hanoi", "volatile", "Alexander Ulyanov", "Beatrix Potter", "Louvre", "James Buchanan", "Volvic", "sola di Paolo", "Colorado is of Spanish origin, meaning \"colored red\"", "Christopher Columbus", "the Balfour Declaration", "a contingency fee basis", "a tortoise", "biretta", "Shinto", "The Simpsons", "hemophilia", "\"Beauty is truth, truth beauty,'", "silk cord", "Bad Boys", "Philadelphia", "Charlie Chaplin Life Story", "The Fresh Prince of Bel-Air", "her abusive husband", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Sylvester the Cat", "Daniel Defoe", "Atlantic Ocean", "Battle of Britain and the Battle of Malta", "the U.S. Consulate in Rio de Janeiro", "the club's board has yet to make it so for all the camps", "three", "in the mouth"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6826802248677248}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7407407407407407, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8065", "mrqa_squad-validation-6255", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-708", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-4663", "mrqa_searchqa-validation-4983", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-15238", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-4717", "mrqa_newsqa-validation-1466"], "SR": 0.609375, "CSR": 0.6205357142857143, "EFR": 0.96, "Overall": 0.7257165178571429}, {"timecode": 14, "before_eval_results": {"predictions": ["Protestantism", "1281", "Tower Theatre", "the Mi'kmaq and the Abenaki", "basic design typical of Eastern bloc countries", "Palestine", "large, stiffened cilia that act as teeth", "the convenience of the railroad and worried about flooding", "Ireland", "18 April 1521", "Melus of Bari", "Warszawa", "the Migration period", "2012", "35", "Trump", "a landmark", "the Blue Nile", "Solomon", "Edie Sedgwick", "Eragon", "Rawhide", "a p puppy", "Judy Garland", "Kevin Freibott", "silk", "Donna Summer", "Fantastic Four", "John Mahoney", "Murder by Death", "Dave Brubeck", "Emmett Dalton", "Washington", "Gilda Radner", "\"People, I just want to say, you know, can we all get along?", "Franklin D. Roosevelt", "rice straw", "soba", "Sirhan Sirhan", "the Moon", "June 20 or 21", "held", "Mountain Dew", "Omaha", "Sammy Hagar", "actress", "the Erie Canal", "The Piano", "Baltic Sea", "senex", "Earvin \"Magic\" Johnson Jr.", "Filippo Brunelleschi", "hoo-hoo", "a syllable", "N\u0289m\u0289n\u0289\u0289", "Renishaw Hall, Derbyshire, England, UK", "Imola Circuit", "sheep", "to provide travel agencies in Japan with booking and ticketing capabilities for a wider range of international airlines.", "Berea College", "returned to Pakistan in October after President Pervez Musharraf signed an amnesty lifting corruption charges.", "Sharon Bialek", "the European Council", "Antigua"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6104166666666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2943", "mrqa_squad-validation-4621", "mrqa_squad-validation-1062", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-534", "mrqa_naturalquestions-validation-129", "mrqa_triviaqa-validation-1936", "mrqa_hotpotqa-validation-5184", "mrqa_newsqa-validation-846", "mrqa_triviaqa-validation-2317"], "SR": 0.5625, "CSR": 0.6166666666666667, "EFR": 1.0, "Overall": 0.7329427083333333}, {"timecode": 15, "before_eval_results": {"predictions": ["the preservation of public order is easier and more efficient than anywhere else", "arrested", "Paul Revere", "the death of Elisabeth Sladen in early 2011", "3.6%", "nearly 42,000", "14", "Riverside-San Bernardino", "Asia", "19 April 1943", "actions-oriented", "rapidly evolve and adapt", "republic of present-day Upstate New York and the Ohio Country", "the republic of Naples", "Debbie Abrahams", "South Africa", "Will Carling", "the Chatham House Rule", "haiti", "America Online", "the D'Artagnan", "bees", "The Firm", "The Streets", "violin", "a blazer", "the 4-minute mile", "the Titanic", "groin", "the outermost buttock muscle", "Luigi", "Georgia", "Massachusetts", "the London theatre district, ballet and circus", "La Boh\u00e8me", "Adams", "republic of Alfarid Corporation", "Ethel Skinner", "Queen Victoria", "My Fair Lady", "Austria", "cellulose", "Adolphe Adam", "a razor", "Barcelona", "the coelacanth", "mono", "cats", "Love Never Dies", "Elizabeth I", "jazz", "haiti", "the ISS", "the Marcy Brothers", "1770 BC", "costume party", "the oil platforms in the North Sea", "Sydney", "Chancellor Angela Merkel", "CNN", "Bob Krulwich", "the Professor", "Burundi", "the Howard Stern Show"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5553926542207792}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.5, 0.0, 1.0, 0.7272727272727273, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1830", "mrqa_squad-validation-6702", "mrqa_squad-validation-7872", "mrqa_squad-validation-10108", "mrqa_squad-validation-5893", "mrqa_squad-validation-2849", "mrqa_squad-validation-10151", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-871", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-7024", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3616", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8359", "mrqa_hotpotqa-validation-1537", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-339", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-8487"], "SR": 0.453125, "CSR": 0.6064453125, "EFR": 1.0, "Overall": 0.7308984374999999}, {"timecode": 16, "before_eval_results": {"predictions": ["computational complexity theory", "secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies.", "tensions over slavery and the power of bishops in the denomination", "mannerist architecture", "489", "the Grainger Town area", "the Great Fire of London", "Theory of the Earth", "Ward", "Newton", "bilaterians", "one of the daughters of former King of Thebes, Oedipus", "the forces of Andrew Moray and William Wallace", "lowered the river's base level ( its lowest point )", "between two and 30 eggs", "biannually", "Arthur Chung", "18", "9 February 2018", "Yuzuru Hanyu", "marley & Me", "26.617 \u00b0 N 81.617", "the 2013 non-fiction book of the same name by David Finkel", "Duck", "McKim Marriott", "Hanna Alstr\u00f6m", "31", "Austin, Texas", "the suprachiasmatic nucleus", "the Twelvers", "vasoconstriction of most blood vessels", "a person has the usual two copies of chromosome 21, plus extra material from chromosome 21 attached to another chromosome", "Rockwell", "All Hallows", "Sylvester Stallone", "In Time", "Rodney Crowell", "10 national ( significant ) numbers after the `` 0 '' trunk code", "in organelles", "Melissa Disney", "Darren McGavin", "UNESCO / ILO", "Thaddeus Rowe Luckinbill", "ummat al - Islamiyah", "William DeVaughn", "Tavares", "Brevet Colonel Robert E. Lee", "Poverty, Detachment from the things of the world, Contempt of Riches, Love of the Poor", "Santiago Ram\u00f3n y Cajal", "boy", "seven", "novella", "Southwest Florida region", "21 June 2007", "four", "Oregon, west of Idaho, and south of the Canadian province of British Columbia", "England", "Roc Me Out", "\"Dear John,\"", "root out terrorists within its borders", "Rudolf Nureyev", "43", "the Kurdish area of northern Iraq", "Mohamed Alanssi"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6141582069890894}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.27272727272727276, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.2222222222222222, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9525", "mrqa_squad-validation-6640", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5348", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-3649", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-4917", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-814"], "SR": 0.546875, "CSR": 0.6029411764705883, "EFR": 0.9655172413793104, "Overall": 0.7233010585699797}, {"timecode": 17, "before_eval_results": {"predictions": ["in Sydney", "Doritos", "Hans Vredeman de Vries", "$20.4 billion", "\u00dcberlingen", "Finsteraarhorn", "seven", "a Time Lord as well and able to regenerate", "Lippe", "the American Revolutionary War", "25", "Chuck Noland", "Donna Mills", "March 31, 2013", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "before the first year begins", "Kansas", "the President of the United States", "UNESCO / ILO", "asexually", "The Republic of Tecala", "2008", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Travis Tritt and Marty Stuart", "March 16, 2018", "Cee - Lo", "The Sun", "a living prokaryotic cell ( or organelle )", "1966", "Charlotte Hornets", "Julie Adams", "April 29, 2009", "J. Presper Eckert and John William Mauchly's ENIAC", "201", "1983", "Thursdays at 8 : 00 pm ( ET )", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T )", "MacFarlane", "to regulate the employment and working conditions of civil servants", "Hercules", "Angel Island ( California )", "Rufus and Chaka Khan", "moist temperate climates", "After Shawn's kidnapping", "7.6 mm", "sorrow regarding the environment", "1955", "B.F. Skinner", "2020", "continues the pre-existing appropriations at the same levels as the previous fiscal year ( or with minor modifications ) for a set amount of time", "during World War II in Berlin", "2004", "Orangeville, Ontario, Canada", "semicubical parabola", "Switzerland", "John Wilkes Booth", "St Augustine's Abbey", "two Russian bombers have landed at a Venezuelan airfield where they will carry out training flights for several days,", "the insurgency", "Joe DiMaggio", "Sandra Bullock", "NBA 2K16", "Baltimore", "September 25, 2017"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6895497384559884}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.14285714285714288, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.72, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.4444444444444445, 1.0, 0.0, 0.18181818181818182, 0.25, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6428571428571429, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.17142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2998", "mrqa_squad-validation-9285", "mrqa_squad-validation-7700", "mrqa_squad-validation-7288", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-582", "mrqa_triviaqa-validation-2922", "mrqa_hotpotqa-validation-538", "mrqa_newsqa-validation-3489", "mrqa_hotpotqa-validation-4735"], "SR": 0.578125, "CSR": 0.6015625, "EFR": 0.9259259259259259, "Overall": 0.7151070601851852}, {"timecode": 18, "before_eval_results": {"predictions": ["1904", "papal legate Cardinal Cajetan Luther stated that he did not consider the papacy part of the biblical Church", "Michael Heckenberger and colleagues of the University of Florida", "declined significantly", "quantum mechanics", "prices", "Miller", "Super Bowl XX", "gold", "p", "from 1962 to 2012", "to collect menstrual flow", "Loudents excessive dilation of the heart in cases of acute volume overload", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "12 to 36 months old", "the mixing of sea water and fresh water", "her abusive husband", "HTTP / 1.1", "the mainland coast of Queensland", "the 2009 model year", "December 2, 1942", "John Cooper Clarke", "`` Grand Dieu, sauvez le Roi ``", "Amitabh Bachchan, Akshay Kumar, Bobby Deol, Divya Khosla Kumar, Sandali Sinha and Nagma", "Annette Strean", "the Bee Gees", "September 25, 1987", "the first official Twenty20 matches were played on 13 June 2003 between the English counties in the Twenty20 Cup", "the Senate", "a little girl ( Addy Miller )", "Ritchie Cordell", "9.0 -- 9.1 ( M )", "the right to peaceably assemble, or to petition for a governmental redress of grievances", "2010", "James Corden", "the development of electronic computers in the 1950s", "the internal reproductive anatomy ( such as the uterus in females )", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "the ark of the covenant", "the population", "beta decay", "Rachel Sarah Bilson", "the NIRA", "March 5, 2014", "Haikou on the Hainan Island", "19 June 2018", "stromal connective tissue", "following Sunday night results show ( with certain exceptions )", "Stephen Curry of Davidson", "Ben Willis", "Rufus and Chaka Khan", "Jodie Foster", "Rory McIlroy", "Aristotle", "Cubs", "1919", "Lithuania", "Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "the Impeccable", "dana point bail", "infectious", "Pete Seeger", "Kiss Me, Kate", "Wigan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5738099415328781}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.9090909090909091, 0.0, 0.5714285714285715, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5106382978723404, 1.0, 1.0, 0.0, 0.2222222222222222, 0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.888888888888889, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9152542372881356, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2268", "mrqa_squad-validation-10388", "mrqa_squad-validation-845", "mrqa_squad-validation-5", "mrqa_squad-validation-9213", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-1259", "mrqa_hotpotqa-validation-4927", "mrqa_newsqa-validation-686", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-10336"], "SR": 0.453125, "CSR": 0.59375, "EFR": 0.9142857142857143, "Overall": 0.7112165178571428}, {"timecode": 19, "before_eval_results": {"predictions": ["outer and inner membranes of the ancestral cyanobacterium's gram negative cell wall", "well before Braddock's departure for North America.", "General Hospital", "the last surviving Time Lord", "night", "assertive teacher who is prepared to impose their will upon a class", "generally antagonistic", "whether or not to plead guilty", "parallelogram", "Valley Falls", "22 July 1930", "Sam Raimi", "New Hyde Park", "political thriller", "Timothy McVeigh", "Robert \"Bobby\" Germaine, Sr.", "1995\u201396", "the NYPD's 83rd Precinct", "Azeroth", "October 5, 1969", "1999", "Musicology", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.", "Gene Serdena", "combed his hair, first", "the Mayor of the City of New York", "841", "My Boss, My Teacher", "Roy Spencer", "Bardney", "Op\u00e9ra comique", "Everglades", "Hibbing, Minnesota", "1891", "Eielson Air Force Base in Alaska", "Serhiy Paradzhanov", "Germany", "eight teams", "Carson City", "Lindsey Islands", "2008", "October 12, 1962", "Eli Roth", "Paige O'Hara", "Northern Ireland", "St. Louis, Missouri", "Valhalla Golf Club inLouis, Kentucky", "Prince Rogers Nelson", "every aspect of public and private life wherever feasible", "Burnley", "Russian Empire", "between 11 or 13 and 18", "the onset and progression of Alzheimer's disease", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "7 June 2005", "Imola", "Sufjan Stevens", "producing rock music with a country influence.", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "Balanchine Jewels", "Irving Berlin", "$1.5 million", "Sen. Barack Obama", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6609375}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.8, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.8750000000000001, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.375]}}, "before_error_ids": ["mrqa_squad-validation-8687", "mrqa_squad-validation-6024", "mrqa_squad-validation-7740", "mrqa_squad-validation-1944", "mrqa_squad-validation-1551", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-904", "mrqa_hotpotqa-validation-5485", "mrqa_naturalquestions-validation-2629", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-7828", "mrqa_newsqa-validation-3659"], "SR": 0.515625, "CSR": 0.58984375, "EFR": 1.0, "Overall": 0.727578125}, {"timecode": 20, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10019", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1939", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2918", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-386", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4867", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15836", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-346", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-4917", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5845", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8487", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9432", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10151", "mrqa_squad-validation-10192", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10381", "mrqa_squad-validation-10388", "mrqa_squad-validation-10399", "mrqa_squad-validation-10408", "mrqa_squad-validation-1062", "mrqa_squad-validation-1082", "mrqa_squad-validation-1131", "mrqa_squad-validation-1180", "mrqa_squad-validation-1248", "mrqa_squad-validation-1272", "mrqa_squad-validation-1334", "mrqa_squad-validation-1348", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-150", "mrqa_squad-validation-1530", "mrqa_squad-validation-1551", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1939", "mrqa_squad-validation-2003", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-217", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2361", "mrqa_squad-validation-2416", "mrqa_squad-validation-2481", "mrqa_squad-validation-2511", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2667", "mrqa_squad-validation-2772", "mrqa_squad-validation-2861", "mrqa_squad-validation-2881", "mrqa_squad-validation-2906", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3148", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3270", "mrqa_squad-validation-3355", "mrqa_squad-validation-3385", "mrqa_squad-validation-3411", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3830", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4159", "mrqa_squad-validation-4186", "mrqa_squad-validation-423", "mrqa_squad-validation-4331", "mrqa_squad-validation-434", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4430", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4681", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-501", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5198", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5265", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-552", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5640", "mrqa_squad-validation-5653", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5849", "mrqa_squad-validation-5893", "mrqa_squad-validation-5986", "mrqa_squad-validation-6024", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6294", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-6728", "mrqa_squad-validation-680", "mrqa_squad-validation-6841", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6917", "mrqa_squad-validation-7029", "mrqa_squad-validation-7164", "mrqa_squad-validation-7175", "mrqa_squad-validation-7302", "mrqa_squad-validation-7362", "mrqa_squad-validation-7366", "mrqa_squad-validation-7435", "mrqa_squad-validation-744", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7709", "mrqa_squad-validation-7740", "mrqa_squad-validation-7766", "mrqa_squad-validation-7775", "mrqa_squad-validation-7818", "mrqa_squad-validation-7821", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8144", "mrqa_squad-validation-8163", "mrqa_squad-validation-828", "mrqa_squad-validation-8336", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8372", "mrqa_squad-validation-848", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8600", "mrqa_squad-validation-8687", "mrqa_squad-validation-8747", "mrqa_squad-validation-8759", "mrqa_squad-validation-8807", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-9050", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9348", "mrqa_squad-validation-9401", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9830", "mrqa_squad-validation-9831", "mrqa_squad-validation-9893", "mrqa_squad-validation-9930", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4825", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7542", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-871"], "OKR": 0.876953125, "KG": 0.459375, "before_eval_results": {"predictions": ["one way", "since at least the mid-14th century", "Marlee Matlin", "biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "Classic", "88", "between 25-minute episodes", "leishmaniasis", "Raimond Gaita", "Theodore Roosevelt Mason", "1996", "1964", "Kinnairdy Castle", "James Edward Franco", "(Thai: \u0e44\u0e17\u0e22\u0e41\u0e2d\u0e4c\u0e40\u0e0a\u0e35\u0e22 \u0e43\u0e2d\u0e01\u0e0b\u0e4c )", "December 24, 1973", "Baden-W\u00fcrttemberg", "James Dean", "Rob Reiner", "Kansas", "Westfield Tea Tree Plaza", "26,000", "the Alemannic", "1992", "Darci Kistler", "the EN World web site", "(25 March 1948 \u2212 27 December 2013)", "Pigman's Bar-B- Que", "Columbus", "Emilia-Romagna", "Miami Gardens, Florida", "The Tower of London", "John Sullivan", "churros", "Franklin, Indiana", "Ub Iwerks", "mentalfloss.com", "Omega SA", "Flashback", "from 1942 to 1945", "Kiernan Shipka", "1993", "Zaire", "Brett Ryan Eldredge", "The pronghorn", "Walcha", "French", "FBI", "BMW X6", "David Michael Bautista Jr.", "the Cherokee River", "15", "Taylor Swift", "The T1 line consists of 23 bearer ( B ) channels and one data ( D ) channel for control purposes", "2017 / 18 Divisional Round", "Robin", "Melbourne, Australia", "Felipe Massa", "The Washington Post", "Constantine XI Palaiologos", "Viva Las Vegas", "vesper bat", "Democritus", "Tennessee"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6174158868092692}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1459", "mrqa_squad-validation-7707", "mrqa_squad-validation-6653", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-238", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2837", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3052", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3402", "mrqa_searchqa-validation-15983", "mrqa_triviaqa-validation-694"], "SR": 0.515625, "CSR": 0.5863095238095238, "EFR": 1.0, "Overall": 0.7341369047619047}, {"timecode": 21, "before_eval_results": {"predictions": ["between 1000 and 1900", "Imperialism", "beneficial", "Approximately one million", "Ted Ginn Jr.", "Harrods", "Coptic Cathedral", "Philadelphia", "Disco", "mixed martial arts", "Kentucky River", "Tiberius", "1995", "public", "Lewis Madison Terman", "Fitzroya cupressoides", "Beno\u00eet Jacquot", "Nanna Popham Britton", "Sada Carolyn Thompson", "a British archaeologist", "Nelson Mandela", "Dissection", "German", "The A41", "New South Wales", "Bronwyn Kathleen Bishop", "The Timekeeper", "Jena Malone", "water", "Hong Kong First Division League", "Four Weddings and a Funeral", "from 1993 to 1996", "July 16, 1971", "influenced by the music genres of electronic rock, electropop and R&B", "Theodore Anthony Nugent", "Reverend Lovejoy", "Geet or The Song", "October 5, 1930", "Pamelyn Wanda Ferdin", "The Ninth Gate", "the Government of South Australia", "neuro-orthopaedic", "Saint Motel", "About 200", "War Is the Answer", "Richard I of England", "Trilochanapala", "Amy Poehler", "The Division of Cook", "Peter Townsend", "Blue Origin", "Taylor Swift", "18 December 1975", "convergent plate boundary", "it failed to enforce its rule", "Dame Kiri Te Kanawa", "melbourne", "30-minute", "the area was sealed off, so they did not know casualty figures", "Richard Thomas", "the wind band", "`` how now ''", "24 hours", "Nigel Lythgoe, Mia Michaels"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6161525106837606}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9904", "mrqa_squad-validation-3060", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-4466", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-883", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4172", "mrqa_naturalquestions-validation-10448", "mrqa_triviaqa-validation-6874", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-5447", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-7852"], "SR": 0.515625, "CSR": 0.5830965909090908, "EFR": 1.0, "Overall": 0.7334943181818182}, {"timecode": 22, "before_eval_results": {"predictions": ["the actioner Zorro", "Zagreus", "January 1985", "Computational complexity theory", "to plan the physical proceedings, and to integrate those proceedings with the other parts", "post-World War I", "Esteban Ocon", "New Orleans Saints", "Clarence Nash", "The Dressmaker", "1,800", "February 14, 1859", "Urijah Faber", "Mary-Kay Wilmers", "baeocystin", "a name in the 1623 First Folio of his plays", "Sunil Grover", "Ars Nova Theater", "Monticello", "a large portion of rural Maine, published six days per week in Bangor, Maine", "\"Menace II Society\"", "16 March 1987", "the luxury Holden Calais (VF) nameplate", "Louis \"Louie\" Zamperini", "1968", "PlayStation 4", "The Andes or Andean Mountains", "the Knight Company", "Parlophone Records", "Nicolas Winding Refn", "South America", "the Kingdom of Morocco", "Gerard Marenghi", "1958", "126,202", "31 July 1975", "orange", "Roseann O'Donnell", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "SAVE", "29,000", "the extraterrestrial hypothesis", "Larnelle Harris", "4,613", "a morir so\u00f1ando or orange Creamsicle", "Ramzan Kadyrov", "Albert", "Tetrahydrogestrinone", "5,042", "Captain B.J. Hunnicutt", "the Duchess's daughter, the future Queen Victoria", "the Masked Bandit", "the Gilbert building", "dead plant and animal material", "Elena Anaya", "Rome", "The Archers", "the raven", "the IV cafe", "during a world tour in the mid-1990s", "Daniel Radcliffe", "Milla Jovovich", "a sword", "a spoiled brat"], "metric_results": {"EM": 0.609375, "QA-F1": 0.675414839848983}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6010", "mrqa_squad-validation-6956", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-460", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3419", "mrqa_hotpotqa-validation-3463", "mrqa_naturalquestions-validation-5396", "mrqa_triviaqa-validation-1447", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-11933"], "SR": 0.609375, "CSR": 0.5842391304347826, "EFR": 1.0, "Overall": 0.7337228260869566}, {"timecode": 23, "before_eval_results": {"predictions": ["new technology and machinery", "Edmonton, Canada", "Ollie Treiz", "the Mongols", "Richard Trevithick", "coughing and sneezing", "9", "the outdoors", "The Backstreet Boys", "November of that year", "Hawaii", "American rock band Tool", "1919", "May 5, 2015", "Arthur Miller", "Edmonton, Alberta", "Brent Robert Barry", "1955 or February 12, 1956", "The LA Galaxy", "Nicholas Kristof", "film and short novels", "Flushed Away", "first baseman and third baseman", "River Welland", "The Process", "200,167", "Bohemia", "\"Jawbreaker\"", "Armin Meiwes", "1933", "Flula Borg", "Carl Michael Edwards II", "In a Better World", "the 45th Infantry Division", "Iron Man 3", "Julie 2", "Conservatorio Verdi in Milan", "6'5\"", "a terrible date", "June 26, 1970", "Lawrenceburg, Indiana", "14 directly elected members", "About a Boy", "1982", "1978", "domestic cat", "Denmark", "1999", "Uchinaanchu (\u6c96\u7e04\u4eba, Japanese: \"Okinawa jin\")", "Peter Thiel", "Summerlin, Nevada", "Cersei Lannister", "Kida", "about 26,000 light - years", "Mahinda Rajapaksa", "iron", "na", "\"FBI Smith and Jones\"", "British broadcaster Channel 4 has been criticized for creating a new television show which looks at how children as young as eight would cope without their parents for two weeks.", "naples", "2.5 million", "1,000,000", "keta", "the \"Watergate Seven\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.6125685307017544}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4210526315789474, 0.0, 1.0, 0.0, 0.75, 0.05555555555555555, 0.0, 0.4, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-5431", "mrqa_squad-validation-6220", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-2654", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-1564", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3634", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-4060", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-865", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-3074"], "SR": 0.515625, "CSR": 0.5813802083333333, "EFR": 1.0, "Overall": 0.7331510416666667}, {"timecode": 24, "before_eval_results": {"predictions": ["1965", "a large need for the positions (high demand)", "A tundra", "article 30", "The mermaid", "south of Kabul in the eastern Afghan province of Logar", "oversee them in a fair and independent manner", "daughter Paige, 15, and \"the crew\": Isaac, 8, Hope, 7, Lydia Beth, 2, Annie, also 2,", "Kurt Cobain", "seven", "two years", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "piano", "Charlotte Gainsbourg and Willem Dafoe", "The Devil Went Down to Georgia.", "that Turkey is one of the few -- possibly the only -- NATO member that has deep religious, cultural and historic knowledge of both Afghanistan and Pakistan.", "curfew", "Missouri", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics", "five victims", "30-minute", "in front of what remained of her Rancho Bernardo home.", "Iran", "cancer", "a baseball bat", "the U.S. Navy", "$249", "the coalition seeks to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "sportswear", "retiring from a 21-year career", "eight", "the FAA received no reports from pilots in the air of any sightings", "December 7, 1941", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "NATO fighters", "Jaime Andrade", "Somali President Sheikh Sharif Sheikh Ahmed", "\"I wasn't sure whether I was going to return to 'E! News' this week or after the new year.\"", "that school staff and security should patrol campuses, especially violence-prone areas, during and after school events.", "Orbiting Carbon Observatory", "German Chancellor Angela Merkel", "Joan Rivers", "Ryder Russell", "Scarlett Keeling", "the U.S. Navy", "his mother, Katherine Jackson, his three children and undisclosed charities.", "Asashoryu", "the District of Columbia National Guard", "the Interior Ministry", "the Kapoor family", "$3 billion", "question people if there's reason to suspect they're in the United States illegally", "Everest creative Maganlal Daiya", "Elizabeth Dean Lail", "1926", "cover", "Wyoming", "Kenny Everett", "November 23, 1996", "Richa Sharma", "The Frost Place", "The Chase", "Valentina Tereshkova", "the hand"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5614397197107794}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 0.26666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.06060606060606061, 1.0, 0.0, 0.0, 0.16216216216216214, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.8, 0.10256410256410256, 0.125, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666669, 1.0, 0.888888888888889, 0.0, 0.0, 0.4, 1.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7407", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3010", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6214", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-733", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-5986"], "SR": 0.484375, "CSR": 0.5775, "EFR": 0.9696969696969697, "Overall": 0.726314393939394}, {"timecode": 25, "before_eval_results": {"predictions": ["phycobilisomes", "WatchESPN", "independent prescribing authority", "10,000", "it infringed on democratic freedoms.", "the U.N. High Commissioner for Refugees", "Crandon, Wisconsin,", "chairman of the House Budget Committee,", "the cellar", "propofol, a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "the anti-trust unit", "company Polo", "\"no illegal substances\" were in her system at the time of her death", "Nkepile M abuse", "a Royal Air Force helicopter", "Juri Kibuishi,", "the restaurant next to the home of Mona Lisa", "the administration's progress, while we also encourage him to 'evolve faster' on supporting full marriage equality,\"", "1981", "Jewish", "al Fayed's security team", "Lifeway Christian Stores.", "Dan Parris, 25, and Rob Lehr, 26,", "18", "the festivities run from mid-November until the holidays end.", "Mad Men", "Jason Chaffetz", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "a woman who accuse him of sexual harassment", "the Form Design Center.", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "21", "in the heart of an urban center like Los Angeles.", "they would not be making any further comments, citing the investigation.No one was inside the apartment at the time of the fire,", "the United States has not yet decided to seek permission to board and inspect a North Korean vessel it suspects of carrying illicit weapons or technology in violation of U.N. sanctions", "the 3rd District of Utah", "House-passed bill that eliminates the 3% withholding requirement for government contractors", "\"peregruzka,\"", "only one", "Lee Probert.", "the listeria outbreak is the deadliest food-borne illness outbreak in the United States since 1998.", "23", "American", "\"we have identified a problem -- let's go solve it together.\"", "Monday.", "Frank Ricci,", "Old Trafford", "Guinea, Myanmar, Sudan and Venezuela.", "the Carrousel du Louvre,", "the cause of the child's death will be listed as homicide by undetermined means,", "Les Bleus", "Madison", "Gibraltar", "the need to repent in time", "John McCarthy", "The Rime of the Ancient Mariner", "the narwhals'", "Atlas ICBM", "Lionel Eugene Hollins", "Duchess Eleanor of Aquitaine", "John Irving", "diabetes", "the Capital"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4383542927660574}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.07142857142857142, 0.15384615384615385, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.15384615384615385, 0.0, 1.0, 0.14285714285714285, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6354", "mrqa_squad-validation-8392", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-981", "mrqa_newsqa-validation-2745", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2955", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2673", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-5238", "mrqa_searchqa-validation-11369"], "SR": 0.34375, "CSR": 0.5685096153846154, "EFR": 1.0, "Overall": 0.7305769230769231}, {"timecode": 26, "before_eval_results": {"predictions": ["Eero Saarinen", "according to a multiple access scheme", "composite number", "Climate fluctuations during the last 34 million years", "Chicago Cubs", "Private Eye", "an \u201caerophone,\u201d", "Norman Hartnell", "1948", "Millbank in London", "Poland", "Adam Ant", "the ancient mass extinction in the history of the Earth", "New Democracy", "Missouri", "six", "mushrooms", "Turkey", "1979", "Pooh and his friends", "the fictional London Borough of Walford in the East End of London", "ancient Latin baccalaureate", "Laurence Olivier", "four rows", "Leonard Bernstein", "\"I am trying get the hang of this new fangled writing machine, but am not making a shining success of it.", "no.", "touch reading", "blood", "aircraft", "passion fruit", "jumper", "The Lone Ranger", "caridean shrimp", "Yemen", "Welles", "King George IV.", "Barry Briggs", "Joseph Smith", "Cornwall", "Ukrainian steppes", "cancer", "Beaujolais Nouveau", "rowing", "$100", "(28 July 1925 - 14 April 1988)", "A Dangerous Man: Lawrence After Arabia", "'Lord Nelson'", "ancient Testament", "Argentina", "lion", "Sinclair Lewis", "Phosphorus pentoxide", "Bindusara", "Norwegian", "the County of York", "Cheshire County", "Blue", "Phillip Myers,", "11th year in a row", "Opryland", "Heroes", "singer", "Alzheimer's disease"], "metric_results": {"EM": 0.53125, "QA-F1": 0.579403409090909}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.18181818181818182, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-118", "mrqa_triviaqa-validation-1872", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-2496", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-2740", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-6073", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-459", "mrqa_naturalquestions-validation-946", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-1554"], "SR": 0.53125, "CSR": 0.5671296296296297, "EFR": 0.9666666666666667, "Overall": 0.7236342592592593}, {"timecode": 27, "before_eval_results": {"predictions": ["water-cooled", "Genghis Khan Mausoleum", "the most cost efficient bidder", "February 1, 2016", "Kylie Jenner's first child", "53", "Aman", "2017", "Elizabeth Dean Lail", "Anthony Caruso", "Peter Finch", "Jerry Ekandjo", "detention camp", "775", "north", "23 %", "vivre", "Roanoke", "a flash music video", "Central Germany ( German : Mitteldeutschland )", "King Saud University", "berkshire", "southeastern coast of the Commonwealth of Virginia", "Orlando", "epidemiology", "Kyrie Irving", "brain", "Paul Hogan", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "1996", "Emma Thompson", "in capillaries", "September 6, 2019", "two", "Shenzi", "August 6", "3000 BC", "peninsular", "1963", "on the ureters", "October 2008", "Broken Hill", "arts liberales", "Mockingjay -- Part 1 ( 2014 )", "1773", "Escherichia coli", "15th century", "radioisotope thermoelectric generator", "Carol Worthington", "at slightly different times when viewed from different points on Earth", "interstitial fluid in the `` interstitial compartment ''", "2014", "english patient", "dorset", "pressure", "Frederick Martin \"Fred\" Mac Murray", "Franz Ferdinand", "extreme nationalist, and nativist", "Tim McAuliffe", "tripplehorn,", "carol stinson", "Portugal", "grow old", "caruso"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5797788149350649}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 0.7499999999999999, 0.33333333333333337, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.8, 1.0, 1.0, 0.5, 0.0, 1.0, 0.060606060606060615, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-8625", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1427", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-1180", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-3827", "mrqa_hotpotqa-validation-5286", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-3635", "mrqa_searchqa-validation-7864"], "SR": 0.4375, "CSR": 0.5625, "EFR": 0.9444444444444444, "Overall": 0.7182638888888889}, {"timecode": 28, "before_eval_results": {"predictions": ["the father of the house when in his home", "Welsh", "data link", "blue", "Mead", "Vietnam", "John Peel,", "Moscow", "insect", "Tallinn,", "Siberia", "malaria", "1992", "Kent", "Arthur, Prince of Wales", "Israel", "butterflies", "New Jersey", "Philippines", "Euskadi Ta Askatasuna,", "the number thirteen", "batha Sulis", "Eric Coates", "charlie wenn", "to make a furrow or furrows in", "Brothers In Arms", "Mexico State (Toluca)", "Aberystwyth", "Eric Morley", "prairie provinces", "Mickey Spillane,", "Erik Aunapuu", "Frank Sinatra", "Alberto Salazar", "the Washington Post", "Niger", "The Lone Gunmen", "Ivan Owen", "piano", "Tritons", "Addis Ababa", "pascal (Pa)", "heart", "Nova Scotia", "\u201clone wolf\u201d", "Mike Gatting", "Nigeria", "Dead Sea", "40", "Gibraltar territory", "1929", "long", "Gerald Ford", "Spanish missionaries", "William Wyler", "Nardwuar the Human Serviette", "1887", "Portal", "Mohmand agency district.", "a plaque", "Phillip A. Myers.", "Priscilla Beaulieu", "Cold Mountain (2003)", "\"deadwood\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5671875}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false], "QA-F1": [0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2318", "mrqa_squad-validation-4968", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-836", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-3536", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4114", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-3924", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-388", "mrqa_newsqa-validation-2885", "mrqa_searchqa-validation-1846", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2066"], "SR": 0.5, "CSR": 0.5603448275862069, "EFR": 0.9375, "Overall": 0.7164439655172414}, {"timecode": 29, "before_eval_results": {"predictions": ["Albert C. Outler", "African Union chairman Jakaya Kikwete", "Drogo", "labrador", "stedian", "New South Wales", "chipmunks", "Amnesty International", "copenhagen", "Australia", "Labrador Retrievers", "Rio Grande", "horror", "patriarchs", "Poland", "copenhagen", "Brooklyn", "Octavian", "gibes", "copenhagen", "jones", "Gryffindor", "gibes", "cop", "lion-o", "Ann Dunham", "Rapa Nui", "copenhagen", "baku", "domenico Ghirlandaio", "sesame Street", "Hindu", "hindfoot", "Alanis Morissette", "high sewing", "Herbert Henry Asquith,", "maple", "index fingers", "plants and animals", "handball", "times", "50", "function", "shakespears Sister", "purple", "Fenn Street School", "stid jones", "niner", "copenhagen", "copertina", "times", "succussu", "March 31, 2017", "1973", "as of 2017, the following 15 countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "Dutch", "1981", "Humberside", "\"wildcat\" strikes", "times", "\"Toy Story,\"", "copenhagen", "tap", "microwave"], "metric_results": {"EM": 0.296875, "QA-F1": 0.3650140224358974}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.625, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.7692307692307693, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8383", "mrqa_triviaqa-validation-7292", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-2279", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4925", "mrqa_triviaqa-validation-7733", "mrqa_triviaqa-validation-4169", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-6764", "mrqa_hotpotqa-validation-2997", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-124", "mrqa_searchqa-validation-10920"], "SR": 0.296875, "CSR": 0.5515625, "EFR": 1.0, "Overall": 0.7271875000000001}, {"timecode": 30, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1132", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-2642", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3117", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-394", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-478", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-5368", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-998", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2364", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-913", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5986", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-9310", "mrqa_squad-validation-10141", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-1131", "mrqa_squad-validation-1272", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-1551", "mrqa_squad-validation-1639", "mrqa_squad-validation-1641", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1732", "mrqa_squad-validation-1830", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2849", "mrqa_squad-validation-2881", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3205", "mrqa_squad-validation-3270", "mrqa_squad-validation-3385", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3841", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4166", "mrqa_squad-validation-423", "mrqa_squad-validation-4385", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4655", "mrqa_squad-validation-4673", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4806", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5198", "mrqa_squad-validation-5234", "mrqa_squad-validation-5265", "mrqa_squad-validation-5331", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5590", "mrqa_squad-validation-5640", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5986", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6614", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-668", "mrqa_squad-validation-6727", "mrqa_squad-validation-6894", "mrqa_squad-validation-6956", "mrqa_squad-validation-7029", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-7435", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7740", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8144", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-839", "mrqa_squad-validation-848", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8600", "mrqa_squad-validation-8646", "mrqa_squad-validation-8656", "mrqa_squad-validation-8687", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-915", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-939", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9813", "mrqa_squad-validation-9878", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1021", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2789", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3512", "mrqa_triviaqa-validation-3595", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4717", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4858", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-884"], "OKR": 0.865234375, "KG": 0.4359375, "before_eval_results": {"predictions": ["cellular respiration", "centre-left Australian Labor Party (ALP),", "the most cost efficient bidder", "Prussian", "850 m", "armani, Esprit", "media", "capital of the Socialist Republic of Vietnam", "Stephen Mangan", "saloon", "power 108", "Vixen", "Adult Swim", "Venice", "Tropical Storm Ann", "1974", "tokamak", "March 17, 2015", "American", "1958", "China Airlines", "Wayne County", "former Senator of the College of Justice", "Republican", "Greek", "Honey Irani", "2000", "Beauty and the Beast", "137th", "half", "nevadius DeMun Wilburn", "27 November 1956", "Francis Nethersole", "hiphop", "47,818", "Salisbury", "Lakshmibai", "Tony Aloupis", "sarod", "Anne Arundel County", "Austria wien", "the local midnight", "Robert A. Iger", "Netherlands", "Taquini Plan", "2 March 1972", "Terry the Tomboy", "Gracie Mansion", "parlophone Records", "R-8 Human Rhythm Composer", "obergruppenf\u00fchrer", "World War I", "84", "Linda Davis", "Sunni Muslim family", "mona Lisa", "eight", "highlands", "an open window", "staff sergeant", "\"It has never been the policy of this president or this administration to torture.\"", "trans fats", "Anna Mary Robertson", "Vienna"], "metric_results": {"EM": 0.5, "QA-F1": 0.5636904761904762}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2885", "mrqa_hotpotqa-validation-3524", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-1370", "mrqa_hotpotqa-validation-4594", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-3473", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-2529", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-276", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-4709", "mrqa_newsqa-validation-3819", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-15972"], "SR": 0.5, "CSR": 0.5498991935483871, "EFR": 1.0, "Overall": 0.7206048387096774}, {"timecode": 31, "before_eval_results": {"predictions": ["medical services", "phagocytes", "Finland", "Gulf of Aden", "Natty Bumppo", "San Francisco", "Amsterdam", "pectins", "moles", "Ryan O' Neal", "Sicily", "Howard Keel", "cobra Bubbles", "Charlie henderson", "Sweet Home Alabama", "Alopecia", "Quin Ivy", "make Me an Offer", "mrs henderson's men", "Man V Food", "eighteenth-century", "Kajagoogoo", "George Fox", "Croatian", "Manchester United Community", "mike Adkins", "Schlitz", "Esau", "South Africa", "fidelio", "ABBA", "Some Like It Hot", "silver", "Cleopatra", "fumage", "Enrico Caruso", "the invasion of Russia cut off those precious supplies", "proton", "nitric acid", "Tasmania", "flesh", "Mille Miglia", "tiger", "rhododendron", "Uranus", "Utrecht", "Puente del Arzobispo", "mousetrap", "shorthand", "caliper", "arts", "Joseph Vissarionovich Stalin", "the player to the dealer's right", "March 27, 2017", "Black Mesa Research Facility", "4,613", "Swiss Super League", "Benj Pasek and Justin Paul", "Mark Sanford", "environmental efforts", "skyscrapers", "chicago", "The Planets", "mrs henderson & Gamble"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4797991071428571}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6, 0.0, 0.0, 1.0, 0.8571428571428571, 0.7499999999999999, 0.5, 0.0, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4462", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-5359", "mrqa_triviaqa-validation-7201", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-740", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-2298", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-7145", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-10606", "mrqa_hotpotqa-validation-1690", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-745", "mrqa_searchqa-validation-199", "mrqa_searchqa-validation-10166"], "SR": 0.421875, "CSR": 0.5458984375, "EFR": 0.9459459459459459, "Overall": 0.7089938766891892}, {"timecode": 32, "before_eval_results": {"predictions": ["K-9 and Company (1981)", "Thomas Sowell", "hitler", "Ye Shiwen", "pangrams", "October", "wyoming", "Leicester", "hiters", "kangaroos", "lourdes", "1929", "hypopituitarism", "February", "piano", "Gloucestershire", "Jupiter Mining Corporation", "John Maynard Keynes", "Scooby snack", "Yulia Tymochenko", "Lake Union", "Adriatic Sea", "hitler hitler", "fife", "Goran Ivanisevic", "Francis Drake", "Wikipedia", "Baku", "Truro", "frasier", "tundras tundra", "Madness", "Barings", "Anne Boleyn", "Professor Plum", "Ken Norton", "Yann Martel", "cabbage", "John Denver", "Fleet Street", "hitler", "one-third", "India", "hitler", "sorrento", "Edward III", "Bill Bryson", "hitler", "Triumph and Disaster", "Spanish", "Norman Mailer", "a \"major science finding from the agency's ongoing exploration of Mars.\"", "between 1923 and 1925", "98,200 police", "capillaries", "sarod", "Laban Movement Analysis", "king James I of England", "\"falling space debris,\"", "on the bench", "Bright Automotive", "Will & Grace", "prehensile", "the English Channel"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5905505952380953}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_triviaqa-validation-1725", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-952", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-7073", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-1770", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-10495", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-3994"], "SR": 0.53125, "CSR": 0.5454545454545454, "EFR": 1.0, "Overall": 0.7197159090909091}, {"timecode": 33, "before_eval_results": {"predictions": ["the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea,", "1580s", "george smiley's People", "French Revolution", "Ted Heath", "wuthering Heights", "Portugal", "Chelsea", "vice-admiral", "Sweeney Todd", "emerald", "Benfica", "6", "1984", "11", "Buzz Aldrin", "MC Hammer", "Archie Shuttleworth", "eddie", "The IT Crowd", "The Cream of Manchester", "red wildebeest", "turtle", "Cold Comfort Farm", "eisbach", "Ruth Rendell", "wales", "Indian Love Call", "eddie", "chardonnay", "John Constable", "sheep", "willow", "carmen Miranda", "nottingham", "jump jump", "1882", "venus", "powys", "seaweed", "sea otter", "dot-com", "Tunisia", "george machell", "earache", "scar", "Gorbachev", "croquet", "low-cost", "east coast", "diamonds", "wigan Warriors", "Taittiriya Samhita", "minimum viable", "The United States Secretary of State", "Jos\u00e9 Bispo Clementino dos Santos", "1979", "Umberto II", "Spc. Megan Lynn Touma,", "suppress the memories", "40", "ID", "charlie williams", "copper"], "metric_results": {"EM": 0.453125, "QA-F1": 0.49441964285714285}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [0.47619047619047616, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-2759", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-748", "mrqa_triviaqa-validation-7609", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7162", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-1809", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-6224", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-11614"], "SR": 0.453125, "CSR": 0.5427389705882353, "EFR": 1.0, "Overall": 0.7191727941176471}, {"timecode": 34, "before_eval_results": {"predictions": ["Central business districts", "1973", "france", "father's day", "Kenya", "September", "jessica", "Albert Camus", "james stewart", "Martin Luther King, Jr.", "petticoat", "harappa", "can be used in a positive sense, meaning that a region or territory is so aligned, supportive, and conducive with the United States, that it is like a U.S. state.", "180 degrees", "Charles Taylor", "conchita wurst", "Ireland", "The Savoy", "niki lauda", "france", "Andaman", "france", "Massachusetts", "Boutros Ghali", "PETER FRAMPTON", "Uranus", "mole", "Crowley", "Greek", "Spain", "Mumbai", "kitsune", "frottage", "collage", "cows", "eriksson", "jessica evertsen", "Angus Deayton", "Dean Martin", "Emily Davison", "Loki", "penny", "canary stewart", "ethel stewart", "Peter Blake", "ghee", "octavian", "Ragman Rolls", "commitment", "anastasia Dobromyslova", "s\u00e8vres", "procol harum", "Victory gardens", "artes liberales", "in the New Testament", "AT&T", "1998", "Dachshunds", "\"totaled,\"", "Iran of trying to build nuclear bombs,", "\"will be considered as are all requests for money from Michael's estate.\"", "nicholas", "\"b're stupidity\"", "anemia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5651041666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-5862", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-3475", "mrqa_triviaqa-validation-3914", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-7019", "mrqa_triviaqa-validation-405", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-2249", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-6010", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3770", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-16252"], "SR": 0.546875, "CSR": 0.5428571428571429, "EFR": 1.0, "Overall": 0.7191964285714285}, {"timecode": 35, "before_eval_results": {"predictions": ["Seven Days to the River Rhine", "Seerhein", "2,579 steps", "to collect menstrual flow", "New Jersey Devils of the National Hockey League ( NHL )", "the Near East", "Sunday evenings", "Charlie Wilson", "Hook", "will first encounters on the planet that his family crash lands on", "with nearby objects show a larger parallax than farther objects when observed from different positions", "four", "Leonard Bernstein", "the Atlantic coast of Africa from 1418", "9 February 2018", "1970s", "2001", "the 18th century", "her abusive husband", "moral", "two - third of the total members present", "federal republic", "July 14, 1969", "Frank Langella", "Tennessee Titan", "a container often made of papier - m\u00e2ch\u00e9, pottery, or cloth", "April 26, 2005", "Castleford", "Action Jackson", "New England Patriots", "the world's first collected descriptions of what builds nations'wealth", "Patrick Warburton", "when the cell is undergoing the metaphase of cell division", "`` One Son ''", "Mara Jade", "revenge and karma", "Kyla Coleman from Lacey, Washington", "Nathan Hale", "Rachel Kelly Tucker", "far lesser degree by blood capillaries extending to the outer layers of the dermis", "during World War II", "S\u00e9rgio Mendes", "spherical boundary of zero thickness", "David Tennant", "Kelly Reno", "Brooke Wexler", "$1.84 billion", "patronymic surname", "Leonard Nimoy", "Billy Hill", "2005", "Pangaea or Pangea", "Kent", "petula skinner", "arm", "Tufts University", "2002", "2004", "work together to stabilize Somalia and cooperate in security and military operations.", "Communist Party of Nepal", "Robert Barnett", "macGyver", "vikings", "chicago"], "metric_results": {"EM": 0.5, "QA-F1": 0.6255288662361032}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 0.6666666666666666, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.21052631578947367, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.15384615384615385, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8338", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6190", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-702", "mrqa_hotpotqa-validation-2788", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-8558"], "SR": 0.5, "CSR": 0.5416666666666667, "EFR": 0.9375, "Overall": 0.7064583333333334}, {"timecode": 36, "before_eval_results": {"predictions": ["$5,000,000", "The Handmaid's Tale", "Austrian", "Archbishop of Canterbury", "1945", "102,984", "Emmanuel Ofosu Yeboah", "Donald Duck", "Bulgarian-Canadian", "Macomb County", "Dusty Dvoracek", "Vice President", "1972", "Disney California Adventure", "Indiana", "Travis County", "major", "orange", "Regional League North", "15-lap race was won by BRM driver Graham Hill after he started from second position", "ragby", "life insurance", "Ukrainian", "Cape Cod", "actress and model", "BBC Focus", "George Clooney", "Joe Scarborough", "Tottenham ( ) or Spurs", "the attack on Pearl Harbor", "Alemannic", "Vienna", "Jesper Myrfors", "Paper", "Amway", "Ogallala Aquifer", "What's Up", "21st Century Fox", "vigorish", "12", "Rockbridge County", "sexual activity", "Italian", "Heathrow", "Carlos Santana", "two Nobel Peace Prizes", "Dutch", "a game setting created by TSR, Inc. in the late 1980s", "2013 Cannes Film Festival", "Aiden English", "actor", "Bill Belichick", "Achal Kumar Jyoti", "a set of connected behaviors, rights, obligations, beliefs, and norms as conceptualized by people in a social situation", "jazz", "17", "plac\u0113b\u014d", "a federal judge in Mississippi", "when the economy turns unfriendly", "\"It was perfect work, ready to go for the stimulus package,\"", "fly larvae", "pinnipeds", "the United States", "Harry S. Truman"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5946886446886447}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.3076923076923077, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3145", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-2776", "mrqa_newsqa-validation-2449", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-13135"], "SR": 0.53125, "CSR": 0.5413851351351351, "EFR": 1.0, "Overall": 0.718902027027027}, {"timecode": 37, "before_eval_results": {"predictions": ["Robert Lane and Benjamin Vail", "pjaro carpintero", "20", "cannibalism", "the College of William and Mary", "China", "Bling-bling", "Biggie Smalls", "Marsha Hunt", "chinook", "Feodor Ivanovich", "Sarah Hughes", "Sonnets to Orpheus", "Caesar salad", "Sheathed", "David Berkowitz", "Catch Me if You Can.", "Taos Pueblo", "a 3500 lb. sculpture of one of these spheres used by early astronomers to represent the circles of the heavens", "licorice stick", "Eugene O'Neill's last play, A Moon for the Misbegotten", "Donovan", "flag", "Dublin", "mathematical", "George II", "Suzuki Grand Vitara", "Yogi Bear", "Lebanon's parliament", "Judas Iscariot", "Christopher", "Stripes", "Little Red Riding Hood", "a wavelength of about 400 nm", "Deryl Dwaine Dodd", "Cherokee", "the cause of the French people", "Gettysburg National Military Park", "a wonderful lamp", "Jackie Kennedy", "\"Sensible and responsible women don't want to vote.\"", "Orange County", "The Picture of Dorian Gray", "Arkansas", "Helen of Troy", "magistrates", "Eduard von Blow", "Aaron Burr", "violins", "ethanol", "Adam Smith", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "Paul", "Pericles, the Athenians pursued a policy of retreat within the city walls of Athens, relying on Athenian maritime supremacy for supply", "red", "Cole Porter", "a horse", "New York City", "Westminster system", "Phil Collins", "nuclear", "Egypt", "Jackson sitting in Renaissance-era clothes and holding a book", "boxing"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49559997294372293}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.4, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06666666666666667]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_searchqa-validation-12628", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12401", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-11435", "mrqa_searchqa-validation-2181", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-4949", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-8404", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-10312", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-14370", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-2068", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-822", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-10156", "mrqa_hotpotqa-validation-4081", "mrqa_naturalquestions-validation-9459"], "SR": 0.40625, "CSR": 0.537828947368421, "EFR": 1.0, "Overall": 0.7181907894736843}, {"timecode": 38, "before_eval_results": {"predictions": ["1954", "Monument", "cloves", "Cressida", "The United States", "St Martin-in-the-Fields", "a body", "Greenpeace", "coelacanth", "the ice creammake", "a canton", "a Palestinian city", "Caracas", "Giza", "Truman Capote", "to be a Horseplayers", "Faneuil Hall", "Babe", "shrimp", "balsa", "Anwar El Sadat", "prostitutes", "an anteater", "field goal", "Diana", "Tasmania", "the Taj Mahal", "cobalt", "Louisa May Alcott", "boat propulsion", "Spider-Man", "lubricant", "insulin", "a fracas", "Dell", "Van Helsing", "Hugh Grant", "the Great Wall of China", "hand", "Hormel Foods", "Cressida", "Clara Barton", "Kauai", "the esophagus", "Joseph", "Otsego County", "The Apartment", "Colombia", "Robert Livingston", "Venezuela", "canticles", "Donna", "outside the playing season, or before November 1", "a phone number", "Apollon", "Chicago", "Andrew Lloyd Webber", "1776", "Tim Howard", "Princess Jessica", "Hutus", "a Florida girl who disappeared in February", "Monday and Tuesday", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5393139367816092}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.4827586206896552]}}, "before_error_ids": ["mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-7187", "mrqa_searchqa-validation-8722", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-6833", "mrqa_searchqa-validation-1238", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-9939", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-3861", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-11351", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-6570", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-1716", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1534", "mrqa_searchqa-validation-14173", "mrqa_searchqa-validation-16849", "mrqa_searchqa-validation-6780", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-6886", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-4264", "mrqa_hotpotqa-validation-5676", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3774", "mrqa_naturalquestions-validation-4021"], "SR": 0.46875, "CSR": 0.5360576923076923, "EFR": 1.0, "Overall": 0.7178365384615385}, {"timecode": 39, "before_eval_results": {"predictions": ["movements of nature", "401(k)", "Hill Street Blues", "tuberculosis", "(Ross) Perot", "fathered", "a crayon", "a dice fracture", "Lance Armstrong", "Kung Fu", "tennis elbow", "a sienna", "hindu", "the Village Voice", "Nacho Libre", "(St) Fields", "Cygnus", "lago de", "nougat", "a Scotch egg", "the Manhattan Project", "the Eiffel Tower", "Roger Federer", "a sculpere", "a yolk", "a cheddar", "the Anglo-Iranian (formerly Anglo-Persian) Oil Company", "Florida", "The Virgin Spring", "(the three sons)", "the Nome", "(Queen) Victoria", "Atlanta", "the Deviled eggs", "Zorro", "assume", "Jack Sprat", "offbeat", "Uranium", "ismene", "(Matt) Bastianich", "Gannett", "(William) Shakespeare", "(George) Sand", "a glacier", "( Dick) Gephardt", "a 4.0 GPA", "Lord Louis Mountbatten", "a master's degree", "(American) Idol)", "Captain (Bob) Bartlett", "back with terminal velocities much lower than their muzzle velocity when they leave the barrel of a firearm", "Hellenism", "James W. Marshall", "achilles", "linseed", "Die Hard", "Wojtek", "Montreal, Quebec, Canada", "Broad Sands Bay", "\"Buus shows the world that you love the environment and hate using fuel,\"", "6-2", "\"First lady Michelle Obama listens to remarks during a health care forum at the White House on Friday.", "$1,500"], "metric_results": {"EM": 0.5, "QA-F1": 0.5383936403508772}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.08, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.21052631578947367, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-2577", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-12788", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-9039", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-262", "mrqa_searchqa-validation-3491", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-4826", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-1888", "mrqa_triviaqa-validation-6258", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-31", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-3934"], "SR": 0.5, "CSR": 0.53515625, "EFR": 1.0, "Overall": 0.71765625}, {"timecode": 40, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2920", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3586", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-514", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1316", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-13508", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-2066", "mrqa_searchqa-validation-222", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4829", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9908", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10088", "mrqa_squad-validation-10388", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1248", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2608", "mrqa_squad-validation-2831", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3331", "mrqa_squad-validation-349", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4046", "mrqa_squad-validation-4155", "mrqa_squad-validation-4331", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4634", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5248", "mrqa_squad-validation-5307", "mrqa_squad-validation-5389", "mrqa_squad-validation-5469", "mrqa_squad-validation-5506", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5728", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6024", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6224", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6702", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7211", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-801", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-9140", "mrqa_squad-validation-915", "mrqa_squad-validation-9285", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9525", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-1425", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.82421875, "KG": 0.48359375, "before_eval_results": {"predictions": ["toward the center of the curving path", "a random number generator", "Hinduism", "a turtle", "Donkey", "Jane Eyre", "Aiden", "William Shakespeare", "the Sun King", "Montmartre", "The Dying Swan", "jailhouse", "a protractor", "voter registration", "The Kite Runner", "white granite", "Islamabad", "crabs", "Stephen Crane", "trespass", "Jack Dempsey", "London", "Val Kilmer", "Pakistan", "Milwaukee", "a kiwi", "Pop-Tarts", "sugar", "Enrico Fermi", "Tiger Woods", "the Madding Crowd", "The FCC recognizes the important role of local broadcasters in helping federal, state, and local officials provide the general public with advanced notification of", "Grace Kelly", "the at symbol", "Bilbo", "Oliver Wendell Holmes", "the Articles of Confederation", "proverbs", "a photon", "Maria Montessori", "an orchid", "an orchid", "the sun", "Michelangelo", "Spain", "ale", "Superman", "a verb", "Brazil", "Puget Sound", "order", "Yahya Khan", "Prince Bao", "1038", "Bubba", "Easter Parade", "Edward Yorke", "beer", "Keeper of the Privy Seal of Scotland", "Martin O'Malley", "1983", "homicide", "six", "Edgehill"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7452237866300366}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.08333333333333333, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-7659", "mrqa_searchqa-validation-12159", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-7222", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-7370", "mrqa_naturalquestions-validation-3485", "mrqa_triviaqa-validation-1475", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-2627"], "SR": 0.65625, "CSR": 0.538109756097561, "EFR": 1.0, "Overall": 0.7168407012195122}, {"timecode": 41, "before_eval_results": {"predictions": ["metals", "2004", "to capitalize on her publicity", "Karen Gillan", "Moira Kelly", "Lola", "Albert Einstein", "Miami Heat of the National Basketball Association ( NBA )", "in Poems : Series 1", "shortwave radio", "the east African coast", "420 mg", "James Madison", "Tom Brady", "asphyxia", "1975", "Thomas Edison", "in a cell", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Flag Day in 1954", "Erica Rivera", "Afghanistan", "Gettysburg College", "The Geography of Oklahoma", "Eydie Gorm\u00e9", "into the bloodstream", "a fuel ( the reductant )", "1 mile ( 1.6 km )", "1871", "refers a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "1970", "solids", "Presley Smith", "a combination of genetics and the male hormone dihydrotestosterone", "Norma's half - brother", "A wide range", "Tami Lynn", "Christianity", "radians", "1931", "Help!", "May 2010", "24", "August 8, 1945", "Bichu", "31 March 1909", "the Comprehensive Crime Control Act of 1984", "the meridian", "Buffalo Springfield", "the times sign", "the title page", "The Plantagenets: History of a Dynasty", "asphalt", "Mar del Sur", "Julie Taymor", "an organ", "Tony Aloupis", "the American Civil Liberties Union", "Nigeria", "attempted burglary", "Pancho Gonzales", "William Henry Harrison", "orchids", "Britain"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6703849706328154}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.3076923076923077, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.9655172413793104, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.16, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-5976", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-9237", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-7595", "mrqa_newsqa-validation-2835", "mrqa_searchqa-validation-2827"], "SR": 0.578125, "CSR": 0.5390625, "EFR": 0.9259259259259259, "Overall": 0.7022164351851852}, {"timecode": 42, "before_eval_results": {"predictions": ["high voltage", "water buffalo", "Texas", "Song of Solomon", "Zohan", "Denmark", "Battlestar Galactica", "Slayer Sainte-Marie", "sheep", "Mary Tudor", "\"Macbeth\"", "a blackbird", "Patty Duke", "Superman", "Judas Iscariot", "3,000th", "grow old along with me", "savanna", "Primo Levi", "a long ton", "Kellogg's", "The Fall Guy", "the phoenix", "depth and breadth", "life with Judy Garland: Me & My Shadows: Me and My Shadows", "crocodiles", "greece", "Slovenia", "baboon", "Morris West", "outta nowhere", "El burlador de Sevilla", "Hawaii", "the Empire State Building", "the League of Nations", "Sally Ride", "bulletproof", "Queen Elizabeth I's Speech to the Troops at Tilbury", "75%", "creme brulee", "the Civil War", "gravity", "Greek Meatballs", "Holden Caulfield", "the Caucasus mountains", "Firebird", "fourteen", "Lecompton", "Midnight Cowboy", "Rosetta Stone", "Louisiana", "poetry", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "a scuffle with the Beast Folk", "Arabian Gulf", "Ross Kemp", "Marine One", "Juan Manuel Mata Garc\u00eda", "Vietnam War", "Taylor Swift", "Sharon Bialek", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Kaka", "an undated photo of Alexandros Grigoropoulos, whose death has sparked riots across Greece.\""], "metric_results": {"EM": 0.4375, "QA-F1": 0.5500376793345543}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.125]}}, "before_error_ids": ["mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-9124", "mrqa_searchqa-validation-15998", "mrqa_searchqa-validation-8140", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-8429", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-6562", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-11902", "mrqa_searchqa-validation-1970", "mrqa_searchqa-validation-9663", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-448", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-1406", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-1411", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-115"], "SR": 0.4375, "CSR": 0.5367005813953488, "EFR": 1.0, "Overall": 0.7165588662790697}, {"timecode": 43, "before_eval_results": {"predictions": ["Southeastern U.S.", "gin", "Leicester", "a mile", "the Jets", "scurvy", "Japan", "\"Razor\"", "falcon", "Niger", "Norman Brookes", "Billy Crystal", "Jaipur", "Goran Ivanisevic", "14", "\"Alastair Burnet\"", "Spain", "Henry Hudson", "Bridge", "a raven", "Much Ado About Nothing", "Felix Leiter", "Louis XV", "Mrs Mainwaring", "Australia", "Robert A. Heinlein", "USS Constitution", "Aug. 24, 1572", "fertilization", "Decoupage", "Massachusetts", "Sherlock Holmes", "Delaware", "Olivia Smith", "Costa Concordia", "elliptical and spiral", "orange juice", "Jim Morrison", "Mona Lisa", "graphite", "Bash", "Mercury", "Ireland", "Gandalf", "Andile Smith", "Michael Curtiz", "Minder", "a star", "a turkey", "Eva", "Eeyore", "Saint Alphonsa", "the Tin Woodman", "March 15, 1945", "Portsea", "1861", "McLaren", "romantic", "Diego Milito", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Zyrtec", "kids", "3.14159", "Leo Frank"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6484375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-6528", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-2978", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7617", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-5446", "mrqa_newsqa-validation-2751", "mrqa_searchqa-validation-11984"], "SR": 0.609375, "CSR": 0.5383522727272727, "EFR": 1.0, "Overall": 0.7168892045454545}, {"timecode": 44, "before_eval_results": {"predictions": ["the inclusion of Lake Constance and the Alpine Rhine", "Lori McKenna", "elm tree at Shackamaxon, in what is now the city's Fishtown neighborhood", "Marcus Atilius Regulus", "the state in which both reactants and products are present in concentrations which have no further tendency to change with time, so that there is no observable change in the properties of the system", "Lucknow", "Kristy Swanson", "Hendersonville, North Carolina", "hydrogen", "Abraham", "Kelly Osbourne", "1773", "the final episode of the series", "a god of the Ammonites", "the Han", "the Super Bowl", "the 1980s", "31 October 1972", "The Italian Agostino Bassi", "graduation with a Bachelor of Medicine, Bachelor of surgery degree", "Germany", "Real Madrid", "LED illuminated display", "Beyonc\u00e9", "about 24 hours", "a person making an initial assessment of another person, place, or thing will assume ambiguous information based upon concrete information", "2015", "UMBC", "Tommy Shaw", "November 2016", "Thomas Alva Edison", "B.R. Ambedkar", "the Indian Hockey Federation ( IHF )", "Niles", "Judy Collins", "Oklahoma", "Grand Inquisition", "British pop band T'Pau", "Angel Island Immigration Station", "Johannes Gutenberg", "Terry Reid", "a mark", "Johannes Gutenberg", "Domhnall Gleeson", "the Soviet Union", "Pandavas", "2020 National Football League ( NFL ) season", "1994 season", "a Parable of the Unjust Judge", "the church at Philippi", "Chuck Noland", "The Green Mile", "Cal Ripken, Jr.", "1905", "Workers' Party", "Revolver", "140 million", "cell phones", "space shuttle Discovery", "750", "lew springsteen", "nantucket", "Microsoft", "liverpool"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6336843711843712}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 1.0, 0.2857142857142857, 0.0, 1.0, 0.1923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9113", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4592", "mrqa_triviaqa-validation-7676", "mrqa_searchqa-validation-7952", "mrqa_searchqa-validation-3760", "mrqa_triviaqa-validation-4825"], "SR": 0.53125, "CSR": 0.5381944444444444, "EFR": 0.9, "Overall": 0.6968576388888889}, {"timecode": 45, "before_eval_results": {"predictions": ["westward", "off the coast of Somalia", "Felipe Massa", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Tibet", "\"handful\" of domestic disturbance calls to police since 2000", "President Bush", "Brad Blauser", "Thursday", "President Obama", "at a depth of about 1,300 meters in the Mediterranean Sea", "one", "raping and murdering", "Ryder Russell", "\"thoroughness\" of the deputy's response", "his father", "the club's board", "insect stings", "U.N. Security Council resolution in 2006", "christopher henderson", "Fullerton, California", "Chinese and international laws", "$2.6 million from Enyimba in Nigeria", "Rev. Alberto Cutie", "ceo Herbert Hainer", "Kate Hudson's ex, Black Crowes rocker Chris Robinson", "269,000", "surrounding areas of the bustling capital faced further inundation at the next high tide.", "Dharamsala, India", "state's attorney", "will remain in custody until a bond hearing Friday,", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola on Saturday,", "two", "Spaniard", "Steve Williams", "theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg, a major art museum in Zurich, said.", "Oxbow, Minnesota", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels", "Donald Trump", "6,000", "gossip girl", "Public Citizen", "Diversity", "650", "Yemen", "South Africa", "Daniel Radcliffe", "President Sheikh Sharif Sheikh Ahmed", "cities throughout Canada", "Florida's Everglades", "Dollree Mapp", "Tyrion", "June 8, 2009", "'Q'", "wolf", "an iron lung", "quantum theory", "mountaineer", "Mel Blanc", "\"eponymos\"", "Colorado", "the CIA", "Billy Bob Thornton"], "metric_results": {"EM": 0.5, "QA-F1": 0.5855350718447023}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.5454545454545454, 0.4, 0.0, 0.09523809523809525, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9268292682926829, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-5370", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-5862", "mrqa_searchqa-validation-6261"], "SR": 0.5, "CSR": 0.5373641304347826, "EFR": 1.0, "Overall": 0.7166915760869565}, {"timecode": 46, "before_eval_results": {"predictions": ["every four years", "a part husky or other Nordic breed, and possibly part terrier", "to oversee the local church", "enterocytes of the duodenal lining", "warmth", "a limited period of time", "during meiosis", "federal government", "alveolar process", "Katharine Hepburn", "Norway", "multiple origins of replication on each linear chromosome that initiate at different times ( replication timing ), with up to 100,000 present in a single human cell", "each country provides public healthcare to all UK permanent residents that is free at the point of use, being paid for from general taxation", "Gladys Knight & the Pips", "to solve its problem of lack of food self - sufficiency", "January 2004", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "276", "the French CYCLADES project directed by Louis Pouzin", "when each of the variables is a perfect monotone function of the other", "the Kansas City Chiefs", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "especially in Western cultures", "Husrev Pasha", "in 2001", "Squamish, British Columbia, Canada", "Mankombu Sambasivan Swaminathan", "players must choose, in advance, whether they wish to collect a jackpot prize in cash or annuity", "1962", "prophets and beloved religious leaders", "Best Picture, Best Director for Fincher, Best Actor for Pitt and Best Supporting Actress for Taraji P. Henson", "interstate communications by radio, television, wire, satellite, and cable", "the phrase `` in the name of the Father, and of the Son, and the Holy Spirit", "William Chatterton Dix", "1987", "Stefanie Scott", "Gamora", "in the fovea centralis", "Ephesus", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah", "Ferraro", "seven", "Uralic languages", "when an individual noticing that the person in the photograph is attractive, well groomed, and properly attired", "scrolls", "British and French Canadian fur traders", "Lori McKenna", "2017", "October 27, 1904", "in the mid - to late 1920s", "Firoz Shah Tughlaq", "Runic", "backgammon", "Malcolm Bradbury", "Wade Boggs", "The 2004\u201305 NBA season was the 16th season for the Minnesota Timberwolves in the National Basketball Association.", "Holberg's comedy \"Den V\u00e6gelsindede\"", "Pakistan's intelligence agency", "Maj. Nidal Malik Hasan, MD,", "February 12.", "Charles Dickens", "the Montague & the Montagues", "plutonium", "24"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6058819768194768}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.16666666666666666, 0.7272727272727273, 0.6666666666666666, 0.30769230769230765, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.5, 0.16, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8148148148148148, 0.0, 0.7499999999999999, 0.09999999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.5833333333333334, 0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-1692", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-13101"], "SR": 0.46875, "CSR": 0.5359042553191489, "EFR": 0.9411764705882353, "Overall": 0.7046348951814767}, {"timecode": 47, "before_eval_results": {"predictions": ["Max Martin", "2002 Mitsubishi Lancer OZ Rally - Provided for Brian", "21 June 2007", "Lewis Carroll", "the government - owned corporation of Puerto Rico", "Austria - Hungary", "Mace Coronel", "libretto", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "Harry", "1800", "in the dress shop", "As of January 17, 2018, 201", "Miller's Diet Beer", "Instagram's own account", "Experimental neuropsychology", "30 years after Return of the Wars", "Selena Gomez", "14 \u00b0 41 \u2032 34", "Dido", "during initial entry training", "Eddie Murphy", "1997", "Bonnie Aarons", "1960", "during the summer of 1979", "Part XI of the Indian constitution", "the Constitution of India came into effect on 26 January 1950", "by Appalachian artists Clarence `` Tom '' Ashley and Gwen Foster", "methane, ethane, and propane", "the 1980s", "Tom Tucker", "Frankie Laine's `` I Believe '' in 1953", "1898", "Flex Builder ( built on Eclipse ) targeted the enterprise application development market", "Matt Jones", "the members of the actual club with the parading permit as well as the brass band", "The Confederate States Army ( C.S.A. ) was the military land force of the Confederate States of America ( Confederacy ) during the American Civil War ( 1861 - 1865 )", "three", "Olivia", "Zachary John Quinto", "Sanchez Navarro", "Have I Told You Lately", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "John Travolta", "Scherie or Phaeacia", "1 October 2006", "2017 Georgia Bulldogs football team", "Eda Reiss Merin", "Michael Clarke Duncan", "the Origination Clause of the United States Constitution", "Gestapo", "Belgium", "heartburn", "Baltimore and Ohio Railroad", "Vanessa Hudgens", "two", "Johannesburg", "a sort of robot living inside.", "hardship for terminally ill patients and their caregivers", "The Odyssey", "Jeff Goldblum", "Prego", "marillion"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6049097019375552}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.7692307692307693, 0.0, 0.08695652173913042, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.923076923076923, 0.0, 0.5714285714285715, 0.6666666666666666, 0.8333333333333334, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6428571428571429, 0.0, 1.0, 0.0, 0.5454545454545454, 0.5, 0.0, 0.0, 0.1, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-10381", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-89", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-2606", "mrqa_newsqa-validation-886"], "SR": 0.453125, "CSR": 0.5341796875, "EFR": 0.9142857142857143, "Overall": 0.6989118303571428}, {"timecode": 48, "before_eval_results": {"predictions": ["Daylight Saving Time", "Cygnus", "Heraclius Novus Constantinus", "Lautrec's first and most well-known poster", "Nigeria's", "Hawaii", "Jeremiah", "The \"park\"", "Don Knotts", "cinnamon Life", "Kbenhavn", "Porgy and Bess", "Dutchman", "Deimos", "Frasier: Head Game", "Robert Frost", "Thelma Dickinson", "Underground Chapel", "the sky", "Maksim Chmerkovskiy", "New Zealand", "the First Folio of 1623", "The region lies just south of Orange County and west of Imperial County,", "Fat Man, you shoot a great game of pool", "snuggle", "Led Zeppelin", "the Book of Kells", "great grandmother tatarabuelo", "uncontrolled", "a Heisman", "Gulf of Tonkin", "Stephen Vincent Bent", "coelacanth", "Prague", "the Federal Reserve", "alliterative combustibles", "Afghanistan's", "Cheetah", "Ambrose Bierce", "the American Lung Association", "a croquet", "Aphrodite", "New age", "Chico and the Man", "Budapest", "John Mahoney", "pythons", "Nit-A-Nee", "chirac", "Beverly Cleary", "Afghanistan", "2009", "a recognized group of people who jointly oversee the activities of an organization", "since 3, 1, and 4 are the first three significant digits of \u03c0", "eight Grand Slam singles championships", "The YMCA Net Worth", "Lutwidge Dodgson", "the theory of direct scattering and inverse scattering", "43rd", "South America", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "$4 a gallon.\"", "fill a million sandbags and place 700,000 around our city,\"", "Gabi"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5301339285714286}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-9339", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-4404", "mrqa_searchqa-validation-12001", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-7598", "mrqa_searchqa-validation-8359", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-8727", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-12165", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-793", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-4738", "mrqa_hotpotqa-validation-1316", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-675", "mrqa_triviaqa-validation-336"], "SR": 0.453125, "CSR": 0.5325255102040816, "EFR": 1.0, "Overall": 0.7157238520408162}, {"timecode": 49, "before_eval_results": {"predictions": ["William Wyler", "Redford's adopted home state of Utah", "pelvic floor", "Kanawha Rivers", "glycine and arginine", "Brian Steele", "1937", "1920s", "832 BCE", "2005", "New York City", "Beorn", "Jonathan Cheban", "1992", "Montreal", "18", "Kristy Swanson", "roughly 230 million kilometres ( 143,000,000 mi )", "Camping World Stadium in Orlando", "C. dromedarius", "sorrow regarding the environment", "the New York Yankees", "Woodrow Wilson", "Phosphorus pentoxide", "Brooklyn", "Secretary of Commerce Herbert Hoover", "the Gemara", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "Tom Brady", "Around 1200", "Walter Egan", "The Crossing", "to stay", "Taiwan", "origins of replication, in the genome", "in the late 1930s in southern California, where people raced modified cars on dry lake beds northeast of Los Angeles under the rules of the Southern California Timing Association ( SCTA )", "2012", "Jackie Robinson", "Coton in the Elms", "the American Civil War", "Pradyumna", "1979", "James Madison", "UVA", "Roger Federer", "17 - year - old", "Kylie Minogue", "5,534", "pumped through the semilunar pulmonary valve into the left and right main pulmonary arteries ( one for each lung ), which branch into smaller pulmonary arteries that spread throughout the lungs", "electron donors", "A rear - view mirror", "Geroge W. Bush", "USS Constitution", "Lucas McCain", "Martin Scorsese", "2014", "2014", "the \"bystander effect\"", "The number of deaths linked to cantaloupes contaminated with the Listeria monocytogenes bacteria has risen to 28,", "United States", "Sagamore Hill", "hyperthyroidism", "a parrot", "a measure of how well a sunscreen will protect skin from both UVA and UVB rays"], "metric_results": {"EM": 0.625, "QA-F1": 0.708107689079374}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 0.6976744186046512, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13793103448275862, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333]}}, "before_error_ids": ["mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-7213", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-558", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2074", "mrqa_searchqa-validation-14922", "mrqa_searchqa-validation-10011", "mrqa_triviaqa-validation-7608"], "SR": 0.625, "CSR": 0.534375, "EFR": 0.9583333333333334, "Overall": 0.7077604166666667}, {"timecode": 50, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-561", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9724", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3331", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.828125, "KG": 0.50234375, "before_eval_results": {"predictions": ["alla capella", "caterpillar", "Zeus", "8", "Lady Gaga", "John Alec Entwistle", "Esmeralda's Barn night", "Compeyson", "Paris", "December 18, 1958", "Cumberland sausage", "Bristol", "Queen Victoria and Prince Albert", "Car ferry", "Amanda Barrie", "Sharjah", "Madagascar", "arid climate", "miss Havisham", "oxygen", "Macbeth", "Gentlemen Prefer Blondes", "Khrushchev", "Japan", "the hose", "Erie Canal", "John Key", "Subway", "geodetics", "Dylan Thomas", "Fred Astaire", "the Tower of London", "Bridge", "quarter", "cirrus uncinus", "Manchester", "Harry patch", "cheese", "Klaus dolls,", "Argentina", "Nick Faldo", "James Valentine", "the Count Basie Orchestra", "the Be beast", "Sir John Houblon", "Top Gun", "elbow", "Hitler", "isohyet", "Naples", "Denmark", "July 14, 1969", "1923", "Massillon, Ohio", "1937", "LA Galaxy", "2017 season", "The BBC", "Frank Ricci,", "The Stooges", "sweatshirt", "Susan B. Anthony", "ivory", "in the north and west of the country,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6148897058823529}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3529411764705882]}}, "before_error_ids": ["mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6534", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-4693", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1927", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2793", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2191"], "SR": 0.5625, "CSR": 0.5349264705882353, "EFR": 0.9642857142857143, "Overall": 0.71281118697479}, {"timecode": 51, "before_eval_results": {"predictions": ["Samson", "smith", "convict pigeon", "catalytic converter", "Sir Edwin Landseer", "smith", "Albert Einstein", "smith", "scales", "gumm", "dave walliams", "tepuis", "into one of the Vikings nine realms", "smith", "mare", "hypopituitarism", "Delaware", "bees", "Treaty of Amiens", "water", "every year", "stanley", "Brittany", "mantle", "havre", "algae", "shine", "Algeria", "Churchill Downs", "smith", "Leonard Bernstein", "smith", "dunes", "japan", "20", "Ken Platt", "smith", "smith", "Sweden", "smith", "muppet Christmas Carol", "wiziwig", "jump", "dessert", "lethal injection", "smith", "10", "netherlands", "dublin", "Babylonian", "the Continental Marines", "Joe Spano", "Gary Grimes", "skeletal muscle", "CBS News", "Herman's Hermits", "the United States Congress", "theocracy", "clothing", "2-0", "Brunei", "a Pringles can", "a peanut butter cup", "1983"], "metric_results": {"EM": 0.40625, "QA-F1": 0.471875}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-1635", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-805", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-7735", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7762", "mrqa_naturalquestions-validation-692", "mrqa_hotpotqa-validation-4406", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2260", "mrqa_searchqa-validation-3853"], "SR": 0.40625, "CSR": 0.5324519230769231, "EFR": 1.0, "Overall": 0.7194591346153847}, {"timecode": 52, "before_eval_results": {"predictions": ["horicle", "australia", "australia", "vodka", "vinnie Barbarino", "Snark", "Prussian", "silversmith", "Joshua Tree National Park", "carpathia", "Superman", "letchworth", "velvet", "1", "jennifer scissorhands", "crackerjack", "white Ferns", "australia", "carburetors", "spanish", "labrador retriever", "what", "Delaware", "thieves", "clara wieck", "squeeze", "Apocalypse Now", "a crew of ten trying to hunt the Snark", "st moritz", "david hockney", "Scafell Pike", "Edgar Allan Poe", "christopher mrs", "Tony Blackburn", "Donna Summer", "france", "kennel", "wolf", "the Titanic", "kennifer purdy", "trumpet player", "Andrew Lloyd Webber", "Malawi", "australia", "eric barker", "m Mikhail Baryshnikov", "Norwich", "Ruth Rendell", "smiths", "ontario", "Ohio", "Miami Heat", "Joanna Page", "1961 during the Cold War", "1902", "FCI Danbury", "25 June 1971", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "Caster Semenya", "a \"happy ending\" to the case.", "universal and equal suffrage", "lauer", "jennifer feldman", "Colonel"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6515625}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2767", "mrqa_triviaqa-validation-57", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-4518", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3898", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5354", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-11787"], "SR": 0.59375, "CSR": 0.5336084905660378, "EFR": 0.9230769230769231, "Overall": 0.7043058327285923}, {"timecode": 53, "before_eval_results": {"predictions": ["albinism", "Jack Ruby", "Google", "hugh dowding", "squash", "Tennessee Williams", "Jim Smith", "injecting a 7 percent solution intravenously three times a day", "David Bowie", "tobacco", "canoeist", "Louren\u00e7o Marques", "Christian Louboutin", "Ironside", "the need to toss logs across narrow chasms", "James Dean", "Mars", "once", "about Eve", "chicken Marengo", "George Orwell", "ernest hemingal Cocoa", "homeless", "Derbyshire", "art movement of the 20th century", "polynesian", "vincent", "hemingtha", "charlie brown", "Ruth Rendell", "nottingham", "ernestony", "hot Chocolate", "utah", "1921", "Spain", "tony cl Clash", "Praseodymium", "Bruce Alexander", "slap", "the brain", "razor", "Arthur Hailey", "hart", "madonna", "little arrows", "a nerve cell cluster", "noel edmonds", "ernestacks", "27", "orchid", "104 colonists", "aiding the war effort", "Benzodiazepines", "Cesar Millan", "2004 Paris Motor Show", "Gracie Humait\u00e1", "contraband", "ernest henderson", "Vicente Carrillo Leyva", "Guinea", "steel", "place bet", "2012"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5635416666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-3136", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-2371", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-31", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-3119", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-6040", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4525", "mrqa_naturalquestions-validation-3962", "mrqa_hotpotqa-validation-3655", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-3551", "mrqa_searchqa-validation-16488", "mrqa_searchqa-validation-14449"], "SR": 0.546875, "CSR": 0.5338541666666667, "EFR": 1.0, "Overall": 0.7197395833333334}, {"timecode": 54, "before_eval_results": {"predictions": ["Big Mamie", "Surveen Chawla", "Canadian", "Sunyani", "six", "the Big 12 Conference", "Armidale", "Alfred Preis", "forest of Bowland", "CAC/PAC JF-17 Thunder", "Daniel Espinosa", "Switzerland", "The Keeping Hours", "Duff Goldman", "Ang Lee", "Mineola", "67,038", "Kentucky Wildcats", "Eileen Atkins", "Parliamentarians", "Harry Potter series", "Haitian Revolution", "alcoholic drinks", "john hollot", "John Meston", "York County", "Cleveland Cavaliers", "Believe", "Hillsborough", "Cartoon Network Too", "Del Mar Fairgrounds", "Quahog, Rhode Island", "mathematician and physicist", "October 4, 1970", "George Raft", "korea", "1944", "the British Army", "311", "Jennifer Taylor", "Cartoon Network", "Columbia Records", "British", "6,241", "WWE 2K18", "Les Clark", "Marilyn Martin", "Hawaii", "Gregg Popovich", "1979", "john Malkovich", "Lauren Tom", "from 0.1 to 0.3 mm", "Spektor", "Maine", "monta", "vanilla", "the local political representative", "three", "Louvre", "Michelson", "hemming", "will", "the Capitoline Wolf"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5274553571428571}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.33333333333333337, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-3161", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-4449", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-873", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-1310", "mrqa_naturalquestions-validation-8962", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5383", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2609", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-1776"], "SR": 0.421875, "CSR": 0.5318181818181817, "EFR": 0.972972972972973, "Overall": 0.713926980958231}, {"timecode": 55, "before_eval_results": {"predictions": ["The 2016 United States Senate election in Nevada", "1952", "private Roman Catholic university located in Great Falls, Montana within the Diocese of Great Falls\u2013Billings", "voice-work", "400 MW", "Kerry Marie Butler", "Esp\u00edrito Santo Financial Group", "July 10, 2017", "November 23, 2011", "leopard", "Salman Rushdie", "Tom Rob", "Big Bad Wolf", "Rockland County", "Personal History", "Australian-American", "the Holston River Valley", "1943", "1996", "Tamil Nadu", "Tallahassee City Commission", "Markov Random Field", "three different covers", "Hechingen in Swabia", "Floyd Mutrux and Colin Escott", "\" Anne of Green Gables\", \"Bridge to Terabithia\", \"All Summer in a Day\", \" Jacob Have I Loved\", \"The Chronicles of Narnia\" series", "\"\", and the 2013 Marvel One- Shot short film of the same name", "2008", "from July 2, 1967 to August 21, 1995", "Kennedy Road", "Mark Neveldine and Brian Taylor", "the fictional city of Quahog, Rhode Island", "Spiro Agnew", "The String Cheese Incident", "three", "Noel (who had previously only sung lead on B-sides)", "Thorgan", "6 February 1692", "9", "the twelfth", "Turkmenistan", "26,000", "Tallaght, South Dublin", "Indian state of Gujarat", "attorney, politician, and the principle founder of the Miami Dolphins", "punk rock", "Matt Lucas", "The 1990\u201391 UNLV Runnin' Rebels basketball team", "Bill Clinton", "John D Rockefeller's Standard Oil Company", "\"script\"", "pigs", "A 2011 American dystopian science fiction action thriller film written, directed, and produced by Andrew Niccol", "The flag of Vietnam, or `` red flag with a gold star ''", "pulsar", "The North Sea is more than 970 km long and 580 km wide, with an area of 570000 km2 and a volume of 54000 km3", "the first eight seasons", "\"totaled,\" which might give buyers the peace of mind knowing they will get a replacement vehicle.", "Steven Gerrard", "ketamine", "the Caspian tern", "The Southern Hemisphere", "Diebold", "Andr\u00e9s Iniesta"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5712113254392666}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true], "QA-F1": [0.8333333333333333, 1.0, 0.13333333333333333, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1818181818181818, 0.4, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.058823529411764705, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-5615", "mrqa_hotpotqa-validation-855", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-343", "mrqa_hotpotqa-validation-3340", "mrqa_hotpotqa-validation-5280", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3512", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-2859", "mrqa_hotpotqa-validation-5228", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5846", "mrqa_triviaqa-validation-4662", "mrqa_triviaqa-validation-3487", "mrqa_newsqa-validation-456", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-6593"], "SR": 0.453125, "CSR": 0.5304129464285714, "EFR": 1.0, "Overall": 0.7190513392857143}, {"timecode": 56, "before_eval_results": {"predictions": ["35,000", "Vishal Bhardwaj", "Macau", "Russian film industry", "no. 3", "The Government of Ireland", "Washington Street", "Eielson Air Force Base", "Galo (], \"Rooster\")", "Tom Shadyac", "Michael Phelps", "Richard Strauss", "Marie Fraser", "Iynx", "David Starkey", "Edward III", "Anne Perry", "September 13, 1994, by Bad Boy Records and Arista Records", "DI Humphrey Goodman", "\"Darconville\u2019s Cat\"", "An invoice, bill or tab", "October 16, 2015", "Dan Bilzerian", "The Andes", "Srinagar", "STS-51-C", "Gregg Harper", "The Ones Who Walk Away from Omelas", "Minnesota's 8th congressional district", "\"Three Colours\" trilogy", "November 27, 2002", "Hank Azaria", "UFC Fight Pass", "The Sun", "Peter Yarrow", "The Frost Report", "Armada", "1,925", "7 October 1978", "June 2, 2008", "Ravenna", "Norman Macdonnell", "the Battelle Energy Alliance", "a co-op of grape growers", "The Spiderwick Chronicles", "goalkeeper", "George Orwell", "Croatian", "Labour Party", "Fortunino", "Warsaw", "Texhoma", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "September 2017", "Ken Norton", "america", "Anne", "the UK", "Al Nisr Al Saudi", "5 1/2-year-old", "cobalt", "Batman to Rock Out", "the bassoon", "time in exchange for detailed public disclosure of an invention"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6497421962095875}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.4615384615384615, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.4347826086956522, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-675", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-7526", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-6632"], "SR": 0.5625, "CSR": 0.5309758771929824, "EFR": 0.9642857142857143, "Overall": 0.7120210682957394}, {"timecode": 57, "before_eval_results": {"predictions": ["1961", "Ballon d'Or", "Elsie Audrey Mossom", "trans-Pacific flight", "House of Hohenstaufen", "BraveStarr", "The White Knights of the Ku Klux Klan", "Benjam\u00edn Arellano F\u00e9lix", "lion", "Tony Award", "The German concept of Lebensraum", "China", "Esperanza Emily Spalding", "1854", "Art Deco-style skyscraper", "Starachowice", "Debbie Harry", "England", "Mineola, New York", "Kagoshima Airport", "Melbourne's City Centre", "926 East McLemore Avenue", "bioelectromagnetics", "Germany", "casting, job opportunities, and career advice", "ten episodes", "2012", "Ford Falcon", "Igor Stravinsky", "Euripides", "second segment", "Clitheroe Football Club", "Peter Wooldridge Townsend", "Paper", "November 27, 2002", "Campbellsville", "1837", "2006", "al-Qaeda", "Humberside Airport", "Algiers", "racehorse breeder", "Nia Kay", "11 November 1918", "1891", "Bank of China Tower", "Lerotholi Polytechnic FC", "achieved the best winning percentage of any Colgate coach with seven or more years at the helm of the Raiders", "the twelfth and thirteenth centuries", "hastings", "Richard Allen Street", "latitude 90 \u00b0 North", "a radius 1.5 times the Schwarzschild radius", "photoelectric receiver", "Japan", "Apollo", "l Leeds", "bragging about his sex life", "the shovel-ready infrastructure spending", "Fernando Torres", "Alicia Keys", "The first hotel in Tybee Island, Georgia", "Feb 19, 2014", "Gillis Grafstr\u00f6m"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6576140873015872}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.1111111111111111, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-4341", "mrqa_hotpotqa-validation-2367", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2122", "mrqa_naturalquestions-validation-3499", "mrqa_triviaqa-validation-1256", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2325", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-6491", "mrqa_triviaqa-validation-4287"], "SR": 0.53125, "CSR": 0.5309806034482758, "EFR": 1.0, "Overall": 0.7191648706896552}, {"timecode": 58, "before_eval_results": {"predictions": ["Karl Kr\u00f8yer", "an open window that fits neatly around him", "King Gyanendra", "Susan Atkins", "Palestinian prisoners", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "\"Mammograms are known to be uncomfortable,\"", "\"He wanted to kill all of us,\"", "The iconic Abbey Road music studios made famous by the Beatles are not for sale,", "10 municipal police officers", "his past and his future", "\"Empire of the Sun,\"", "Steven Chu", "uncomfortable,\"", "\"We want to have basic, durable, timeless, beautiful clothes,\"", "55-year-old", "capital murder and three counts of attempted murder", "the South Korean military responded by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "eight Indians whom the rebels accused of collaborating with the Colombian government,", "denying the Swiss maestro a 16th grand Slam title.", "curfew", "\"Nude, Green Leaves and Bust,\"", "demolishing American third seed Venus Williams in the final of the Sony Ericsson Open in Miami on Saturday.", "The Glasgow, Scotland concert", "an antihistamine and an epinephrine auto-injector for emergencies,", "Damon Bankston", "Afghanistan's", "a ban on inflatable or portable signs and banners on public property", "Some people ask: Why do genocides and mass atrocities happen?", "warns business owners to close their shops during daily prayers, or they will be temporarily shut down,", "humans", "Vancouver, British Columbia", "immediately releasing all civilians and laying down arms,\"", "two years ago.", "is a businessman, team owner, radio-show host and author.", "Sen. Barack Obama", "his own sense of spirit and energy and opportunity in this country that we've never seen.", "fake his own death by crashing his private plane into a Florida swamp.", "his visit to Istanbul last April, Basaran and a group of her friends started up a Turkish-American friendship club at her university.", "one-of-a-kind navy dress with red lining by the American-born Lintner,", "The Rev. Alberto Cutie", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "damage from Hurricane Irene and Tropical Storm Lee in Bradford, Dauphin, Columbia, Wyoming and Luzerne counties.", "the earthquake's devastation.", "a mammoth", "\"She had a smile on her face, like she always does when she comes in here,\"", "the creation of an Islamic emirate in Gaza,", "in the lawless southern provinces and especially in the Taliban stronghold of Helmand, poppy production was going on largely unchecked.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "2-1.", "says Somalia's piracy problem was fueled by environmental and political events.", "Francisco Pizarro", "Owen Vaccaro", "2003", "Renault", "cheese", "Alan Freed", "Woolsthorpe-by-Belvoir", "War Is the Answer", "arts manager", "The Antarctic ice sheet", "Ms.", "achieve", "prevent the fish from freezing"], "metric_results": {"EM": 0.34375, "QA-F1": 0.43712906013658903}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 1.0, 0.4, 0.0, 0.15789473684210525, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.15384615384615383, 0.36363636363636365, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.2222222222222222, 0.0, 0.5833333333333334, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18749999999999997, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.7586206896551725, 0.0, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-112", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1055", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-4517", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-1521"], "SR": 0.34375, "CSR": 0.5278072033898304, "EFR": 0.9761904761904762, "Overall": 0.7137682859160613}, {"timecode": 59, "before_eval_results": {"predictions": ["Holley Wimunc.", "Sunday", "five suspects,", "Mike Griffin,", "U.N. Security Council", "Democratic VP candidate", "Ken Choi,", "second child", "on the bench", "leftist Workers' Party.", "Robert", "Christiane Amanpour", "Narayanthi Royal", "Mexico", "five", "going somewhere very special, far away, because under the Communist regime you didn't travel that much and Prague was \"wow.\"", "a 10-day retreat,", "Keating Holland", "Asashoryu,", "Mutassim,", "Dr. Jennifer Arnold and husband Bill Klein,", "at the Lindsey oil refinery,", "Sunday.", "future relations between the Middle East and Washington", "prostate cancer,", "the southern city of Naples", "at least 300", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "2002", "Spc. Megan Lynn Touma,", "how preachy and awkward cancer movies can get.", "Kingdom City", "strife in Somalia,", "Miami Beach, Florida,", "Charles Darwin", "the New Jersey Economic Development Authority's 20% tax credit", "President Obama.", "people who have faith in this community that they want to do the right thing,\"", "six giant pumpkins, specially delivered from nearby Half Moon Bay (some weighing well over 1,000 pounds)", "baseball bat", "Las Vegas.", "an organ procurement agency in Southern California,", "Jaime Andrade", "Alfredo Astiz,", "ties", "KARK,", "The father of Haleigh Cummings,", "Amir Zaki", "$8.8 million", "Tuesday night", "Alan Graham", "Canada south of the Arctic", "Glenn Close", "Joseph M. Scriven", "Richard Krajicek", "Manchester", "Robert Plant", "Richie McDonald", "Cannes Film Festival", "Usher", "5-7-5", "Rooster Cogburn", "Korea", "1936"], "metric_results": {"EM": 0.453125, "QA-F1": 0.581291335978836}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true], "QA-F1": [0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.4, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3095", "mrqa_naturalquestions-validation-1872", "mrqa_triviaqa-validation-5522", "mrqa_hotpotqa-validation-1961", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-11013"], "SR": 0.453125, "CSR": 0.5265625, "EFR": 1.0, "Overall": 0.71828125}, {"timecode": 60, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1899", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-382", "mrqa_hotpotqa-validation-4012", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-892", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-266", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2558", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1143", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5687", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6157", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7783", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.828125, "KG": 0.50546875, "before_eval_results": {"predictions": ["2017", "1648 - 51 war", "Thomas Edison's assistants, Fred Ott", "season four", "Master Christopher Jones", "Juliet", "the status line", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "The Maginot Line", "1937", "Fix You", "Hirschman", "Bobby Eli, John Freeman and Vinnie Barrett", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "approximately 11 %", "Jonny Buckland", "Austria - Hungary", "Sharyans Resources", "Alex Rodriguez", "Seattle, Washington", "420", "Wylie Draper", "Pinar del R\u00edo Province", "DNA", "Magnavox Odyssey", "the ball is fed into the gap between the two forward packs and they both compete for the ball to win possession", "Sunday night", "tolled ( quota ) highways", "close by the hip, and under the left shoulder,", "innermost in the eye while the photoreceptive cells lie beyond", "Saturday", "Glenn Close", "reservoirs at high altitudes", "to increase support for the war effort", "into smaller pulmonary arteries that spread throughout the lungs", "Nepal", "the symbol \u00d7", "Ace", "Phillip Paley", "the tax rate paid by a small business", "the liver and kidneys", "Lee County, Florida, United States", "tenderness of meat", "Stephen Graham", "Florida", "2010", "Andy Kim", "infection, irritation, or allergies", "ABC", "August 1991", "The Maidstone Studios in Maidstone, Kent", "two", "helium", "horsepower", "Edward James Olmos", "Terrina Chrishell Stause", "Toyota Hilux", "Omar bin Laden,", "a man's lifeless, naked body", "jazz", "Falkland Islands", "a cement pond", "Seventy-six trombones", "Fifty Shades of Grey"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6141657464267758}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.5, 1.0, 0.3076923076923077, 0.0, 0.2666666666666667, 0.47058823529411764, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2, 0.0, 1.0, 1.0, 0.3076923076923077, 0.4444444444444444, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-8998", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-6052", "mrqa_hotpotqa-validation-4674", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-15624", "mrqa_searchqa-validation-893"], "SR": 0.515625, "CSR": 0.5263831967213115, "EFR": 0.967741935483871, "Overall": 0.7096844014410364}, {"timecode": 61, "before_eval_results": {"predictions": ["17 - year - old", "This procedure can be performed at any level in the spine ( cervical, thoracic, or lumbar ) and prevents any movement between the fused vertebrae", "Dido", "international aid", "at least US $2 trillion by GDP in nominal or PPP terms", "January 15, 2007", "Gorakhpur railway station", "Marin \u010cili\u0107", "Battle of Antietam", "1933", "the hardships of growing older and has no relationship to drug - taking", "the person compelled to pay for reformist programs", "John Travolta", "August 5, 1937", "Rory McIlroy", "Jonathan Breck", "the 4th century", "Smith Jerrod, the one man whose sincerity got to Samantha", "Acid rain", "2018 NCAA Division I Men's Basketball Tournament for their third National Championship", "Mulberry Street", "Pete Seeger", "The pacemaking signal generated in the sinoatrial node travels through the right atrium to the atrioventricular node, along the Bundle of His and through bundle branches to cause contraction of the heart muscle", "1994", "Owen Fielding", "10 May 1940", "Malloy as Pierre, Phillipa Soo as Natasha, Lucas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne, Brittain Ashford as Sonya, Shaina Taub as Mary", "Denver Broncos", "Judiththia Aline Keppel", "Vincent Price", "the customer's account", "Prairie Creek Redwoods State Parks", "The film follows a child with Treacher Collins syndrome trying to fit in", "metaphase", "Sally Field", "Eric Clapton", "from the most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "Aamir Khan and Mansoor Khan", "Colman", "Karen Gillan", "Easter", "Beijing, China", "`` 5 lakh rupees ''", "Havana Harbor", "1824", "1961", "Qutab Ud - Din - Aibak", "Prince Henry", "the inferior thoracic border -- made up of the diaphragm", "USCS or USC", "a legal decision", "floating ribs", "Melinda Messenger (Glamour Model)", "Romania", "Helensvale", "Kathryn Jean Martin \"Kathy\" Sullivan AM", "over 20 million", "\"I wanted to come here, and I wanted to see my kids graduate from this school district.\"", "torture and indefinite detention", "Tuesday's iPhone 4S news,", "sleeping sickness", "Jacob Marley", "John Hersey", "22 September 2015"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6000983991749547}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.125, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.15384615384615385, 0.0, 0.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.16, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.9, 1.0, 1.0, 1.0, 0.12500000000000003, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.2608695652173913, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10172", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8382", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6596", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-451", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-2373", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-2249", "mrqa_searchqa-validation-14622"], "SR": 0.46875, "CSR": 0.525453629032258, "EFR": 0.9705882352941176, "Overall": 0.7100677478652752}, {"timecode": 62, "before_eval_results": {"predictions": ["Christian Dior", "cranberry", "the Arch", "fibula", "fort boyard", "a whale", "the Mississippi", "Kevin Costner", "Lil Jon", "the Boers", "the Colosseum", "Goldeneye", "soup", "tarmes", "Churchill", "Scampton, Lincolnshire", "the Lincoln Tunnel", "Pinta", "Louisiana", "Lew Wallace", "Rembrandt", "Canada", "Tony Banks", "Pope Benedict XVI", "Strategic Petroleum Reserve", "Prague", "the engineer of Byzantium", "electricity", "Harry Dean Stanton", "fort boyard", "Beck", "the College of William & Mary", "the Sheepeater", "the oboe", "Montpelier", "the Brutes and Drones", "a pound of Antonio's flesh", "Bahrain", "Best Picture", "Hamlet", "Heroes", "Russia", "beer", "the Hawks", "a large gay crowd", "wood", "the Lord", "Dustin Hoffman", "Nittany", "Sicily", "Socrates", "more than 1,000", "the large area needed for effective gas exchange", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "fort boyard", "Honolulu", "fort boyard", "Esperanza Spalding", "2000", "Robert Sargent Shriver", "Daytime Emmy Lifetime Achievement Award", "Impeccable", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "the par 4 third hole"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5508637422360247}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.8695652173913043, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.47619047619047616, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-7337", "mrqa_searchqa-validation-4677", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-1906", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-13940", "mrqa_searchqa-validation-5976", "mrqa_searchqa-validation-1751", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-8943", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-481", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-16438", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-8676", "mrqa_searchqa-validation-6211", "mrqa_searchqa-validation-14898", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-8062", "mrqa_searchqa-validation-546", "mrqa_searchqa-validation-7407", "mrqa_searchqa-validation-15398", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6028", "mrqa_triviaqa-validation-7643", "mrqa_hotpotqa-validation-5895", "mrqa_newsqa-validation-3961", "mrqa_triviaqa-validation-1787"], "SR": 0.453125, "CSR": 0.5243055555555556, "EFR": 0.9714285714285714, "Overall": 0.7100062003968254}, {"timecode": 63, "before_eval_results": {"predictions": ["the Voting Rights Act", "a 919mm Parabellum pistol", "Disabilities", "Harry Potter and the Chamber of Secrets", "George John Mitchell", "Shampoos", "the Mycenaean palatial civilization", "the People's Party", "the Tower of London", "Daniel", "The Jinx", "a mall", "the United States", "Cosmopolitan", "David Beckham", "Minoan", "Japan", "the Stick Horse Rodeo", "a crowbar", "Vietnam", "The Stanford Tree", "the D major", "a prism schism", "Alaska", "Wallis Warfield Simpson", "a centipede", "Greek", "Sisters Rosensweig", "Brooklyn", "Penn State Hershey", "Easter Island", "Nasser", "the Hell is Hell", "Stephen Hawking", "Labour Day", "Mozambique", "landfills", "The Silence of the Lambs", "15", "Capone", "Paul McCartney", "Anne Murray", "Tennessee", "Buenos", "contagious", "Jane Austen", "a haploid hybrid", "baking soda", "peanuts", "philosophy", "Byzantium", "Icona Pop", "Rent", "Miami Heat", "gluten", "an embroidered cloth", "Easter", "2004", "Pacific War", "Tsavo East National Park", "Casa de Campo International Airport", "100", "an incentive and a method to reach car owners who haven't complied fully with recalls", "great-grandfather of Miami Marlin Christian Yelich"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6564140720390721}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.7692307692307693, 0.6666666666666666, 0.22222222222222224, 0.2857142857142857]}}, "before_error_ids": ["mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-3380", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-7830", "mrqa_searchqa-validation-2172", "mrqa_searchqa-validation-4616", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-7562", "mrqa_searchqa-validation-6957", "mrqa_searchqa-validation-16319", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-15049", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-14995", "mrqa_searchqa-validation-16029", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-7085", "mrqa_hotpotqa-validation-4005", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2362", "mrqa_hotpotqa-validation-798"], "SR": 0.515625, "CSR": 0.524169921875, "EFR": 1.0, "Overall": 0.715693359375}, {"timecode": 64, "before_eval_results": {"predictions": ["Malawi", "empire", "knife hones", "the short-beaked echidna and the duck-billed platypus", "British Airways", "Yoshi", "1961", "Blades", "The Booker Prize 2001", "Dante", "repechage", "uranium", "Wildcats", "the Comte de la F\u00e8re", "John Adams", "The Merchant of Venice", "volume", "Paul Rudd", "Tanzania", "Julian", "Yves Saint Laurent", "saxophone", "Firecracker Maria Elena", "September", "Grantham,", "Muriel Spark", "a state", "kvetch", "the Netherlands", "Lome", "what", "Christian Dior", "Pat Houston", "King William IV", "Dutch", "Jack Lemmon", "Trainspotting", "Australia", "the southwestern Pacific Ocean", "Diptera", "India and Pakistan", "obi", "Broccoli", "Heisenberg", "1976", "cyclopes", "phrenology", "Full Metal jacket", "Tokyo", "California", "Borrowdale", "third", "the fictional elite conservative Vermont boarding school Welton Academy", "the Old French tailleur ( `` cutter '' )", "Daniel Richard \" Danny\" Green", "between 11 or 13 and 18", "Franz Ferdinand Carl Ludwig Joseph Maria", "Chile", "the single-engine Cessna 206 went down,", "helping on the sandbags lines as the community raced to fill 1 million of them.", "Rachel Carson", "Italy", "Toni Morrison", "Copts"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6013480392156862}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3529411764705882, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-2156", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-2127", "mrqa_triviaqa-validation-4208", "mrqa_triviaqa-validation-825", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-7398", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6308", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1819", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8858", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-1926", "mrqa_searchqa-validation-4044", "mrqa_newsqa-validation-2435"], "SR": 0.546875, "CSR": 0.5245192307692308, "EFR": 1.0, "Overall": 0.7157632211538462}, {"timecode": 65, "before_eval_results": {"predictions": ["abraham barrie", "drums", "a wedge", "$200 million", "dreamgirls", "barry Taylor", "1986", "right-hand side", "commercial", "manson", "Uganda", "ash", "temperature", "satirical", "Alaska", "iron", "willier drogba", "the C\u00e9vennes", "bagram", "Mnemosyne", "william iv", "The Pillow Book", "arthur", "ethiopia", "Denver", "smack", "Massachusetts", "jet streams", "william williams", "The Apprentice", "altamont", "jennifer william lank", "brixham", "billy ray cyrus", "Ghana", "solo", "Nelson Mandela", "abraham hall", "Illinois", "sailing", "a hood", "two-half", "Oman", "CBS", "noah beery, Jr.", "Nebraska\u2013Lincoln", "Sarajevo", "power", "Beyonce", "b\u00e9arn", "MI5", "number of games where the player played, in whole or in part", "( `` Something to Sing About '' )", "a writ of certiorari", "\"Little Dixie\" area of western Missouri", "American Way", "polyglot", "fluoroquinolone", "Alina Cho", "eight.\"", "the Penguin", "Passover", "Johann Strauss II", "John Lennon and George Harrison,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5266826923076923}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true], "QA-F1": [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-4892", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3599", "mrqa_triviaqa-validation-2184", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-7203", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-2918", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-5878", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-3265", "mrqa_naturalquestions-validation-716", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-150", "mrqa_newsqa-validation-1804", "mrqa_searchqa-validation-14825"], "SR": 0.46875, "CSR": 0.5236742424242424, "EFR": 1.0, "Overall": 0.7155942234848485}, {"timecode": 66, "before_eval_results": {"predictions": ["sleepless in seattle", "Harriet Harman", "Portugal", "typhoid fever", "Sheryl Crow", "Melvil Dewey", "j Jamaica", "Pancho Villa", "wild bunch", "mikhail gorbachev", "South Korea", "prince bride", "tiptoe through the Tulips", "eriennia", "Lesley Garrett", "erinys", "the C\u00e9vennes", "Les Invalides", "1861", "brown", "london", "Dvorak", "arise", "aslan", "37", "narwhal", "wishbone", "photographer", "Charlie Chan", "taekwondo", "phosphorus", "Hogwarts School of Witchcraft and Wizardry", "lead", "Mercury", "trenches", "dorset", "j. S. Bach", "Groucho Marx", "lacrosse", "Queen Elizabeth I", "eri london", "1825", "bordered by Albania", "clydebank", "Alice", "Bette Davis", "heating device", "Chrysler", "\"The closest approach to the original sound\"", "west africa", "Carl Sagan and his wife and co-writer, Ann Druyan", "2010", "Pepsi", "mass of alcohol per volume of blood", "FAI Junior Cup", "the Knight Company", "north Miami-Dade County", "military veterans", "January 24, 2006.", "after giving birth to baby daughter Jada,", "a retronym", "Ovid", "spooky", "Latin"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6119791666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.6363636363636364, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4413", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-4502", "mrqa_triviaqa-validation-458", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-1834", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-1794", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-4523", "mrqa_hotpotqa-validation-1030", "mrqa_newsqa-validation-2498", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-8081"], "SR": 0.578125, "CSR": 0.5244869402985075, "EFR": 0.9629629629629629, "Overall": 0.7083493556522941}, {"timecode": 67, "before_eval_results": {"predictions": ["the Rheingold", "Mahatma Gandhi", "Battlestar Galactica", "humpback whale", "South Africa", "Brett Favre", "Peril", "Texas", "angioplasty", "anxiety", "phaser", "Mary Pickford", "a termite", "Sayonara", "cat", "india", "Lone Ranger", "Mars", "the Battle of Verdun", "statistic", "Andes", "India", "c Cecil John Rhodes", "Houston Rockets", "sirloin", "apartheid", "Boston", "Red Dead Redemption", "Van Helsing", "Thomas", "pesos", "Shop", "His Majesty's Ship", "Urban Outfitters", "Baskin-Robbins", "Andrew Wyeth", "smallpox", "jimmy", "Al Jolson", "Risk", "recessive", "Don Quixote", "Chesterfield", "midnight", "The Age of Innocence", "Students for a Democratic Society", "Bering Sea", "the Caucasus", "Alisa Harris", "tritonic", "With a Little Help from My friends", "the Brewster family", "After World War II", "acronym", "brighton", "Microsoft", "Fleet Street", "1966", "Juilliard School", "City of Starachowice", "success as a recording artist", "Mombasa, Kenya,", "Thaksin Shinawatra,", "peninsulas"], "metric_results": {"EM": 0.625, "QA-F1": 0.6640625}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-15118", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4010", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-5960", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-14069", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-3274", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-7165", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2674", "mrqa_naturalquestions-validation-2064"], "SR": 0.625, "CSR": 0.5259650735294117, "EFR": 0.9583333333333334, "Overall": 0.7077190563725491}, {"timecode": 68, "before_eval_results": {"predictions": ["Davenport", "soup", "(Ulysses) S. Grant", "the egg and sperm", "the High Plains area", "the gap", "The Bodyguard", "South Dakota", "neurons", "Kirk Lazarus", "the megaton", "the king", "electrons", "the Communist Party", "Alfred Binet", "the wife of Bath", "the Atlantic Ocean", "the Billboard", "James Buchanan", "Hinduism", "the Miasa French Countryside Ruby", "( Wayne) Brady", "Trotsky", "Arkansas", "chromosomes", "Cuba", "The Computer Age", "airplanes", "Surgeons", "Thomas Nast", "3", "freezing", "Picabo Street", "National Security Agency", "Kate", "Honey Nut Cheerios", "Hercules", "5", "Rod Laver", "Eczema Therapy", "Ivy Dickens", "(Bud) Wellington", "vote", "Herbert Hoover", "Joe Hill", "IHOP", "Bubba\\'s Book Club", "Lou Gehrig\\'s disease", "Samuel Beckett", "William Pitt the Younger", "Independence Day", "Jethalal Gada", "Kevin Sumlin", "Charles Carson", "Albert Einstein", "Orion", "the dodo", "1986", "James Gandolfini", "Mandarin Airlines", "at least two and a half hours.", "39,", "Buenos Aires.", "Citizens for a Sound Economy"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6270833333333333}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.4, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-30", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-15848", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-15032", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-5152", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-6721", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-8996", "mrqa_searchqa-validation-2684", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-14015", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466"], "SR": 0.578125, "CSR": 0.5267210144927537, "EFR": 1.0, "Overall": 0.7162035778985507}, {"timecode": 69, "before_eval_results": {"predictions": ["Manitoba", "a research organization", "60 Minutes", "the Christmas", "midnight", "Socrates", "Al Gore", "the Louvre", "the House of Wessex", "(Erwin) Rommel", "Baton Rouge", "Langston Hughes", "a manacle", "Wisconsin", "Cleveland", "a slave", "Toronto", "Stevie Wonder", "Typhoid Mary", "an inch", "from 700 to 2000", "Tokyo", "Tennessee Williams", "the United Arab Emirates", "Lurch", "a chancellor", "(Thomas) Woodrow Wilson", "Old Yeller", "Proverbs", "2001: A Space Odyssey", "A Brief", "Prince", "Genesis", "William Shakespeare", "(Yitzhak) Rabin", "Brisbane", "Jason Bourne", "the Holocaust", "a globe", "Iowa", "Mephistopheles", "Samsonite", "a second lieutenant", "Vietnam", "Genve", "Punxsutawney, Pennsylvania", "Sports Illustrated", "Venus", "Shia LaBeouf", "Annapolis", "Princess Anne", "Hungary", "January 15, 2007", "Introverted Sensing ( Si )", "(Ed) I", "Full Monty", "Stella McCartney", "1898", "Anne of Green Gables", "Daniil Shafran", "a Columbian mammoth", "Disney", "can help the convicts find calmness in a prison culture fertile with violence and chaos.", "then-Sen. Obama"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6300595238095239}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.1904761904761905, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-4759", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-234", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-14167", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-3164", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13160", "mrqa_searchqa-validation-7011", "mrqa_searchqa-validation-6746", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-2651", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-3564", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3695"], "SR": 0.59375, "CSR": 0.5276785714285714, "EFR": 1.0, "Overall": 0.7163950892857143}, {"timecode": 70, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4260", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5757", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1840", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10156", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-1971", "mrqa_squad-validation-2082", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3215", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4299", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-513", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6067", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6917", "mrqa_squad-validation-6960", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9401", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.806640625, "KG": 0.50546875, "before_eval_results": {"predictions": ["Doc Holliday", "Fritos", "cross", "the fowls", "a fruitcake", "Carrie Underwood", "Subclue 2", "Christa McAuliffe", "Kilimanjaro", "Misbegotten", "a pumpkin", "Jumbo", "Stoke", "J!", "Adolf Hitler", "Portland", "the imagist movement", "Rudolf Diesel", "a palace", "a prison hulk", "the Spanish Republic", "Ruth", "sugar", "Cheaper by the Dozen", "Hans Christian Andersen", "San Francisco", "Betsey Johnson", "Wanted", "the bugle", "BORE", "Emma Peel", "The Homestead Act", "Liberty", "the Max Factor", "plantain", "Marie Curie", "German", "Aladdin", "the rain", "Samson", "Toy Story", "the Boston Tea Party", "George Sand", "Jim Jarmusch", "Afghanistan", "Minos", "salamander", "James Fenimore Cooper", "the rose hips", "Led Zeppelin", "Nice", "Michigan and surrounding states and provinces", "Texas A&M Aggies", "Alex Burrall, Jason Weaver and Wylie Draper played Michael Jackson in different eras", "Samuel", "The Kentucky Derby", "Iwo Jima", "Charles L. Clifford", "Vince Staples", "Sam Bettley", "Al-Shabaab", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Lucky Dube", "Florida"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7191695971867007}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1739130434782609, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-4747", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-2880", "mrqa_searchqa-validation-15349", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-2455", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-16761", "mrqa_searchqa-validation-8455", "mrqa_naturalquestions-validation-2870", "mrqa_triviaqa-validation-1148", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-585"], "SR": 0.671875, "CSR": 0.5297095070422535, "EFR": 1.0, "Overall": 0.7128950264084507}, {"timecode": 71, "before_eval_results": {"predictions": ["The Beatles", "The Andy Griffith Show", "Inigo Montoya", "Pocono Mountains", "trip", "watermelon", "a karting", "tanks", "Simon & Garfunkel", "Albert Pujols", "Andean", "4", "nebulae", "George W. Bush", "Eastwick", "The Who", "Cy Young", "A trilogy", "a mime instructor", "conga drums", "fern", "Nellie Bly", "IBM", "Pizza sauce", "A Lesson from Aloes", "menudo-mdo", "debts", "cloven", "Nicole Kidman", "Aristophanes", "Wimbledon", "Lumbini,", "restrictive", "The Jungle Book", "turquoise", "Papua New Guinea", "Rooster Cogburn", "Halo 3", "Boz", "Sayonara", "touch", "a crescent", "The Moment of Truth", "aardvark", "harpy", "a play", "James A. Garfield", "a duck", "The Dream Factory", "a rocket", "Henry Cavendish", "three degrees of freedom", "Olivia", "May 31, 2012", "Syria", "Otto Hahn", "nynorsk", "World War II", "Peter Kay\\'s Car Share", "Oneida Limited", "Zac Efron", "the punishment for the player", "the murders of his father and brother.\"", "mantle"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6783854166666666}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11101", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-1282", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-10741", "mrqa_searchqa-validation-2234", "mrqa_searchqa-validation-10201", "mrqa_searchqa-validation-12152", "mrqa_searchqa-validation-1831", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-8738", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-2850", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-5808", "mrqa_newsqa-validation-2382"], "SR": 0.65625, "CSR": 0.5314670138888888, "EFR": 1.0, "Overall": 0.7132465277777778}, {"timecode": 72, "before_eval_results": {"predictions": ["133", "the United States", "Comoros Islands", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "$2 billion", "\"Slumdog Millionaire\"", "17", "London's O2 arena,", "Joel \"Taz\" DiGregorio", "second child", "in her home", "forgery and flying without a valid license,", "U.S. 93", "a progressive neurological disease", "64,", "Muslim festival", "figure out a way that was practical to get a drum set on a plane.\"", "The supplemental spending bill", "The detainees are ethnic Uighurs,", "a satellite", "Mokotedi Mpshe,", "cross-country skiers", "Jeff Klein", "Saturn", "the reality he has seen is \"terrifying.\"", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "Haeftling,", "severe flooding", "Consumer Reports", "Now Zad in Helmand province, Afghanistan.", "a clear strategy", "second", "through the weekend", "Sen. Barack Obama", "South Africa's president", "the abduction of minors.", "three", "Fernando Caceres", "Joan Rivers", "the L'Aquila earthquake,", "Sgt. Barbara Jones", "country directors", "Tehran", "\"We must eliminate the perceived stigma, shame and dishonor of asking for help,\"", "when times get tough,", "Filippo Inzaghi", "Acura MDXA", "Kabul", "the bombers", "Mom", "Asashoryu", "king", "a form of fixed - mobile convergence ( FMC )", "Ant & Dec", "Robert A. Heinlein", "a falcon", "friends", "IT", "St. Louis Cardinals", "Dumfries and Galloway, south-west Scotland", "Popcorn", "3", "an elephant", "prime minister"], "metric_results": {"EM": 0.53125, "QA-F1": 0.612006353021978}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.125, 0.25, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.7499999999999999, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1373", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-1128", "mrqa_naturalquestions-validation-6294", "mrqa_triviaqa-validation-179", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-2653", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-3590", "mrqa_hotpotqa-validation-364"], "SR": 0.53125, "CSR": 0.5314640410958904, "EFR": 1.0, "Overall": 0.7132459332191781}, {"timecode": 73, "before_eval_results": {"predictions": ["$40 and a bread.", "Justicialist Party,", "\"Vaughn,\"", "U.N. nuclear watchdog agency", "11,", "\"The Sopranos,\"", "average of 25 percent", "April 2", "Robert Mugabe", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "OneLegacy,", "upper respiratory infection,\"", "10:30 p.m. October 3,", "Arabic, French and English,", "United Nations", "\"Maude\" and the sardonic Dorothy on \"The Golden Girls,\"", "Bill,", "\"Rin Tin Tin: The Life and the Legend\"", "ancient Jewish tradition", "\"The Lost Symbol,\"", "J.G. Ballard,", "protective shoes", "immediate release into the United States of 17 Chinese", "2008.", "12 hours", "Now Zad in Helmand province, Afghanistan.", "Russian concerns that the defensive shield could be used for offensive aims.", "American", "Los Alamitos Joint Forces Training Base", "Al-Shabaab,", "safer surroundings.", "$60 billion on America's infrastructure.", "upper respiratory infection,\"", "\"Test scores and graduation rates", "Sri Lanka", "\"CNN Heroes: An All-Star Tribute\"", "\"medical escorts\" for deportation since 2003.", "the wars in Iraq and Afghanistan through the remainder of his presidency and into spring 2009.", "scientific reasons.", "innovative, exciting skyscrapers", "\"My father and brother were betrayed by a Dutch nurse who was arrested and taken to the headquarters to be interrogated.\"", "Pakistani territory", "hanged in 1979 for the murder of a political opponent two years after he was ousted as prime minister in a military coup.", "Pew Research Center", "Symbionese Liberation Army -- perhaps best known for kidnapping Patricia Hearst --", "38,", "\"stressed and tired force\"", "speed attempts.", "Leo Frank,", "At least 38", "Iran", "Port Said to the southern terminus of Port Tewfik at the city of Suez", "Philadelphia, which is Greek for brotherly love", "September 2017", "St John University College", "Tony Cozier", "Hallam F.C.", "Dulwich, South Australia", "New York Giants", "South America", "Vulcan", "Beloved", "a flapjack", "Hungarian"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5908181278104047}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false], "QA-F1": [0.7499999999999999, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.25, 0.0, 0.8, 0.0, 0.5263157894736842, 1.0, 1.0, 0.0, 1.0, 0.2608695652173913, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.28571428571428575, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2763", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-666", "mrqa_hotpotqa-validation-5414", "mrqa_searchqa-validation-1383"], "SR": 0.46875, "CSR": 0.5306165540540541, "EFR": 1.0, "Overall": 0.7130764358108108}, {"timecode": 74, "before_eval_results": {"predictions": ["South Korea's", "\"Dr. Jennifer Arnold and husband Bill Klein,", "16", "the program was made with the parents' full consent.", "five", "Marie-Therese Walter.", "Sixteen", "1979", "federal help", "consumer confidence", "$81,8709", "Climatecare,", "a Florida girl", "state senators who will decide whether to remove him from office", "August 19, 2007.", "Ghana", "62,000", "1-0", "Brian Smith.", "fight against terror will respect America's values.", "Haiti National Police (HNP)", "Sunday,", "city of romance, of incredible architecture and history.", "1,500 Marines", "\"Chadian authorities immediately accused the charity of kidnapping the children and concealing their identities.", "American Bill Haas", "Silicon Valley.", "\"The number of Americans who think things are going very badly has dropped from 40 percent in December to 32 percent now,\"", "at least nine people", "British", "Cash for Clunkers", "Yemen", "the Dalai Lama", "800,000", "fascinating transformation that takes place when carving a pumpkin.", "Russia", "one", "Democratic VP candidate", "Former U.S. soldier Steven Green", "Cologne, Germany,", "President George H.W. Bush", "Kurdish militant group in Turkey", "6-4", "Karen Floyd", "the Arctic north of Murmansk down to the southern climes of Sochi", "flights", "1-1 draw", "South African", "Alaska or Hawaii.", "30", "equality,", "Virgil Ogletree", "Robert Hooke", "the American Civil War", "giraffe", "hiking and climbing", "World Bank", "Idaho", "Robert Matthew Hurley", "Ashanti Region", "Michael Kors", "Katherine Heigl", "Lenin", "australia"], "metric_results": {"EM": 0.5, "QA-F1": 0.594426159518038}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.10526315789473685, 0.8, 1.0, 0.20689655172413796, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-258", "mrqa_naturalquestions-validation-2482", "mrqa_triviaqa-validation-280", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-5300", "mrqa_triviaqa-validation-6077"], "SR": 0.5, "CSR": 0.5302083333333334, "EFR": 0.96875, "Overall": 0.7067447916666667}, {"timecode": 75, "before_eval_results": {"predictions": ["Bear Grylls", "euros", "Adam Faith", "Elbe", "Australia", "Edward Pierce", "Stockholm", "Leo", "Henry VIII", "a power outage Sunday,", "Clarice Starling", "Pluto", "Deep Blue", "violet", "1996", "squid and other fish", "James Scott", "Isaac", "New Israel Shekel", "serbia", "The Lost Weekend", "Althorp", "the conquest of Peru", "June", "Persuasion", "Robert Taylor", "the AllStars", "Hannibal", "\"The Greatest Generation\"", "fishes", "floating", "Florence", "hallmarks", "pascal", "\"gruppetto\"", "the British pop band Go West", "11", "Israel", "football", "Porteous Riots", "English", "Gentlemen Prefer Blondes", "bulls", "Kenya", "Conrad Murray", "Shuttle Launch", "Amelia Earhart", "Spinach", "John Gorman", "Benedict", "cirrocumulus", "Ariana Clarice Richards", "Turner Layton", "The Parlement de Bretagne", "Campbellsville University", "Daniel Craig", "Vanilla Air", "HPV (human papillomavirus)", "\"This is not something that anybody can reasonably anticipate,\"", "21 percent suggesting that", "the vacuum effect", "Ohio", "Parkinson\\'s disease", "John Gotti, the family's boss"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6448660714285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-5929", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-771", "mrqa_triviaqa-validation-2332", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-5132", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-1221", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7059", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-3838", "mrqa_naturalquestions-validation-7021", "mrqa_hotpotqa-validation-684", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-13638", "mrqa_hotpotqa-validation-4564"], "SR": 0.578125, "CSR": 0.5308388157894737, "EFR": 0.9259259259259259, "Overall": 0.6983060733430799}, {"timecode": 76, "before_eval_results": {"predictions": ["Steve Jobs", "Les Bleus", "Islamic", "Cpl. Richard Findley,", "two women", "at least seven", "drug cartels", "suicide car bombing", "Illinois Reform Commission", "senior vice-chairman of Hussein's Revolutionary Command Council.", "high tide.", "pipelines and hostage-taking", "183", "80,", "british state Sen. Jeff Klein", "\"Hawaii Five-O\"", "back at work.", "Mexico", "The Rev. Alberto Cutie", "California, Texas and Florida,", "3-3 draw", "Iran of trying to build nuclear bombs,", "withdrawing most U.S. forces by the end of his current term,", "dancing", "Bridgestone Invitational", "Jenny Sanford,", "\"It was a wrong thing to say,", "Sheikh Sharif Sheikh Ahmed", "8,", "\"and I am not confident to what degree our sincerity was conveyed.\"", "bipartisan", "Haiti", "Mashhad", "10", "Buenos Aires.", "Coast Guard", "Dennis Davern,", "Oaxaca", "Sony Ericsson Open", "Aravane Rezai", "1.0-magnitude earthquake sent a quarter-mile pier crumbling into the sea along with two of his trucks.", "\"the most dangerous precedent in this country, violating all of our due process rights.\"", "more than 15,000", "Jenny Sanford,", "on vacation", "The father of Haleigh Cummings,", "\"I am sick of life -- what can I say to you?\"", "E. coli", "southwestern Mexico,", "1,500", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "Jyoti Basu", "60", "Stax Records songwriters Homer Banks, Carl Hampton and Raymond Jackson", "leeds", "edward ii", "seattle", "strings", "Dante Bonfim Costa Santos", "1958", "british general", "british", "air pressure", "crocodile"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6403844615529397}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.18181818181818182, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.88, 0.08695652173913043, 1.0, 1.0, 1.0, 0.3333333333333333, 0.3076923076923077, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-990", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-4796", "mrqa_hotpotqa-validation-1902", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-12792", "mrqa_triviaqa-validation-1454"], "SR": 0.515625, "CSR": 0.5306412337662338, "EFR": 1.0, "Overall": 0.7130813717532468}, {"timecode": 77, "before_eval_results": {"predictions": ["Bill Haas", "a team of eight surgeons", "more than 200.", "The forward's lawyer", "American Civil Liberties Union", "Ronald Cummings", "Dr. Death in Germany", "Ralph Lauren", "Charlie Chaplin", "behind the operator's station.", "Missouri.", "four", "Jenny Sanford,", "British", "hundreds", "more than 78,000 parents of children ages 3 to 17,", "archdiocese Web page", "The Israeli Navy", "Buddhism", "in Seoul,", "President Obama", "the United States", "Prince George's County police Cpl. Richard Findley,", "John and Elizabeth Calvert", "more than $2 billion in disaster assistance", "oppose racial intolerance.", "Spain", "Michael Jackson", "Brazil", "Pixar's", "Pixar's", "1,500", "Bridgestone Invitational", "Two United Arab Emirates based companies", "fascinating transformation that takes place when carving a pumpkin.", "the best-of-three series.", "Transport Workers Union leaders", "Amanda Knox's aunt", "in the Muslim north of Sudan", "Melbourne", "cancer", "a man headed to Washington was threatening the inaugural,", "the ship", "338", "finance", "Haeftling,", "bribing other wrestlers to lose bouts,", "in the northwestern province of Antioquia,", "outfit from designer", "350 U.S. soldiers", "\"Taz\" DiGregorio,", "active absorption of water from the soil by the root", "Robin", "17 -- 15", "three Worlds", "the South China Sea", "Egypt", "The satirical", "Valhalla Highlands Historic District, also known as Lake Valhalla,", "boxer", "push", "rain", "Marsalis", "radon"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5288862179487179}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7777777777777777, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6153846153846153, 0.5, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2633", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-2997", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-394", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-4358"], "SR": 0.453125, "CSR": 0.5296474358974359, "EFR": 1.0, "Overall": 0.7128826121794872}, {"timecode": 78, "before_eval_results": {"predictions": ["In the battle for the fourth and final Champions League qualifying spot,", "Aung San Suu Kyi", "1957,", "Fort Bragg in North Carolina.", "their emergency plans", "South Africa", "Al-Shabaab,", "black is beautiful,\"", "assassination of President Mohamed Anwar al-Sadat", "Diego Milito's", "11 countries,", "Robert", "11.4 million orphans", "a senior at Stetson University studying computer science.", "to places like Southeast Asia and India.", "did not go into further detail about her heart condition or the medical procedure.", "200 human bodies at various life stages -- from conception to old age,", "Phillip Myers,", "Congress", "bipartisan", "a rally at the State House next week", "1912.", "Steven Green", "The plane, an Airbus A320-214,", "Casey Anthony,", "Lifeway's 100-plus stores nationwide", "way American men and women dress", "some deaths", "Seattle home.", "Dr. Christina Romete,", "sRI International,", "\"The Real Housewives of Atlanta\"", "an independent homeland since 1983.", "Air traffic delays began to clear up Tuesday evening after computer problems left travelers across the United States waiting in airports,", "Ames, Iowa,", "Barbara Streisand's", "the death of a pregnant soldier", "raping", "five", "Takashi Saito,", "Bob Johnson", "$41.1 million)", "in the vast expanses of subtropical Africa and Asia.", "natural disasters", "Joe Jackson,", "red varnished cover with the word \"Album\" inscribed on it in gold lettering,", "the House,\"", "his bullet-riddled body", "John Demjanjuk,", "following in Arizona's footsteps would take states in the wrong direction.", "seven", "song's central theme of loss", "antimeridian", "instructions", "The Undertones", "Johnny Mercer.", "Fenn Street School", "American actor and former fashion model", "Vietnam War", "1967", "Ukraine", "capitals.", "Scott Fitzgerald", "genome"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5980909715284716}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714288, 0.15384615384615385, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727272, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-889", "mrqa_newsqa-validation-374", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-584", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2118", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9852", "mrqa_triviaqa-validation-6200", "mrqa_hotpotqa-validation-5128", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-10570"], "SR": 0.515625, "CSR": 0.5294699367088608, "EFR": 0.9354838709677419, "Overall": 0.6999438865353205}, {"timecode": 79, "before_eval_results": {"predictions": ["Dick Rutan and Jeana Yeager", "maintenance fees", "The eighth and final season", "Rigg", "March 2016", "clara notitia cum laude, `` brilliant celebrity with praise ''", "the name announcement of Kylie Jenner's first child", "2003", "in the U.S.", "seven", "February 16, 2018", "Coroebus of Elis", "Jonathan Breck", "George Harrison", "Howard Rollins", "Ancy Lostoma duodenale", "January 17, 1899", "eight hours ( UTC \u2212 08 : 00 )", "Thespis ( / \u02c8\u03b8\u025bsp\u026as / ; Greek : \u0398\u03ad\u03c3\u03c0\u03b9\u03c2 ; fl. 6th century BC )", "biological taxonomy", "Wednesday, September 21, 2016", "Robber baron", "three", "Tbilisi, Georgia", "off the rez", "1895", "King Harold Godwinson", "Sir Ronald Ross", "May 29, 2018", "1979", "In the television series's fourth season", "ancient Athens", "Russia", "the magnetic stripe `` anomalies '' on the ocean floor", "United Nations", "2018", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "Real Madrid", "Angel Benitez", "capillary action", "1830", "1st Earl Mountbatten", "foreign investors", "Bhupendranath Dutt", "In June 22, 1942", "775 rooms", "In 1984", "senators", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Paul Lynde", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "treaty of Waitangi", "James Garfield", "Gambia", "England", "Yekaterinburg", "47,818,", "seven", "Samoa", "to close their shops during daily prayers,", "Versailles", "Westminster College", "Bob Dylan", "Mount Pelee"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7226571120872591}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [0.5714285714285715, 0.0, 0.6666666666666666, 1.0, 1.0, 0.761904761904762, 0.7999999999999999, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.6, 0.2222222222222222, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727272, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-7848", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-5144", "mrqa_searchqa-validation-13149", "mrqa_searchqa-validation-13773"], "SR": 0.5625, "CSR": 0.5298828125, "EFR": 0.8571428571428571, "Overall": 0.6843582589285714}, {"timecode": 80, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11088", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.82421875, "KG": 0.51015625, "before_eval_results": {"predictions": ["the 12th century", "14 December 1972 UTC", "China", "1919", "Abraham Gottlob Werner", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "near the Afghan - Pakistan border", "September, 2016", "July 2014", "peroxidase", "at a given temperature", "Skat", "Claudia Grace Wells", "Bill Russell", "China in American colonies", "Eobard", "B cells", "Melanie Lynskey", "four", "1923", "San Francisco, California", "2018 Winter Olympics", "Lilian Bellamy", "Denver Broncos", "Zoe Badwi, Jade Thirlwall's cousin", "1997", "William Wyler", "Ray Harroun", "one person", "NIRA", "New Mexico", "Kaley Christine Cuoco", "Strabo", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "the United Kingdom", "the Gupta Empire", "the oculus, or `` eye point ''", "James Madison", "Matt Monro", "Burton upon Trent", "Angola", "June 1992", "prokaryotic", "Germany", "the English", "Inti, the sun god of the Inca religion", "Jason Flemyng", "blue", "unknown origin", "New Zealand to New Guinea", "1963", "Kaiser Chiefs", "Donna Jo Napoli", "claire goose", "\"The 8th Habit\"", "science fiction", "Earvin \"Magic\" Johnson Jr.", "President Sheikh Sheikh Ahmed", "a Burmese python", "posting a $1,725 bail,", "Shimon Peres", "anesthesia", "( Jesse) Owens", "carbon"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6477092352092353}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.2, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-5711", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-4432", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2802", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-1713", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-11202", "mrqa_searchqa-validation-16652"], "SR": 0.546875, "CSR": 0.5300925925925926, "EFR": 1.0, "Overall": 0.7201591435185185}, {"timecode": 81, "before_eval_results": {"predictions": ["William the Conqueror", "1963", "Sachin Tendulkar and Kumar Sangakkara", "The President of Zambia", "Ray Charles", "in Ephesus in AD 95 -- 110", "the four - letter suffix", "in the 1970s", "Dan Stevens", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "North Dakota", "Tatsumi", "1969", "Ethiopia", "1937", "artes liberales", "(Paul) Lynde", "( Italian : MSC Crociere S. p.A. )", "2020", "2007", "Hodel", "the coffee shop Monk's", "Siddharth Arora / Vibhav Roy", "1991", "From 1900 to 1946", "between 2 World Trade Center and 3 World Trade center -- as well as to public concourses under the various towers in the World Trade Centre complex", "Munich, Bavaria, during 8 -- 9 November 1923", "Steve Russell", "October 30, 2017", "Paul Lynde", "to regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Frank Oz", "Sam Waterston", "Sunday night", "Johnny Darrell", "in the 7th century at Rendlesham in East Anglia", "2018", "Jesse Triplett", "(Gopaulsingh as Caleb Dunbar", "2013", "two", "Chris Martin", "Bartemius Crouch Jr impersonating Alastor `` Mad - eye '' Moody ( book four )", "Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluated", "Jackie Robinson", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "Brazil, Turkey and Uzbekistan", "Times Square in New York City west to Lincoln Park in San Francisco", "two", "The pia mater", "United Nations Peacekeeping Operations", "the Kinks", "Inishtrahull Island", "the Indus Valley", "the junction with Interstate 95", "mythology", "as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "$1.45 billion", "Lindsey Vonn", "(Watchmen)", "Ptolemy", "Dixie", "San Salvador", "Snickers candy bars"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6577016641983747}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 0.5714285714285715, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.7999999999999999, 0.4, 0.631578947368421, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-1642", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-1861", "mrqa_newsqa-validation-3916", "mrqa_newsqa-validation-3156", "mrqa_triviaqa-validation-7778"], "SR": 0.53125, "CSR": 0.5301067073170731, "EFR": 0.9333333333333333, "Overall": 0.7068286331300813}, {"timecode": 82, "before_eval_results": {"predictions": ["the Four Seasons", "Kathleen Erin Walsh", "in 1603", "Stephen Graham", "in 2010", "Tulsa, Oklahoma", "beta decay", "Nickelback", "1998", "John Adams", "Ford", "NCIS Special Agent in Charge", "Ed Furlong", "(Gustave) Biva", "July 2, 1928", "Dottie West", "in northern China", "Blood is the New Black", "1977", "arm", "Professor Eobard Thawne", "Jack Nicklaus", "arm regions", "Newfoundland and Labrador", "December 25", "a young husband and wife", "May 29, 2018", "Anglican", "September 27, 2017", "April 10, 2018", "Daniel Suarez", "the Confederate States of America ( 1861 -- 1865 )", "January 15, 2007", "the Soviet Union", "the optic chiasma", "Japan -- Korea Protectorate Treaty", "Bangladesh -- India border", "Norway", "Jenny Slate", "( 1 ) 2013", "Prince William, Duke of Cambridge", "Nicklaus", "1904", "Jim Carrey", "Matt Monro", "a little warmth", "in the thylakoid lumen", "( born November 28, 1973 )", "energy from light is absorbed by proteins called reaction centres", "in Rome in 336", "March 29, 2018", "Charles Darwin", "Hiram Kilborn", "Alfa Romeo", "five months", "Wanda", "April 24, 1934", "40", "49,", "nearly $2 billion in", "Sex and the City", "coffee", "(Bryan) Adams", "Oldham, in Greater Manchester, England"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6548067384004883}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 0.0, 0.07692307692307691, 1.0, 1.0, 1.0, 0.28571428571428575, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.8, 0.4, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9687", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-3677", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2444", "mrqa_triviaqa-validation-6822"], "SR": 0.484375, "CSR": 0.5295557228915663, "EFR": 0.9393939393939394, "Overall": 0.7079305574571012}, {"timecode": 83, "before_eval_results": {"predictions": ["American 3D computer-animated comedy", "1964", "Hindi", "eight", "Flula Borg", "Nevada", "Swiss", "1999", "Congo River", "4,613", "George Washington Bridge", "Harry Robbins \"Bob\" Haldeman", "Guardians of the Galaxy Vol.", "2.1 million", "6'5\" and 190 pounds", "526 people per square mile", "eastern Cheshire", "people working in film and the performing arts", "Saint Elgiva", "Scotland", "2006", "Carrefour", "infinite sum of terms", "Stravinsky\\'s", "Wayne Rooney", "second half of the third season", "Johnson & Johnson", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "the Blue Ridge Parkway", "Backstreet Boys", "45th", "Foxborough, Massachusetts", "Wichita", "animation", "Sir John Major", "sandstone", "Reginald Engelbach", "Dutch", "BC Dz\u016bkija", "Anne", "James Fox", "Rebirth", "Big Mamie", "banjo player", "Maxwell Atoms", "1903", "King of France", "Alemannic", "Larry Alphonso Johnson Jr.", "Marlon St\u00f6ckinger", "Taylor Swift", "October 27, 2016", "tenderness of meat", "Tigris and Euphrates rivers", "Mozambique Channel", "the Astor family", "Tashkent", "Afghanistan,", "Samson D'Souza,", "black bear", "speed", "Yogi Bear", "sakura", "fermenting dietary fiber into short - chain fatty acids ( SCFAs ), such as acetic acid and butyric acid, which are then absorbed by the host"], "metric_results": {"EM": 0.578125, "QA-F1": 0.671119907139644}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.33333333333333337, 0.0, 0.15384615384615383, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.9777777777777777]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-2285", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-1512", "mrqa_hotpotqa-validation-245", "mrqa_naturalquestions-validation-1728", "mrqa_triviaqa-validation-5311", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3680", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5496", "mrqa_naturalquestions-validation-7393"], "SR": 0.578125, "CSR": 0.5301339285714286, "EFR": 1.0, "Overall": 0.7201674107142858}, {"timecode": 84, "before_eval_results": {"predictions": ["As of January 17, 2018, 201 episodes", "Atlanta", "around 1872", "1978", "Hodel", "Central Germany", "Kida", "the Outfield", "infant, schoolboy, lover, soldier, justice, Pantalone and old age", "in Christian eschatology", "Bill Irwin", "artes liberales", "the sinoatrial node", "by pulse - width modulation of the pump voltage", "Ellen is restored to life and is married to Bobby", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "December 11, 2014", "A vanishing point", "Janie Crawford", "UTC \u2212 09 : 00", "mining", "Orange Juice", "small amounts of transcellular fluid such as ocular and cerebrospinal fluids in the `` Transcellular compartment ''", "nearly 92 %", "Missouri River", "Macintosh High Sierra", "Jason Marsden", "The Chipettes", "Linda Davis", "Fats Waller", "Thespis", "Bill Patriots", "May 19, 2017", "season two", "south of Newfoundland", "the status line", "Samantha Jo `` Mandy '' Moore", "Dr. Lexie Grey", "Nodar Kumaritashvili", "TLC - All That", "Welch, West Virginia", "Nicki Minaj", "Steve Russell", "Julia Roberts", "the church at Philippi", "data", "a valuable way to feed the poor, and would relieve some pressure of the land redistribution process", "Bed and breakfast", "The Lykan Hypersport", "The first five seasons", "Geothermal gradient", "soft contact lenses", "(Zak) Starkey", "fire insurance", "Seoul, South Korea", "Saturday", "Al Capone", "14 years", "ambassadors", "Russia", "Dustbin", "the Equator", "Caesar", "There's no chance"], "metric_results": {"EM": 0.671875, "QA-F1": 0.77255661005661}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-1767", "mrqa_triviaqa-validation-6303", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-16205", "mrqa_newsqa-validation-2213"], "SR": 0.671875, "CSR": 0.5318014705882352, "EFR": 0.9523809523809523, "Overall": 0.7109771095938375}, {"timecode": 85, "before_eval_results": {"predictions": ["the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "The uvea", "25 June 1932", "Felicity Huffman", "to control gene expression and mediate the replication of DNA during the cell cycle", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Richard Carpenter", "peninsular", "at least 28 vowel forms", "high - performance luxury coupe", "eleven separate regions of the Old and New World", "As of January 17, 2018, 201 episodes", "the Tennessee Titans", "Monk's", "B.R. Ambedkar", "the septum", "Cecil Lockhart", "Executive Residence of the White House Complex", "Deputy Director David Motl", "invertebrates", "as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "in the beta cells of the islets of Langerhans", "New Croton Reservoir in Westchester and Putnam counties", "Professor Eobard Thawne", "Freddie Highmore", "the closing scene of the final episode of the first season", "1939", "over 74", "during prenatal development in the central part of each developing bone", "Nebuchadnezzar", "A footling breech", "Robert Gillespie Adamson IV", "Waylon Jennings", "A rear - view mirror", "into the central canal of the spinal cord or into the subarachnoid cisterns via three small foramina : the central median aperture and the two lateral apertures", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Doug Diemoz", "$19.8 trillion", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "Frank Zappa's", "1956", "Camping World Stadium in Orlando, Florida", "Louis XV's", "the final episode of the series", "Jonathan Breck", "Ludacris", "UK Kennel Clubs", "126 by Wilt Chamberlain from October 19, 1961 -- January 19, 1963", "James Chadwick", "Olympia", "Wyatt `` Dusty '' Chandler ( George Strait )", "Swiss", "1875", "Las Vegas", "Hindi", "Brookhaven", "January 2004", "the International Space Station", "ordered the release of the four men", "\"Empire of the Sun,\"", "frittatas", "a crabitat", "a meter", "Matlock"], "metric_results": {"EM": 0.59375, "QA-F1": 0.677706114011261}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 0.35294117647058826, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5185185185185185, 0.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6485", "mrqa_naturalquestions-validation-6066", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2863", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-7912", "mrqa_triviaqa-validation-2264", "mrqa_newsqa-validation-1345", "mrqa_newsqa-validation-3807", "mrqa_searchqa-validation-9160", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-10904"], "SR": 0.59375, "CSR": 0.5325218023255813, "EFR": 0.9615384615384616, "Overall": 0.7129526777728086}, {"timecode": 86, "before_eval_results": {"predictions": ["San Francisco", "a Polaroid picture", "( Rudolf) Nureyev", "flibutor", "Christo", "a song", "Cyrus McCormick", "the Ziegfeld Follies", "ballpoint pen", "the tabernacle", "a butterfly life cycle", "tequila", "The Plains tribes", "Einstein", "gambling", "a roller coaster", "George H.W. Bush", "Elvis Presley", "a flushing toilet", "a pesticide", "Nikolai Gogol", "Hudson", "Banquo", "Virgo", "Alaska", "pardon", "an alligator", "General Mills", "the comb", "( Ferdinand) Magellan", "Austria", "a drop of Roses lime", "'Jersey Boys", "The Condemned of Altona", "Greyhounds", "a biological clock", "Molson", "a lottery", "Al Lenhardt", "Tanzania", "Colorado", "Christopher Columbus", "the Caribbean Sea", "the Swiss tournament", "polygons", "Frederic Remington", "The father called her his \"St. Patrick's... -and that he had been married one year.", "Evan Almighty", "the tooth Fairy", "third", "a pillar", "Johannes Gutenberg", "203", "4 January 2011", "Neighbours", "curb-roof", "a lion", "Norway", "Big Machine Records", "Campbell Soup Company", "Stoke City.", "identity documents", "Los Angeles.", "Venezuela"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6267361111111112}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10854", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-6274", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-5062", "mrqa_searchqa-validation-2269", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-632", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-971", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6649", "mrqa_newsqa-validation-1516"], "SR": 0.53125, "CSR": 0.532507183908046, "EFR": 1.0, "Overall": 0.7206420617816092}, {"timecode": 87, "before_eval_results": {"predictions": ["Shopgirl", "the South Pacific Ocean", "True", "The Bridge on the River Kwai", "Motown Legends", "Virgin Atlantic", "dill", "Indira Gandhi", "pew", "Hitchcock", "the cardiovascular system", "London Bridge", "Ho Chi Minh", "Vesuvius", "hari", "Kodachrome", "House", "Louis Armstrong", "Pakistan", "A Prairie Home Companion", "the American Union", "dill", "Thames", "Taipei", "Dame Nellie Melba", "the Caspian Sea", "Babe Ruth", "the Edo era", "dromboli", "apples", "chocolates", "bay leaf", "the Canterbury Tales", "Japan", "Ben & Jerry", "Sweater girl", "Krakauer", "Ali", "Sweden", "Neil Armstrong", "the Andes", "the ink gland", "Mitt Romney", "Goofy", "Peter Jackson", "an inch", "Neptune", "Hawaii", "the shark", "Simon Cowell", "the Northern Mockingbird", "origins of replication, in the genome", "27 January -- 16 April 1898", "57 days", "Mr. Humphries", "Mason", "Tinie Tempah", "Richard Arthur", "Ghanaian national team", "Labour", "citizenship", "1,500", "African President Thabo Mbeki,", "Louis XV"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7390625}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4530", "mrqa_searchqa-validation-10848", "mrqa_searchqa-validation-3533", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-13168", "mrqa_searchqa-validation-8024", "mrqa_searchqa-validation-3772", "mrqa_searchqa-validation-2075", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-8536", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-7364", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-6778", "mrqa_searchqa-validation-6947", "mrqa_naturalquestions-validation-1277", "mrqa_triviaqa-validation-1234", "mrqa_newsqa-validation-3391"], "SR": 0.640625, "CSR": 0.5337357954545454, "EFR": 0.9565217391304348, "Overall": 0.7121921319169962}, {"timecode": 88, "before_eval_results": {"predictions": ["Charles Dickens", "a fish", "Sherlock", "Grant Wood", "( Jeff) Probst", "All\\'s Well That Ends Well", "the Black Sea", "Eggs Benedict", "lovebird", "Agatha Christie", "a church", "a memoir", "Mossad", "a backstroke", "Swahili", "a defibrillator", "Katrina", "a rocket", "a proscenium arch", "(Thunnus) alalunga", "Pocahontas", "sinuses", "a fish", "Jane Eyre", "Kandahar", "(Nolan) Ryan", "miso", "(William) Harrison", "clay", "the Jutland Peninsula", "( Alma) Thomas", "Fourteen Points", "Misery", "(Cora) Munro", "a crossword", "the Osmonds", "a guitar", "(Robert) Flack", "Belgium", "Shield", "Chicago", "an actuary", "a latkes", "Montana", "dulcimer", "McCarthy", "lead", "the apocrypha", "a discus", "an Ayhuasca", "Top Gun", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "an electrochemical proton gradient that drives the synthesis of adenosine triphosphate ( ATP ), a molecule that stores energy chemically in the form of highly strained bonds", "Nick Sager", "horse racing", "Uruguay", "Mauna Kea", "Deftones", "The Design Inference", "Dutch", "(Norra Grangesbergsgatan 4)", "3-0", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "Ronald Lyle \" Ron\" Goldman"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7007440476190476}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.9047619047619047, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-13978", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-16336", "mrqa_searchqa-validation-8423", "mrqa_searchqa-validation-10749", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-12604", "mrqa_searchqa-validation-2486", "mrqa_searchqa-validation-7014", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-13957", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5284", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-2402", "mrqa_hotpotqa-validation-3258", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-692", "mrqa_hotpotqa-validation-2410"], "SR": 0.609375, "CSR": 0.5345856741573034, "EFR": 0.96, "Overall": 0.7130577598314607}, {"timecode": 89, "before_eval_results": {"predictions": ["Four bodies", "Ali Bongo,", "African National Congress Deputy President Kgalema Motlanthe,", "Dan Parris, 25, and Rob Lehr, 26,", "that the six imprisoned leaders of the religious minority were held for security reasons and not because of their faith.", "18th", "monarchy.", "that he acted in self defense in punching businessman Marcus McGhee.", "Ron-owned company has been under fire from the SEIU,", "near the equator,", "U.S. senators", "\"we have more work to do,\"", "a one-shot victory in the Bob Hope Classic", "that she was lured to a dorm and assaulted in a bathroom stall.", "Egyptians", "death", "a businessman", "his business dealings", "plastic surgery", "Meira Kumar", "The exact cause of IBS remains unknown,", "The Valley Swim Club", "The Mexican military", "The FBI's Baltimore field office", "British", "a steam-driven, paddlewheeled overnight passenger boat.", "Adidas,", "can be traced, in part, to a \"stressed and tired force\" made vulnerable by multiple deployments,", "Zachac Efron", "that he wants a \"happy ending\" to the case.", "1 percent", "4.6 million", "Miss USA Rima Fakih", "the Airbus A330-200", "the Internet", "228", "London's O2 arena,", "at Sea World in San Antonio,", "London Heathrow's Terminal 5.", "123 pounds of cocaine and 4.5 pounds of heroin,", "Bob Dole,", "Section 60.", "Muqtada al-Sadr,", "Leaders of more than 30 Latin American and Caribbean nations", "At least 38 people", "two years,", "heavy turbulence", "Alwin Landry's supply vessel Damon Bankston", "Florida", "tax", "Luis Evelis Andrade", "The genome", "2010", "Jurchen Aisin Gioro clan in Manchuria", "the Maeotium", "w WJacobs", "france", "wine", "Tsavo East National Park", "\"Lucky\"", "bananas", "a magpies", "Midas", "Home alone"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5622311586082682}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4210526315789474, 0.5, 0.923076923076923, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.125, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-1036", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-7311", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-6077", "mrqa_hotpotqa-validation-3474"], "SR": 0.46875, "CSR": 0.5338541666666667, "EFR": 1.0, "Overall": 0.7209114583333334}, {"timecode": 90, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-11678", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.841796875, "KG": 0.540625, "before_eval_results": {"predictions": ["1974", "bat and ball", "Rebecca Lynn Forstadt", "\"Switzerland of England\"", "Ford Island", "the Food and Agriculture Organization", "Beno\u00eet Jacquot", "Lincoln", "World War II", "Les Temps modernes", "Agra", "1986", "\"King of Cool\"", "841", "Missouri River", "1.23 million", "32", "Clarence Nash", "2015", "James Franco", "1970", "Sam Kinison", "1918", "Robert Gibson", "3730", "Washington State University", "1939", "Forbes", "Univision", "an English Wesleyan minister and biographer", "1865", "Dealey Plaza", "237", "The State of Franklin", "Orlando", "Oklahoma City", "Suspiria", "The School Boys", "Michele Marie Bachmann", "2007", "Japanese", "fixed-roof", "Singapore", "Balloon Street, Manchester", "\"Ted\"", "The National League", "Anne Erin \"Annie\" Clark", "the Atlantic Ocean", "Samantha Spiro", "Fort Hood, Texas", "Hong Kong", "Clarence Darrow", "Ren\u00e9 Verdon", "Instagram's own account", "(Rory) Bremner", "Adam Smith", "geese", "Bill Klein,", "last summer.", "speed attempts", "Andrew Wyeth", "calico", "biddy", "to the U.S. Holocaust Memorial Museum"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7212549603174603}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-2970", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-7442", "mrqa_triviaqa-validation-6410", "mrqa_newsqa-validation-2420"], "SR": 0.65625, "CSR": 0.5351991758241759, "EFR": 0.9090909090909091, "Overall": 0.7141705169830169}, {"timecode": 91, "before_eval_results": {"predictions": ["Tim Tolstoy", "Montana", "Tigger", "Charlesfort", "Nintendo", "Lexington and Concord", "a ray", "mint", "Roxanne", "Florida State", "Roald Dahl", "The 50 Best TV Dramas of All Time", "Buffalo Bill", "a pager", "Hawaii", "Sir Isaac Newton", "Radiohead", "Cain", "Lignite", "the Vietnam War", "Algebra", "catherine", "McCartney and Lennon", "Blondie", "Drumline", "Donnie Wahlberg", "the nucleus", "the Unabomber", "Tom Petty", "Harry Potter", "The Sixth Sense", "American New Wave band Talking Heads", "Gin", "Santa", "fashion", "Billy the Kid", "The Stone Age", "Cecil John Rhodes", "James Garner", "inn", "Tim Hutton", "a double take", "Michael Lewis Webster", "Michael Phelps", "Papua New Guinea", "downslope", "a prism", "sesame", "a quart", "pawn", "Late Night Show", "UNICEF's global programing", "defense against rain rather than sun", "Vijaya Mulay", "Peru", "Istanbul", "World War II", "Dante", "George Raft", "Grace O'Malley", "Zed", "Communist", "an empty water bottle", "The Fixx"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6808035714285714}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-9909", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-12917", "mrqa_searchqa-validation-5080", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-14637", "mrqa_searchqa-validation-12260", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-2465", "mrqa_searchqa-validation-15001", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-11934", "mrqa_searchqa-validation-3638", "mrqa_searchqa-validation-6925", "mrqa_searchqa-validation-13368", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-11404", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-2238", "mrqa_triviaqa-validation-3479", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-8584"], "SR": 0.578125, "CSR": 0.5356657608695652, "EFR": 0.9629629629629629, "Overall": 0.7250382447665056}, {"timecode": 92, "before_eval_results": {"predictions": ["John Foster Dulles", "aluminum", "\" Look, Momno cavities!\"", "The New York Times", "the 1978 Foreign Intelligence Surveillance Act", "hock", "John Madden", "Hemingway", "a rubaiyat", "a jurist", "Thailand", "a yam", "Mariner 1", "the Chunnel", "President Lincoln", "Smithfield", "Scorsese", "Poland", "a mausoleum", "a bagel", "the Tabernacle", "The Indianapolis 500", "the Galapagos", "Boston", "Nautilus", "the Phoenician", "parez", "Los Angeles", "Athens", "Calvin Klein Eternity", "Plummer", "Pennsylvania", "Cotton Mather", "the Berlin Wall", "Suzanne Valadon", "the will", "Elephants", "a pocket axe", "Erwin Rommel", "France", "wheat", "Vermont", "Mending Wall", "Thomas Jefferson", "copper", "Wrigley", "rum", "a Towel", "Brian Curtis Wimes", "steel", "Cormac McCarthy", "Cynthia Weil", "A turlough, or turlach", "8,850", "Tutankhamun", "Funchal", "Ruth Rendell", "Anthony Lynn", "The 2016 United States Senate election", "Martin Scorsese", "African National Congress Deputy President Kgalema Motlanthe,", "Angela Merkel", "in September,", "Somali-based"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6189236111111112}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-13482", "mrqa_searchqa-validation-5947", "mrqa_searchqa-validation-16916", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-15747", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4976", "mrqa_searchqa-validation-4760", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-1937", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-12309", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-13222", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-335", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-4189"], "SR": 0.515625, "CSR": 0.5354502688172043, "EFR": 1.0, "Overall": 0.7324025537634408}, {"timecode": 93, "before_eval_results": {"predictions": ["Cub Scout", "Romeo & Juliet", "doughboy", "Georgetown", "Dalmatian", "Cricket", "Duke", "Two and a Half Men", "a sun", "Lot", "a hull", "aluminum", "Kevin Smith", "Jabez Stone", "Major", "the Monitor", "Pasteur", "the Channel Islands", "Lake Superior", "wheat", "Edgar Allan Poe", "(Thomas) Edison", "Manhattan", "(Curly) Lambeau", "T.E. Lawrence", "Blake Lively", "an ape", "Eliot Spitzer", "licorice stick", "the union", "a fox", "Edgar Allan Poe", "a star", "impeachment", "Dominican Republic", "a Dagger", "French", "Nightingale", "Leonard Cohen", "San Francisco Mayor George Moscone", "butterfly", "a Macintosh", "the devil's food cake", "Goodyear", "corpulent", "Edinburgh", "a crumpet", "trailgator bars", "the Great Smoky Mountains", "(Sir) Walter Scott", "Punjabi", "October 1986", "in the duodenum", "Anthony Hopkins", "six", "Dubai", "horse racing", "Abdul Razzak Yaqoob", "England", "Colonel Gaddafi", "first team to overcome Australia at home since the West Indies in 1992-93.", "Haiti", "whether findings revealed if Gadhafi suffered the wound in crossfire or at close-range --", "Roberto Micheletti,"], "metric_results": {"EM": 0.625, "QA-F1": 0.6758289453601953}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3704", "mrqa_searchqa-validation-3150", "mrqa_searchqa-validation-14177", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-14527", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-7436", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-5257", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-221", "mrqa_hotpotqa-validation-3442", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2385"], "SR": 0.625, "CSR": 0.5364029255319149, "EFR": 1.0, "Overall": 0.732593085106383}, {"timecode": 94, "before_eval_results": {"predictions": ["John Mayer", "Benjamin Franklin", "the Amstel River", "Constantinople", "Georgie Porgie", "an Autistic Chad", "Methuselah", "Puerto Rico", "hearth", "Quebec", "The Handwriting", "the Cincinnati Reds", "Once", "Shelley", "China", "Frederick Douglass", "Sitka", "the Amazons", "Debussy", "Aziraphale", "Bojangles", "Aunt Jemima", "Frank Sinatra", "a saucer", "California", "KLM", "(Fred) Zeller", "a carriage", "a vest", "The Adventures of Sherlock Holmes", "Ned Kelly", "his once-sure-thing bid for Louisiana's open governorship", "World of Warcraft", "Shakespeare", "the Inca", "Alaska", "Sam Kinison", "the Wii", "the high jump", "Champagne", "a stick hoovers", "Danica Patrick", "the pancreas", "Midway", "stars", "Henry Cisneros", "the sacristy", "the Great Seal", "Rihanna", "\"24\"", "Tom Brady", "an ornament", "in the final scene of the fourth season", "Billy Hill", "iron", "Henry Ford", "Saint Bartholomew", "1943", "Battle of Britain and the Battle of Malta", "Kaep", "did not", "David Beckham", "the vicious brutality which accompanied the murders of his father and brother.\"", "Clarkson"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7377164502164502}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10187", "mrqa_searchqa-validation-9190", "mrqa_searchqa-validation-16247", "mrqa_searchqa-validation-13912", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-15733", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-615", "mrqa_searchqa-validation-4359", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-13716", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-8277", "mrqa_hotpotqa-validation-5383", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-2196"], "SR": 0.6875, "CSR": 0.5379934210526316, "EFR": 1.0, "Overall": 0.7329111842105263}, {"timecode": 95, "before_eval_results": {"predictions": ["The Buckwheat Boyz", "1998", "the south to West Bengal in the north through Andhra Pradesh and Odisha", "the largest among the tropical oceans, and about 3 times faster than the warming observed in the Pacific", "Gatiman", "Harendra Coomar Mookerjee", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "no more than 38 inches ( 965 mm )", "the Gentiles", "art of the Persian Safavid dynasty", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "typically the player to the dealer's right", "a sign of good luck", "February 6, 2005", "a global cultural icon of France", "the episode `` Kobol's Last `` ''", "Philippe Petit", "Terrell Suggs", "anion", "four", "1952", "The User State Migration Tool ( USMT )", "2017", "restoring someone's faith in love and family relationships", "an illustration by Everest creative Maganlal Daiya back in the 1960s", "if the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "solemniser", "1961", "36 months", "an integral membrane protein that builds up a proton gradient across a biological membrane", "during the natural period of activity ( night )", "Ethel `` Edy '' Proctor", "Randy VanWarmer", "Isle Vierge", "Kevin Spacey", "Miller Lite", "the body - centered cubic ( BCC ) lattice", "in 1986", "their son Jack ( short for Jack - o - Lantern )", "Spektor", "after 9pm ET ( UTC - 5 )", "in February 2017 in Japan and in March 2018 in North America and Europe", "July 21, 1861", "Holden Nowell", "Muhammad Yunus", "Jules Shear", "Juliet", "Kansas and Oklahoma", "Ethel Merman", "The New Croton Aqueduct", "Judith Cynthia Aline Keppel", "Kenny Everett", "Trinidad", "Ghost", "Hawaii", "William Bradford", "Chuck Noll", "Serie A", "three", "suicides", "John Henry", "manure", "Two and a Half Men", "the 2004 Paris Motor Show"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6555358861265191}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.2222222222222222, 0.17391304347826086, 1.0, 0.0, 0.9428571428571428, 0.7142857142857143, 0.0, 0.0, 1.0, 0.7272727272727272, 0.6, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 0.5454545454545454, 0.2857142857142857, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.07272727272727272, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.7499999999999999, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-9683", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-10294", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-6035", "mrqa_hotpotqa-validation-4553", "mrqa_newsqa-validation-2754", "mrqa_searchqa-validation-5787"], "SR": 0.515625, "CSR": 0.5377604166666667, "EFR": 0.8709677419354839, "Overall": 0.7070581317204301}, {"timecode": 96, "before_eval_results": {"predictions": ["Shetlands", "painting", "to mark last weekend's mass shooting at a central Florida nightclub,", "Pisces", "The Law Society of Scotland", "Russell Crowe", "two", "Khomeini", "Clint Eastwood", "1921", "Spain", "David Bowie", "7. Jaws", "German", "the Volcanoes", "Porridge", "South Africa", "New Orleans", "the eye", "Pooh Bear", "Ringo Starr", "John Mortimer", "bushfires", "Idaho", "Danny DeVito", "Sweden", "four", "Tony Washington, Willie Woods and Victor Thomas", "Rastafarians", "Sydney", "a long sprint", "Tolstoy", "Vanwall", "the Rock of Gibraltar", "Rick Wakeman", "Benghazi", "Brazil", "The Mary Tyler Moore Show", "Gordon Jackson", "Scotland", "Kate Smith", "West Sussex", "Laputa", "Colombia", "Lurch", "rabia", "Tanzania", "Darby and Joan", "Robert Boyle", "Thailand", "Hugh Laurie", "Beijing for the 2020 Winter Olympics, Paris for the 2024 Summer Olympics, and Los Angeles for the 2028 Summer Olympics", "sometime between 124 and 800 CE", "the first to develop lethal injection as a method of execution", "1 January 1788", "guitar feedback", "Stephen Lee", "Arroyo and her husband", "Apple's only model with a sort of robot living inside.", "North Korea", "The Vietnam War", "white granite", "the humerus", "pinch"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6616666666666666}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4799999999999999, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-1554", "mrqa_triviaqa-validation-6629", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-6139", "mrqa_triviaqa-validation-1431", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-6747", "mrqa_triviaqa-validation-1866", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-1433", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-1941", "mrqa_newsqa-validation-887"], "SR": 0.609375, "CSR": 0.5384987113402062, "EFR": 1.0, "Overall": 0.7330122422680412}, {"timecode": 97, "before_eval_results": {"predictions": ["40 militants and six Pakistan soldiers dead,", "recall notices", "Javier Hernandez", "Nechirvan Barzani,", "Madrid's Barajas International Airport", "The monarchy's end after 239 years of rule", "Kerstin and two of her brothers,", "56,", "video cameras", "more than 200.", "Princess Diana", "1831", "maintain an \"aesthetic environment\" and ensure public safety,", "Caylee Anthony", "presidential challenge", "Alexandre Caizergues,", "Arroyo and her husband", "\"It was terrible,", "hackers", "India", "non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "Haleigh Cummings,", "another state senator", "toffelmakaren.", "ended his playing career", "researchers", "in his late 30s and early 40s.", "new restaurant next to the home of Mona Lisa as something completely normal.", "3,000", "10 percent", "$250,000", "April 22.", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Yusuf Saad Kamel", "Obama", "flights affected", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Jeffrey Jamaleldine", "eight years in prison.", "it was unjustifiable", "up to $50,000 for her,", "Citizens", "41,280", "Adam Lambert", "President Clinton.", "allegedly involved in forged credit cards and identity theft", "Iran's nuclear program.", "many riders say it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "133", "suppress the memories", "deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "Richard Parker", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "electron pairs", "fast bowler", "groszy", "roman numbers", "Hawaii", "stolperstein", "Paper", "Las Vegas", "the hypothalamus", "the orangutan", "mrs blyton"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5539342085219745}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [0.923076923076923, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.25, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.19047619047619047, 0.8, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.2553191489361702, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-4055", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-582", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-7701", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-1532"], "SR": 0.46875, "CSR": 0.5377869897959184, "EFR": 1.0, "Overall": 0.7328698979591837}, {"timecode": 98, "before_eval_results": {"predictions": ["it does not", "Swedish Foreign Ministry", "through the Rockies and climbs to 9,000 feet.", "Friday,", "Iranian", "Daniel Radcliffe", "South African teenager Caster Semenya", "the mine", "Susan Atkins,", "Pakistan's High Commission in India", "\"Wicked,\"", "magazine's Friday edition.", "Tsvangirai", "the oceans, like the land,", "Asashoryu,", "an Airbus A320-214,", "autonomy.", "South Africa", "\"A good vegan cupcake has the power to transform everything for the better,\"", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "between 1917 and 1924", "Les Bleus", "Too many glass shards left by beer drinkers in the city center,", "collapsed ConAgra Foods plant in the town of Garner,", "40-year-old", "Karthik Rajaram,", "the island's dining scene", "in the U.S.", "clogs", "Nirvana frontman,", "Turkey,", "Moody and sinister,", "United States, NATO member states, Russia and India", "Ryder Russell,", "spend billions to improve America's education, infrastructure, energy and health care systems.", "Palestinian Islamic Army, which has links to al Qaeda,", "Haeftling,", "off the coast of Dubai", "change course", "Sri Lanka,", "eight-week", "drug cartels", "head for Italy.", "Anil Kapoor.", "series", "38 feet", "St. Louis, Missouri.", "64,", "ultra-high-strength steel and boron", "fifth", "order after demonstrators rose up across Greece Monday in a third day of rioting over Saturday's killing of a 15-year-old boy that has left dozens injured and scores of properties destroyed.", "1977", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Alexandra Alstr\u00f6m and Bj\u00f8rn Floberg", "Nasdaq", "Van Rijn", "swans", "\"thirtysomething\"", "hulder", "Lucille Ball", "Ziploc", "South Africa", "ivory", "Kraftwerk"], "metric_results": {"EM": 0.578125, "QA-F1": 0.631547619047619}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.06666666666666667, 1.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-4167", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-122", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-2399"], "SR": 0.578125, "CSR": 0.5381944444444444, "EFR": 1.0, "Overall": 0.7329513888888889}, {"timecode": 99, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1232", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13865", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14527", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-622", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8108", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9502", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1713", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3633", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3883", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.83984375, "KG": 0.5375, "before_eval_results": {"predictions": ["Number Ones", "\"I wasn't the best \"coach,\"", "$40 billion", "\"I am sick of life -- what can I", "10 municipal police officers", "Lifeway's 100-plus stores nationwide", "\"The extensive use of advanced technologies and materials in the 2009 F-150", "2009", "two years,", "A group of college students of Pakistani background", "gasoline", "Her husband and attorney, James Whitehouse,", "North Korea,", "four Impressionist paintings", "Omar bin Laden,", "nude beaches.", "1-1", "Jason Chaffetz", "Martin \"Al\" Culhane,", "President Obama", "A member of the group dubbed the \"Jena 6\"", "Animal Planet", "Elizabeth Birnbaum", "Mandi Hamlin", "41,280", "Arlington National Cemetery's Section 60,", "Bill Haas", "Caster Semenya", "Michael Jackson", "Kerstin Fritzl,", "Djibouti,", "123 pounds of cocaine and 4.5 pounds of heroin,", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Brazil, Argentina, Mexico, Colombia and Venezuela.", "second-degree aggravated battery.", "two Emmys", "Madonna", "the forward's lawyer", "Turkish President Abdullah Gul,", "Lee Myung-Bak", "to secure more funds from the region.", "five female pastors", "14", "South Carolina Republican Party Chairwoman Karen Floyd", "African National Congress Deputy President", "1913,", "second", "Hu Jintao", "Michael Arrington,", "asylum in Britain.", "eight Indian army troopers, including one officer, and 17 militants,", "New York City", "2004", "Massachusetts", "Phil Mickelson", "the Quran", "Shanghai", "University of Columbia", "an advertisement figure", "Aldosterone", "freezing", "Power", "Maine", "November 17, 2017"], "metric_results": {"EM": 0.625, "QA-F1": 0.7358833874458874}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.3, 0.5, 0.4, 1.0, 1.0, 0.6060606060606061, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3689", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3895", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-644", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-397"], "SR": 0.625, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.723515625}]}