{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]_result.json', stream_id=3, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4070, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["The Jones et al. and Briffa reconstructions", "help transfer and dissipate excess energy", "Historical and Critical Dictionary", "human settlement and development of the land", "over three days", "eight", "polynomial algebra", "60 days", "American Association of University Women", "mid-Cambrian period", "1926", "The outcome of most votes can be predicted beforehand", "Newton", "a disaster", "between the 1960s and 1990s", "James Hutton", "British", "growth and investment", "Each step had to be successfully accomplished before the next ones could be performed", "Downtown Riverside", "plague of Athens in 430 BC", "expelled Jews", "interleukin 1", "DuMont Television Network", "The city of Fresno", "1762", "colloblasts", "self", "Alta California", "300 km long and up to 40 km wide", "two-phased system", "On the Councils and the Church", "Super Bowl LI", "1968", "\"right\", \"just\", or \"true\"", "pastors", "meritocracy", "Vistula River", "arrows, swords, and leather shields", "23 November 1963", "since the 1960s", "zero net force", "2012", "CD4", "income inequality", "Tuition Fee Supplement", "April 1523", "the California State Automobile Association and the Automobile Club of Southern California", "Von Miller", "lymphocytes or an antibody-based humoral response", "unsuccessful", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Percy Shelley", "education, sanitation, and traffic control", "six divisions", "six series of theses", "Pi\u0142sudski", "Disney\u2013ABC Television Group", "SAP Center", "Genghis Khan", "materials melted near an impact crater", "William the Silent", "182 million tons", "John G. Trump"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8592347756410257}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4191", "mrqa_squad-validation-9426", "mrqa_squad-validation-368", "mrqa_squad-validation-3776", "mrqa_squad-validation-4769", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_squad-validation-2812", "mrqa_squad-validation-6559", "mrqa_squad-validation-2153", "mrqa_squad-validation-7246"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["occupational stress", "Sunni Arabs from Iraq and Syria", "The Lower Rhine", "Mike Tolbert", "silt up the lake", "cigarette advertising from all television and radio networks", "Dorothy and Michael Hintze", "climate change", "northwest", "Turkey", "Economist", "The Tyneside flat", "73", "Derek Jacobi", "ten minutes", "Sydney", "Basel", "ideal strings", "unstable molecules", "lasting damage", "December 2014", "other locations throughout Scotland", "eastern", "Newton", "Westwood One", "2008", "the colonies of British America and New France", "early 1546", "lower bounds", "2011", "became the University of Northumbria at Newcastle", "between 1859 and 1865", "8\u20134\u20134 system", "a green algal derived chloroplast", "over 200", "William the Conqueror", "The Service Module was discarded", "18 million volumes", "north", "detention", "collenchyma tissue", "Louis Adamic", "cytokine T IGF-\u03b2", "England", "Yuri Gagarin", "antigenic variation", "Knaurs Lexikon", "Chester, South Carolina", "1992", "three", "9.1 million", "rich and well socially standing", "pharynx", "1969", "Wardenclyffe Tower", "Tower Theatre", "700,000", "Pittsburgh", "jeopardy/2516_Qs.txt at master  jedoublen/jeopardy", "Heroes struggle... Animated, Action, Adventure, Fantasy, Sci-Fi.... Lego Star Wars: The Yoda Ch", "American baseball, until the late 1940s, excluded, with some big exceptions in... The color line was broken for good when Jackie Robinson signed with the Brooklyn", "the hand of cards which he supposedly held at the time of his death... killed by the assassin Jack McCall in Deadwood, Black Hills, August 2, 1876", "His... (initial capital letter) a German-built enciphering machine developed for commercial", "Europe"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8128918650793651}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.16, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9752", "mrqa_squad-validation-5549", "mrqa_squad-validation-8718", "mrqa_squad-validation-8891", "mrqa_squad-validation-9744", "mrqa_squad-validation-5337", "mrqa_squad-validation-3885", "mrqa_squad-validation-1568", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-4596"], "SR": 0.78125, "CSR": 0.8046875, "EFR": 1.0, "Overall": 0.90234375}, {"timecode": 2, "before_eval_results": {"predictions": ["John Sutcliffe", "can also concentrate wealth, pass environmental costs on to society", "6800", "medical treatment for the sick and wounded French soldiers", "Ticonderoga Point", "October 16, 1973", "1980s", "seven", "books and articles for magazines and journals", "Roger NFL", "the oceans and seas", "2 million", "by over 100%", "1350", "North America", "Euclid's fundamental theorem of arithmetic", "Maria de la Queillerie", "to encourage investment", "Julia Butterfly Hill", "dry areas", "587,000", "March 1974", "quickly", "end of the 19th century", "DeMarcus Ware", "University of North Florida", "motivated students", "platyctenids", "elementary school education certificate", "1220", "two", "30.0%", "Raghuram Rajan", "VHF channel 7", "The Pink Triangle", "ideological", "Methodist institutions", "Ticonderoga", "at least six daughters", "Los Angeles Dodgers", "19th", "school trips", "a delay costs money", "Funchess", "Watt", "1,000 m3/s", "Thuringia", "rivers", "Nazi Third Reich", "visor helmet", "Catholic", "Hollywood", "Long Island Sound", "Sweden's", "Ireland", "an African American", "Wyoming", "an athlete who plays cricket", "conifer", "Alaska", "an Austrian and American film actress and inventor", "Daniel Defoe", "the Association of American Universities", "50JJB Sports Fitness Clubs"], "metric_results": {"EM": 0.640625, "QA-F1": 0.70546875}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.6, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665]}}, "before_error_ids": ["mrqa_squad-validation-7525", "mrqa_squad-validation-10258", "mrqa_squad-validation-1565", "mrqa_squad-validation-85", "mrqa_squad-validation-802", "mrqa_squad-validation-10114", "mrqa_squad-validation-9061", "mrqa_squad-validation-8322", "mrqa_squad-validation-1960", "mrqa_squad-validation-5678", "mrqa_squad-validation-6101", "mrqa_squad-validation-2786", "mrqa_squad-validation-1877", "mrqa_squad-validation-2497", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-13452", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-5603"], "SR": 0.640625, "CSR": 0.75, "EFR": 0.9565217391304348, "Overall": 0.8532608695652174}, {"timecode": 3, "before_eval_results": {"predictions": ["public schools", "connection id in a table", "males", "electrical, water, sewage, phone, and cable facilities", "the level of the top tax rate", "\"Wise up or die.\"", "a universal Ku band LNB", "highly-paid", "pump this into the mesoglea", "Fred Silverman", "atmospheric engine", "force", "ctenophores", "the trial and rehabilitation of Joan of Arc", "John Hurt", "an Australian public X.25 network operated by Telstra", "Arizona Cardinals", "Von Miller", "Indianapolis Colts", "42%", "1957", "imprisonment", "orogenic wedges", "one", "Catholic", "Edict of Fontainebleau", "Fort Caroline", "Pittard Sullivan", "wealth", "Niagara Falls", "Hugh Downs", "the Saracens", "3D printing technology", "Daniel Burke", "internal strife", "Matthew Murray", "400 AD", "the United States", "Satya Nadella", "the difference between a problem and an instance", "Joan Atkinson", "Inner Mongolia", "cortisol and catecholamines", "a third group of pigments found in cyanobacteria", "isopentenyl pyrophosphate synthesis", "1963", "hotel room", "Italy", "Joan Otto", "a parish beadle", "Khartoum", "John Quincy Adams", "Playboy rabbit", "he", "Puerto Rico", "Court TV", "Howard Carter", "Joann Hesse", "\"Inhospitable Sea\"", "Moshe Dayan", "Joan Van Dinh", "active athletes", "helicopters and boats", "$17,000"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6828563797313798}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 0.4615384615384615, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7103", "mrqa_squad-validation-2743", "mrqa_squad-validation-7357", "mrqa_squad-validation-4452", "mrqa_squad-validation-3247", "mrqa_squad-validation-230", "mrqa_squad-validation-363", "mrqa_squad-validation-10083", "mrqa_squad-validation-3139", "mrqa_squad-validation-5542", "mrqa_squad-validation-1670", "mrqa_squad-validation-7885", "mrqa_squad-validation-6263", "mrqa_squad-validation-8839", "mrqa_squad-validation-7711", "mrqa_searchqa-validation-8170", "mrqa_searchqa-validation-12199", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-8423", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-6624", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-3588"], "SR": 0.59375, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 4, "before_eval_results": {"predictions": ["whether he stood by their contents", "1850s", "Troika", "complexity classes", "Parliament of the United Kingdom at Westminster", "teaching", "Dancing with the Stars", "8 November 2010", "a combination of anthrax and other pandemics", "coastal beaches and the game reserves", "1524", "form 2p \u2212 1, with p a prime", "horizontal", "Crash the Super Bowl", "collenchyma tissue", "around a billion years ago", "Croatia", "Port of Long Beach", "Edinburgh", "McManus", "Papin", "Cricket", "William Morris", "T. J. Ward", "the Daleks", "San Diego", "Saracen", "heat and pressure", "1072", "Chevron", "Africa", "New York City", "Marshall Cohen", "Hypersensitivity", "Business Connect", "Henry Young Darracott Scott", "not break any law", "income inequality", "Bainbridge's", "1294", "chao", "Apollo 17", "Steicho Mile", "Helen Thorpe", "Stephen Hawking", "Hooterville Monkey Racing Track", "Europe", "helium-4", "Simn Bolvar", "Pam Anderson's", "Rhinegraves", "Moscow", "crystal anniversary", "prairie crocus", "Detroit River", "Cleveland", "New Testament manuscripts were written on papyrus, made from a reed that grew", "Dublin", "Steir praia no Rio e mostra boa forma aos 63", "Ben", "Albert Einstein", "Doctor Who", "1990", "Zimbabwe"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6976617132867133}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5878", "mrqa_squad-validation-8945", "mrqa_squad-validation-821", "mrqa_squad-validation-5344", "mrqa_squad-validation-7615", "mrqa_squad-validation-2679", "mrqa_squad-validation-1061", "mrqa_squad-validation-4147", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-9601", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-4300"], "SR": 0.671875, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 5, "before_eval_results": {"predictions": ["November 2006", "achievement-oriented motivations (\"pull\")", "a malfunction in the chameleon circuit", "SAP Center", "March 2011", "above the top of the range", "12th", "1226", "a special episode of The Late Late Show", "Esel", "P", "theNP-complete knapsack problem", "1928", "when the immune system is less active than normal", "disturbances", "Fraud", "from 1562 to 1598", "Emmerich Rhine Bridge", "ten", "1993", "a primitive intermediate between cyanobacteria and the more evolved chloroplasts", "oxygen", "US$100,000", "Bakersfield", "Bruno Mars", "patients' prescriptions and patient safety issues", "2009", "the Common Core", "Recognized Student Organizations", "a cubic interpolation formula", "Thomas Edison", "the Swiss Reformation", "phagocytes", "15", "Satyagraha", "two", "microorganisms", "1968", "Aloha \u02bbOe", "over 1.6 million", "Eric Whitacre", "mouthbows", "Angus Young", "New York City", "the waltz Gunstwerber", "Cherokee River", "Odisha", "January 28, 2016", "138,535", "Adam Rex", "1933", "Sivakumar", "1968", "a Chaplain to the Forces", "astronomer and composer", "Warrington", "1866", "We'll Burn That Bridge", "the Provisional Irish Republican Army", "pneumonoultramicroscopicsilicovolcanoconiosis", "Boutros Ghali", "Vertikal-T", "The Wizard of Oz", "he was mad at the U.S. military because of what they had done to Muslims"], "metric_results": {"EM": 0.625, "QA-F1": 0.6866987179487178}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8531", "mrqa_squad-validation-434", "mrqa_squad-validation-9185", "mrqa_squad-validation-1759", "mrqa_squad-validation-1860", "mrqa_squad-validation-8696", "mrqa_squad-validation-3496", "mrqa_squad-validation-2634", "mrqa_squad-validation-664", "mrqa_squad-validation-3240", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_newsqa-validation-1468", "mrqa_searchqa-validation-13492", "mrqa_newsqa-validation-3290"], "SR": 0.625, "CSR": 0.6901041666666667, "EFR": 1.0, "Overall": 0.8450520833333334}, {"timecode": 6, "before_eval_results": {"predictions": ["UHF", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "Africa", "1976", "the law as an equality; the relative units of force and mass then are fixed.", "mathematical models", "60", "Warsaw", "5,000 years", "Edgar Atheling", "Writers Guild of America", "United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO)", "Conservative", "11 million", "visor helmet", "the Queen", "3 in 1,000,000", "2009 onwards", "Super Bowl Opening Night", "Fresno Street and Thorne Ave", "southern and central parts of France", "2014", "40%", "John F. Kennedy", "innate immune system", "15\u20131", "the history of arms", "Industry and manufacturing", "this contact with nature made him stronger, both physically and mentally.", "1543", "\u015ar\u00f3dmie\u015bcie", "Hmong or Laotian", "Stromules", "atmospheric pressure", "Johnny Herbert", "\"jus sanguinis\"", "Pharrell Williams", "Adrian Lyne", "J\u00f3zsef Pulitzer", "June 17, 2007", "\"The Frost Report\"", "National Basketball Development League", "Danish", "24 January 76", "Kealakekua Bay", "Dave Lee Travis", "Northrop P-61 Black widow", "People v. Turner", "\"Veyyil\"", "Helena Sternlicht", "Highlands Course", "Illinois", "Cartoon Cartoon Fridays", "Timo Hildebrand", "2016", "Ginger Rogers", "Corps of Discovery", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "801,200", "the Giraffa camelopardalis", "navy", "Ecuador", "Joseph Barnes", "South School Land Judging Contests"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7225524475524476}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10424", "mrqa_squad-validation-1114", "mrqa_squad-validation-8523", "mrqa_squad-validation-7770", "mrqa_squad-validation-3764", "mrqa_squad-validation-1232", "mrqa_squad-validation-3483", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2183", "mrqa_naturalquestions-validation-4953", "mrqa_triviaqa-validation-1134", "mrqa_newsqa-validation-3782", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-5374"], "SR": 0.671875, "CSR": 0.6875, "EFR": 1.0, "Overall": 0.84375}, {"timecode": 7, "before_eval_results": {"predictions": ["convection of the mantle", "Continental Edison Company in France", "South", "T. J. Ward", "25 minutes of transmission length", "1903", "1993", "King George III", "occupancy permit", "Hereford", "a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "Arts & Entertainment Television (A&E)", "NASA", "Albert Einstein", "Karl von Miltitz", "a matter of custom or expectation, such as isolating businesses to a business district and residences to a residential district", "Silk Road", "a war erupted in the Philippines", "39", "Bolshevik leaders", "University of Aberdeen", "transportation, sewer, hazardous waste and water", "430 BC", "Pedro Men\u00e9ndez de Avil\u00e9s", "expansion", "a deterministic Turing machine", "a D loop mechanism", "when the oxygen concentration is too high", "1290", "17,786,419,", "smallest state on the Australian mainland", "Montreal Montreal", "7000301604928199000 \u2660 3.016 049 281 99 ( 23 ) u", "was an incident on March 5, 1770, in which British Army soldiers shot and killed people while under attack by a mob", "Pakistan, India, and Bangladesh are among the largest individual contributors with around 8,000 units each", "2000", "Christina Calvano", "Moore", "the human hands and face have a much larger representation than the legs", "Martin Lawrence as Agent Malcolm Turner / Big Momma   Nia Long as Sherry Pierce", "Steve Hale", "a multilayer", "January 2017 patch", "Idaho is to the west and southwest, and three Canadian provinces, British Columbia, Alberta, and Saskatchewan, are to the north", "September 6, 2019", "multinational retail corporation", "Anita", "Jack Gleeson", "claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "when the forward reaction proceeds at the same rate as the reverse reaction", "heart", "a writ of certiorari", "a take - it - or - leave - it contract, or a boilerplate contract )", "Shawn", "September 30", "Kelly Osbourne, Ian `` Dicko '' Dickson, Christina Monk and Eddie Perfect", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "a Californio nobleman living in Los Angeles during the era of Spanish rule", "\"Household Words\",", "56", "Hindu scriptures", "St. Augustine", "a vowel", "gold"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6147036645215687}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8571428571428571, 0.07692307692307691, 0.0, 0.0, 0.0, 0.4, 0.33333333333333337, 0.2666666666666667, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.326530612244898, 0.0, 0.5, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1239", "mrqa_squad-validation-2315", "mrqa_squad-validation-6878", "mrqa_squad-validation-10007", "mrqa_squad-validation-360", "mrqa_squad-validation-6500", "mrqa_squad-validation-1819", "mrqa_squad-validation-8746", "mrqa_squad-validation-2881", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-7579", "mrqa_searchqa-validation-12472", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3744"], "SR": 0.515625, "CSR": 0.666015625, "EFR": 0.967741935483871, "Overall": 0.8168787802419355}, {"timecode": 8, "before_eval_results": {"predictions": ["ivory", "The Dornbirner Ach", "a certain number of teacher's salaries are paid by the State", "non-deterministic time", "five", "December 2014", "an inauspicious typhoon", "four", "Zwickau prophet", "10 July 1856 \u2013 7 January 1943", "1999", "CBS Sports apps on tablets, Windows 10, Xbox One and other digital media players", "3\u20132.7 billion years ago", "New Testament from Greek", "Newton", "economists with the Standard & Poor's rating agency", "Germany", "a vestigial red algal derived chloroplast", "two or more teachers", "a computer network funded by the U.S. National Science Foundation", "type III secretion system", "worker, capitalist/business owner, landlord", "tungsten", "the state", "oxidant", "the signals could come from Mars, Venus, or other planets", "the American Revolution", "Book of Discipline", "Fat Albert", "1943", "Big Fucking German", "Chelmsford City", "William Novak", "22,500 acres", "1951", "Abu Dhabi, United Arab Emirates", "86,112", "American", "Firth of Forth Site of Special Scientific Interest", "one live album, one compilation album, nineteen singles and fourteen music videos", "#364", "The Birds", "Battle of the Rosebud", "Andy Miller", "Pablo Escobar", "Brian A. Miller", "26 June 2013", "25 million records", "320 years", "Geraldine Sue Page", "Rochdale", "Charles Reed Bishop", "motor vehicles", "Marco Fu", "2015", "Dusty Springfield", "her translation of and commentary on Isaac Newton's book \"Principia\" containing basic laws of physics", "BeBe Winans", "Henry VIII", "July", "Nancy Pelosi", "Dumont d'Urville Station", "Under normal conditions", "a conversion, change of mind, repentance, and atonement"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7282289478291317}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.125, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855]}}, "before_error_ids": ["mrqa_squad-validation-1156", "mrqa_squad-validation-531", "mrqa_squad-validation-2272", "mrqa_squad-validation-361", "mrqa_squad-validation-1912", "mrqa_squad-validation-4849", "mrqa_squad-validation-1529", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-391", "mrqa_triviaqa-validation-4490", "mrqa_searchqa-validation-703", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-5851"], "SR": 0.609375, "CSR": 0.6597222222222222, "EFR": 1.0, "Overall": 0.8298611111111112}, {"timecode": 9, "before_eval_results": {"predictions": ["1999", "zero", "Mao Zedong", "Chebyshev", "1960", "introduction of Beroe", "1000 CE", "Orkney Vikings", "Sunspot, New Mexico", "Sonderungsverbot", "amending", "the environment in which they lived", "when the immune system is less active than normal", "C. J. Anderson", "Cadeby", "Warraghiggey", "accidentally adding oxygen to sugar precursors", "World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "Sunni pan-Islamism", "11 points", "yes or no, or alternately either 1 or 0", "black", "the Mongols", "English", "Newton", "1940s and 1950s", "arthurice dating from no later than 1618 and possibly founded as early as 1580", "musical theatre", "Taoiseach", "Duval County", "William Harold \"Bill\" Ponsford", "$10\u201320 million", "Manasseh Cutler Hall", "Denmark", "Hindi", "the \"Pour le M\u00e9rite\"", "Giuseppe Fortunino Francesco Verdi", "Edward Trowbridge Collins Sr.", "1946 and 1947", "Christopher McCulloch", "2016\u201317", "Carson City", "Wembley Stadium", "19th", "Cesar Millan", "Robert Allen Zimmerman", "Michael Lewis Greenwell", "from 20 March to 1 May 2003", "The Life of Charlotte Bront\u00eb", "2015", "Bill Curry", "Blunderbuss", "Kim Yoon-seok and Ha Jung-woo", "superhero roles", "Jonathan", "18th congressional district", "BBC", "2008", "Ringo Starr", "arthur", "in a firefight", "not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures", "the Necklace", "Jamaica"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6972085206460206}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.923076923076923, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5714285714285715, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9696969696969697, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9897", "mrqa_squad-validation-1025", "mrqa_squad-validation-3981", "mrqa_squad-validation-6443", "mrqa_squad-validation-8832", "mrqa_squad-validation-260", "mrqa_squad-validation-1652", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-2645", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809"], "SR": 0.5625, "CSR": 0.65, "EFR": 1.0, "Overall": 0.825}, {"timecode": 10, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3894", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4633", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5145", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5527", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-690", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9687", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16753", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4300", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6624", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-10007", "mrqa_squad-validation-10010", "mrqa_squad-validation-10031", "mrqa_squad-validation-10083", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10130", "mrqa_squad-validation-10153", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10234", "mrqa_squad-validation-10249", "mrqa_squad-validation-10258", "mrqa_squad-validation-10305", "mrqa_squad-validation-10345", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10478", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1073", "mrqa_squad-validation-1075", "mrqa_squad-validation-1075", "mrqa_squad-validation-1096", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1232", "mrqa_squad-validation-1239", "mrqa_squad-validation-1254", "mrqa_squad-validation-1296", "mrqa_squad-validation-1372", "mrqa_squad-validation-1529", "mrqa_squad-validation-1543", "mrqa_squad-validation-1586", "mrqa_squad-validation-1632", "mrqa_squad-validation-1652", "mrqa_squad-validation-1681", "mrqa_squad-validation-1723", "mrqa_squad-validation-1730", "mrqa_squad-validation-1731", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-1819", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1877", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-1976", "mrqa_squad-validation-1985", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2111", "mrqa_squad-validation-2153", "mrqa_squad-validation-2181", "mrqa_squad-validation-2189", "mrqa_squad-validation-2246", "mrqa_squad-validation-2247", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2320", "mrqa_squad-validation-2359", "mrqa_squad-validation-2413", "mrqa_squad-validation-2442", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2568", "mrqa_squad-validation-2591", "mrqa_squad-validation-260", "mrqa_squad-validation-2617", "mrqa_squad-validation-2628", "mrqa_squad-validation-2634", "mrqa_squad-validation-2644", "mrqa_squad-validation-2679", "mrqa_squad-validation-2721", "mrqa_squad-validation-2723", "mrqa_squad-validation-2765", "mrqa_squad-validation-2808", "mrqa_squad-validation-2812", "mrqa_squad-validation-2881", "mrqa_squad-validation-2941", "mrqa_squad-validation-2949", "mrqa_squad-validation-2975", "mrqa_squad-validation-2977", "mrqa_squad-validation-30", "mrqa_squad-validation-3111", "mrqa_squad-validation-3139", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3248", "mrqa_squad-validation-3269", "mrqa_squad-validation-3296", "mrqa_squad-validation-3377", "mrqa_squad-validation-3429", "mrqa_squad-validation-3483", "mrqa_squad-validation-3496", "mrqa_squad-validation-3534", "mrqa_squad-validation-3552", "mrqa_squad-validation-360", "mrqa_squad-validation-363", "mrqa_squad-validation-368", "mrqa_squad-validation-3705", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3776", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3885", "mrqa_squad-validation-3952", "mrqa_squad-validation-3981", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4070", "mrqa_squad-validation-4095", "mrqa_squad-validation-4107", "mrqa_squad-validation-4121", "mrqa_squad-validation-4121", "mrqa_squad-validation-4147", "mrqa_squad-validation-4191", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4516", "mrqa_squad-validation-457", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4689", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4758", "mrqa_squad-validation-4797", "mrqa_squad-validation-480", "mrqa_squad-validation-4840", "mrqa_squad-validation-4898", "mrqa_squad-validation-4988", "mrqa_squad-validation-4997", "mrqa_squad-validation-502", "mrqa_squad-validation-5029", "mrqa_squad-validation-5061", "mrqa_squad-validation-5096", "mrqa_squad-validation-5108", "mrqa_squad-validation-5200", "mrqa_squad-validation-5222", "mrqa_squad-validation-5270", "mrqa_squad-validation-5272", "mrqa_squad-validation-5287", "mrqa_squad-validation-531", "mrqa_squad-validation-5337", "mrqa_squad-validation-5344", "mrqa_squad-validation-5347", "mrqa_squad-validation-5382", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5549", "mrqa_squad-validation-5551", "mrqa_squad-validation-556", "mrqa_squad-validation-5621", "mrqa_squad-validation-5670", "mrqa_squad-validation-5741", "mrqa_squad-validation-5794", "mrqa_squad-validation-5839", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6000", "mrqa_squad-validation-6027", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6259", "mrqa_squad-validation-6260", "mrqa_squad-validation-6263", "mrqa_squad-validation-6277", "mrqa_squad-validation-6295", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-65", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6538", "mrqa_squad-validation-6548", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6600", "mrqa_squad-validation-6612", "mrqa_squad-validation-6624", "mrqa_squad-validation-664", "mrqa_squad-validation-6656", "mrqa_squad-validation-6657", "mrqa_squad-validation-6666", "mrqa_squad-validation-6695", "mrqa_squad-validation-6749", "mrqa_squad-validation-6858", "mrqa_squad-validation-6861", "mrqa_squad-validation-6878", "mrqa_squad-validation-6880", "mrqa_squad-validation-6888", "mrqa_squad-validation-689", "mrqa_squad-validation-6898", "mrqa_squad-validation-6900", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-6967", "mrqa_squad-validation-7018", "mrqa_squad-validation-7021", "mrqa_squad-validation-7033", "mrqa_squad-validation-7036", "mrqa_squad-validation-7062", "mrqa_squad-validation-7123", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7246", "mrqa_squad-validation-7268", "mrqa_squad-validation-7302", "mrqa_squad-validation-7312", "mrqa_squad-validation-7323", "mrqa_squad-validation-7357", "mrqa_squad-validation-7362", "mrqa_squad-validation-7373", "mrqa_squad-validation-738", "mrqa_squad-validation-7391", "mrqa_squad-validation-741", "mrqa_squad-validation-7450", "mrqa_squad-validation-7458", "mrqa_squad-validation-7466", "mrqa_squad-validation-7470", "mrqa_squad-validation-755", "mrqa_squad-validation-7562", "mrqa_squad-validation-7603", "mrqa_squad-validation-764", "mrqa_squad-validation-767", "mrqa_squad-validation-7686", "mrqa_squad-validation-7711", "mrqa_squad-validation-7736", "mrqa_squad-validation-7744", "mrqa_squad-validation-7765", "mrqa_squad-validation-7770", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7892", "mrqa_squad-validation-7902", "mrqa_squad-validation-7957", "mrqa_squad-validation-7970", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8232", "mrqa_squad-validation-8236", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8572", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8685", "mrqa_squad-validation-8696", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8743", "mrqa_squad-validation-8760", "mrqa_squad-validation-8763", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8794", "mrqa_squad-validation-8797", "mrqa_squad-validation-8832", "mrqa_squad-validation-8837", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-8965", "mrqa_squad-validation-904", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9236", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-9317", "mrqa_squad-validation-9330", "mrqa_squad-validation-938", "mrqa_squad-validation-9401", "mrqa_squad-validation-9426", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-9579", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9628", "mrqa_squad-validation-9744", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9817", "mrqa_squad-validation-9890", "mrqa_squad-validation-9892", "mrqa_squad-validation-9897", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7579"], "OKR": 0.91796875, "KG": 0.4328125, "before_eval_results": {"predictions": ["Thermochemical", "executive producer", "1987", "James O. McKinsey", "North", "one-eighth", "coastal beaches and the game reserves", "Vicodin", "\u00a34.2bn", "Katharina von Bora", "9 March 1508", "Shoushi Li", "his means of seizing power", "ideal strings", "after the Franco-German War", "German", "San Andreas Fault", "NYPD Blue", "Northern Chinese", "Oireachtas funds", "the Marconi Company", "countries with bigger income inequalities", "John Robert Cocker", "King Kelly", "a U.S. Army major and psychiatrist", "Richard Masur", "1988", "Bergen County", "The Ones Who Walk Away from Omelas", "hiphop", "Lithuania", "historic buildings, arts, and published works", "Disney Parks Christmas Day Parade", "Esp\u00edrito Santo Financial Group", "David S. Goyer", "Wolf Creek", "YouTube", "onset and progression of Alzheimer's disease", "San Francisco 49ers", "Lake Placid, New York", "Iron Man 3", "Bury St Edmunds", "singer", "Chicago", "Martin \"Marty\" McCann", "247,597", "Mandalay Entertainment", "actor", "Boeing EA-18G Growler", "Barnoldswick", "Pacific War", "A41", "Heather Langenkamp", "Leona Lewis", "The God of Small Things", "BAFTA TV Award Best Actor winner in 1956", "Rodney Crowell", "Andy Serkis", "having or seeing nosebleeds or bleeding", "a pete", "Musharraf", "cancer", "is a special Jewish toast that...   Haim", "a nitrogen gas"], "metric_results": {"EM": 0.625, "QA-F1": 0.714586195054945}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.7692307692307693, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7887", "mrqa_squad-validation-2976", "mrqa_squad-validation-1480", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3822", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-1133", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-7417", "mrqa_newsqa-validation-850", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-5418"], "SR": 0.625, "CSR": 0.6477272727272727, "EFR": 1.0, "Overall": 0.7500923295454545}, {"timecode": 11, "before_eval_results": {"predictions": ["April 1887", "semi-arid savanna", "2014", "Levi's Stadium", "misguided", "San Jose Marriott", "1972", "second-largest", "Decision Time", "Victorian Government", "the American Revolutionary War", "pep rally", "human", "the Treaties establishing the European Union", "the Command Module's heat shield to survive a trans-lunar reentry", "Amazonia", "Daniel Andrews", "UNESCO's World Heritage list", "Richard E. Grant", ", running up a 31\u20130 halftime lead and then holding off a furious second half comeback attempt to win 31\u201324,", "NCAA's Division I", "Mark Helfrich", "Walmart", "\"Louie\" Zamperini", "Che Guevara", "Carol Ann Duffy", "Karl-Anthony Towns", "1978", "Danish", "Ukrainian", "1977\u20132012", "John John Florence", "\" Brainwash\")", "9Lives", "'valley of the hazels'", "Art Bell", "Sophie Winkleman", "Eminem", "Point Place", "Knowlton School", "Delilah Luke", "Don Bluth", "Columbus", "Czech Kingdom", "Anne Fletcher", "Sacramento Kings", "South Asian Games", "Tufts College", "Harrods", "Flamingo Las Vegas", "Ben Savage", "\"Suspiria\"", "City of Newcastle", "Japan", "Canada", "Arg15 - Ile16", "privatized", "silver", "insects", "were attacked by small-arms, machine-gun and RPG fire", "Jan Brewer", "Irving Berlin", "CBS Radio Network", "\"Mourner's Kaddish\""], "metric_results": {"EM": 0.65625, "QA-F1": 0.7610863095238095}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.6666666666666666, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8447", "mrqa_squad-validation-7288", "mrqa_squad-validation-4015", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-1825", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-818", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-3580", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7583"], "SR": 0.65625, "CSR": 0.6484375, "EFR": 1.0, "Overall": 0.750234375}, {"timecode": 12, "before_eval_results": {"predictions": ["late 1545", "several hundred thousand", "five", "every two years", "one-man", "The Hoppings", "Mycobacterium tuberculosis", "C. J. Anderson", "Harvey Martin", "stratigraphic", "environmental determinism", "Wellington", "problem instance", "at rest", "a sin", "Small subunit ribosomes", "\"Isel\"", "Edmonton, Canada", "Anthony Stephen Burke", "Eugene O'Neill", "Max Kellerman", "UVM Agriculture Department and the Agricultural Experiment Station", "Bundesliga club VfL Wolfsburg", "July 22, 1946", "Julia Verdin", "Lancashire", "McLaren-Honda", "\"Bismarck\"", "Comedy Film Nerds", "2016 World Indoor Championships", "MG Cars", "January 18, 1977", "North Greenwich Arena", "The Soloist", "Nikita Khrushchev", "Hal Linden", "the fourth President of Pakistan from 1971 to 1973", "February 18, 1965", "automobiles", "Republican", "Allison J71", "Chad", "NCAA Basketball Association", "Emilia Fox", "Freeform", "Mark Masons' Hall", "Cristiano Ronaldo", "American", "Chief Strategy Officer", "November 15, 1903", "De La Soul", "American", "Archbishop of Canterbury", "Via Vai", "1985", "after releasing Xander from the obligation to be Sweet's `` bride '', tells the group how much fun they have been ( `` What You Feel -- Reprise '' )", "when the cell is undergoing the metaphase of cell division", "California", "(multi-user dungeon)", "Dubai", "Iran", "Halle Berry", "Sinbad", "Abid Ali Neemuchwala"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7480105744949496}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-914", "mrqa_squad-validation-3765", "mrqa_squad-validation-8852", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-3242"], "SR": 0.703125, "CSR": 0.6526442307692308, "EFR": 1.0, "Overall": 0.7510757211538461}, {"timecode": 13, "before_eval_results": {"predictions": ["Tower District", "computational", "\"social and political action,\"", "Duran Duran", "30", "NYPD Blue", "chemical bonds", "John and Benjamin Green", "the Lippe", "between AD 0\u20131250", "2 million", "a statement to the chamber setting out the Government's legislative programme for the forthcoming year", "40,000 plant species", "Citadel Broadcasting", "$45,000", "stream capture", "1,149 feet", "about a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "Schleiden and Schawnn", "Eddie Murphy", "oregon", "Human fertilization", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year", "The terrestrial biosphere", "Peter Andrew Beardsley MBE", "Leo Arnaud", "lumbar", "is about the music business", "is currently the least valued currency unit in the Americas, 1 US dollar worth close to 5,770 guaranies", "digitization of social systems", "Yugoslavia was set up as a federation of six republics, with borders drawn along ethnic and historical lines : Bosnia and Herzegovina, Croatia, Macedonia, Montenegro, Serbia and Slovenia", "in Middlesex County", "Sweden had been an active supporter of the League of Nations and most of Sweden's political energy in the international arena had been directed towards the preservation", "in the 1960s", "lightning", "Inequality of opportunity was higher", "first stand - alone instant messenger and the first online instant messenger service", "George Strait", "silk, hair / fur ( including wool ) and feathers", "Anakin Wars", "King James II & VII", "Paspahegh Indians", "an enumeration of 7 spiritual gifts originating from patristic authors", "Spain", "Toronto, Ontario, Canada", "Manchuria", "Ben Savage", "at a given temperature", "Max", "Magyarorsz\u00e1g z\u00e1szlaja", "the church at Philippi", "2003", "is the star at the center of the Solar System", "northern China", "Mackinac", "Barbarella", "Bergen", "Balvenie Castle", "Something lost his job as the supermarket chain he worked for cut staff.Facing the prospect of homelessness and hunger in his own country, he joined the estimated 4,000 Zimbabweans", "is in the midst of an economic collapse, with nearly 80 percent unemployment and inflation estimated to be killed", "oregon", "oregon", "Elizabeth Gaskell", "(WRAMC)"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5485635747354497}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.9777777777777777, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.07407407407407408, 0.6666666666666666, 1.0, 0.07407407407407407, 0.2727272727272727, 0.07142857142857142, 0.0, 0.0, 0.7499999999999999, 0.625, 1.0, 0.25, 0.5, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1600", "mrqa_squad-validation-3599", "mrqa_squad-validation-4304", "mrqa_squad-validation-4263", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5608", "mrqa_triviaqa-validation-3961", "mrqa_hotpotqa-validation-877", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-16103", "mrqa_hotpotqa-validation-3149"], "SR": 0.4375, "CSR": 0.6372767857142857, "EFR": 1.0, "Overall": 0.7480022321428571}, {"timecode": 14, "before_eval_results": {"predictions": ["reaffirmed Catholicism as the state religion", "an all-time high between 2005 and 2010", "Shropshire", "social", "cartels", "Anglo-Saxon", "Ancient Egypt", "Battle of Olustee", "patrimonial feudalism", "Daniel arap Moi", "near Millingen aan de Rijn, close to the Dutch-German border with the division of the Rhine into Waal and Nederrijn", "an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing", "1303", "oxidant", "three, later four,", "Lance Cpl. Maria Lauterbach", "1994", "Empire of the Sun", "54 bodies", "Roger Federer", "sodium dichromate", "helicopters and boats, as well as vessels from other agencies,", "July", "citizenship", "40", "18", "to fill sandbags", "Expedia", "Kabul", "\"I'm just getting started.\"", "Communist Party of Nepal", "Bob Dole", "Eden Park", "in her home for 12 of the past 18 years", "12.3 million people worldwide", "as soon as 2050", "in all of Lifeway's 100-plus stores nationwide", "Osan Air Base", "18", "National Park Service", "3-2", "Bob Bogle", "40 militants and six Pakistan soldiers dead", "1959", "Pakistan's High Commission in India", "his father", "Obama's race", "Obama", "Mark Fields", "Howard Bragman", "a peace sign", "Muslim festival of Eid al-Adha", "Larry Ellison", "National Indigenous Organization of Colombia", "an unknown recipient", "Jules Shear", "Frank Warner", "Vienna", "2008\u201309 UEFA Champions League", "310", "New England", "Achaemenid Empire", "2017", "Algernod Lanier Washington"], "metric_results": {"EM": 0.46875, "QA-F1": 0.572250796078921}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.4000000000000001, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3163", "mrqa_squad-validation-1886", "mrqa_squad-validation-1064", "mrqa_squad-validation-5276", "mrqa_squad-validation-9076", "mrqa_squad-validation-1287", "mrqa_squad-validation-3532", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-1038", "mrqa_triviaqa-validation-7584", "mrqa_hotpotqa-validation-2080", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-1723", "mrqa_hotpotqa-validation-5370"], "SR": 0.46875, "CSR": 0.6260416666666666, "EFR": 1.0, "Overall": 0.7457552083333333}, {"timecode": 15, "before_eval_results": {"predictions": ["Robert R. Gilruth", "Arthur Woolf", "Ten", "multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "seven classes", "Funchess", "St. George's United Methodist Church", "1914", "Wales", "70,000", "Nikita Khrushchev", "in proportion to capital inputs, increasing unemployment (the \"reserve army of labour\")", "X is no more difficult than Y, and we say that X reduces to Y", "February 5,", "he acted in self defense in punching businessman Marcus McGhee.", "MDC head Morgan Tsvangirai.", "so powerful and intimidating that police were hesitant to build a case against them.", "in Spain and at Harvard Law School.", "power-sharing talks to take place in the next few weeks.", "Senate", "15,000", "Kim Jong Un", "Pfc. Bowe Bergdahl", "A 22-year-old college student", "Democratic", "IV cafe", "North Korea intends to launch a long-range missile in the near future,", "District of Columbia National Guard,", "forgery and flying without a valid license,", "work is the hardest and least rewarding work we have ever tried to do.", "14", "All three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "school", "Monday and Tuesday", "27-year-old", "almost 100 vessels off Somalia's coast", "Venus Williams", "allergies to peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy", "as spies for more than two years,", "Manchester United", "Nafees A. Syed", "Dick Cheney", "military commissions are inherently illegitimate, unconstitutional and incapable of delivering outcomes we can trust.", "Alfredo Astiz,", "a Muslim with Lebanese heritage,", "Illness", "procedures", "Jaime Andrade", "56", "Michael Jackson", "Raymond Thomas", "racially motivated", "Adam Lambert", "Arlington National Cemetery's Section 60", "The U.S. state of Georgia", "along the coast of northern California", "citric acid", "Denali", "The New Yorker", "death", "the Mason-Dixon Line", "high and dry", "a palace on US soil", "New Orleans Saints"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6426055641694943}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.2222222222222222, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.4210526315789474, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.888888888888889, 0.2857142857142857, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.16666666666666669, 0.7272727272727273, 1.0, 1.0, 0.0, 0.17391304347826086, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-7260", "mrqa_squad-validation-7183", "mrqa_squad-validation-1748", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-1965", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-6596", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-3685"], "SR": 0.515625, "CSR": 0.619140625, "EFR": 0.967741935483871, "Overall": 0.7379233870967742}, {"timecode": 16, "before_eval_results": {"predictions": ["\"exterminate\" all non-Dalek beings.", "San Diego", "30\u201375%", "favors the reordering of government and society", "James Watt", "comb jellies", "NewcastleGateshead", "1989", "Serbian Orthodox priest", "Denmark, Iceland and Norway", "the need for alliances", "Anderson", "1969", "Debbie Gibson", "to collect", "at least 18 or 21 years old", "acid", "the UK", "1917", "three", "Montgomery", "Upon closure at birth", "it was to last four years unless renewed by the Reichstag", "December 1, 2009", "Miami Heat", "Jim Capaldi", "Alan Shearer", "The 1700 Cascadia earthquake", "the Central and South regions, and by people of Mexican ancestry living in other places, especially the United States", "Portugal. The Man", "the blood to the liver", "1960", "two - year terms", "Harry", "September 19, 2017", "Andy Serkis", "President Franklin Roosevelt", "John Smith", "Idaho", "Holly", "Abraham Gottlob Werner", "Jason Paige", "Jackie Robinson", "Rufus and Chaka Khan", "the body - centered cubic", "William J. Bell", "merengue", "3", "Olivia Olson", "erosion", "Office of Inspector General,", "an integral membrane protein", "Conrad Lewis", "Justice Harlan", "a person employed to write or type what another dictates or to copy what has been written by another,", "purple", "Oakland, California", "World Famous Gold & Silver Pawn Shop", "Crandon, Wisconsin,", "loyalty and the way that I stood by this guy through thick and thin...\"", "Psycho", "sapphires", "Warren Gamaliel Harding", "yellow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6237704136141636}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false], "QA-F1": [0.888888888888889, 1.0, 1.0, 0.6666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.42857142857142855, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7778", "mrqa_squad-validation-9610", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-3385", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-7642", "mrqa_hotpotqa-validation-5030", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-3751", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-262"], "SR": 0.515625, "CSR": 0.6130514705882353, "EFR": 1.0, "Overall": 0.7431571691176471}, {"timecode": 17, "before_eval_results": {"predictions": ["diversity", "\"Provisional Registration\"", "the Tyne and wear Metro", "phlogiston", "Creon", "Half Business Big Game", "1892", "The coordinating lead authors", "Six", "ideal pulleys", "\u00d6gedei Khan", "Princes Park", "47 years", "Polk", "Mrs. Eastwood & Company", "first train robbery", "Las Vegas", "attorney, politician, and the principle founder of the Miami Dolphins", "Amy Babcock", "The visit", "Hong Kong First Division League", "Unbreakable", "rap parts", "The Maze Runner", "Agra", "1.6 million passengers", "film actress and dancer", "Gaius Julius Caesar Augustus Germanicus", "Jeff Van Gundy", "Joachim Trier", "Tamil", "1972", "2013", "Golden Globe Award for Best Actor", "Ronald Lyle \" Ron\" Goldman", "David Allen", "Cleopatra VII Philopator", "political", "Tomorrowland", "16,116", "footballer", "Bishop's Stortford", "Joanna No\u00eblle Levesque", "late eighteenth century", "The School Boys", "Operation Iceberg", "Texas Longhorns", "Hordaland", "1968", "30", "the \"Pierement Waltz\"", "October 22, 2012", "a Soldier in Truck", "a young girl", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Neptune", "Lyon, France", "70,000 or so", "Miami Beach, Florida,", "the second pilot episode of the science fiction television series Star Trek", "Estonian", "Amalienborg", "Aristophanes", "The Red Badge of Courage"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6489211309523809}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.2, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.4, 0.33333333333333337, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2788", "mrqa_squad-validation-543", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-1308", "mrqa_naturalquestions-validation-9028", "mrqa_newsqa-validation-1720", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-3630", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-3516"], "SR": 0.546875, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.742421875}, {"timecode": 18, "before_eval_results": {"predictions": ["Lagos and Quiberon Bay", "seven months old", "Sava Kosanovi\u0107", "a noble death", "\"Monte Carlo\")", "huge", "Graz", "5,792", "Lucas\u2013Lehmer", "Wahhabism", "February 26, 1948", "Hordaland", "the town of El Nacimiento in M\u00fazquiz Municipality", "Stephen James Ireland", "Koch Industries", "Washington", "the Isle of Man", "2010", "High Falls Brewery", "technical director", "Mike Holmgren", "Nathan Bedford Forrest", "Hyuna", "green and yellow", "Isobel", "Urijah Faber", "Barack Obama's", "Guthred", "Peel Holdings", "1 million copies worldwide", "College Football Friday Primetime", "Marko Tapani \" Marco\" Hietala", "In 2017, Pachulia won his first NBA Championship as a member of the Warriors", "Rebecca", "An invoice, bill or tab", "jimmy carter", "Clarence Nash", "Minneapolis", "Marco Fu", "Syracuse", "Durban International Convention Centre (ICC Arena)", "Ryan Babel", "Bob Dylan", "\"continuously operating\" roller", "Luca Guadagnino", "Jennifer Lynne \" Jennings\" Brown", "11 Grands Prix wins", "National Collegiate Athletic Association", "Bill Cosby", "thirteen", "Argentinian", "Dana Fox", "Sunday, November 2, 2003", "northwest Washington", "Jenny", "Nadia Comaneci", "The Mayor of Casterbridge", "Friday", "Luca di Montezemolo", "the Social Democratic", "they must be combined with one of the higher pectin fruits", "sexual misconduct", "President Obama and Britain's Prince Charles", "a woman"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5781994047619048}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.14285714285714285, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9057", "mrqa_squad-validation-9255", "mrqa_squad-validation-8020", "mrqa_squad-validation-9592", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-4757", "mrqa_newsqa-validation-2163", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-3464", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2844"], "SR": 0.484375, "CSR": 0.602796052631579, "EFR": 1.0, "Overall": 0.7411060855263157}, {"timecode": 19, "before_eval_results": {"predictions": ["CEPR", "the Commission and Council", "14,000", "scoil phr\u00edobh\u00e1ideach", "90 to 95 percent", "the famous rock Lorelei", "56.2%", "computational power", "Christ's message and teachings", "Bayern Munich", "fifth", "once", "A123 Systems, LLC", "the backside", "Bothtec", "the Manhattan Project", "1975", "3,000", "youngest TV director ever", "Nigel Godrich", "About 200", "Marco Fu", "Orfeo ed Euridice", "seventh", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn.", "Golden Calf for Best Actor in 2013", "Houston Rockets", "Summerlin, Clark County, Nevada", "Argentinian", "Europe", "Noel", "the Savannah River Site", "a family member", "Switzerland", "second largest", "Frank Lowy", "Fat Man", "Robert Marvin \"Bobby\" Hull, OC", "Nye County", "Herman's Hermits", "Marian Delario", "Pendlebury, Lancashire", "300 km north west", "1932", "Arrowhead Stadium", "1885", "House of Borromeo", "Democracy", "Sydney", "Michael Redgrave", "KlingStubbins", "Big 12", "a kind of oil lamp", "2017", "RAF", "the A-Rod", "Mater", "the deployment of unmanned drones", "a one-shot victory in the Bob Hope Classic on the final hole", "the white supremacists system", "Dune", "the fire", "\"teenagers in Versailles.\"", "Jupiter"], "metric_results": {"EM": 0.625, "QA-F1": 0.6721876214063713}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7001", "mrqa_squad-validation-9093", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-119", "mrqa_naturalquestions-validation-954", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-3281", "mrqa_newsqa-validation-1445", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-2686"], "SR": 0.625, "CSR": 0.60390625, "EFR": 1.0, "Overall": 0.741328125}, {"timecode": 20, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1582", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2022", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2295", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3693", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5718", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4155", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-402", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-642", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-1723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10115", "mrqa_squad-validation-10249", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10328", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1064", "mrqa_squad-validation-1064", "mrqa_squad-validation-1114", "mrqa_squad-validation-1162", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1255", "mrqa_squad-validation-1296", "mrqa_squad-validation-1366", "mrqa_squad-validation-1480", "mrqa_squad-validation-1529", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1597", "mrqa_squad-validation-1723", "mrqa_squad-validation-1748", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-2040", "mrqa_squad-validation-2059", "mrqa_squad-validation-2111", "mrqa_squad-validation-2144", "mrqa_squad-validation-2153", "mrqa_squad-validation-2315", "mrqa_squad-validation-234", "mrqa_squad-validation-2475", "mrqa_squad-validation-2536", "mrqa_squad-validation-2568", "mrqa_squad-validation-2628", "mrqa_squad-validation-2701", "mrqa_squad-validation-2786", "mrqa_squad-validation-287", "mrqa_squad-validation-2898", "mrqa_squad-validation-2976", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3198", "mrqa_squad-validation-3240", "mrqa_squad-validation-3483", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4015", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4259", "mrqa_squad-validation-4263", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-4545", "mrqa_squad-validation-457", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4646", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5200", "mrqa_squad-validation-5276", "mrqa_squad-validation-531", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5542", "mrqa_squad-validation-556", "mrqa_squad-validation-5611", "mrqa_squad-validation-5655", "mrqa_squad-validation-5678", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6263", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-6500", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6523", "mrqa_squad-validation-6538", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6657", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-689", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-7001", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7062", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7152", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7302", "mrqa_squad-validation-7302", "mrqa_squad-validation-7323", "mrqa_squad-validation-7391", "mrqa_squad-validation-7403", "mrqa_squad-validation-7450", "mrqa_squad-validation-7603", "mrqa_squad-validation-7692", "mrqa_squad-validation-7732", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8077", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8219", "mrqa_squad-validation-8225", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8447", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-8612", "mrqa_squad-validation-864", "mrqa_squad-validation-8668", "mrqa_squad-validation-8685", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-876", "mrqa_squad-validation-8836", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9061", "mrqa_squad-validation-9076", "mrqa_squad-validation-9101", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9240", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9652", "mrqa_squad-validation-9716", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9956", "mrqa_squad-validation-9991", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4403", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7642"], "OKR": 0.892578125, "KG": 0.41875, "before_eval_results": {"predictions": ["Bento de Moura Portugal", "an unmarked grave somewhere in Mongolia", "polynomial-time", "All India Muslim League", "WatchESPN", "1967", "antigen from a pathogen", "Encoded Archival description (EAD)", "Trey Parker and Matt Stone", "First Family of Competitive eating", "Democratic Unionist Party", "local South Australian and Australian produced content", "Naked Soldier", "7 June 1926 to 17 December 1926", "Summer Olympic Games unofficial programme in 1900", "1937", "John Lee Hancock", "Hordaland", "Agent 99", "Bonnie Franklin", "Edmonton, Alberta", "Skyscraper", "Love Actually", "a creek", "Orange County, California", "Humvee", "sea eagle", "32", "fifty-word", "Philadelphia Naval Shipyard", "The Books", "Rochdale, North West England", "2013\u201314", "Clara Petacci", "Marie Fraser", "Lionel Brockman Richie Jr.", "The Two Noble Kinsmen", "Eucritta melanolimnetes", "Jenji Kohan", "Johnson & Johnson", "PewDiePie", "Germanic", "Most 2002 United States Senate election in Minnesota took place on November 5, 2002", "Tamara Ecclestone Rutland", "Thomas Joseph \"T. J.\" Lavin", "Stalybridge Celtic", "Adelaide Lightning", "Kohlberg K Travis Roberts", "My Love from the Star", "Tottenham ( ) or Spurs", "Sam Bettley", "Ernest Hemingway", "Nia Kay", "The Fixx", "The Crossing", "1876", "dynamite", "London transit bombings", "head injury", "The Tomb of Sheikh Salim Chishti", "teeth", "Profit maximization", "electron shells", "1901"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6328125}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.4, 0.4, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.8, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1764", "mrqa_squad-validation-525", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-4819", "mrqa_naturalquestions-validation-8584", "mrqa_triviaqa-validation-7266", "mrqa_newsqa-validation-894", "mrqa_searchqa-validation-9565"], "SR": 0.515625, "CSR": 0.5997023809523809, "EFR": 1.0, "Overall": 0.7224404761904762}, {"timecode": 21, "before_eval_results": {"predictions": ["a Serbian Orthodox priest", "December 12", "the Hostmen", "Apollo 8", "Antigone", "three membranes", "Sugarfoot", "\"Crossed: Family Values\"", "Forbes", "Mitsubishi Motors Corporation", "tabasco", "1 January 1788", "Lowestoft", "Christopher Nolan", "Taylor Swift", "Al Horford", "Johan Leysen", "1854", "Switzerland", "New York Giants", "professional footballer", "Logar Province", "Lauren Alaina", "Ian Fleming", "Mary Bonauto, Susan Murray, and Beth Robinson and the 1999 Vermont Supreme Court case \"Baker v. Vermont\"", "27 November 1956", "American heavy metal band", "Hindi", "United States Auto Club", "\"SpongeBob SquarePants\"", "Albany School for Educating People of Color", "BAFTA TV Award Best Actor", "\"The Bob Edwards Show\"", "\"the heaviest album of all\"", "Field Marshal Lord Gort", "15,024", "Prime Minister of Denmark 1852\u20131853 as head of the Cabinet of Bluhme I", "\"Cind Cinderella\"", "the Cylon Number Six", "2007", "3,000", "Chinese Coffee", "Arnold M\u00e6rsk Mc\u00f8ller", "Mineola", "KWPW", "John Richard Schlesinger", "fourth-ranking", "Atomic Kitten", "Esperanza Spalding", "the Ruul", "Lonestar", "1916", "American", "16", "The Golden Gate Bridge", "David Jason", "Olive", "Turkey", "Michael Jackson may soon return to the stage, at least for a \"special announcement.\"", "Entourage", "the Tet Offensive", "Erica Rivera", "Spanish missionaries", "Madison"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6739594606782107}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 1.0, 1.0, 0.25, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8651", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-5216", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-1446", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-347", "mrqa_naturalquestions-validation-1640", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-3348"], "SR": 0.546875, "CSR": 0.5973011363636364, "EFR": 0.9655172413793104, "Overall": 0.7150636755485894}, {"timecode": 22, "before_eval_results": {"predictions": ["in numerous noble palaces and churches during the later decades of the 17th century", "WLQP-LP", "Sunni extremist groups", "mid-Eocene", "the Soviet Union", "teacher enthusiasm", "Sir Kenneth Arthur Noel Anderson", "receive the benefits of the Morrill Acts of 1862 and 1890", "Prussian", "Violet", "German", "the British series of the same name", "Indianola", "the gods themselves", "Nassau County", "the early 19th century", "Andries Jonker", "President John F. Kennedy and Jacqueline Kennedy", "Mollie Elizabeth King", "1959", "129,007", "San Francisco 49ers", "Big Machine Records", "the Parthian Empire", "almost 3 million people", "Matt Groening", "June 10, 1982", "Philip K. Dick", "Jack White of The White Stripes", "Samuel Burl \"Sam\" Kinison", "Boston", "Lisa", "perjury and obstruction of justice", "Galleria Vittorio Emanuele II", "Paul Avery", "31 October 1783", "Puli Alam", "the Peninsular War in Spain during the Napoleonic Wars of the early 19th Century", "1838", "Margaret Thatcher", "Ashland is home to Scribner-Fellows State Forest", "Manchester Victoria station in air rights space", "the east of Ireland", "Estadio de L\u00f3pez Cort\u00e1zar", "March 21, 2004", "Jesus", "Agent Carter", "Plies", "Tim \"Ripper\" Owens", "Walt Disney Productions", "chalk quarry", "Rhode Island", "electron donors", "Johnson", "Sarafina", "King George VI", "the River Thames", "the D.C.", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "died of cardiac arrest on June 25.", "the run-of-the-mill security guard", "the dugout canoe", "we/wee", "Tim Russert"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7045710076960077}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.29629629629629634, 0.5454545454545454, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-968", "mrqa_squad-validation-3754", "mrqa_squad-validation-9647", "mrqa_squad-validation-2146", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-3736", "mrqa_naturalquestions-validation-5288", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-257", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-2945", "mrqa_searchqa-validation-11779"], "SR": 0.609375, "CSR": 0.5978260869565217, "EFR": 0.96, "Overall": 0.7140652173913044}, {"timecode": 23, "before_eval_results": {"predictions": ["MBH99 reconstructions", "the Moselle", "rotating discs", "Pittsburgh Steelers", "14,000", "Scandinavia and northern Europe", "1911", "ESPN's \"SportsCenter\"", "Prince Antoni Radziwi\u0142\u0142", "northern Italy's Lombardy region", "Caesars Palace Grand Prix", "Ford Field in Detroit, Michigan", "Rotterdam Square", "Sun", "Point of Entry", "Wilmette, Illinois", "Malayalam cinema", "Pendlebury, Lancashire", "Leona Lewis", "Laura Dern", "Melbourne", "democracy and personal freedom", "Cool Runnings", "Indian", "Labour Party", "Thor", "Copa Airlines", "Chiltern Hills", "Rudebox", "Washington", "Big Mamie", "teacher Jaime Escalante", "sunday", "Slaughterhouse-Five", "Nashville", "simple language", "Telugu and Tamil film industries", "Peshwa", "Joseph I", "Oracle Corporation", "July 1999", "William Shakespeare", "January 23, 1898", "1995", "Bergen", "Scribner", "Apprendi v. New Jersey", "Beavers", "Jack Elam", "August 17, 1907", "The Design Inference", "R&B vocal group", "pilgrimages to Jerusalem", "art pottery", "series about restoring someone's faith in love and family relationships", "California Chrome", "Moby Dick", "Christine Keeler", "137", "townsend of Aberdeen, Washington,", "Sunday", "Jackie Robinson", "sunday line", "a narrow fellow in the grass"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7195240705931495}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.888888888888889, 0.5, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.8571428571428571, 0.5, 0.0, 0.7368421052631577, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8702", "mrqa_squad-validation-3469", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-354", "mrqa_hotpotqa-validation-3552", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2866", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-3186", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1059", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-10472"], "SR": 0.5625, "CSR": 0.5963541666666667, "EFR": 0.9642857142857143, "Overall": 0.7146279761904762}, {"timecode": 24, "before_eval_results": {"predictions": ["Los Angeles", "combustion chamber", "13th-century", "spinat", "ACL", "League One", "Las Vegas", "Suzuki YZF-R6", "Ahold N.V.", "east", "The Gettysburg Address", "Engineering", "Robert \"Bobby\" Germaine, Sr.", "American 3D computer-animated comedy", "the Asia-Pacific War", "Amy Poehler", "current manager", "British Labour", "USC Marshall School of Business", "Theme Hospital", "1936", "a tailor", "Maxwell Smart", "\"The Walking Dead\"", "2008", "Yasir Hussain", "Let's Make Sure We Kiss Goodbye", "Ronald Ryan", "James Brolin", "soccer", "Peel Holdings", "Chechen Republic", "alcoholic drinks", "Zaire", "Debbie Harry", "Barbara Lee Alexander", "novelist and poet", "1986", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Hilary Duff", "100 million", "Albanian political party", "2015 Masters Tournament", "John Schlesinger", "Venice", "Rockstar San Diego", "A.S. Roma", "Genderqueer", "Gothic Revival", "Melbourne's City Centre", "South West Peninsula League", "Bury St Edmunds, Suffolk, England", "1608", "Sedimentary rock", "Rugrats in Paris", "auk", "Sidecar", "Rose-Marie", "Sen. Barack Obama", "David Wymore.", "World number two Roger Federer", "aluminum", "calcium", "Anne Frances Reagan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7052703373015873}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.25, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.7499999999999999, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-10449", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1741", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-234", "mrqa_naturalquestions-validation-9626", "mrqa_triviaqa-validation-6356", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-1361", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-14782"], "SR": 0.5625, "CSR": 0.595, "EFR": 1.0, "Overall": 0.7215}, {"timecode": 25, "before_eval_results": {"predictions": ["August 15, 1971", "Geordie", "Peter Howell", "many elements of the old language", "formalize a unified front in trade and negotiations with various Indians", "French", "Muncie", "2017", "Logan International Airport", "GZA", "Serhiy Paradzhanov", "no. 3", "John \"John\" Alexander Florence", "Two Is Better Than One", "July 16, 1971", "Microsoft Office file formats", "Baldwin, Nassau County, New York, United States", "Elton John", "Firestorm", "the Ruul", "March 14, 2000", "David Wells", "Northern Lights", "the Chengdu Aircraft Corporation (CAC) of China", "the Bhaktivedanta Institute of ISKCON", "Minnesota", "Oklahoma", "Jim Aaron Diamond", "Smithfield", "Julie Taymor", "29 September\u20132 October 2011", "Columbia Records", "1943", "Maria Brink", "Scottish folk song", "Cody Miller", "Darkroom", "Tainted Love", "Christopher Nolan", "Weezer", "1992", "2016 United States elections", "pro-Confederate partisan rangers (\"bushwhackers\")", "Princes Park in Melbourne", "\"The Late Late Show\"", "2012", "1978", "Donald Carl \"Patsy\" Swayze", "John Morgan", "June", "an organ", "Macau", "49 cents", "issued upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Jocelyn Flores", "the purpose of emphasis or heightened effect", "Egyptian", "tiger", "1492", "June 2002,", "A growing percentage of the Somali population has become dependent on humanitarian aid.", "Thomas Nast", "ice hockey", "Ugly Betty"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6799231150793651}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8888888888888888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-3386", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-1718", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-2092", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-699", "mrqa_triviaqa-validation-3925", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-540"], "SR": 0.578125, "CSR": 0.5943509615384616, "EFR": 0.9629629629629629, "Overall": 0.713962784900285}, {"timecode": 26, "before_eval_results": {"predictions": ["Italian physicist", "illiberal Islamic regimes", "The Judicial Council", "CBS", "egypt", "The David Bowie Song Lyric Cut-Up Game", "ibex", "Hebridean isle", "prostate", "Vitus Bering", "egypt", "Rwanda", "larva", "egypt", "Der Zauberberg", "egypt", "Canada", "Komodo", "a laparoscope", "eight", "Ice Cream Salesman", "eulliver", "evanhoe", "radio waves", "eEE", "egyptian", "Eliza Doolittle", "fibrous", "En banc", "Franklin D. Roosevelt", "six ounces", "Violeta Barrios de Chamorro", "eiorgio Moroder", "Rafael Nadal", "Canberra", "Ich bin ein Berliner", "egypt", "Good fiction", "egypt", "Platoon", "egypt", "e e letra da msica", "Nanjing", "zZ", "euf Wiedersehen", "blubber", "catalysts", "the L-M version", "egypt", "Deep Purple", "egypt", "egypt", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Speaker of the House of Representatives", "the Bulgarian 2nd Army", "20", "Time Bandits", "egypt", "Kerry Marie Butler", "35", "three centuries", "Consumer Product Safety Commission", "file papers shortly with an appeals court seeking an emergency stay of the order in its tracks.", "the United Nations"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4076121794871794}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615383, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15646", "mrqa_searchqa-validation-10928", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-2775", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-4299", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-4939", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-6143", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-10252", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-4190", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-7196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-2819", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-1748"], "SR": 0.3125, "CSR": 0.583912037037037, "EFR": 1.0, "Overall": 0.7192824074074075}, {"timecode": 27, "before_eval_results": {"predictions": ["May 18, 1756", "canceled", "modern fashion", "46% of the world's wealth", "Lismore", "STS-51-L", "Paradise", "European", "Newcastle upon Tyne, England", "Colonel", "Cody Miller", "Virginia", "Hertz Corporation", "Wiz Khalifa", "Disney California Adventure", "Maria Brink", "The Sound of Music", "G\u00f6tene in Sweden", "Argentine", "novelty songs, comedy, and strange or unusual recordings", "January", "6teen", "South America", "Princes Park", "Edgar Rice Burroughs", "the Knight Company", "My Gorgeous Life", "Ashanti Region", "Raden Panji", "Culiac\u00e1n, Sinaloa", "Northampton, England", "Black Panthers", "Fred Willard", "Trappist beer", "Mary Wayte", "nine", "1909", "about 5320 km", "3,384,569", "anvil", "House of Hohenstaufen", "James G. Kiernan", "Tony Bennett", "Forrest Gump", "2009", "1919", "a skerry", "1620 to 1691", "Charles Edward Stuart", "Mickey Gilley", "Blue Origin", "2015 Orange Bowl", "2017", "Eleanor Roosevelt", "Utah, Arizona, Wyoming, and Oroville, California", "Google", "the earth-moon system", "English rock group the Kinks", "throwing three punches but said only one connected.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997", "1975", "The sperm whale", "libraries", "NASA"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7216637529137528}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.8, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7559", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-296", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4650", "mrqa_hotpotqa-validation-4645", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-2829", "mrqa_triviaqa-validation-1527", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2586"], "SR": 0.609375, "CSR": 0.5848214285714286, "EFR": 1.0, "Overall": 0.7194642857142858}, {"timecode": 28, "before_eval_results": {"predictions": ["British patrons", "Laverne & Shirley", "South", "Roger Bacon", "bacteria", "perigee", "Arthur Pitney", "The Office", "apogee", "September 20, 1934", "Old Quebec City", "Edith Piaf", "the Krntnertor Theater", "Sappho", "apogee", "Colorado River", "Mars", "Timothy Leary", "3800", "the Thought Police", "The Street Lawyer", "apogee", "Anthony Afterwit", "Doctor", "a 1.5 km swim", "calcium", "Mikel Arteta", "Wisconsin", "Raphael", "To Build a Fire", "auk", "Around 2500 B.C. the Harappan Civilization", "apse", "centigrade", "silver", "BBC", "The jackass penguin", "Jack Lewis", "Blackwater USA", "apogee", "N. Hilton", "December", "Arsinoe II", "New Jersey", "\"E-T\"", "a con", "the C&D Canal", "asthma", "a duck", "a trumpet", "Narcissus", "Norman Bates", "liquids", "The Senate is composed of senators, each of whom represents a single state in its entirety, with each state being equally represented by two senators", "Sarah Silverman", "Anwar Sadat", "apogee", "Barings", "Esp\u00edrito Santo Financial Group", "Earvin \"Magic\" Johnson Jr.", "John DeMita", "free laundry service", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "the north coast of Puerto Rico"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5136817528735631}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.06896551724137931, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.23999999999999996, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-13291", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-8603", "mrqa_searchqa-validation-4398", "mrqa_searchqa-validation-4388", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-6355", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-11293", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-841", "mrqa_searchqa-validation-4866", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-4972", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3961"], "SR": 0.4375, "CSR": 0.5797413793103448, "EFR": 0.9444444444444444, "Overall": 0.7073371647509579}, {"timecode": 29, "before_eval_results": {"predictions": ["NBA", "11 million", "GTE", "High school", "John Lee Hancock", "2007", "Westfield Tea Tree Plaza", "Philadelphia", "237 square miles", "Gal Gadot", "1860", "Eddie Izzard", "1966", "Miracle", "Richard Wayne Snell", "writer", "Humberside Airport", "8/7c on Fox", "2012", "Kind Hearts and Coronets", "Centennial Olympic Stadium", "Leatherheads", "Polka", "Sinngedichte", "James G. Kiernan", "Australia", "heavy metal band", "Anah\u00ed", "private", "Tampa Bay Lightning", "tabasco peppers", "Patricia Arquette", "Secrets and Lies", "Richard John Bingham", "non-alcoholic", "Square Enix", "Geraldine Page", "pornographicstar", "Europe", "460", "five", "Sam the Sham", "pinball", "High Falls Brewing Company", "Las Vegas", "PPG Paints Arena", "new king", "J35", "Chief Minister of Tamil Nadu", "Romance", "New York", "Macomb County", "birth", "biochemistry", "United States Internal Revenue Service", "Classical Archives", "Stockholm syndrome", "magnetism", "Morgan Tsvangirai", "Empire of the Sun", "Amanda Knox's aunt", "Robert the Bruce", "(Larry Hagman", "a gull"], "metric_results": {"EM": 0.5, "QA-F1": 0.5841021825396826}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444444, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.5714285714285715, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5826", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-925", "mrqa_triviaqa-validation-5397", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-3391", "mrqa_searchqa-validation-5455"], "SR": 0.5, "CSR": 0.5770833333333334, "EFR": 1.0, "Overall": 0.7179166666666668}, {"timecode": 30, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1180", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1342", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1488", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-172", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2199", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2825", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-3071", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3191", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4285", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4467", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5561", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5713", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-704", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-98", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1199", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11434", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-13831", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-2686", "mrqa_searchqa-validation-3516", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6045", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-8245", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9601", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10223", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10424", "mrqa_squad-validation-10449", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1064", "mrqa_squad-validation-1096", "mrqa_squad-validation-1114", "mrqa_squad-validation-1177", "mrqa_squad-validation-1232", "mrqa_squad-validation-1255", "mrqa_squad-validation-1258", "mrqa_squad-validation-1296", "mrqa_squad-validation-1480", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1723", "mrqa_squad-validation-1940", "mrqa_squad-validation-1976", "mrqa_squad-validation-2153", "mrqa_squad-validation-2272", "mrqa_squad-validation-2301", "mrqa_squad-validation-234", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2591", "mrqa_squad-validation-287", "mrqa_squad-validation-2976", "mrqa_squad-validation-306", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3296", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3716", "mrqa_squad-validation-3767", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4026", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4259", "mrqa_squad-validation-4286", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4545", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4810", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5276", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5532", "mrqa_squad-validation-556", "mrqa_squad-validation-5578", "mrqa_squad-validation-5611", "mrqa_squad-validation-5779", "mrqa_squad-validation-5839", "mrqa_squad-validation-5881", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6443", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6888", "mrqa_squad-validation-6967", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7125", "mrqa_squad-validation-7152", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7323", "mrqa_squad-validation-7403", "mrqa_squad-validation-7458", "mrqa_squad-validation-7603", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-8219", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-864", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8839", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9061", "mrqa_squad-validation-9191", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9521", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9647", "mrqa_squad-validation-9652", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2654", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-830"], "OKR": 0.85546875, "KG": 0.43515625, "before_eval_results": {"predictions": ["any member of the Scottish Government", "quadratic time", "1879", "Xiu Li Dai", "St. Theodosius Russian Orthodox Cathedral", "The 256 - acre ( 1.04 km ; 0.400 sq mi ) campus often referred to as `` The Mecca '' is located in northwest Washington", "1924", "England", "Bud Light", "the status line", "Benjamin Franklin", "December 2, 2013", "the intermembrane space", "Chinese", "Laodicean Church", "the U.S. Senate", "electric potential generated by muscle cells when these cells are electrically or neurologically activated", "thick skin", "an Islamic shrine located on the Temple Mount in the Old City of Jerusalem", "Sylvester Stallone", "Anakin Skywalker", "September 27, 2017", "convert single - stranded genomic RNA into double - stranded cDNA", "the economy", "war gardens", "Michael `` Crocodile '' Dundee inCrocodile Dundee ( 1986 )", "961", "northern China", "gathering money from the public", "a beach in Malibu, California", "Kitty Softpaws", "homicidal thoughts of a troubled youth", "an autonomous constituent country within the Kingdom of Denmark between the Arctic and Atlantic Oceans, east of the Canadian Arctic Archipelago", "Sun Tzu", "18th century in the United Kingdom", "Setsuko Thurlow", "temporal lobes", "the Douze at Pont l'Abb\u00e9", "DNA", "mining", "Keith Thibodeaux", "six - hoop game", "Atlantic", "butane", "Julia Roberts", "12 November 2010", "Brazil, China, France, Germany", "Aaron Harrison", "Panning", "CBS Television City", "Johnny Depp", "James Chadwick", "the Swirral Edge ridge", "Nutrient enrichment", "Worcester Cathedral", "Germany", "Rachel, Nevada", "Atlanta", "more than 30 Latin American and Caribbean nations", "two women", "police dogs", "Observatory", "(Fahrid) Murray Abraham", "Hannibal of Carthage"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5290604070015835}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.4864864864864865, 0.0, 0.4, 0.0, 1.0, 1.0, 0.47058823529411764, 0.6666666666666666, 0.0, 1.0, 0.16666666666666666, 1.0, 0.4166666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.25, 1.0, 0.6666666666666666, 0.5454545454545454, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1818", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-6465", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-782", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2873", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-414", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-4098"], "SR": 0.359375, "CSR": 0.5700604838709677, "EFR": 1.0, "Overall": 0.7217464717741936}, {"timecode": 31, "before_eval_results": {"predictions": ["weight in burning was hidden by the buoyancy of the gaseous combustion products", "During the genus Beroe the juveniles, like the adults, lack tentacles and tentacle sheaths", "September 19 - 22, 2017", "Oliver Goldsmith", "John Roberts", "in contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "between the Eastern Ghats and the Bay of Bengal", "a bow bridge with 16 arches shielded by ice guards", "12 to 36 months old", "helb McKittrick", "Chelsea", "Darlene Cates", "fascia surrounding skeletal muscle", "Jerry Leiber and Mike Stoller", "Bokm\u00e5l : Et dukkehjem", "Lisa Stelly", "Only Fools and Horses", "A standard form contract ( sometimes referred to as a contract of adhesion, a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract )", "Robin", "Jack Barry", "Missouri River", "Randy", "August 18, 1945", "19 June 2018", "Daniel A. Dailey", "to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "international educational foundation headquartered in Geneva, Switzerland and founded in 1968", "October 28, 2007", "Jaydev Shah", "domestication of the wild mouflon in ancient Mesopotamia", "Prabhu Deva", "in a thousand years", "a state or other organizational body that controls the factors of production", "90 \u00b0 N 0 \u00b0 W", "in Ephesus in AD 95 -- 110", "2006", "sport utility vehicles", "Americans", "1997 squad voted atop the final AP Poll", "30 years after Return of the Jedi", "John Joseph Patrick Ryan", "an American musical group founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O' Neal and Dougy Williams", "the Rashidun Caliphs", "off the rez", "Woody Paige", "Ren\u00e9 Descartes", "2007 via Valve's Steam content distribution platform", "Asuka", "A Turtle's Tale : Sammy's Adventures and the TV show Suburgatory", "three", "Steve Biko", "Rudolph", "xiangqi", "Anne Fletcher", "October 21, 2016", "Centers for Medicare and Medicaid Services (CMS)", "21", "Robert Barnett", "Oaxaca, Mexico", "Algeria", "birth inside creates a vacuum effect, so the egg is sucked in by the differential in this between the inside & outside of the bottle", "ppa"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5894409551695867}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false], "QA-F1": [0.09090909090909091, 0.0, 1.0, 0.0, 1.0, 0.0625, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.5, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.28571428571428575, 0.32786885245901637, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.8125000000000001, 1.0, 0.9859154929577464, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.6666666666666666, 0.0, 1.0, 0.4, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3494", "mrqa_squad-validation-4566", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7412", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-853", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-2748", "mrqa_hotpotqa-validation-1803", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-3785", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-4046"], "SR": 0.453125, "CSR": 0.56640625, "EFR": 0.8857142857142857, "Overall": 0.6981584821428571}, {"timecode": 32, "before_eval_results": {"predictions": ["September 1565", "the Anglo-Saxon populations who migrated to and conquered much of England after the end of Roman Imperial rule", "Pin the Tail on the Donkey", "the martini", "the Wiener Sangerknaben", "cinnamon", "a big bang", "Halloween", "R.E.M.", "Gail Sayers", "French Presidential Power and the Stability of the French Fifth Republic", "Russia", "Thomas Jefferson", "the Yellow River", "mandeopary Questions page 1236 - SHAKESPEARE - TriviaBistro.com", "Angelina Jolie", "Sharon Epatha Merkerson", "air pressure", "Alec Douglas-Home", "bony frill around the back of its head", "Sharks", "NOBEL", "AUGUSTA", "mandeverous lay", "the orangutan", "anaphylactic shock", "camels", "gangrene", "``ex- comes from Latin, where it has the meaning \"out of\" or \"outside\"", "Bonnie and Clyde", "John Harvard", "the Germanic branch", "``Dante Daniel \" Danny\" Bonaduce", "``Shmoop", "Guatemala", "`` Something you should not say after I love you", "Raul Castro", "Jos Joaqun de Olmedo", "Albert Einstein", "school", "Barnsdall", "Little Women", "mandlin", "``I love you\"", "Providence", "Tasmanian devil John Quincy", "Mother Vineyard", "South Africa", "Swan Lake", "dry ice", "`` 1947 Birthday - Avril \" Kim\" Campbell", "tooth", "Gibraltar", "1999", "9 February 2018", "Argentina", "Sarah Sawyer", "RAC plc", "Russian Empire", "1967", "44,300", "228", "Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5408463064713065}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 0.4, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_squad-validation-5276", "mrqa_searchqa-validation-10043", "mrqa_searchqa-validation-5405", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-5348", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-12676", "mrqa_searchqa-validation-15919", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-9970", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-4645", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-2559", "mrqa_searchqa-validation-3246", "mrqa_searchqa-validation-391", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-554", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-5763", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-2395"], "SR": 0.453125, "CSR": 0.5629734848484849, "EFR": 1.0, "Overall": 0.720329071969697}, {"timecode": 33, "before_eval_results": {"predictions": ["11", "R\u00fcdesheim", "Dirty Diana", "Tennessee Williams", "Ring", "the dough", "(John) Henry", "Zombies", "Colombia", "belle", "Friday Night Lights", "Halloween", "the Mummy", "a port-wine stain", "the Empire State Building", "Pinta", "Czechoslovakia", "Ferris B Mueller", "Mike Judge", "Unforgiven", "Court TV", "the galaxies", "Germany", "Gunsmoke", "astronomer", "Candy", "AT&T", "asthma", "Microsoft", "blue", "Puerto Rico", "18", "a flying saucer", "Shakespeare", "a liter", "Edward", "The Silence of the Lambs", "Prymatt Conehead", "stuffing", "a fraction", "carbonite", "Spain", "the phi phenomenon", "the obelisk", "Sam Kinison", "Katharine Hepburn", "(Harry S. Truman)", "Kublai Khan", "the South Ossetia", "New York City", "a bow", "Newfoundland", "538", "a belover", "Ross Elliott", "a googol", "Laos", "Wigan", "1995", "Champion Jockey", "Donald J. Trump", "Bush administration", "200", "8 p.m."], "metric_results": {"EM": 0.640625, "QA-F1": 0.6901041666666666}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9098", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-15910", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-6554", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-9672", "mrqa_searchqa-validation-2404", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-227", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-5234", "mrqa_naturalquestions-validation-6326", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3445", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-591"], "SR": 0.640625, "CSR": 0.5652573529411764, "EFR": 1.0, "Overall": 0.7207858455882353}, {"timecode": 34, "before_eval_results": {"predictions": ["Hisao Yamada", "Percy Shelley", "3D computer-animated comedy", "aluminum foil", "Montreal, Quebec, Canada", "Lego", "Daniil Borisovich Shafran", "Doc Hollywood", "Richard L. Thompson", "Virgin label", "John Christopher Lujack Jr.", "26 June 2013", "Freddie Jackson", "Michael Swango", "Roman Polanski", "322,520", "1754", "Cate Blanchett", "Westfield Marion", "Montana State University", "1961", "Eisenhower Executive Office Building", "137th", "Mohsin Fani", "High Falls Brewery", "2016", "1998", "Sean", "2015", "Mel Blanc", "Corendon Airlines", "Tamil", "number five", "Champion Jockey", "University of the District of Columbia", "Nancy Dow", "Larry Eustachy", "Anne Perry", "March 17, 2015", "Julie Taymor", "Mika H\u00e4kkinen", "nine", "a bass", "Buck Owens and the Buckaroos", "January 1788", "Lord Chancellor of England", "MGM Resorts International", "Cleveland, Ohio", "rap parts from Darryl, RB Djan and Ryan Babel", "Mark Anthony \"Baz\" Luhrmann", "Syracuse University", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "1969", "Georgia Groome", "the referee", "Botticelli", "the American circus", "John Keats", "a French team", "it has not", "Arsene Wenger", "the basic building blocks of matter", "Bellerophon", "Monica Lewinsky"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7581845238095238}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5530", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-421", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2989", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-2977", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-7800"], "SR": 0.65625, "CSR": 0.5678571428571428, "EFR": 1.0, "Overall": 0.7213058035714286}, {"timecode": 35, "before_eval_results": {"predictions": ["2014", "highly diversified", "Walter Pauk", "2018", "noble gas", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "Peter Hansen", "Ishaan Anirudh Sinha", "Charlotte Hornets", "one person", "Orographic lift", "1945", "On 6 March 1983", "An alternative is to cool all the atmosphere by spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "Around 1200", "2015", "1854", "Mount Mannen in Norway", "1937", "Kennedy Space Center ( KSC ) in Florida", "Rocky Dzidzornu -- congas", "employment in which a person works a minimum number of hours defined as such by his / her employer", "Monastic orders, especially the Cistercians and the Carthusians", "since the early 20th century", "Authority", "warning signs", "Magnetically soft ( low coercivity ) iron", "southern Anatolia", "1992", "ulcerative colitis", "April 20, 1983", "Gooducken", "the brain and spinal cord", "abdicated in November 1918", "Massachusetts", "Hans Zimmer", "c. 1000 AD", "from Times Square in New York City west to Lincoln Park in San Francisco", "Jerry Leiber", "two", "49 cents", "1969", "Central Germany", "1978", "Javier Fern\u00e1ndez", "by observing the magnetic stripe `` anomalies '' on the ocean floor", "art", "the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council ( UNSC )", "peninsular mainland", "by revolutionaries named their newly independent country La Rep\u00fablica Dominicana", "in season two", "born November 28, 1973", "The Battle of the Somme", "Frederick William III", "Majorca", "National Football League", "Arlo Looking Cloud", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Pervez Musharraf", "spend billions to improve America's education, infrastructure, energy and health care systems.", "Wigan Athletic", "Ireland", "Asteroid impact avoidance", "Colorado"], "metric_results": {"EM": 0.375, "QA-F1": 0.5639727688247426}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.5, 0.4, 0.1111111111111111, 0.0, 0.5, 0.6666666666666666, 1.0, 0.5, 1.0, 0.8, 0.09090909090909091, 0.0, 0.8571428571428571, 1.0, 0.3076923076923077, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.6, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, 0.13333333333333333, 0.6666666666666666, 0.0, 0.8, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-9165", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-1985", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-6276", "mrqa_hotpotqa-validation-3613", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1970", "mrqa_searchqa-validation-13764"], "SR": 0.375, "CSR": 0.5625, "EFR": 1.0, "Overall": 0.720234375}, {"timecode": 36, "before_eval_results": {"predictions": ["crust and lithosphere", "Kanun\u00ee Sultan S\u00fcleyman", "Skatoony", "number 1", "Satchmo, Satch or Pops", "San Antonio", "Polish", "Danish", "Milwaukee Bucks", "1908", "glee", "1965", "over 100 million", "Oneida Limited", "Wilmington, North Carolina, United States", "Pieter van Musschenbroek", "Southbank in Victoria", "London", "Australian", "Rochdale", "Bardot", "Mario Lemieux", "To SquarePants or Not to Squarepants", "The Sun", "2000 Summer Olympics", "Ferdinand Magellan", "King of France", "1694", "Liam Cunningham", "Nanna Popham Britton", "Minette Walters", "leopard", "Moselle", "Anne and Georges", "Bank of China Tower", "Nobel laureate in Literature", "Cheshire County", "Robert Gibson", "1770", "1974", "Great Northern Railway transcontinental railway line", "Woody Woodpecker", "2", "Edward Albert Heimberger", "IFFHS World's Best Goalkeeper", "three", "1989 until 1994", "Pittsburgh Steelers", "9 venues", "1993 to 2001", "Double Crossed", "economic recession", "needle - like teeth", "Bacon", "human rights lawyer", "mathematically obsolete", "1812", "energy propels the boat that travels between 5 and 10 knots an hour.", "anti-M Mafia judges Giovanni Falcone and Paolo Borsellino", "Kingdom City", "Queen Wilhelmina", "greece", "Santa Fe", "1992"], "metric_results": {"EM": 0.59375, "QA-F1": 0.719920183982684}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [0.25, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4952", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-2883", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3417", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-2319", "mrqa_naturalquestions-validation-5119", "mrqa_triviaqa-validation-994", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-743", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-6519"], "SR": 0.59375, "CSR": 0.5633445945945945, "EFR": 1.0, "Overall": 0.7204032939189189}, {"timecode": 37, "before_eval_results": {"predictions": ["lasting damage", "Favre", "Bob Turner", "Wool Sack dress", "Billy the Kid", "Oliver Twist", "Hans Christian Andersen", "Topaz", "xataina Volcanic Centre Geology / volcano Geology", "Cameroon Pidgin English", "x", "Destiny's Child", "Bishop of Rome", "California", "Danny Ocean", "valkyries", "Little Women & Good Wives: 1868's First Novel With Feminist Ideals", "Ich bin ein Berliner", "ximperial", "Leyte Gulf", "difference", "Gogol", "Malcolm X", "Purple violet", "vonetics", "Michigan", "Sigmund Freud", "red meat", "T. S. Eliot", "Dumpling", "Trinity Church Cemetery", "New Zealand", "rum", "theology", "Alexander Nevsky", "Stephen Decatur", "the Greek god", "Paraguay", "R2-D2", "6 to 8 glasses", "tense", "Vassar College", "forensic medicine", "National Air and Space Museum", "Vespa", "Warren G. Harding", "Emma Peel", "Tennessee", "Richard I of England", "Will Rogers", "Bee Gees", "Honor\u00e9 Mirabeau", "in 2018", "Hanna Alstr\u00f6m", "NUPE", "website", "Scotland", "1898", "1887", "Bonkyll Castle", "more and more suspicious of the way their business books were being handled.", "Top Gun", "Dame Elizabeth,", "1875"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5743055555555556}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-7080", "mrqa_searchqa-validation-3714", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-8720", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9251", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-10345", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-13049", "mrqa_searchqa-validation-8295", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-10821", "mrqa_searchqa-validation-2859", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-8890", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-7359", "mrqa_triviaqa-validation-6174", "mrqa_hotpotqa-validation-1884", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-2552", "mrqa_hotpotqa-validation-4203"], "SR": 0.46875, "CSR": 0.5608552631578947, "EFR": 1.0, "Overall": 0.7199054276315789}, {"timecode": 38, "before_eval_results": {"predictions": ["the butcher Market", "Mardi Gras", "grommet", "Soundgarden", "a pew", "Russia", "the Penguin", "water", "Canada", "sopra", "pole vault", "California", "Jordan", "a hickey", "the Battle of Waterloo", "Ukraine", "goombah", "Nuku'alofa", "David Geffen", "Exxon", "Joan of Arc", "John Tyler", "George Walker Bush", "La-Z-Boy", "water", "a subgenus", "Narnia", "East Germany", "Ginger Rogers", "Judges", "Dracula", "Marlee Matlin", "frogs", "Qatar", "debts", "Lady Jane Grey", "yellow fever", "Westin", "Guatemala", "Harold Edward \"Red\" Grange", "Simon", "indirect", "couscous", "1917", "Colonel (Tom) Parker", "the lilac", "American Pie", "a diamond", "a whale", "Ohio State", "Sweet Home Alabama", "American country music singer George Strait", "Toledo", "1960", "the Nutcracker", "Dodoma", "Red Lion", "martial arts action", "Black Swan", "the Sun", "girls around 11 or 12.", "Vernon Forrest", "President Barack Obama", "Idaho"], "metric_results": {"EM": 0.625, "QA-F1": 0.6826636904761905}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-6090", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-14171", "mrqa_searchqa-validation-12204", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-5033", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-3197", "mrqa_naturalquestions-validation-3087", "mrqa_triviaqa-validation-6420", "mrqa_hotpotqa-validation-770", "mrqa_hotpotqa-validation-1192", "mrqa_triviaqa-validation-700"], "SR": 0.625, "CSR": 0.5625, "EFR": 1.0, "Overall": 0.720234375}, {"timecode": 39, "before_eval_results": {"predictions": ["substantially increased the asking price", "sexual bodies at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "gratitude", "Lucky Dube", "grocery store", "fallen comrades lost in the heat of battle", "Samuel Herr, 26, and Juri Kibuishi,", "around 1918 or 1919", "participate in Iraq's government", "political dead-end", "Honduras", "Neptune Pool at Hearst Castle", "FBI", "201-262-2800", "stop selling unapproved pain-relief drugs.", "directly involved in an Internet broadband deal with a Chinese firm", "Iraq", "40 militants and six Pakistan soldiers", "1973", "Argentina", "Washington", "water", "Laura Ling and Euna Lee", "January 24, 2006", "\"bystander effect\"", "Haleigh Cummings", "The Casalesi Camorra clan", "nine", "cars making an annual road trip", "two", "two", "bankruptcy", "Kurt Cobain", "war crimes", "Hartsfield-Jackson International Airport", "trading goods and services without exchanging money", "\"Dance Your Ass Off.\"", "1994", "Larry Zeiger", "the United States", "upper respiratory infection", "\"He's crying like a baby,\"", "\"Stuntman\"", "African National Congress", "abuse", "The man", "two", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip", "At least 14", "11", "Royal Navy servicemen", "Clarence Darrow", "water - soluble", "in the attempt to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "1768", "chariots", "writer", "\" rated R\"", "1955", "\"I'm Shipping Up to Boston\"", "a lock", "(St.) Whistler", "\"Greece\"", "email"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4880248397435898}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.24, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.07692307692307691, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2946", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-1259", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-4211", "mrqa_triviaqa-validation-7637", "mrqa_hotpotqa-validation-1585", "mrqa_searchqa-validation-16932", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-11411", "mrqa_triviaqa-validation-90"], "SR": 0.4375, "CSR": 0.559375, "EFR": 1.0, "Overall": 0.719609375}, {"timecode": 40, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1348", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2749", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3171", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-543", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6680", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-2423", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-8890", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-9163", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9967", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-138", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3341", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3787", "mrqa_squad-validation-3813", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4107", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5222", "mrqa_squad-validation-5237", "mrqa_squad-validation-525", "mrqa_squad-validation-5269", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-531", "mrqa_squad-validation-5319", "mrqa_squad-validation-5483", "mrqa_squad-validation-5501", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5741", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-689", "mrqa_squad-validation-7001", "mrqa_squad-validation-7002", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-741", "mrqa_squad-validation-7458", "mrqa_squad-validation-7470", "mrqa_squad-validation-7525", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7736", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9744", "mrqa_squad-validation-9845", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9969", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830"], "OKR": 0.859375, "KG": 0.4828125, "before_eval_results": {"predictions": ["pressure the lazy, inspire the bored, deflate the cocky, encourage the timid, detect and correct individual flaws, and broaden the viewpoint of all", "Gov. Jan Brewer.", "Casey Anthony,", "Tehran,", "Britain's", "such joint exercises between nations are not unusual. \"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "were sentenced in 2005 to 75 and 45 years in prison, respectively, after being convicted of conspiring to provide material support and resources to foreign terrorist organizations.", "Gordon Brown", "regulators in the agency's Colorado office", "A Biography", "Vivek Wadhwa,", "\"stand tall, stand firm.\"", "former Mobile County Circuit Judge Herman Thomas", "eight-day", "a long-range missile", "Iggy and the Stooges created the blueprint for punk rock and made an album that would one day be regarded as a landmark in rock music.", "committed to equality,", "Harry Nicolaides,", "in central Cairo,", "we're worried that we might find ourselves in five or 10 years saying we've made a big mistake.", "opium poppies", "animal products.", "firefighter", "the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "at the age of 23", "secretary of defense", "The Real Housewives of Atlanta", "dancing", "September,", "eric blair, who later miscarried their unborn child.", "Waterloo Bridge", "two", "ties", "a missile", "Nicole", "was arrested earlier in the afternoon after a traffic stop south of the city, police said.", "Drew Kesse,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the execution.", "14", "Graham", "vitamin injections that promise to improve health and beauty.", "delivers a big speech", "Ennis, County Clare", "$17,000", "Swedish Prime Minister Fredrik Reinfeldt", "on Anjuna beach in Goa", "the patient,", "South African", "died after shooting himself three times in the head with a.40-caliber pistol,", "a rally", "J. Presper Eckert", "Latitude", "excessive growth", "Cecilia", "One Thousand and One", "King George I", "2011", "Violet", "Kinnairdy Castle", "the hippopotamus", "boll weevil", "Solzhenitsyn", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5617374401913875}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true], "QA-F1": [0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 0.65, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6, 1.0, 0.6666666666666666, 0.0, 0.25, 1.0, 0.8, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.24242424242424246, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.20000000000000004, 0.33333333333333337, 0.0, 0.5, 0.7368421052631579, 0.4, 0.0, 0.0, 0.22222222222222224, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1919", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-4059", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-705", "mrqa_searchqa-validation-16464"], "SR": 0.40625, "CSR": 0.555640243902439, "EFR": 0.9736842105263158, "Overall": 0.720786765885751}, {"timecode": 41, "before_eval_results": {"predictions": ["Jim Nantz and Phil Simms", "Texas", "Detroit", "birds", "george iv d'espionnage", "Taps", "\"The Magic of David Copperfield XVI: Unexplained Forces\"", "Jul 27, 2013", "a cat", "Atlanta", "on a brief comment made by Carnap in the... metaphysics are without meaning because they are not deducible", "the new baseball movie starring Kevin", "Coors Field", "Boise State", "Doc Holliday", "Chicken Run", "Hercules", "\"Rama, #1)", "hydrogen", "george du Maurier", "2011", "Mammoth Cave National Park", "a sousaphone", "earthquakes", "Poseidon", "Queen Elizabeth II", "\"The 39 Steps\"", "kynikos", "Judges", "oreos", "St. Lawrence", "sea salt", "Indiana Jones", "America", "Bill Clinton", "Cloverfield", "Paraguay", "zenda", "the East Sea", "to rest or relax, or to rest on something for support", "Resentment", "adverbs", "fischertechnik", "Tuesday", "Olivia Newton-John", "Robert", "oil", "South Africa", "Jim's", "Arnold J. Toynbee", "Heathers", "2004", "Rachel Sarah Bilson", "Pradyumna", "Accrington Stanley\u2019s biggest league win was 8-0 in 1934", "Franklin D. Roosevelt", "blood", "heavier than a feather", "Indooroopilly Shopping Centre", "freshman", "opium", "Adam Yahiye Gadahn,", "on an island stronghold of the Islamic militant group Abu Sayyaf,", "Elmo Lincoln"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5439980158730159}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [0.20000000000000004, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-597", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-8412", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-4782", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-2758", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4268", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-12223", "mrqa_searchqa-validation-2756", "mrqa_naturalquestions-validation-3124", "mrqa_triviaqa-validation-6163", "mrqa_triviaqa-validation-5356", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-294", "mrqa_newsqa-validation-3404", "mrqa_triviaqa-validation-2080"], "SR": 0.453125, "CSR": 0.5531994047619048, "EFR": 1.0, "Overall": 0.7255617559523809}, {"timecode": 42, "before_eval_results": {"predictions": ["Divine Right of Kings", "Mussolini", "Madonna", "tennis", "Tarsus", "Charles Chaplin", "Jayne Torvill and Christopher Dean", "Hermes", "Vietnam", "Foil", "Agatha Christie", "Cold Blood", "laugh", "Jackie Joyner", "the Bowhead whale", "Nelson Mandela", "the Perseid", "Cuba Libre", "Thomas Jefferson", "Tanzania", "Oscar Wilde", "Mexico City", "Delaware", "Borneo", "mckinley", "Walla Walla", "Netflix", "Roger Bannister", "Le Corbusier", "(Scott) Peterson", "an enigma", "Franco", "Bolivia", "rugby", "Ireland", "Vanna White", "Catherine II", "yellow", "the ignition coil", "ROE", "Elizabeth Cady Stanton", "the Wetterstein Mountains", "Francis Ford Coppola", "wives and concubines", "meander", "The Wind in the Willows", "Jack Dempsey", "hexadecimal", "The Two Gentlemen of Verona", "the chimpanzee", "the Red Cross", "pigs", "August 2012", "Jack Nicklaus", "the Central African Republic", "six", "Yalta", "Emad Hashim", "Lexy Gold", "The Daily Stormer", "Bayern Munich", "the United States", "cancer", "the Boston Fern"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6463541666666667}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-16519", "mrqa_searchqa-validation-11329", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-2589", "mrqa_searchqa-validation-16809", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-6757", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-10935", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-12644", "mrqa_searchqa-validation-12772", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-7323", "mrqa_searchqa-validation-14283", "mrqa_naturalquestions-validation-7702", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2135", "mrqa_hotpotqa-validation-5307", "mrqa_newsqa-validation-3131", "mrqa_triviaqa-validation-6337"], "SR": 0.546875, "CSR": 0.5530523255813953, "EFR": 0.9655172413793104, "Overall": 0.7186357883921412}, {"timecode": 43, "before_eval_results": {"predictions": ["the third component", "Henry III", "Tomorrow Never Dies", "Liechtenstein", "on the last lap", "Jonathan", "Columbus", "Brett Favre", "South African", "Brian Deane", "(Fr) Pinter", "Argentina", "William Conrad", "1875", "Lloyd Webber", "Iran", "Fairey", "the Isle of Arran", "London", "Playboy", "a barbeor", "Matalan", "the Chancery", "boise", "a griffin", "white", "the Pussycats", "Jersey City", "Leon Blair", "a bird", "Karl Marx and Friedrich Engels", "Utrecht", "Union of Post Office Workers", "Strangeways", "Carousel", "14", "Richard Wagner", "the brain and the spinal cord", "Adrian Cronauer", "(Frederick) William Herschel", "Belgium", "October 31st", "beetles", "St Volvo Dan", "Pompey", "Denali", "auction houses", "haddock", "L. P. Hartley", "Italy", "a snake", "Andy Serkis", "2001", "the human hands", "the U.S. military's second highest", "Shenae Grimes", "five-time", "prostate cancer,", "the U.S. Holocaust Memorial Museum,", "on its final scheduled voyage this week.", "Dale", "a game show", "London", "Uru-Salim"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10394", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-6459", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-4052", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-2462", "mrqa_hotpotqa-validation-3137", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2244", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-9414"], "SR": 0.46875, "CSR": 0.5511363636363636, "EFR": 0.9705882352941176, "Overall": 0.7192667947860962}, {"timecode": 44, "before_eval_results": {"predictions": ["co-chair", "a cappella group", "iceland", "Evita", "Victoria", "Sikhism", "william wilson", "Sinclair Lewis", "Argentina", "glaze", "Guatemala", "olive", "Munich", "violin", "glazing", "Paul Nash", "a marsupials", "first among equals", "a robin", "Indira Priyadarshini Gandhi", "Colombia", "jean Baptiste Say", "Uranus", "Prince Igor", "h2g2", "a full-contact 7 a side game", "an electrical component", "The Wicker Man", "nathaniel Poe", "nizhny", "South Africa", "hovercraft", "john mcenroe", "red", "john Mellencamp", "Tina Turner", "gloucester", "brash", "bees", "harold wilson", "harold wilson", "john john wilson", "Roger Ebert", "hair", "Wolfgang Amadeus Mozart", "bubba", "monaco", "Richard Lester", "December", "peregrines", "steel", "1 October 2006", "Billboard magazine", "Hitler's military forces", "Marcus T. Reynolds", "35,000", "Tel Aviv University", "response to a civil disturbance call,", "the BBC building in central London", "\"Hillbilly Handfishin'\"", "diogenes", "Roosevelt, Churchill", "halfbacks", "an integral membrane protein that builds up a proton gradient across a biological membrane"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6031622023809524}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.2857142857142857, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.7499999999999999, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8581", "mrqa_triviaqa-validation-5869", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-7543", "mrqa_triviaqa-validation-57", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-954", "mrqa_hotpotqa-validation-773", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-220", "mrqa_searchqa-validation-13219", "mrqa_searchqa-validation-9558"], "SR": 0.5625, "CSR": 0.5513888888888889, "EFR": 0.9642857142857143, "Overall": 0.7180567956349206}, {"timecode": 45, "before_eval_results": {"predictions": ["The Apollo spacecraft", "1853", "Daniel A. Dailey", "adrenal medulla", "anakin Skywalker", "Plank", "Ann Gillespie", "the London Virginia Company", "a loanword of the Visigothic word guma `` man ''", "March 26, 1973", "drizzle, rain", "Tommy Shaw", "1757", "Charles Perrault", "John Daly", "1997", "Elizabeth Dean Lail", "March 31, 2017", "Jonathan Breck", "Aristotle", "2007", "plate tectonics", "eight", "more than a million", "the Church of the East", "1995", "Rock Island, Illinois", "1926", "2014", "the Southern Ocean", "on the pelvic floor", "Donny Osmond", "the British group Ace", "from the Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s, meaning `` herdsman '' or `` plowman ''", "Bobby Weinstein, and Lou Stallman", "the liver", "Christopher Jones", "to avoid the inconvenienceiences of a pure barter system", "current day Poole Harbour towards mid-Channel", "Bill Patriots", "balance sheet", "London, United Kingdom", "Hellenism", "232", "January 2, 1971", "Andy Cole", "\u20b9 39.50 lakh", "natural killer cells", "a crust of potatoes", "team", "$72", "Amy Johnson", "12", "buk'wus", "Westgate Las Vegas Resort & Casino", "Gloria Trevi", "postal delivery", "was killed in an attempted car-jacking as he dropped his children off at a relative's house,", "an army major assigned to a guard unit protecting Mexican President Felipe Calderon.", "\"Slumdog Millionaire\"", "Clifford Odets", "Margaret Mitchell", "Microsoft", "Sherlock Holmes"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6590613371913324}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.8, 0.0, 0.5, 0.6956521739130436, 0.0, 0.0, 0.5, 0.7058823529411764, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-8596", "mrqa_triviaqa-validation-2110", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-3832", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-947"], "SR": 0.53125, "CSR": 0.5509510869565217, "EFR": 0.9, "Overall": 0.7051120923913043}, {"timecode": 46, "before_eval_results": {"predictions": ["Gamal Abdul Nasser", "2016", "a line of committed and effective Sultans", "Germany", "United Nations Peacekeeping Operations", "Steve Hale", "King Saud University", "Parashara", "John Dalton", "Vienna", "pepsinogen", "Carol Worthington", "ancient Rome", "1928", "April 29, 2009", "19 June 2018", "Todd Bridges", "Ben Savage", "the senior-most judge of the supreme court", "multinational retail corporation", "nachos", "Andrea Brooks", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "Max Martin", "July 2010", "232", "forested parts of the world", "3.5 mya", "1998", "The Union", "cytokinesis", "an Aldabra giant tortoise", "Texhoma", "solemniser", "Warhol", "The Royalettes", "centigrade", "Vienna", "Andrew Johnson", "microfilament", "four distinct levels of protein structure", "if the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "the American League ( AL ) champion Cleveland Indians", "David Kaye", "his band Kid Creole and the Coconuts", "Stephen Graham", "(Bill Mumy)", "the plane crash", "Tatsumi", "Ernest Hemingway", "14 November 2001", "Venus", "Laos", "nastase", "Bennett Cerf", "Scotty Grainger Jr.", "uncle", "Steve Jobs", "motion for a preliminary injunction against a Mississippi school district and high school in federal court Tuesday", "jailed, with bail set at $500,000 each.", "the Poverty line", "uterus", "panting", "(John) Y. Brown Jr."], "metric_results": {"EM": 0.5, "QA-F1": 0.6044887404262405}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.5454545454545454, 1.0, 0.0, 0.8, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.846153846153846, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-7027", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-3771", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-5936", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-959", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-1475", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-14439", "mrqa_searchqa-validation-15864"], "SR": 0.5, "CSR": 0.5498670212765957, "EFR": 1.0, "Overall": 0.7248952792553192}, {"timecode": 47, "before_eval_results": {"predictions": ["2014", "Ren\u00e9 Descartes", "Sauron", "Dante Pastula", "Havana Harbor during the Cuban revolt against Spain", "sedimentary rock", "April 10, 2018", "Ted Mosby", "the North Atlantic Ocean", "a flood defense system", "111", "around 2 %", "an Irish feminine name", "the head of the Imperial Family and the traditional head of state of Japan", "November 17, 2017", "an instant messaging client that was first developed and popularized by the Israeli company Mirabilis in 1996", "appellate court", "her abusive husband and meets Alex", "84", "William Chatterton Dix", "Killer Within", "Broken Hill and Sydney", "in the axial skeleton ( 28 in the skull and 52 in the torso ) and 126 bones in the appendicular skeleton ( 32 \u00d7 2 in the upper extremities including both arms", "a database maintained by the United States federal government, listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "a couple broken apart by the Iraq War", "Gutenberg", "National Industrial Recovery Act ( NIRA )", "Pittsburgh", "the ulnar nerve", "the Nationalists", "31 October 1972", "twelve Wimpy Kid books", "Matt Flinders", "The person who has existence in two parallel worlds", "SIP ( Session Initiation Protocol )", "Ra\u00fal Eduardo Esparza", "The post translational modification of proinsulin to mature insulin only occurs in the beta cells of the islets of Langerhans", "4 September 1936", "drivers who meet more exclusive criteria", "H.L. Hunley", "Orangeville, Ontario", "the Bactrian camels", "the Gentiles", "James Hutton", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "the late 1970s", "2005", "March 18, 2005", "Fats Waller", "in the fovea centralis", "the Russian army", "into an afterlife and into one of the Vikings nine realms", "Rudyard Kipling", "THE PengUIN", "Otto Eduard Leopold, Prince of Bismarck, Duke of Lauenburg", "Ukrainian", "237", "Apple employees", "\"We connected meaningfully about the important issues that have emerged over recent days, and I offered him my sincere apologies for any offense to our veterans caused by this report.", "in July", "Margaret Mitchell", "a cross with the Risen Christ licit", "Lisa Lisa Lisa & Cult Jam", "right-hand batsman"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6607226804595225}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.14814814814814814, 0.8108108108108109, 0.0, 0.6666666666666666, 0.888888888888889, 0.25, 1.0, 1.0, 1.0, 0.38095238095238093, 1.0, 1.0, 0.8571428571428571, 1.0, 0.3, 1.0, 0.3333333333333333, 0.4, 0.8, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.10256410256410257, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-8179", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-893", "mrqa_naturalquestions-validation-10321", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-6040", "mrqa_triviaqa-validation-3828", "mrqa_hotpotqa-validation-1056", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-272", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-16263", "mrqa_hotpotqa-validation-181"], "SR": 0.484375, "CSR": 0.5485026041666667, "EFR": 0.9696969696969697, "Overall": 0.7185617897727273}, {"timecode": 48, "before_eval_results": {"predictions": ["Magdalen College", "Rana Daggubati", "13 people", "Veyyil", "1", "Kristy Lee Cook", "LA Galaxy", "The Volvo 850", "February 14, 1859", "\"The Simpsons\"' thirteenth season", "Biola University", "Academy Award for Best Art Direction", "2012", "October 13, 1980", "tomato", "Gracie Mansion", "Arsenal Football Club", "The Major of St. Lo", "Steve Prohm", "a super-regional shopping mall owned by Simon Property Group,", "Lady Charlotte Elliot", "2000 World Rally Championship", "seven players", "28 June 1945", "University of California", "Miami Gardens, Florida", "Indian", "Donna Paige Helmintoller", "Graham Hill", "The Emperor of Japan", "Hillary Clinton", "1896", "the American record for the most time in space (381.6 days)", "the D\u00e2mbovi\u021ba River", "Philip Quast", "Pierce County", "PPG Paints Arena", "May 10, 1976", "Devon Bostick", "Operation Julin", "BAFTA TV Award Best Actor", "the Slavic women accompanying their husbands in the First Balkan War", "1641", "Marty Ingels", "Carl David Tolm\u00e9 Runge", "1941", "June 2, 2008", "Charice", "Sleepy Brown", "Waimea Bay", "Nikhil Banerjee", "the bottom of the brain immediately below the hypothalamus", "his last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "restoring someone's faith in love and family relationships", "Eddie", "gold anniversary", "auk", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of disaster assistance for parts of the Midwest", "the Spieker family", "The Sopranos", "Winslow Homer", "Elizabeth II", "the Crimean War", "Alexey Pajitnov"], "metric_results": {"EM": 0.625, "QA-F1": 0.7203013591800356}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5454545454545454, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.823529411764706, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-4551", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-1757", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5373", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5535", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1997", "mrqa_searchqa-validation-14284"], "SR": 0.625, "CSR": 0.550063775510204, "EFR": 0.9583333333333334, "Overall": 0.7166012967687074}, {"timecode": 49, "before_eval_results": {"predictions": ["Blind Faith", "Dubai", "Silverstone", "Ted", "Triumph and Disaster", "1720", "King Henry V", "\"The Ram\" Robinson", "beetle", "Bot", "100 years", "Rudyard Kipling", "dragonflies", "Cole Porter", "liriope", "Britain", "Big Brother", "nippori", "plaujolais", "Timaru", "Paul Dukas", "Tom Watson", "9", "the dog's middle ear", "Tokyo", "low-E", "keeper", "God bless America", "Dangerous Minds", "psychology", "Apollo", "Ken Platt", "North Korea", "Boxing Day", "Decanter", "fish", "wainscoting", "plaphylococcus", "Scarborough", "Alan Turing", "Isaac Newton", "Calcium", "Bombay", "Little Eleanor", "Paul Revere", "karpathos", "bowls", "Hitachi", "plutarch", "Belgrade", "gizzard", "Tony La Russa", "counter clockwise", "3.5 million years old", "Tampa Bay Lightning", "Theatre Ventures, Inc.", "Battle of Dresden", "U.S. Agency for International Development", "$150 billion", "root out terrorists within its borders.", "St. Peter's", "alligator", "Jerry Rice", "Johnny Weissmuller"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5677083333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-3276", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-2114", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7216", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6646", "mrqa_triviaqa-validation-293", "mrqa_triviaqa-validation-7667", "mrqa_naturalquestions-validation-2621", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-93", "mrqa_searchqa-validation-7000", "mrqa_searchqa-validation-2086"], "SR": 0.53125, "CSR": 0.5496875, "EFR": 0.9666666666666667, "Overall": 0.7181927083333334}, {"timecode": 50, "UKR": 0.65625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2584", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5405", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3559", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14232", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7260", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7002", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96"], "OKR": 0.822265625, "KG": 0.4578125, "before_eval_results": {"predictions": ["medusa", "Hawaii", "Easy Rider", "scrabble", "Percy Bysshe Shelley", "Billy Joel", "pardon", "south of Fulton Street", "dawn at Tiffany's", "Roman Polanski", "Dogberry", "Battle Creek", "the Red Sea", "Mary Todd Lincoln", "Mary Poppins", "Russia", "phonetics", "niger", "Julianne Moore", "saddle bags", "Holly Golightly", "quilting", "anemoi", "butter", "the Tagus", "the Social Democratic", "acting", "USS nautilus", "giblet drums", "Denmark", "student loans", "kidney pie", "aulos", "seattle", "Michael Jordan", "John Quincy Adams", "the Legion of Honour", "Louis XIII", "kimchi", "December 23, 1777", "chancellor of West Germany", "roosevelt", "Crimea", "almonds", "the White House", "a seashell", "Caesar", "della", "Dean Acheson", "Pittsburgh Steelers", "jury dutyserve with pride, said. New York Chief", "Malina Weissman", "Kyla Pratt", "Jonathan Goldstein", "9 imperial gallons", "centurion", "Spain", "Sean Yseult", "1754", "Robert Harper", "the capital of Pakistan's North West Frontier Province,", "home to a hero's welcome in his native Philippines", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "September 21, 2014"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5032828282828283}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.12121212121212123, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6167", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-11542", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-14873", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-11463", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-9320", "mrqa_searchqa-validation-4385", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-1251", "mrqa_searchqa-validation-9809", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-15599", "mrqa_searchqa-validation-9986", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-674", "mrqa_hotpotqa-validation-3612", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-551", "mrqa_hotpotqa-validation-4911"], "SR": 0.421875, "CSR": 0.5471813725490196, "EFR": 1.0, "Overall": 0.6967018995098039}, {"timecode": 51, "before_eval_results": {"predictions": ["Reddi-wip", "the Dachshund", "Saturn", "Dagny Taggart", "Risk", "a Bar Mitzvah", "cauliflower ear", "Clark Gable", "Katharine Hepburn", "Metacomet", "surrender", "Tarsus", "the Niagara Falls", "Hannibal Lecter", "The Man Without A Country", "the Arc de Triomphe", "George Frideric Handel", "the cologne", "Indonesia", "Toralv Maurstad", "Linus Pauling", "gold", "the Niagara Falls", "a cavallo", "water", "Ohio State", "Million Dollar Baby", "rum", "an organ", "Papua New Guinea", "Macy\\'s", "Bush", "the Arctic Ocean", "enamel", "Port-au-Prince", "the Barbary Coast", "humility", "Aleksandr Vladimirovich Popov", "rice", "gas masks", "\"to look like\"", "\"Juno\"", "the breast", "water", "Louis XIV of France", "a suspension bridge", "the faerie", "a trudge", "JetBlue", "Ryan Seacrest", "roller skates", "Lake Michigan", "Spanish colonies", "the home state of Texas", "piscinae", "the house sparrows", "Islamabad", "Indianapolis Motor Speedway", "$26 billion", "Deftones", "Adam Lambert and Kris Allen", "27-year-old's", "more than two years,", "James Long"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6067708333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-14737", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-13086", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-16862", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-10726", "mrqa_searchqa-validation-3994", "mrqa_searchqa-validation-10574", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-8155", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-2705", "mrqa_naturalquestions-validation-8099"], "SR": 0.5625, "CSR": 0.5474759615384616, "EFR": 1.0, "Overall": 0.6967608173076923}, {"timecode": 52, "before_eval_results": {"predictions": ["Alfred Preis", "\"Nip/Tuck\"", "Adelaide Lightning", "Homeland", "1983", "3.9 mi", "Hilux", "\"S&amp;M\"", "Snowball II", "Anna Clyne", "Flushed Away", "Elbow", "\"Mickey Mouse\" series", "Ellie Kemper", "Aamir Khan", "Eugene Levy", "25 million", "on the property of the University of Hawai\u02bbi", "Best Actress", "drummer Seb Rochford", "Samantha Spiro", "Martin O'Neill", "Shabab Al-Sahel", "Aamina Sheikh", "Total Nonstop Action Wrestling", "Walt Disney Feature Animation", "Nobel Prize in Physics", "operas", "Leinster", "Blue Grass Airport", "Tim Whelan", "Cleveland", "Ashley Jensen", "Franc Roddam", "Ben R. Guttery", "Pieter van Musschenbroek", "ABC", "Roseann O'Donnell", "\"media for the 65.8 million,\"", "1902", "the USS \"Enterprise\"", "Las Vegas", "Todd Emmanuel Fisher", "human blood, platelets, and plasma", "John M. Dowd", "November 6, 2018", "The MGM Grand Las Vegas", "Stanmore, New South Wales", "Clara Petacci", "from 1986 to 2013", "Bill Ponsford", "being one of Jesus'disciples", "Thomas Lennon", "Paris World's Fair", "Martin Van Buren", "Robert Boyle", "Vienna", "Harrison Ford", "Detroit", "27-year-old's", "the Squirrel", "Farsi (Persian)", "water", "Kitty Kelley"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5967013888888888}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-1584", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-1306", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2426", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-3286", "mrqa_hotpotqa-validation-2153", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-3391", "mrqa_triviaqa-validation-2082", "mrqa_newsqa-validation-2415", "mrqa_searchqa-validation-11381", "mrqa_searchqa-validation-4118"], "SR": 0.515625, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.696640625}, {"timecode": 53, "before_eval_results": {"predictions": ["London", "2018", "Welch, West Virginia", "in 525", "2009", "Mark Jackson", "Brazil", "December 24, 1836", "20 November 1989", "BC Jean", "Toronto", "the southernmost tip of the South American mainland", "Roger Dean Stadium", "Jesse Wesley Williams", "19 June 2018", "Prince William", "Sanchez Navarro", "Nick Kroll", "pigs", "Los Angeles", "2018", "on the table", "headdresses", "a Norwegian town", "1960", "1840s", "heavy tank", "semi-automatic", "Humpty Dumpty and Kitty Softpaws", "displacement", "methane", "200 to 500 mg up to 7ml", "Mel Gibson", "December 19, 2016", "vasoconstriction of most blood vessels", "Firoz Shah Tughlaq", "M\u00e1ximo Gomez and Antonio Maceo", "in muscles", "Robin", "March 26, 1973", "New England Patriots", "New York City", "S - shaped", "31 - member Senate and a 150 - member House of Representatives", "Ajay Tyagi", "from the heraldic crest carved in the lintel on St. Ignatius'family home in Azpeitia, Spain", "in a brownstone in Brooklyn Heights", "book and architecture", "19 June 2018", "Efren Manalang Reyes", "in a quest for new surf spots and introduce locals to the sport", "Barcelona", "nickel", "Henri Paul", "January 4, 1976", "1921", "General Allenby", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "glaciers in the European Alps may melt as soon as 2050,", "22", "a triangle", "Timbaland", "Foo Fighters", "Jennifer Grey"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6316283236779561}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.5714285714285715, 1.0, 0.0, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.19999999999999998, 1.0, 0.11764705882352941, 0.5333333333333333, 0.5, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6137", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-3801", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-3837", "mrqa_triviaqa-validation-7571", "mrqa_hotpotqa-validation-5720", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1638", "mrqa_searchqa-validation-16366"], "SR": 0.515625, "CSR": 0.5462962962962963, "EFR": 1.0, "Overall": 0.6965248842592593}, {"timecode": 54, "before_eval_results": {"predictions": ["the martini", "a final contest", "Oakland", "Petro Poroshenko", "Cottage", "Tony Gwynn", "Atonement", "Kentucky", "Collagen", "Just say no", "Typewriter", "Diane Arbus", "Cincinnati", "PAM TILLIS LYRICS", "Suez Canal", "Planet of the Apes", "garret", "Adam Sandler", "north, east, and west", "to exhort", "a projecting beam", "William Shakespeare", "phobias", "San Jose", "pianissimo", "the Byzantine Empire", "Dunkirk", "the \"Queen of Crime\" Agatha Christie", "Psalms", "a pearl", "Gelato", "Jesus", "viruses", "George Balanchine", "Alfred Stieglitz", "Don Juan De Marco", "Africa", "Gaius Longinus", "Applebee's", "the Mercator", "Robin Hood", "Stegosaurus", "(Boris) Godunov", "Daniel Boone", "William Tecumseh Sherman", "Prison Break", "a hippopotamus", "Black Beauty", "\"Candid Microphone\"", "Sinclair Lewis", "Leo", "85 %", "Jamestown settlement in the Colony of Virginia", "Uruguay", "Lister", "Mr. Humphries", "Boulder Dam", "Personal History", "Hellenism", "Barnoldswick", "bodies and heads", "to the U.S. Holocaust Memorial Museum", "BET", "Havana Harbor"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6626240079365079}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-1885", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-13725", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-1876", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-308", "mrqa_searchqa-validation-7376", "mrqa_searchqa-validation-5449", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-14717", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-1657", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2420"], "SR": 0.609375, "CSR": 0.5474431818181817, "EFR": 0.96, "Overall": 0.6887542613636364}, {"timecode": 55, "before_eval_results": {"predictions": ["a zebra", "Sarah McLachlan", "a cake", "Japan", "C Daryl Whittier Chessman", "Grad school", "a grapefruit juice", "Detroit", "Tiger Woods", "John Paul II", "Blackbeard", "goose", "Jane Goodall", "the Tower of London", "Ethiopian", "first", "Stephen Crane", "Luxor", "gng h", "a nickel", "Clinton", "Wyoming", "a septum", "Nantucket", "Abnormal Psychology", "Elvis Presley", "Gianlorenzo Bernini", "mosquitoes", "Frank Sinatra", "Mark Knopfler", "photons", "National Archives", "low blood pressure", "Mousehunt", "Israel", "honey", "Rugby Football Union", "Romeo", "a palace", "decaffeination", "Knott's Berry Farm", "Phaedra", "Carl Linnaeus", "Australia", "Jodie Foster", "ventricular Fibrillation", "Barbary pirates", "a sandwich", "The Monkees", "Matilda", "Don Juan", "Nothing Gold Can stay", "Master Christopher Jones", "T.J. Miller", "7", "Jim Broadbent, John Cleese and Ricky Gervais", "Jerry Mouse", "AVN Adult Entertainment Expo", "England and Ireland", "John Lennon, Paul McCartney, George Harrison and Ringo Starr", "identity documents", "She told the newspaper she came to her decision based on the combination of the interrogation techniques, their duration and the effect on al-Qahtani's health.", "the 11th anniversary of the September 11, 2001, terror attacks.", "Philip Billard Municipal Airport"], "metric_results": {"EM": 0.546875, "QA-F1": 0.625569538712921}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.11764705882352941, 0.3076923076923077, 0.888888888888889]}}, "before_error_ids": ["mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-3214", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-15527", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-12914", "mrqa_searchqa-validation-4022", "mrqa_searchqa-validation-11601", "mrqa_searchqa-validation-14248", "mrqa_searchqa-validation-7849", "mrqa_searchqa-validation-12284", "mrqa_naturalquestions-validation-10546", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-7361", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-122", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2215", "mrqa_hotpotqa-validation-2840"], "SR": 0.546875, "CSR": 0.5474330357142857, "EFR": 0.9655172413793104, "Overall": 0.6898556804187193}, {"timecode": 56, "before_eval_results": {"predictions": ["Jean Lafitte", "Gylippus", "argyle", "the Pacific Ocean", "Easter", "forgive", "Dalai Lama", "the heptathlon", "a raincatcher", "Thomas L. Friedman", "tea", "arteries", "Nicholas II", "Amerigo Vespucci", "Patrick Henry", "Essen", "The Clash", "Ho Chi Minh", "the pituitary gland", "Ben Johnson", "Kentucky", "the Byzantine Empire", "9 to 5", "Cambodia", "lunch", "Velvet Revolver", "Sears", "vitamin A and K.", "a cherries", "Florence", "Ma Barker", "Joe DiMaggio", "Tie", "Naples", "Nick", "the Baruch Plan", "the Big Dipper", "wine", "silk", "\"The Safety Dance\"", "Manx", "the Moors", "a GPS", "North Carolina", "Green", "a cake knife", "Tchaikovsky", "the Tuileries", "the Panama Canal", "Cessna 172", "Alexander Gardner", "Andy Serkis", "parthenogenesis", "Pangaea", "Germany", "mule", "61", "Silvia Navarro", "26 November", "John Morgan", "\"I loved the convenience [of the train] say it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "Mary Phagan,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "off the coast of Somalia"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6827682913351016}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 1.0, 0.4615384615384615, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-6160", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10639", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-9458", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-11186", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-11589", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-13439", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-15973", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-3262", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-1022"], "SR": 0.609375, "CSR": 0.5485197368421053, "EFR": 1.0, "Overall": 0.696969572368421}, {"timecode": 57, "before_eval_results": {"predictions": ["a spectator", "French Toast", "Mexico", "plug in", "William Faulkner", "Samwise Gamgee", "Franklin Pierce", "Hindu", "Juno", "Intel", "Hank Williams Jr.", "George C. Wallace", "northern Wyoming", "a three-line of scrimmage", "West Virginia", "Edward Hopper", "asteroids", "Mark Twain", "the Large Orbiting Telescope", "Pop-Tarts", "Jimi Hendrix", "John Miller", "Linnaeus", "Tootsie", "roots", "albino", "Bonn", "junk", "chinchillas", "Tennessee", "No Child Left Behind", "William S. Hart", "The Piano Guys", "Francisco Franco", "Tennessee Williams", "S", "Douglas Fairbanks, Jr.", "West Point", "The Beatles", "Steely Dan", "word", "Norway", "kotleta po-kyivsky", "George Clooney", "a diamond", "Chicago White Sox", "postcards", "Kentucky", "Skateboarding", "Gaul", "blasters", "funding at a rate or formula based on the previous year's funding", "Havana Harbor", "Melbourne", "Atticus Finch", "elephant", "Geheimrat Dr. Max", "Ella", "Ed O'Neill", "Darkroom", "Dancing With the Stars", "south-central Washington,", "the World Cup giant slalom in Lienz on Monday.", "Upstairs"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6483716475095787}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20689655172413793, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-2264", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-6362", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-14269", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6688", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-5196", "mrqa_searchqa-validation-12764", "mrqa_searchqa-validation-8729", "mrqa_searchqa-validation-11120", "mrqa_naturalquestions-validation-10533", "mrqa_triviaqa-validation-2878", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-1161", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-1727", "mrqa_triviaqa-validation-7365"], "SR": 0.578125, "CSR": 0.5490301724137931, "EFR": 1.0, "Overall": 0.6970716594827586}, {"timecode": 58, "before_eval_results": {"predictions": ["Gov. James Grant", "Sri Lanka", "John Glenn", "Hinduism", "Lady Sings the Blues", "wedlock", "trans fat (i.e.)", "Joe Friedland", "Edward", "Hello, Dolly!", "the Mesozoic Era", "Gettysburg", "Martin Lawrence", "plantain", "Heracles", "Bob Fosse", "stem cells", "a cutlass", "the Bodleian library", "the pupil", "a front", "James Franco", "salmon", "The Crow", "cheese", "James Watt", "1945", "a birthstone", "Ichabod Crane", "Morrie: An Old Man, a Young Man", "Heather Locklear", "noun", "Holden Caulfield", "nuts", "Saudi Arabia", "lamb", "LaDainian Tomlinson", "wheat", "Duke", "a photoelectric cell", "Cape Town", "egg", "\"I have a dream that one day this nation will rise up and live out the true meaning of its creed\"", "sourdough", "a diamond", "run as Eisenhower's running mate on the Democratic ticket if Douglas MacArthur won the", "Joseph Stalin", "La Guardia", "Chastity", "Turandot", "Texas Rangers", "Jean - Eug\u00e8ne", "ice giants", "Amenhotep IV", "Zimbabwe", "colony", "phobia", "Haiti", "Argand lamp", "1891", "Secretary of State Hillary Clinton,", "1-1", "an upper respiratory infection,", "Germany"], "metric_results": {"EM": 0.609375, "QA-F1": 0.65078125}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-4326", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5137", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-10741", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-11795", "mrqa_searchqa-validation-13956", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2685", "mrqa_hotpotqa-validation-1482", "mrqa_newsqa-validation-2402", "mrqa_newsqa-validation-2472"], "SR": 0.609375, "CSR": 0.5500529661016949, "EFR": 1.0, "Overall": 0.6972762182203389}, {"timecode": 59, "before_eval_results": {"predictions": ["the UK\u2019s Trade Mark", "Mary Magdalene", "The Pillow Book", "General Paulus", "the Grail", "butcher", "The Double", "Stafford Stafford", "Jessica Simpson", "Humble Pie", "the gallbladder", "peterloo", "Aaron", "Leo Tolstoy", "Birmingham", "the Penrose triangle", "Magnificent Seven", "the Eureka Flag", "the 26th,", "raven", "John of Gaunt", "typhoid fever", "rk", "Microsoft", "Saint Laurent", "the Big Bang", "Willie Nelson", "horseracing", "Stars on 45 Medley", "Lundy Island", "Guinea", "Nadia Comaneci", "Belgium", "Del", "Turnbull & Asser", "non-Orthodox synagogues", "Lilo & Stitch", "Herbert Henry Asquith, 1st Earl of Oxford", "Nirvana and Kiss", "Mr. Humphries", "Paul Gauguin", "wildebeest", "French", "50", "Harper", "nirvana", "Pet Sounds", "purple", "Brigit Forsyth", "aardvark", "Charles Darwin", "5.7 million", "Oklahoma", "the Parable of the Unjust Judge", "8,515", "villanelle", "Awake", "2008", "China, Taiwan, Hong Kong and Mongolia", "Patrick McGoohan", "the coyote", "heating", "A Tale of Two Cities", "Charlie Wilson"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6803819444444443}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4273", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-3489", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-6759", "mrqa_triviaqa-validation-7013", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-6132", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-577", "mrqa_triviaqa-validation-3208", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-2061"], "SR": 0.609375, "CSR": 0.5510416666666667, "EFR": 0.92, "Overall": 0.6814739583333334}, {"timecode": 60, "UKR": 0.63671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1121", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-122", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4518", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5828", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-1071", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10905", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-12032", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15373", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16915", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3329", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4232", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6187", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8373", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9809", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4887", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9613", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1078", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-5183", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-998"], "OKR": 0.80078125, "KG": 0.42890625, "before_eval_results": {"predictions": ["15", "G\u00f6tene in Sweden", "William Walton", "Rensselaer County", "more than 20 principal operations and manufacturing facilities worldwide", "Beauty and the Beast", "authoritarian tendencies", "thomas", "Overtime", "north", "Hockey Club Davos", "2 April 1940", "Domingo \"Sam\" Samudio", "Sunday", "Taylor Swift", "Asif Kapadia", "hunt for the Death Star, the Galactic Empire's super weapon", "graffiti artists", "ESPN", "Bangor International Airport", "October 29, 1985", "Point of Entry", "Mickey's Christmas Carol", "the Harpe brothers", "the 1940s and 1950s", "Port Clinton", "deadpan sketch group", "Bharat Ratna", "Ronald Ryan", "broadcast internationally", "the 2011 Pulitzer Prize in General Nonfiction", "Eliot", "IT products and services", "American", "1865", "Critics' Choice Television Award", "Jeff Meldrum", "Picric acid", "23 March 1991", "1979", "Hannaford", "1968", "the post-Roman Republic period of the ancient Roman civilization", "Rigoletto", "Bill Clinton", "Race Through New York Starring Jimmy Tonight", "94", "horror film", "baa, Baa, Black sheep", "Italian nationality law", "28,776", "a Canaanite god associated with child sacrifice", "on the microscope's stage by slide clips, slide clamps or a cross-table", "commemorating fealty and filial piety", "Cain", "video", "charlie", "the country's Supreme Court.", "U.N. agencies", "al-Maqdessi,", "the Thames", "Pamela Anderson", "Henry Ford", "methane"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6391865079365079}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5333333333333333, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.8, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-804", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-5589", "mrqa_hotpotqa-validation-2263", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-978", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-1570", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-3314", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-2732"], "SR": 0.515625, "CSR": 0.5504610655737705, "EFR": 0.967741935483871, "Overall": 0.6769218502115283}, {"timecode": 61, "before_eval_results": {"predictions": ["Awake", "Law Adam", "Daniel Craig", "Sven Magnus \u00d8en Carlsen (] ; born 30 November 1990) is a Norwegian chess grandmaster and the current World Chess Champion", "Volvo 850", "400 MW", "1991", "Dra\u017een Petrovi\u0107", "Lord's Resistance Army", "Scott Dunlop", "New York City", "IFFHS World's Best Goalkeeper", "David May", "New Jersey", "Nicol Williamson", "Waimea Bay", "Willie Nelson and Kris Kristofferson", "the German Luftwaffe", "Nick on Sunset", "American Wrestler:The Wizard", "holy servant of Christ", "Give Up", "Matt Winer", "University of Kentucky", "WB Television Network", "\"Black Christmas\"", "2004", "democracy and personal freedom", "Australian", "Norse mythology", "Konstant\u012bns Raudive", "Melville", "5,922", "White Horse", "Black Abbots", "The Thieves", "Kentucky, Virginia, and Tennessee", "2011", "five", "Veronica Hamel", "literary magazine", "French, English and Spanish", "Edward James Olmos", "John R. Leonetti", "\"Alceste\"", "Perth", "Cersei Lannister", "Baltimore and Ohio Railroad", "countdown timer", "Tayeb Salih", "\"The Simpsons\"' thirteenth season", "Ferm\u00edn Francisco de Lasu\u00e9n, the 13th of 21 missions in California", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W \ufeff / \ufefb\ufffd 22.000 \u00b0 N \u00b0 W \ufffd\u1eff & 22.00 \u00b0 W", "Yuzuru Hanyu", "Jose Antonio Reyes", "Jordan", "Don Black", "sniff out cell phones.", "18", "four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday", "Prison Break", "Oscar Wilde", "Sicily", "Yukon"], "metric_results": {"EM": 0.625, "QA-F1": 0.677944506151743}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-1670", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-2187", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5451", "mrqa_triviaqa-validation-1648", "mrqa_newsqa-validation-4032"], "SR": 0.625, "CSR": 0.5516633064516129, "EFR": 0.9583333333333334, "Overall": 0.6752805779569891}, {"timecode": 62, "before_eval_results": {"predictions": ["happy being at home, attending every soccer game and knowing what his kids like to eat for breakfast", "refused Wednesday to soften the Vatican's ban on condom use", "public-television show", "company Polo", "punish participants in this week's bloody mutiny, which killed nearly 100 army officers and civilians,", "the Russian Defense Ministry", "Dore Gold, former Israeli ambassador to the United Nations", "The European Union", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "India", "snowstorm", "Bryant Purvis,", "fire", "a first document to recognize the legal right to freedom from tyranny,", "Dead Weather's \"Horehound\"", "Cash for Clunkers", "San Diego", "returning combat veterans", "Robert Park", "The Mexican military", "tusks", "David Design", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "Friday", "his wife's name", "$17,000", "helping on the Iraqi missions by the civil affairs division of the U.S. military,", "Bob Johnson", "Matthew Fisher", "26", "an angel, she is God-sent,", "$1,500", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "American Civil Liberties Union", "the company's products are roadworthy.", "1994", "High Court Judge Justice Davis", "to provide security as needed.", "$83,03013", "$250,000", "kase Ng,", "Islamabad", "hundreds of people joined a campus rally to oppose racial intolerance.", "the North Korean newspaper Rodong Sinmun", "Osama", "her office has launched a criminal investigation into the statements and reports given by the woman.", "was asked about the number of times Mohammed was waterboarded.", "the capital city of Harare.", "Vernon Forrest,", "the case drew nationwide attention.", "subway train accident", "in March 1930", "1961", "Rajendra Prasad", "mseyside", "Secretary of State William H. Seward", "the Cascade Range", "Ice Princess", "Hispania Racing F1 Team", "compact car", "delete", "Kansas", "John James Audubon", "Kim Basinger"], "metric_results": {"EM": 0.375, "QA-F1": 0.4790517186498424}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 0.5333333333333333, 0.33333333333333337, 0.0, 0.6363636363636364, 0.4, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.13793103448275862, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.5714285714285715, 0.0, 0.0, 0.13793103448275865, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.4, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-1289", "mrqa_naturalquestions-validation-7628", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6501", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-3993", "mrqa_searchqa-validation-5326"], "SR": 0.375, "CSR": 0.548859126984127, "EFR": 0.975, "Overall": 0.6780530753968254}, {"timecode": 63, "before_eval_results": {"predictions": ["Israel", "billboards with an image of the burning World Trade Center", "Saturn", "the commissions as a legitimate forum for prosecution,", "Kgalema Motlanthe,", "Kim Il Sung", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "travel with privately armed guards.", "up to $50,000 for her,", "gun charges", "January 24, 2006.", "usion teams", "Philippines", "in the $24,000-30,000 price range.", "July", "her home", "natural gas", "in the mouth.", "jazz", "almost 30 tunnels, including the 6.2-mile Moffat Tunnel,", "beat and binding Andrade,", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "KBR.", "Ralph Lauren", "the Gulf of Aden,", "Al-Shabaab", "the story of the Cowardly Lion", "269,000", "23", "Dube, 43, was killed", "Tim Cahill", "Tuesday,", "super-yacht designers", "Alina Cho", "Inseeded Frenchwoman Aravane Rezai", "the nose, cheeks, upper jaw and facial tissue", "1983", "made one of his strongest statements to date on the sex abuse scandal sweeping the Roman Catholic Church,", "the FARC were not targeting indigenous populations but took the action \"against people who independent of their race, religion, ethnicity, social condition etc.", "three-time road race world champion,", "\"We tortured (Mohammed al-) Qahtani,\"", "Yemen,", "11", "Matthew Fisher", "Afghanistan's restive provinces", "Rob Lehr,", "insect stings,", "Tennessee", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "the chemical at the Qarmat Ali water pumping plant in southern Iraq shortly after the U.S. invasion in 2003.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "Sleeping with the Past", "in the ark of the covenant", "in the pachytene stage of prophase I of meiosis", "the Great Chicago Fire", "4", "Edward III", "fourth", "BAFTA Award for Best Art Direction", "in March 19, 1958", "(Peter) Pushkin", "Bronx Park", "draft horse", "November"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5029294742544234}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.9411764705882353, 1.0, 0.0, 0.4444444444444445, 0.0, 0.2608695652173913, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8750000000000001, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.2666666666666667, 0.10526315789473684, 0.9166666666666666, 1.0, 0.0, 0.7368421052631579, 1.0, 0.0, 1.0, 0.0, 0.8333333333333334, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-3049", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-7035", "mrqa_triviaqa-validation-1159", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-3383", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-10510"], "SR": 0.359375, "CSR": 0.5458984375, "EFR": 1.0, "Overall": 0.6824609374999999}, {"timecode": 64, "before_eval_results": {"predictions": ["We Found Love", "The 19-year-old woman", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "40", "new clashes in Cairo's Tahrir Square that stretched into Wednesday.", "helped make the new truck safer, but also could make it more expensive to repair after a collision.", "19 American tourists and two Egyptians -- the bus driver and a tour guide --", "Paul McCartney and Ringo Starr", "great jazz", "The lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains poses a challenge for prosecutors,", "Sodra nongovernmental organization,", "in Port-au-Prince", "computer-generated animated film", "0300", "on the 12th on the Blue Monster course at Doral", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world --", "signed a power-sharing deal", "collaborating with the Colombian government,", "the Russian air force,", "Rod Blagojevich", "Fiona MacKeown", "50", "the legitimacy of that race.", "President Obama", "John Lennon and George Harrison,", "Sharon Bialek", "1998.", "45 minutes, five days a week.", "Israel and the United States", "Monday.", "Frank Ricci,", "Sixteen", "International Red Cross Committee,", "EU naval force", "Al-Shabaab", "Daytime Emmy Lifetime Achievement Award", "two and a half years", "the foyer of the BBC building in Glasgow, Scotland", "The UNHCR", "EU naval force", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple, with the latest resulting in the arrest of Mesac Damas in January,", "that he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "two", "6-2 6-1", "U.S. Consulate in Rio de Janeiro,", "German authorities", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "citizenship", "secure more funds", "Redwood Original", "Emily Blunt", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "the board base for physically supporting and wiring the", "Richard Wagner", "H. H. Asquith", "Centers for Medicare and Medicaid Services", "FBI", "Macomb County", "Custer", "Louis XIV", "a waterbed", "Risk"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5832191469091623}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.2222222222222222, 0.16666666666666669, 0.33333333333333337, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10526315789473684, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352942, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3, 1.0, 0.5454545454545454, 0.23529411764705882, 0.0, 0.14285714285714288, 0.22222222222222224, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-372", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-574", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-5934", "mrqa_hotpotqa-validation-2837"], "SR": 0.515625, "CSR": 0.5454326923076923, "EFR": 1.0, "Overall": 0.6823677884615384}, {"timecode": 65, "before_eval_results": {"predictions": ["North West England", "Carol Ann Duffy", "Liquidambar styraciflua", "Fredric Warburg", "Battleship", "Hurricane Faith", "the First Balkan War", "Teutonic Knights", "9", "James Harrison", "Germany", "Brian Doyle- Murray", "Ford Island", "2011", "I", "Tim Allen", "Latium in central Italy,", "Paul Avery", "Berea College", "Christopher Lloyd Smalling", "January 4, 1976", "an inversion", "1971", "Clovis I", "Tie Domi", "2007", "poet, and writer", "Quasimodo", "Savin Yeatman-Eiffel", "Pieter van Musschenbroek", "23 July 1989", "actress", "Attorney General and as Lord Chancellor of England", "Plato", "St Andrews, Fife, Scotland", "Henry Daniel Mills", "ribosomal RNA", "Ronald Joseph Ryan", "A Hard Day's Night", "Humberside", "Dumfries and Galloway,", "Rudolph the Grinch Stole Christmas", "from 1989 until 1994", "Cecily Legler Strong", "Polish-Jewish", "Philip Aaberg", "in 2005", "Levon Helm", "Chengdu Aircraft Corporation", "White Knights of the Ku Klux Klan", "Reunited Worlds", "Lou Rawls", "before the first year begins", "Buddy Greene", "Parkinson's disease", "Explain", "Yeats", "Casa de Campo International Airport", "11", "the truth about why you broke up", "Carmen", "New York City", "Massachusetts", "in July"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6716918498168498}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.33333333333333337, 1.0, 0.28571428571428575, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-4711", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-10550", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-6912", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1180", "mrqa_searchqa-validation-2844", "mrqa_newsqa-validation-271"], "SR": 0.53125, "CSR": 0.545217803030303, "EFR": 0.9333333333333333, "Overall": 0.6689914772727272}, {"timecode": 66, "before_eval_results": {"predictions": ["Sirach", "alligator", "best adult and children's hospitals", "quoit", "Ramona", "Tobacco Road", "M*A*S*H", "Opportunity", "Smokey Robinson", "a frog", "Gladiator", "combination colors", "the earpipe", "Cairo", "The Cotton Club", "a sandstorm", "George Byron", "neutrino", "Alexandra Rover", "George Eliot", "clouds", "Sir Arthur Conan Doyle", "PCH", "Auschwitz", "China", "Uganda", "low-calorie", "Edward", "garnet", "Bali", "Montmartre", "2004 Olympic silver medalist", "Elizabeth II", "kings", "Blacklisted", "take a small boat", "U.S.", "Jean", "Yogi Berra", "China", "Hawaii", "birds", "peripheral vision", "Espresso", "Delacorte", "head", "Vanessa Williams", "buttercream", "potential energy", "the Byzantine Empire", "Carson City", "16 seasons", "photoelectric", "the upper peninsula of Michigan, south to northern Louisiana, west to Colorado, and east to Massachusetts", "Joan Crawford", "Bassenthwaite Lake", "the moon", "Vanilla Air Inc.", "Jack Ridley", "military officer", "children's books", "Six people", "attempted burglary", "what is thought to be a long-range missile"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5989118303571429}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.9375, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.25]}}, "before_error_ids": ["mrqa_searchqa-validation-995", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-13562", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-14972", "mrqa_searchqa-validation-5294", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-14263", "mrqa_searchqa-validation-10375", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15880", "mrqa_searchqa-validation-14516", "mrqa_searchqa-validation-488", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-1198", "mrqa_searchqa-validation-16731", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-15274", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-6543", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-2870", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-5467", "mrqa_newsqa-validation-3829", "mrqa_newsqa-validation-1661"], "SR": 0.53125, "CSR": 0.5450093283582089, "EFR": 1.0, "Overall": 0.6822831156716418}, {"timecode": 67, "before_eval_results": {"predictions": ["Steven Spielberg", "Baton Rouge", "Wilbur Wright", "Woodrow Wilson", "King George III", "Stephen Sondheim", "515 ft", "a calculators", "Bill Wyman", "Dr. Murthy", "T.S. Eliot", "lead", "Kevenie Foster", "the 15th century", "the gravitational force", "Fisherman\\'s Wharf", "Santa Fe", "Kenya Barris", "Sex Pistols", "chaturanga", "Michael Jordan", "Roustabout", "doughboy", "Brge Rosenbaum", "Muhammad Ali", "rabbit", "Secretariat", "soup Nazi", "a tooth", "tannins", "Homer", "a rudder", "\"Hell hath no fury like a woman scorned\"", "John Paul II", "Will Rogers", "Hairspray", "Orlando", "Top 3", "Old Ironsides", "River Phoenix", "the Sydney, Australia Harbor", "mutton", "royal icing", "Napoleon", "the flag of Mongolia", "Peter the Great", "barn-raising", "corporal punishment", "Missouri", "Tim Burton", "Paris", "1956", "Tommy James", "Two Days Before the Day After Tomorrow", "jujitsu", "Salvador Dali", "Robert De Niro", "1993", "October 17, 2017", "from 1986 to 2013", "Afghanistan", "a mammoth", "\u00a343.89 ($72.40) per minute", "Gary Grimes"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6444444444444444}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15468", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-9009", "mrqa_searchqa-validation-7771", "mrqa_searchqa-validation-626", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-7053", "mrqa_searchqa-validation-4446", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-4698", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-8561", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-1882", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-512", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2152"], "SR": 0.578125, "CSR": 0.5454963235294117, "EFR": 1.0, "Overall": 0.6823805147058823}, {"timecode": 68, "before_eval_results": {"predictions": ["the \"Fisherman's ring\"", "Omaha", "Antwerp", "the Matterhorn", "Loch Lomond", "Alaska", "Frasier", "a temporary need", "Denmark", "the \"ball in tube\" or electromechanical crash sensor", "someone\\'s", "Pygmalion", "cholera", "E.E. Cummings", "Wilhelm Conrad Roentgen", "Glendening", "Yes", "the Green Hornet", "\"People, people who need\" Peabodys", "geolu", "the amniotic membrane", "\"Spartans\"", "Diner", "Cleopatra", "pizza", "St. Petersburg", "Japan", "the Jordan River", "Derek Jeter", "Hans Christian Andersen", "a value of type", "defense", "\"Unfathomable Sea\"", "Percy Shelley", "pearls", "vinegar", "an earthquake", "Jr.", "Citizen Kane", "gravity", "Mathew Brady", "Clinton", "the opponent\\'s court", "Tasmania", "Wyoming", "the Fellowship of the Ring", "the quick brown fox", "Denmark", "wheat", "\"Free Bird\"", "the frigate", "a web page above the page in an address bar", "presidential representative democratic republic", "the Pir Panjal Railway Tunnel", "Barcelona", "China", "Leander", "the Lewis and Clark Expedition", "Morocco", "1998", "club managers,", "either heavy flannel or wool", "several weeks", "2011"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6315498737373737}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.888888888888889, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-16760", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-9738", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-7831", "mrqa_searchqa-validation-4154", "mrqa_searchqa-validation-13968", "mrqa_searchqa-validation-4617", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-1342", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-10953", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-15704", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1848", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-3156", "mrqa_newsqa-validation-3500"], "SR": 0.546875, "CSR": 0.5455163043478262, "EFR": 1.0, "Overall": 0.6823845108695652}, {"timecode": 69, "before_eval_results": {"predictions": ["Kentucky Fried Chicken", "a dickey", "Follies", "( Francis) Ford", "Andrew Jackson", "Agamemnon", "spurs", "Robert Bartlett", "\"Bah-dum\"", "a canton", "Louisiana", "tree-lined", "percussus", "Pardon", "Diana", "strawberry", "(the) Constellations", "Indiana Jones", "Fox Network", "20", "Mendel", "Marlain Angelidou", "Hulk Hogan", "Margaret Tobin Brown", "a horse", "A Hard Day\\'s Night", "Making the Band 3", "Judy Garland", "Autumn in New York", "telephone operator", "FDR", "PETRUCHIO", "\"I Have No Mouth, and I Must Scream\"", "La Salle", "lattice", "a penny", "succotash", "the retina", "a prayer", "Idaho", "The Sopranos", "Hark", "Huguenots", "the Brooklyn Dodgers", "king", "yellow", "mascara", "Rooster", "pines", "the homestead act", "Lawrence Wien", "Kyrie Irving", "Mexico", "Andrew Michael Harrison ( born October 28, 1994 ) is an American professional basketball player for the Memphis Grizzlies", "Crete", "Miles Morales", "Peter Nichols", "the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "1858", "McComb, Mississippi", "Newcastle", "eight years", "Camorra -- the name for organized crime in Naples -- is strong.", "a mermaid"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6496180555555555}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 0.32, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-5203", "mrqa_searchqa-validation-843", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-10815", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-16083", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-4606", "mrqa_searchqa-validation-705", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-5659", "mrqa_searchqa-validation-12330", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8460", "mrqa_naturalquestions-validation-5767", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-2762", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-2727"], "SR": 0.578125, "CSR": 0.5459821428571429, "EFR": 1.0, "Overall": 0.6824776785714286}, {"timecode": 70, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-947", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16953", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4642", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7593", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9643", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10254", "mrqa_squad-validation-10328", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1075", "mrqa_squad-validation-1183", "mrqa_squad-validation-1319", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-2975", "mrqa_squad-validation-3139", "mrqa_squad-validation-3168", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4147", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5532", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5826", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6624", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-821", "mrqa_squad-validation-8253", "mrqa_squad-validation-8410", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5488", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6565", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.822265625, "KG": 0.5046875, "before_eval_results": {"predictions": ["James K. Polk", "stanch", "delta", "barroco", "St. Petersburg", "China", "Prohibition", "Onomastic Sobriquets", "The Godfather", "Maria Sharapova", "McDonald\\'s", "(Sonny) Corleone", "11", "The Stars and Stripes Forever", "Jackie Moon", "Pulp Fiction", "expunge", "the Rhine", "a missile", "dilithium", "Schwarzenegger", "the Epstein-Barr virus", "helium", "a cadence", "U.S. Naval Academy", "Iowa", "indirect discourse", "a circle", "Pussycat Dolls", "Shakespeare", "a clump", "Toorop", "Louis Le Vau", "Heath", "the Odyssey", "(Michael) Phelps", "Annapolis", "the Maccabees", "Rolls Royce", "a doses", "the Caucasus", "Lafayette", "the gopher", "Mephistopheles", "The Coca-Cola", "Warren", "apogee", "the Moon", "a mirror", "david archuleta", "Union Carbide", "Paradise, Nevada", "Ireland", "the accession of the princely state of Hyderabad into the Indian Union on 24 November 1949", "the month of May", "Stockholm syndrome", "Ilkley", "Geographical Indication", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "The scarp was first imaged by Voyager 2", "military veterans", "7.14 points behind Kim Yu-Na of South Korea and 2.42 points behind Mao Asada of Japan", "1000 square meters in forward deck space", "Larry Ellison,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6206845238095238}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.375, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.125, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8833", "mrqa_searchqa-validation-7803", "mrqa_searchqa-validation-5802", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-9498", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-13731", "mrqa_searchqa-validation-15266", "mrqa_searchqa-validation-958", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-11878", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-12155", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6489", "mrqa_triviaqa-validation-1074", "mrqa_hotpotqa-validation-2854", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1701"], "SR": 0.546875, "CSR": 0.5459947183098591, "EFR": 1.0, "Overall": 0.7171676936619719}, {"timecode": 71, "before_eval_results": {"predictions": ["Funki Porcini", "119", "560", "100 Greatest Artists of Hard Rock", "Klasky Csupo", "influenced by the music genres of electronic rock, electropop and R&B", "the \"Home of the Submarine Force\"", "Luc Besson", "River Shiel", "What Ever Happened to Baby Jane?", "the Lommel differential equation", "Harry Booth", "Southland", "1.23 million", "Boston", "281", "Northern Ireland", "1916 Easter Rising", "Park Hyung-Sik", "Conservative Party", "quantum mechanics", "gamecock", "Theo James Walcott", "April 8, 1943", "\"L'homme qui voulait savoir\" or \"Without a Trace\"", "their unusual behavior", "11", "Victoria Peak", "\"Back to December\"", "850 saloon", "Hindi", "Statutory List of Buildings of Special Architectural or Historic Interest", "High Falls Brewery", "one child, Lisa Brennan-Jobs", "Frederick Louis", "Tomorrowland and Fantasyland", "Hindi", "Art Deco-style skyscraper", "Brent Wilson", "The song, written by Lamar and Mike Will Made It", "Nova", "Green Chair", "Walker Smith Jr.", "Haleiwa on the North Shore of O'ahu", "Juan Manuel Mata Garc\u00eda", "Umina Beach", "\"Mickey Mouser\"", "Kinnairdy Castle", "Antigua & Barbuda, Argentina, South Africa,", "Stephen James Ireland", "Lola Dee", "1924", "cell nucleus", "inner core and growing bud", "Helen of Troy", "Vince Cable", "-30", "A good vegan cupcake has the power to transform everything for the better,\"", "Sporting Lisbon", "Illness", "computer programming", "bowling", "Gin Rummy", "teeth"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6810763888888889}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-5371", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-1799", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-237", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-3301", "mrqa_hotpotqa-validation-1716", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5242", "mrqa_searchqa-validation-12240", "mrqa_searchqa-validation-3082"], "SR": 0.578125, "CSR": 0.5464409722222222, "EFR": 1.0, "Overall": 0.7172569444444445}, {"timecode": 72, "before_eval_results": {"predictions": ["U2 360\u00b0 Tour", "Brent Wilson was fired", "Peter Kay's Car Share", "Michael Crawford", "Brazilian Jiu-Jitsu", "2012 Olympic bronze medalist", "CMYKOG process", "Colonel", "the first month of World War I", "Germany", "River Clyde", "George Washington Bridge", "Argand lamp", "Lowestoft, Suffolk", "The Ethics of Ambiguity", "Bishop's Stortford F.C.", "The Rural Electrification Act of 1936", "Vitor Belfort", "Carlos Coy", "Hawaii", "Julio Daniel Martinez", "35,000 members", "24", "the Bahamian island of Great Exuma", "research-based study of music", "Bonnie Franklin", "the Cheshire League Premier Division", "Kelly Bundy", "Canada's first train robbery", "Carson City", "Ben R. Guttery", "arts manager", "Montreal", "New Zealand", "August 14, 1848,", "Peel Holdings", "24 December 1692", "film", "Teddy Riley", "672 km2", "140 million", "Lamar Hunt", "Vienna", "Alemannic", "ten", "SpongeBob SquarePants 4-D", "Seti I", "Raabta", "Joseph E. Grosberg", "January 15, 1975", "2015", "historical fiction", "energy loss", "king Gautamiputra Satakarni", "Leopold", "Audi A4", "Valletta", "is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Heshmatollah Attarzadeh", "drug cartels", "New York City", "Turandot", "Champagne", "seabirds"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7063895089285714}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.18749999999999997, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1006", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-6234", "mrqa_triviaqa-validation-6188", "mrqa_triviaqa-validation-4960", "mrqa_newsqa-validation-2194", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3199"], "SR": 0.578125, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.7173437500000001}, {"timecode": 73, "before_eval_results": {"predictions": ["Adonijah", "Poland", "Hillary Clinton", "Hannibal", "Elysium", "Birmingham", "a syndicate", "Hansel and Gretel", "J.M.W. Turner", "Heisenberg", "astronaut", "glockenspiel", "David Hockney", "Kyoto Protocol", "Lassie", "Croatia", "Kansas City", "South Carolina", "Survivor Series", "taxis", "bell peppers", "piscina", "Edward III", "Bruce Wayne", "lighting", "Tesco", "Cologne", "P. b. eous", "Midtown", "Nikola Tesla", "near-field communication", "Tennessee", "Grimbsy", "Shropshire Way", "Robert Guerrero", "Virginia Plain", "Columbia", "Scotland", "Adeola", "Spanish", "toilet", "Medusa", "1911", "alexandrina", "World Heavyweight", "ArcelorMittal Orbit", "Madness", "Mao Zedong", "Sweet Home Alabama", "Greece", "St Helens", "to solve its problem of lack of food self - sufficiency", "required many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the", "biscuit", "High Knob", "Sacramento Kings", "200", "African National Congress Deputy President Kgalema Motlanthe", "Michelle Obama", "Quetta, the capital of Balochistan province", "campanile", "Maverick", "Ronald McDonald House", "#364"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5360119047619047}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.9714285714285714, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4166", "mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-5341", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-5672", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-4399", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-4776", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-5719", "mrqa_triviaqa-validation-614", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-4414", "mrqa_hotpotqa-validation-5878", "mrqa_newsqa-validation-3358"], "SR": 0.484375, "CSR": 0.5460304054054055, "EFR": 0.9393939393939394, "Overall": 0.705053618959869}, {"timecode": 74, "before_eval_results": {"predictions": ["iPod Classic or... Shuffle.", "1-0", "Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "the lower house of parliament,", "Simon Cowell", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "1983", "bread", "and Ethiopian troops are rampant.", "are \"active athletes,\" far from couch potatoes,", "the FBI", "10,000", "his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "former Procol Harum bandmate Gary Brooker", "California, Texas and Florida", "morphine sulfate oral solution", "\"it should stay that way.\"", "Iran", "Iran and Egypt", "Rawalpindi", "death", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "peanuts", "United States", "Samoa", "digital television network", "six", "Dublin", "former supermodel", "13", "Whitney Houston", "Ferraris", "free fixes for the consumer.", "nine-wicket", "10 below", "Madhav Kumar Nepal", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "black is beautiful", "fifth", "Iran", "U.S. State Department and British Foreign Office", "JBS Swift Beef Company", "Hurricane Gustav", "Canadian Prime Minister Stephen Harper", "murder", "Manny Pacquiao", "Itawamba Agricultural High School", "1,073 immigration detainees", "flying", "Alfredo Astiz", "Rancho La Brea", "Super Bowl VII", "3", "once upon a time", "whetstones", "Edinburgh", "true", "Norbertine", "Anheuser-Busch", "Beauty and the Beast", "the Chiefs", "Cleopatra", "Philip", "Sparafucile"], "metric_results": {"EM": 0.5, "QA-F1": 0.6051498802732356}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3157894736842105, 0.0, 0.0, 0.8, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-1514", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1373", "mrqa_triviaqa-validation-3067", "mrqa_searchqa-validation-14806"], "SR": 0.5, "CSR": 0.5454166666666667, "EFR": 0.96875, "Overall": 0.7108020833333333}, {"timecode": 75, "before_eval_results": {"predictions": ["1-1 draw", "South African ministers and the deputy president", "Sharon Bialek", "demolishing American third seed Venus Williams in the final of the Sony Ericsson Open in Miami", "voluntary manslaughter", "The Kirchners", "Sharon Bialek", "Roy Foster", "March 24,", "African-Americans", "Bill Haas", "David Beckham", "Rwanda", "Caster Semenya", "nuclear weapon", "$627,", "Turkish President Abdullah Gul.", "People Against Switching Sides (PASS)", "sedative.", "26", "Bob Dole,", "White Hills, Arizona,", "the assassination of President Mohamed Anwar al-Sadat at the hands of four military officers", "Australia", "re-examine other regions where similar blowback might take place.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Veracruz, Mexico,", "Sharon Bialek", "About 100,000 workers", "The two were separated in June 2004 when the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro, told Goldman", "San Simeon, California,", "\"Don Draper\"", "Fiona Mac Keown", "2001", "\"Toy Story\"", "a violent government crackdown seeped out.\"", "43 percent", "Jezebel.com", "alongside Deepwater Horizon", "Saturday", "that the deadly attack on India's financial capital last month was planned inside Pakistan,", "in 2006 and 2007", "the equator,", "Ralph Lauren,", "have a smile on her face when her kids were around.\"", "71 percent of Americans consider China an economic threat to the United States,", "Kitty Kelley,", "the Bronx", "\"We thought we were doing a humanitarian transport,\" said Antoni Cajal,", "in 1994.", "murder in the beating death of a company boss who fired them.", "February 7, 2018", "1439", "Blue laws", "Shakyamuni", "Prince Bumpo", "Nitrogen", "Araminta Ross", "ABC1 and ABC2", "G\u00f6tene in Sweden", "the kitchen sink", "whey", "Benito Mussolini", "roosevelt"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5232907717282718}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.923076923076923, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4014", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-3564", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-177", "mrqa_searchqa-validation-7856"], "SR": 0.4375, "CSR": 0.5439967105263157, "EFR": 1.0, "Overall": 0.7167680921052633}, {"timecode": 76, "before_eval_results": {"predictions": ["cricket", "\" Bernanke Doctrine\"", "Bolivia", "Matalan", "Ub Iwerks", "lincoln", "Michelle Obama", "Monopoly", "transsexual", "black", "chest cavity", "doubles", "Paul Gauguin", "Ben Jonson", "rhombus", "Willy Lott", "5-7, 6-4", "Dubai", "london", "14", "lice", "platinum", "Hubble Space Telescope", "James Van Allen", "Islamabad", "Mexico", "Philip Glenister", "Jack", "Emma Hamilton", "Beethoven", "Haystacks", "1958", "Margaret Thatcher", "Mauricio Pochettino", "USS Missouri", "Sensurround", "Venus", "Olympic Games", "Blue Ivy Carter", "Rihanna", "Tripoli", "rhianna", "Eva Per\u00f3n", "\"Doctor Who\"", "pink", "Glenn Close and Rade Serbedzija", "Illinois", "The Magnificent Seven", "Uranus", "typhoid fever", "Ross MacManus", "September 9, 2012", "Charles R Ranch, County Road 24, Las Vegas, New Mexico, USA", "seven", "Ricky Marco Marco Bryant", "American pharmaceutical company and one of the largest pharmaceutical companies in the world", "Queens, New York", "Manchester United", "Alwin Landry's supply vessel Damon Bankston", "ceilings", "The Big Sleep", "Dairy Queen", "Paul the Apostle", "Confederate victory"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6386160714285715}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-4855", "mrqa_triviaqa-validation-6274", "mrqa_triviaqa-validation-6019", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-6179", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2631", "mrqa_searchqa-validation-15341", "mrqa_naturalquestions-validation-767"], "SR": 0.59375, "CSR": 0.5446428571428572, "EFR": 1.0, "Overall": 0.7168973214285714}, {"timecode": 77, "before_eval_results": {"predictions": ["nearly $162 billion in war funding", "poor.", "137 children,", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost,", "\"I'm certainly not nearly as good of a great speaker -- probably the greatest we've seen in a generation.", "Transport Workers Union leaders", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "President Bush", "November 26,", "Wednesday.", "change course", "machine guns and two silencers", "Michael Jackson was sitting in Renaissance-era clothes and holding a book.", "off the coast of Dubai", "his past and his future", "Spc. Megan Lynn Touma,", "Hussein's Revolutionary Command Council.", "McDonald's' plans", "$31,000", "Black History Month", "in a tenement in the Mumbai suburb of Chembur,", "Monday.", "weren't taking it well.", "the island's dining scene", "22", "the foyer of the BBC building in Glasgow, Scotland", "mosteller,", "The president,", "Graham's wife", "don Draper", "Three", "their ambassadors", "fritter his cash away on fast cars, drink and celebrity parties.", "Everton", "criminals", "The Ministry of Defense", "civilians,", "21-year-old", "Ignazio La Russa", "Passers-by", "Pixar's", "NATO fighters", "New York City Mayor Michael Bloomberg", "Colorado prosecutor", "16 Indiana National Guard soldiers", "1940's", "The Maraachlis' daughter, Zeina,", "a member of the self-styled revolutionary Symbionese Liberation Army", "South Africa", "\"I am sick of life -- what can I say to you?\"", "a tanker", "Rigor mortis is very important in meat technology", "Walter Pauk", "Louis Prima", "alanis Morissette", "Uranus", "Baroness Thatcher", "February 16, 1944", "Fort Saint Anthony", "Wilmette, Illinois", "the quotient", "Vermont", "Kiribati", "Jay Van Andel"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6122761786824287}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true], "QA-F1": [0.1818181818181818, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.3076923076923077, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-162", "mrqa_newsqa-validation-780", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-199", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-9", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1145", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-6088", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-1874", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-13106"], "SR": 0.53125, "CSR": 0.5444711538461539, "EFR": 1.0, "Overall": 0.7168629807692307}, {"timecode": 78, "before_eval_results": {"predictions": ["the test results", "the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic.", "\"He is a very special member of our family. We miss having his love and compassion in our home,\"", "Isabella", "5 1/2-year-old son, Ryder Russell,", "finance", "gun charges,", "forgery and flying without a valid license,", "Middle, Oregon, in the Willamette Valley to the Pacific coast.", "There's no chance of it being open on time.", "the peace with Israel", "two", "70,000", "Arab Yahiye Gadahn,", "Silvio Berlusconi.", "650", "Expedia.", "the ireport form", "Virgin America", "The Italian government", "\"Sen. Piedad Cordoba is the most likely recipient among three leading contenders,", "sandbags to keep the waters at bay.", "four decades", "a supply vessel Damon Bankston", "Jenny Sanford's e-mails", "HPV (human papillomavirus)", "helicopters and unmanned aerial vehicles", "tickets", "Dr. Maria Siemionow,", "Hillary Clinton", "Barack Obama:", "NATO", "assassination of", "eradication of the Zetas cartel", "June 6, 1944,", "Orbiting Carbon Observatory,", "a face-to-face interview", "the Airbus A330-200", "divorced Goldman and married a Brazilian lawyer.", "poems", "be silent.", "Sri Lanka", "Adidas", "15 African and Asian elephants.", "Victor Mejia Munera was a drug lord with ties to paramilitary groups,", "can indeed help people with irritable bowel syndrome,", "martial arts,", "a student who admitted to hanging a noose in a campus library,", "543", "Barack Obama", "The Louvre", "New Zealand", "Rihanna", "enemy", "johnson", "Count Basie Orchestra", "Billy Cox", "wooden", "Yoruba gods and goddesses", "boxer", "chrysanthemums", "john Wesley", "Colonel Harland Sanders", "a series of newsreel films depicting multiple alternative realities rather than a novel"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5821748224601486}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.18181818181818182, 0.1904761904761905, 1.0, 0.0, 0.5, 1.0, 0.2857142857142857, 0.18181818181818182, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.0, 0.5, 1.0, 0.2857142857142857, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6956521739130436, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3030", "mrqa_newsqa-validation-3240", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2609", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-5101", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-13641", "mrqa_naturalquestions-validation-2729"], "SR": 0.453125, "CSR": 0.5433148734177216, "EFR": 1.0, "Overall": 0.7166317246835444}, {"timecode": 79, "before_eval_results": {"predictions": ["`` Mercy Mercy Me ( The Ecology ) '' was the second single from Marvin Gaye's 1971 album, What's Going On", "in 1989", "McFerrin, Robin Williams, and Bill Irwin,", "Hon July Moyo", "glycine and arginine", "2017", "12951 / 52 Mumbai Rajdhani Express", "the Rolling Stones", "A Christmas Story", "In 2010", "2018", "1975", "the date on which the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "David Tennant", "the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "the Canadian Rockies continental divide east to central Saskatchewan", "Exodus 20 : 1 -- 17", "the Supremacy Clause", "Hans Christian Andersen", "Nicolas Anelka", "each team", "in case of `` a national emergency created by attack upon the United States, its territories or possessions, or its armed forces", "Sauron", "1775", "Supplemental oxygen", "the level of the third lumbar vertebra, or L3, at birth", "`` Six flags over Texas '' is the slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "103", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Reba McEntire and Linda Davis", "New Mexico", "Karen Gillan", "In the fifth century", "The sacroiliac joint or SI joint ( SIJ )", "cut off close by the hip, and under the left shoulder", "Arkansas", "Mickey Rourke", "A rotation", "Phillip Paley", "Number 4, Privet Drive, Little Whinging in Surrey, England", "differs in ingredients", "Sunday evenings", "15 Bonanza Creek Lane, Santa Fe, New Mexico, USA", "a major fall in stock prices", "April 1979", "Reverend J. Long", "2005", "`` save, rescue, savior ''", "It acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Brad Johnson", "Nicole Gale Anderson", "1", "in 1642", "James Taylor", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "goalkeeper", "the Emancipation Proclamation", "4.6 million", "executive director of the Americas Division of Human Rights Watch,", "Pope Benedict XVI", "Afghanistan", "Erie Canal", "\"PANT\"s", "the Black Sea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6723481342721213}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.5882352941176471, 1.0, 0.7692307692307692, 0.4, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 0.22857142857142856, 1.0, 1.0, 0.0, 0.16666666666666666, 0.851063829787234, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.8571428571428571, 0.12500000000000003, 1.0, 1.0, 1.0, 0.5, 0.72, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-4108", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-9409", "mrqa_triviaqa-validation-752", "mrqa_triviaqa-validation-5693", "mrqa_searchqa-validation-10525"], "SR": 0.515625, "CSR": 0.54296875, "EFR": 0.967741935483871, "Overall": 0.7101108870967743}, {"timecode": 80, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3139", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1535", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.810546875, "KG": 0.45546875, "before_eval_results": {"predictions": ["prevent any contaminants in the sink from flowing into the potable water system by siphonage", "all - female", "late - September through early January", "Moscazzano", "Bachendri Pal", "calpurnia", "2009", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "Joe Pizzulo and Leeza Miller", "19th - century India", "the eighth series of the UK version of The X Factor", "Jenny Slate", "Gunpei Yokoi", "1990", "the sixth season", "the leaves of the plant species Stevia rebaudiana", "Julie Deborah Kavner", "Peggy Lipton", "infection", "Jewel Akens", "Wales and Yorkshire", "2002", "their bearers", "12", "Frank Langella", "Number 4, Privet Drive, Little Whinging in Surrey, England", "shared", "Southport, North Carolina", "John C. Reilly", "Wednesday, September 21, 2016", "the Washington metropolitan area", "the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "1885", "Jolyon Coy", "Consular Report of Birth Abroad", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "Monk's Caf\u00e9", "20 locations all within the Pittsburgh metropolitan area", "Spanish", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "butch or Killer", "Norway", "the total number of voting representatives is fixed by law at 435", "1997", "Brooks & Dunn", "12 November 2010", "the homicidal thoughts of a troubled youth", "W. Edwards Deming", "convergent plate", "Sylvester Stallone", "1967", "Harrods,", "Fontane di Roma", "Steve Coogan", "Boston University", "Bay of Fundy", "Lincoln Riley", "The Rosie Show", "Tomas Olsson, the journalists' Swedish attorney.", "2,000 euros ($2,963)", "dishwasher", "the NATO phonetic alphabet", "(Albert) Einstein", "wheezing"], "metric_results": {"EM": 0.5, "QA-F1": 0.6055971582181259}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.16666666666666669, 0.4, 0.0, 0.20000000000000004, 0.5, 0.4, 1.0, 1.0, 0.967741935483871, 0.0, 1.0, 0.16666666666666669, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-937", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8485", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-8699", "mrqa_triviaqa-validation-5589", "mrqa_hotpotqa-validation-4160", "mrqa_newsqa-validation-2045", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-9418"], "SR": 0.5, "CSR": 0.5424382716049383, "EFR": 0.96875, "Overall": 0.6964564043209877}, {"timecode": 81, "before_eval_results": {"predictions": ["Jason Momoa", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "1623", "March 14, 1942", "Lafayette", "April 3, 1973", "5 liters", "Kate Walsh", "13 to 22 June 2012", "2.5 %", "232", "mid November", "Guwahati", "Nala", "1979 / 80", "2017", "Imperium R\u014dm\u0101num", "Leslie", "electron shells", "rain", "compasses", "production -- possibility frontier ( PPF ) or production possibility curve ( PPC )", "in 1987", "in the eye", "Eagle Ridge Outdoor pool in Coquitlam, BC", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "Louis Heston", "the fourth ventricle", "October 27, 2017", "John Cooper Clarke", "Saturday evenings", "Cheryl Campbell", "Spanish", "0.1", "March 2, 2016", "Jeff Bezos", "Erica Rivera", "warm and is considered to be the most comfortable climatic conditions of the year", "January 2, 1971", "Tracy McConnell", "gastrocnemius muscle", "Parker's pregnancy at the time of filming", "7", "the ascension of Akbar the Great to the throne", "1830", "Frank Oz", "Number 4, Privet Drive, Little Whinging in Surrey, England", "In 1967, Celtic became the first British team to win the competition", "Guant\u00e1namo or GTMO", "Morgan Freeman", "Sea of Monsters", "in the muscle tissue", "oche", "a Congregational Quorum", "Katarina Witt", "Bhaktivedanta Manor", "Lowe's", "Honduran", "al-Shabaab", "Columbian mammoth fossil \"Zed.\"", "flee", "Quebec", "churrasco", "a refrigeration unit"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6150494904401155}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.375, 0.0, 0.5, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5185", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2449", "mrqa_hotpotqa-validation-5833", "mrqa_newsqa-validation-1512", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-12256", "mrqa_searchqa-validation-12239"], "SR": 0.515625, "CSR": 0.5421112804878049, "EFR": 0.967741935483871, "Overall": 0.6961893931943351}, {"timecode": 82, "before_eval_results": {"predictions": ["emc-sized", "Hans", "purple", "Charles Lindbergh", "sucrose", "T.S. Eliot", "Superman Returns", "nokomis", "Yale", "tidal streams", "abu diamants", "Over the hifls", "circumnavigate", "southern Maine", "tarzan", "CFO", "manx cat", "rum", "baroque", "WATERS OF THE BODY", "licorice", "Dracula", "skating", "Sweden", "\"Frankenstein\"", "Hannah Montana", "Van Allen", "Mitch McConnell", "bravery or valor", "the gallbladder", "\"Invisibility\"", "Himalaya", "Chile", "the Democratic Socialist Republic of Sri Lanka", "the St. Valentine's Day Massacre", "Bea Arthur", "\"Sayonara\"", "San Francisco", "The Taming of the Shrew", "a proxy", "Andrew Johnson", "the knee joint", "NASA", "Gavin MacLeod", "abraham", "The Count of Monte Cristo", "Valley Stream", "Peter Shaffer", "Wyandotte County", "Cy Young", "Stephen Sondheim", "October 29, 2015", "MFSK", "Andy Cole", "yellow", "Galileo", "alan Dunham", "Afro-American religions", "Midtown Manhattan in New York City", "Awake", "exotic sports cars", "The show allows 10 boys and 10 girls between the age of eight and 11 to create their own mini-societies, organizing everything from what they eat to how they should entertain themselves.", "Saudi Arabia", "U.S. state of Washington"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5625}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-5014", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-13546", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-795", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-15074", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3227", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-10329", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-15465", "mrqa_searchqa-validation-12810", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-7984", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-14521", "mrqa_searchqa-validation-6416", "mrqa_naturalquestions-validation-8934", "mrqa_triviaqa-validation-1023", "mrqa_hotpotqa-validation-5173", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3226", "mrqa_naturalquestions-validation-3281"], "SR": 0.453125, "CSR": 0.541039156626506, "EFR": 1.0, "Overall": 0.7024265813253012}, {"timecode": 83, "before_eval_results": {"predictions": ["Coca-Cola", "Oklahoma State", "a honeybee", "Pippin", "Georgia", "the Chesapeake Bay", "a dugout", "cement", "heat houses", "Tim Robbins", "(Mark) Twain", "Platoon", "Leon Uris", "potato chips", "the Bay of Bengal", "the Clark bar", "to the tonic", "Dresden", "John Ashcroft", "Phil of the Future", "Newman", "Death Valley", "rings", "to pick up and hold until released,", "George Eliot", "American country music band", "(L.) Frank Baum", "J.P. Richardson", "jaded", "Sgt. Pepper's Lonely Hearts Club Band", "palindromes", "a trapezoid", "Scrubs", "Henrik Ibsen", "Elizabeth I", "Canticle", "Friedrich Nietzsche", "(Rodney) King", "Haunted Town Streets", "a prisoner of the gutter", "Tanzania", "Coca-Cola", "(Deepak) Chopra", "Rudolf Hess", "Siberia", "Rings Twice", "Sylvester Stallone", "(Edna) Ferber", "the magic screen", "safari", "Murfreesboro", "During his epic battle with Frieza", "1997", "The stability, security, and predictability of British law and government enabled international trade", "Joseph Priestley", "Granada", "independent music", "the 1940s and 1950s", "Best Musical", "York County", "six", "Michelle Rounds", "Derek Mears", "Adidas"], "metric_results": {"EM": 0.640625, "QA-F1": 0.69609375}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12304", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-16183", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-15025", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-16081", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-6008", "mrqa_searchqa-validation-16469", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-15171", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-13383", "mrqa_searchqa-validation-11096", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-2381", "mrqa_hotpotqa-validation-5309"], "SR": 0.640625, "CSR": 0.5422247023809523, "EFR": 1.0, "Overall": 0.7026636904761905}, {"timecode": 84, "before_eval_results": {"predictions": ["Portugal", "the Kingdom of Norway", "a daisy", "G\u00fcnter Grass", "Belfast", "W W Jacobs", "the Andaman", "East of Eden", "(John) Buchan", "Doncaster Rovers", "Anne", "Honshu", "9", "Supertramp", "Grieg", "Joanne Harris", "a abacus", "the Norse", "a buffalo", "displacement", "(James Valentine) Valentine", "(John Francome)", "the moon", "Jupiter", "White spirit", "aglets", "lemurs", "Manitoba", "Fabio Capello", "Mickey Mouse", "cricket", "1973", "William Neil Connor", "Azerbaijan", "Mathematics", "Spain", "the leader of Nazi Germany,", "HM Inspector of Prisons", "Moulin Rouge", "golf", "a dog", "KG PC", "Hamelin", "the Mysak", "George Osborne", "oxygen", "GM", "toads", "HMS Amethyst", "a hairdresser", "Antony", "Jethalalal Gada", "Spektor", "in San Francisco, California ( the primary setting of the film ), and around Oahu, Hawaii", "Deputy F\u00fchrer", "nearly 8 km", "Lake Buena Vista, Florida", "about 1,300 meters in the Mediterranean Sea.", "Afghanistan,", "in the mountains around Deutschneudorf,", "a chimp", "Tin", "Wings of Desire", "repel bullets and fly at sub-sonic speeds"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5346354166666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.8, 0.8, 0.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.25, 0.1]}}, "before_error_ids": ["mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-303", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-6666", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4166", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2685", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-1068", "mrqa_searchqa-validation-1820", "mrqa_naturalquestions-validation-2309"], "SR": 0.46875, "CSR": 0.5413602941176471, "EFR": 0.9705882352941176, "Overall": 0.6966084558823529}, {"timecode": 85, "before_eval_results": {"predictions": ["the Marsamxett Harbour", "Eurasia", "Tom Ewell", "American", "9,000", "My Beautiful Dark Twisted Fantasy", "secondary school study", "Shut Up", "30.9%", "William Shakespeare", "Nic Cester", "the Kingdom of Morocco", "Mountain goat", "Prince George's County", "Ariel Ram\u00edrez", "The Apple iPod+HP", "four months in jail", "Objectivism", "Vixen", "stunt performances", "Stevie Young", "co-founder and lead guitarist", "dreikaiserbund", "Frederick Alexander Lindemann", "Indianola", "Alexandre Dimitri Song Billong", "Dutch", "Rabies", "Malayalam", "Sim Theme Park", "Outside", "novelty songs, comedy, and strange or unusual recordings", "Sri Lanka Nidahas Pakshaya", "Jennifer Aniston", "the North Atlantic Conference", "Aubrey Posen", "Downtown", "Rockstar San Diego", "Strato of Lampsacus", "Columbus Crew SC", "Beno\u00eet Jacquot", "the Manor of the More", "Anita Dobson", "500-room", "Rajmund Roman Thierry Pola\u0144ski", "Discovery", "\"Orchard County\"", "Saint Michael, Barbados", "Championnat National 3", "Boston, Massachusetts", "The King of Chutzpah", "one person", "Acetate", "Gibraltar", "the Seine", "sarah pierce Brosnan", "daltonism", "Singapore Airlines", "Nicolas Sarkozy", "Zelaya", "doctrine", "Japan", "Stalin", "territories"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7505478896103897}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-2506", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-1202", "mrqa_triviaqa-validation-5716", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-5450"], "SR": 0.65625, "CSR": 0.5426962209302326, "EFR": 1.0, "Overall": 0.7027579941860466}, {"timecode": 86, "before_eval_results": {"predictions": ["California", "the object is placed further away from the mirror / lens than the focal point", "can therefore be any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product, whether or not related to the medicinal", "18 - season", "December 24, 1836", "the bank, rather than the purchaser, is responsible for paying the amount", "Carroll O'Connor", "A patent", "the landowners", "the 1940s", "American production duo The Chainsmoker", "Steve Russell", "the President pro tempore", "Donna", "the Dutch", "turlough, or turlach", "Vancouver, British Columbia", "Elected Emperor of the Romans", "9 February 2018", "1933", "the eighth episode of Arrow's second season", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "supervillains who pose catastrophic challenges to the world", "two", "the Charbagh structure", "Elizabeth Dean Lail", "a password recovery tool for Microsoft Windows", "semi-autonomous organisational units within the National Health Service in England", "After Margaret Thatcher became Prime Minister in May 1979", "from 13 to 22 June 2012", "60", "electron donors", "in the pouring rain", "monitor lizards", "Abbot Suger", "the Mahalangur Himal sub-range of the Himalayas", "The Royalettes", "the winter solstice", "State Bar of Arizona", "Marie Fredriksson", "Geothermal gradient", "2001", "Hellenion", "Urge Overkill", "2000", "blue", "the Mishnah", "1078", "April 1979", "around 1872", "Atlanta, Georgia", "glagolitic", "US", "Wooden Heart", "France", "rapper", "November 10, 2017", "jobs up and down the auto supply chain", "Buenos Aires", "France's famous Louvre", "arbutus", "Dragnet", "Harry Potter", "Fairfax"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5340504228237117}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.912280701754386, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 0.75, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 0.9387755102040816, 1.0, 0.6666666666666666, 0.0, 1.0, 0.058823529411764705, 1.0, 0.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.15384615384615383, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-916", "mrqa_triviaqa-validation-3891", "mrqa_triviaqa-validation-2813", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2614", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-1911", "mrqa_hotpotqa-validation-82"], "SR": 0.421875, "CSR": 0.5413074712643677, "EFR": 0.8108108108108109, "Overall": 0.6646424064150358}, {"timecode": 87, "before_eval_results": {"predictions": ["$10 billion", "Don Draper", "Nafees Syed", "usually high school juniors who serve Congress as messengers", "Marcell Jansen", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "nuclear warheads", "CNN's Charlie Moore", "a floating National Historic Landmark,", "a city of romance, of incredible architecture and history.", "peanuts, nuts, shellfish and fish", "Piers Morgan Tonight", "BERLIN, Germany", "in the bedrooms of their two-floor home in the St. Louis suburb of Columbia, Illinois,", "jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "free services.", "Steven Chu", "Mark Obama Ndesandjo", "Hayden", "30,000", "Adam Lambert and Kris Allen,", "Sunday", "more than $17,000", "250,000 unprotected civilians", "Theikini rocketed to fame in 1960 with Brian Hyland`s hit single, \"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "the Juarez drug cartel.", "Muslim festival", "United States, NATO member states, Russia and India", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Marcus Schrenker,", "150", "cancer for several years.", "U.S. Chamber of Commerce", "Rima Fakih is a Muslim with Lebanese heritage,", "the underprivileged.", "an open window", "Jiverly Wong,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "bronze medal in the women's figure skating final,", "98 people,", "President Obama", "in Austin, Texas,", "Cologne, Germany,", "that former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Tennessee", "five minutes before commandos descended from ropes that dangled from helicopters,", "legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "Mary Procidano,", "The son of Gabon's former president", "Samoa", "Derek Mears", "January to May 2014", "Frederick County", "RMS Titanic", "calcium carbonate", "The Great Leap", "Bangladesh", "Elisha Nelson Manning", "CBS", "Dr. Gr\u00e4sler, Badearzt", "a seal", "glaucoma", "Carl Sandburg", "Meriwether Lewis"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6778219899313649}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6153846153846153, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5925925925925926, 0.5, 1.0, 1.0, 0.26666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.5, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.625, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-931", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-3855", "mrqa_hotpotqa-validation-3563"], "SR": 0.59375, "CSR": 0.5419034090909092, "EFR": 1.0, "Overall": 0.7025994318181819}, {"timecode": 88, "before_eval_results": {"predictions": ["Caylee,", "at least 18", "$22 million", "The federal officers' bodies", "\"We tortured (Mohammed al-) Qahtani,\"", "the area of the 11th century Preah Vihear temple", "police to question people if there's reason to suspect they're in the United States illegally.", "\"Oprah: A Biography,\"", "and Jquante Crews,", "for adoption", "poems telling of the pain and suffering of children", "to express ourselves and expose the lies,\"", "Patrick McGoohan,", "Saturday", "Columbian mammoth", "insurgents", "genocide,", "in 1999,", "in a Utah jail", "269,000 copies", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry", "28", "an independent homeland", "Miguel Cotto", "South Africa", "seven", "to best your own fuel economy achievements,\"", "returning combat veterans", "scientific reasons.", "dismissed all charges", "United States, NATO member states, Russia and India", "75", "The 31-year-old Iranian", "will not support the Stop Online Piracy Act,", "Ma Khin Khin Leh,", "12 brutal rounds", "The Rosie Show", "The station", "Larry Zeiger", "The son of Gabon's former president", "Mogadishu", "Seoul.", "part of the proceeds", "Olympic relay", "vitamin \"drips\"", "Piers Morgan,", "Ciudad Juarez,", "Rolling Stone", "Indonesia", "Robert Park", "At least 14", "late 2018 or early 2019", "Graub\u00fcnden, in the eastern Alps region of Switzerland", "2,050 metres ( 6,730 ft )", "cutis anserina", "battle of Agincourt", "The Blind Beggar", "Belgian", "Geelong Football Club", "Richard L. Thompson", "a fisheye lens", "a clavichord", "orchids", "J. Knox"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5760725539645777}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false], "QA-F1": [0.6666666666666666, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 0.08, 0.0, 1.0, 0.0, 0.2222222222222222, 0.1818181818181818, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.28571428571428575, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.4347826086956522, 0.7777777777777778, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-4007", "mrqa_newsqa-validation-929", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1642", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-306", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-1586", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-3547", "mrqa_newsqa-validation-798", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8794", "mrqa_triviaqa-validation-5342", "mrqa_searchqa-validation-14968", "mrqa_searchqa-validation-2827", "mrqa_searchqa-validation-8747"], "SR": 0.4375, "CSR": 0.5407303370786517, "EFR": 1.0, "Overall": 0.7023648174157303}, {"timecode": 89, "before_eval_results": {"predictions": ["Ariel Binns", "Ignazio La Russa", "because the Indians were gathering information about the rebels to give to the Colombian military.", "collaborating with the Colombian government,", "potential revenues from oil and gas", "2000", "$24.1 million,", "Two United Arab Emirates based companies", "Chadian President Idriss Deby", "U.S. Navy", "two hunters", "U.S. State Department and British Foreign Office", "the hunt for Nazi Gold and possibly the legendary Amber Room", "U.S. President-elect Barack Obama", "to use the Internet for fun and not interfere with government and serious issues,", "News of the World tabloid.", "They're big, strong, and fierce", "not for sale,", "blind Majid Movahedi,", "fear of losing their licenses to fly.", "Jenny Sanford,", "Kurdish Workers' Party,", "Carnival", "\"falling space debris,\"", "At Wilhelmina Kids,", "France", "581 points", "Robert Barnett, a prominent Washington attorney,", "Mexican military", "14", "Venezuela", "41,", "Wednesday at the age of 95.", "Idriss Deby hopes the journalists and the flight crew will be freed,", "Iran could be secretly working on a nuclear weapon is a major development, but not one that should lead the U.S. to consider a military strike against the Tehran regime,", "\"illegitimate.\"", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "2-1", "Haeftling", "Saturday.", "five minutes before commandos descended", "A severely disfigured woman", "to sniff out cell phones.", "homicide", "forcibly injecting them with psychotropic drugs", "for the rest of the year", "July", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Dominic Adiyiah", "Glasgow, Scotland concert", "Caylee Anthony's", "2006 -- 06", "member", "John Vincent Calipari", "New Zealand", "Jessica Simpson", "fox hunting", "Los Angeles", "Samsung Galaxy S7", "Hilux pickup truck", "eggshells", "Manchester", "Gertrude Stein", "Dick Van Dyke"], "metric_results": {"EM": 0.5, "QA-F1": 0.6314175697817646}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.125, 0.125, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.05714285714285715, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.8235294117647058, 0.14634146341463414, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.6666666666666666, 0.42857142857142855, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-1000", "mrqa_naturalquestions-validation-5602", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5380"], "SR": 0.5, "CSR": 0.5402777777777779, "EFR": 0.90625, "Overall": 0.6835243055555555}, {"timecode": 90, "UKR": 0.642578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-342", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.81640625, "KG": 0.49765625, "before_eval_results": {"predictions": ["eels", "chas chandler", "Stevie Wonder", "Sir Tom Finney", "Jackson Pollock", "Black Sea", "Jumping Jack Flash", "Joseph Priestley", "Dumbo", "New Zealand", "Call for the Dead", "Bill Bryson", "Hans Christian Andersen", "Christian Louboutin", "Vespa", "nudity", "Laputa", "a Hungarian", "Jumanji", "Flo Rida", "at", "The Princess bride", "Advanced", "pig", "Dancing With the Stars", "Australia", "Leicester", "E. E.", "Andr\u00e9s Iniesta", "BATH, England", "1924", "a barred, spiral galaxy", "Duty Free", "Mark Twain", "fruit", "carbon", "Caernarfon", "Sir Herbert Kitchener", "dpurves", "Sergio Garc\u00eda Fern\u00e1ndez", "Chad", "Arthur", "Yulia Tymochenko", "E. Nesbit", "Lord Melbourne", "John F. Kennedy", "Sheree Murphy", "Robert Louis Stevenson", "the R34", "Yukon", "\"Eliver Twist\"", "Chlorofluorocarbons", "The tower has three levels for visitors, with restaurants on the first and second levels", "the coffee shop Monk's", "Mani", "DreamWorks Animation", "Port Melbourne", "338", "Pakistan", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "the Marine Band", "Joe Biden", "Daumier", "Austria"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5926533385093167}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.08695652173913043, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-436", "mrqa_triviaqa-validation-2698", "mrqa_triviaqa-validation-5650", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3335", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-268", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-2111", "mrqa_triviaqa-validation-4011", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-6807", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-339", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-724", "mrqa_searchqa-validation-13048", "mrqa_searchqa-validation-8979", "mrqa_hotpotqa-validation-5203"], "SR": 0.515625, "CSR": 0.5400068681318682, "EFR": 0.8387096774193549, "Overall": 0.6670714341102446}, {"timecode": 91, "before_eval_results": {"predictions": ["dumbo", "Ernest Hemingway", "Switzerland", "Mexican orange blossom", "Paul Drake", "India", "James I", "trapezium", "Canada", "a 1934 Austin seven box saloon", "seven", "Vancouver, Canada", "niger", "Switzerland", "the Union Gap", "Cologne", "air Bud", "carol", "gin", "piano", "Dick Cheney", "Auckland", "Virginia", "dysmenorrhea", "pasta", "witch trials", "sailor", "my Favorite Martian", "plutocracy", "Bahrain", "Austria", "Ace of Spades", "the British Parliament", "China", "doxycycline", "Venice", "New Zealand", "1973", "st Pauls", "Brighton", "c.offical", "pius XI", "Jimmy Carter", "(Danny Baldwin) Baldwin", "Argentina", "Genesis", "special sauce", "khrushchev", "arsenic", "john Peel", "Sheffield Wednesday", "December 14, 2017", "mitosis", "Morgan Freeman", "Michael Bruce Fiers", "coca wine", "politician", "\u00a320 million ($41.1 million) fortune", "to do more to stop the Afghan opium trade", "Trevor Rees-Jones,", "(Edvard) GRIEG", "Buddhism", "Thomas Francis Eagleton", "a thermophile"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6116972117794486}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2105263157894737, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-4024", "mrqa_triviaqa-validation-6471", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3775", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-7584", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-3062", "mrqa_triviaqa-validation-2032", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-6212", "mrqa_naturalquestions-validation-9781", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-2960", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-11019"], "SR": 0.5625, "CSR": 0.5402513586956521, "EFR": 0.9642857142857143, "Overall": 0.6922355395962733}, {"timecode": 92, "before_eval_results": {"predictions": ["raping and killing a 14-year-old Iraqi girl.", "Karen Floyd", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "Krishna Rajaram,", "U.S. President-elect Barack Obama", "Kurt Cobain", "a delegation of American Muslim and Christian leaders", "a treadmill", "California-based Current TV", "gun", "\"E! News\"", "1983", "nude beaches", "tennis", "president Barack Obama,", "19", "Max Foster,", "military trials", "Iran of trying to build nuclear bombs,", "a lion Among Men,\"", "if the people closest to him didn't see any indicators or signs that he was going to go off so drastically... how is some public safety officer supposed to recognize this person?\"", "we have seen success in the surge,", "to clean up Washington State's decommissioned Hanford nuclear site,", "will his No. 2 man (or woman) be by his side", "sylt's dining scene", "Saturday,", "women to cover their bodies and heads from view,", "a residential dike", "gasoline", "they did not receive a fair trial.", "2.5 million copies,", "abducting each other for ransoms or retribution.", "Egypt", "Tuesday,", "Barack Obama's", "iTunes Music Store,", "president Robert Mugabe", "Trevor Rees,", "12.3 million", "ties", "three", "to encourage readers to get involved in service and volunteerism in their communities.", "75 percent", "Hu Jintao", "1,500", "can be volatile and dangerous.", "two counts of murder.", "anarchists", "first grand Slam,", "Los Angeles'", "Alberto Espinoza Barron,", "2013", "Vincenzo Peruggia", "April 1979", "margaret thatcher", "Hercule Poirot", "1998", "Debbie Reynolds", "43rd", "USS Essex", "Daylight Saving Time", "Amelia Earhart", "citric acid", "Pakistan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7163709707367796}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9411764705882353, 0.0, 0.6666666666666666, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 0.5, 0.8571428571428571, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-45", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1516", "mrqa_triviaqa-validation-3035", "mrqa_hotpotqa-validation-1316", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-13027"], "SR": 0.546875, "CSR": 0.5403225806451613, "EFR": 1.0, "Overall": 0.6993926411290322}, {"timecode": 93, "before_eval_results": {"predictions": ["David Beckham", "They are co-chairs of the Genocide Prevention Task Force.", "March 8", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "Kearny, New Jersey.", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "five minutes before commandos descended from ropes that dangled from helicopters,", "Democratic", "Kim Il Sung", "Roy Foster's", "South Africa", "managing his time.", "U.S. Consulate in Rio de Janeiro,", "Majid Movahedi,", "Molotov cocktails, rocks and glass.", "Facebook and Google,", "boats heading from Galveston, Texas, to Veracruz, Mexico,", "customers are lining up for vitamin injections that promise to improve health and beauty.", "Oxbow, a town of about 238 people,", "British Prime Minister", "those missing", "summer", "a vast settlement of people left without loved ones, without homes, without life's belongings.", "Friday,", "\"the most important discovery\"", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "producing rock music with a country influence.", "more than 78,000 parents of children ages 3 to 17.", "he said he sees significant bias in Silicon Valley,", "Republican", "salary to the underprivileged.", "March 24,", "North Korea,", "Mexican military", "Bill Haas", "devoted to federal ocean planning.", "young self-styled anarchists", "the piracy incident was discussed as one of the \"tests\" of President Obama that Joe Biden warned about during the campaign.", "Swat Valley.", "Japanese Foreign Ministry spokesman Hidenobu Sobashima", "Grayback forest-firefighters", "Steven Gerrard", "three", "a bronze medal in the women's figure skating final,", "having an affair with a woman in Argentina.", "the Beatles", "At least 88", "the commission", "use of torture and indefinite detention", "\"Watchmen\" (No. 4)", "severe flooding", "Pastoral farming", "Christine Cuoco", "Hermann Ebbinghaus", "U.S. Highway Route 66", "wagner", "Black Wednesday", "Cambridge University", "in early 20th-century Europe.", "American tour", "Mickey Mouse", "margarita", "Israel", "the UNESCO / ILO Recommendation concerning the Status of Teachers"], "metric_results": {"EM": 0.5, "QA-F1": 0.6254566251319369}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.782608695652174, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.21428571428571427, 0.0, 0.7777777777777777, 0.11764705882352941, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.14285714285714285, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.2222222222222222, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_newsqa-validation-1910", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3157", "mrqa_naturalquestions-validation-8734", "mrqa_triviaqa-validation-3951", "mrqa_hotpotqa-validation-3326", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-343", "mrqa_naturalquestions-validation-7261"], "SR": 0.5, "CSR": 0.5398936170212766, "EFR": 1.0, "Overall": 0.6993068484042553}, {"timecode": 94, "before_eval_results": {"predictions": ["Lunsmann, 43, was found at Suba Kampong township on the Philippine island of Basilan", "23-year-old", "Paul McCartney and Ringo Starr", "Thirty to 40", "Larry Ellison,", "California, Texas and Florida,", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "review their emergency plans and consider additional security measures in light of Wednesday's shooting,", "4,000", "Whitney Houston", "Marines", "Lillo Brancato Jr.", "United States, NATO member states, Russia", "Charles Potter and the Order of the Phoenix\"", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "riders love the trip route, which winds through the Rockies and climbs to 9,000 feet.", "$10 billion", "opium trade", "Casa de Campo International Airport in the Dominican Republic", "Pakistan's", "her dancing against a stripper's pole.", "women.", "CNN's Campbell Brown", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Iraqi Prime Minister Nouri al-Maliki", "Alfredo Astiz,", "Sonia Sotomayor,", "Majid Movahedi,", "Reid's dismissal,", "three", "1960", "he would pay for these programs by ending the war in Iraq, reducing government waste, charging polluters for greenhouse gas emissions and ending the Bush tax cuts for wealthy individuals.", "students to engage in learning differently, enjoy a customized approach", "the first", "Larry Zeiger,\"", "Charlotte Gainsbourg and Willem Dafoe", "The public endorsement comes one day after the Register -- Iowa's largest newspaper -- backed Romney in his bid for the Republican presidential nomination", "tie salesman", "fight against terror will once against honor some of the most cherished ideals of our republic: respect for the rule of law, individual rights, and America's moral leadership.", "2005", "Two UH-60 Blackhawk helicopters", "Starr", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Ghana", "Christopher Savoie", "246", "London's Waterloo Bridge", "second-placed", "the last few months,", "84-year-old", "a nuclear weapon", "2019", "John Cooper Clarke", "around the time when ARPANET was interlinked with NSFNET in the late 1980s,", "Chicago", "kolkata", "photographer", "MGM Grand Garden Special Events Center", "The Royal Navy (RN)", "Manchester United", "Bangkok", "roof", "cana", "KXII"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6618447856094452}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.782608695652174, 1.0, 1.0, 0.14285714285714288, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.5, 1.0, 0.1904761904761905, 0.0, 0.3157894736842105, 1.0, 1.0, 0.0, 0.1212121212121212, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-985", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1131", "mrqa_naturalquestions-validation-5649", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-565", "mrqa_hotpotqa-validation-4069"], "SR": 0.53125, "CSR": 0.5398026315789474, "EFR": 1.0, "Overall": 0.6992886513157894}, {"timecode": 95, "before_eval_results": {"predictions": ["Dano-Nor Norwegian author Aksel Sandemose", "pubs, bars and restaurants", "Retina display", "Chick tract", "Julia Verdin", "uncle of Prince Philip, Duke of Edinburgh,", "to prevent the opposing team from scoring goals", "Graffiti", "ARY Digital Network", "Drifting (motorsport)", "Larry Wayne Gatlin", "1980", "Key West, Florida", "its riverside location,", "Scottish", "Skipton", "Atlas ICBM", "PPG Paints Arena", "Kelly Bundy", "John Joseph Travolta", "Love Letter", "SBS", "AVN Adult Entertainment Expo (AEE)", "119 minutes", "50 million", "Intelligent Design: The Bridge Between Science and Theology", "Sinngedichte", "Hidden America with Jonah Ray", "Irish", "Metro-Goldwyn-Mayer", "1 Squadron and 3 Squadron", "The Summer Olympic Games", "1692", "Art Deco-style skyscraper", "was drafted out of high school by the Marlins", "25 November 2015", "Hyuna", "Mathieu Kassovitz", "Kentucky Music Hall of Fame", "Louis King", "Danish", "Hindi", "two", "Mississippi", "D\u00e2mbovi\u021ba River", "24800 mi", "saint", "143,007", "Michael Edward \" Mike\" Mills", "\"New York Daily News\" and the Chicago Tribune New York News Syndicate.", "his fourth term", "2001", "a set of related data", "Marshall Sahlins", "Roberta Flack", "Sir Walter Scott", "Braille", "Caster Semenya", "Heshmatollah Attarzadeh", "London's 20,000-capacity O2 Arena.", "to be drummed out", "Wizard The DresdenFiles", "the Erie Canal", "Andy Warhol"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7548383907758907}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 0.0, 0.923076923076923, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1777", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-46", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2046", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-2013", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-8669", "mrqa_newsqa-validation-3651", "mrqa_searchqa-validation-3482"], "SR": 0.59375, "CSR": 0.5403645833333333, "EFR": 0.9615384615384616, "Overall": 0.691708733974359}, {"timecode": 96, "before_eval_results": {"predictions": ["Black Abbots", "Audrey Mossom", "Nicolas Winding Refn", "PBS", "Ed Lee", "Vic Chesnutt", "Wayne Rooney", "South Asia and the Middle East", "Martin Ingerman", "Rice University", "NCAA Division I", "Tom Rob Smith", "Reich Chancellery", "Sun Woong", "pubs, bars and restaurants", "Brad Pitt", "Milan", "in 1885", "Pac-12 Conference", "Lamar Wyatt", "Homer Hickam, Jr.", "Straits of Gibraltar", "Port Clinton", "Patrick Dempsey", "Issaquah", "Taylor Swift", "Appleby-in-Westmorland", "2000", "Mickey's Christmas Carol", "Klete Keller", "Orson Welles", "1902", "Brittany Snow", "northernmost province", "Elvis' Christmas Album", "CD Castell\u00f3n", "Coronation Street", "Zimbabwe", "Lifestyle Cities", "Tim McIntire", "1998", "Johnny Cash, Waylon Jennings", "Brad Silberling", "ITV", "1982", "the Marx Brothers film", "Tennessee", "Agent Vinod", "Daniel Richard \" Danny\" Green, Jr.", "Harrods", "2007 Formula One season", "January 2004", "toothed", "left - sided heart failure", "Karl Marx", "Laurence Olivier", "Indian Ocean", "Rodong Sinmun", "two-day, two-city charm offensive", "a Texas ranch that's home to members of a polygamist sect,", "Vatican City", "a Porch", "Eric Knight", "United Kingdom and Commonwealth countries"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7715277777777778}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2033", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-2807", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7760", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-776", "mrqa_searchqa-validation-69", "mrqa_naturalquestions-validation-9071"], "SR": 0.671875, "CSR": 0.5417203608247423, "EFR": 1.0, "Overall": 0.6996721971649484}, {"timecode": 97, "before_eval_results": {"predictions": ["New Orleans", "poker", "Budapest", "Hoppin' John", "a bird", "bass", "Campeador", "Vestal Virgins", "contract", "Akihito", "lead", "Israel", "Matthew", "Nancy Astor", "imperative", "a bald eagle", "high altitude", "Bergen", "leap year", "Little Miss Muffet", "Gila", "The Hague", "Zyrtec", "Buddhism", "Carson City", "Syria", "Cherry, Cherry", "the Council of Better Business Bureaus", "Linda Tripp", "a shooting-brake", "Aqua Teen Hunger Force", "James Webb", "economics", "Korean War", "diseases", "Rocky Mountain Fever", "euros", "Lebanese", "typewriters", "Isadora Duncan", "Jaws 2", "Custer", "nag", "Homer", "Motor Trend", "the U.S. Federal Aviation Administration", "Staten Island", "Naxos", "M.E.s", "Andorra", "Samuels", "Lord Banquo", "Agra Cantonment - H. Nizamuddin Gatimaan Express", "Paul Lynde", "John Part", "March 10, 1997", "Hubble's Hubble Space Telescope", "Household Words", "Rockland County", "Trilochanapala", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "United States, the European Union", "The first line of law and order", "Hoyo de Monterrey"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6325563108766233}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.1818181818181818, 1.0, 1.0, 0.5, 0.8571428571428571, 1.0, 1.0, 0.0, 0.6875000000000001, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-10502", "mrqa_searchqa-validation-2543", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-1155", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-2127", "mrqa_searchqa-validation-4397", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-12404", "mrqa_searchqa-validation-2730", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-9423", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-2784", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-2927", "mrqa_searchqa-validation-15833", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-3416", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-1504", "mrqa_hotpotqa-validation-2278", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-1504", "mrqa_triviaqa-validation-5852"], "SR": 0.53125, "CSR": 0.5416135204081632, "EFR": 1.0, "Overall": 0.6996508290816326}, {"timecode": 98, "before_eval_results": {"predictions": ["Scary Movie", "a mall", "piety", "TIME", "Annunciation", "the Thames", "Alyssa Milano", "drowsiness", "lily", "Alaska", "Yellowstone", "Duchamp", "Little Red Riding Hood", "Eu-", "the tongue", "the English Channel", "Michelin", "a celebration", "Simon", "hot chocolate", "vibrations", "a metronome", "gigabytes", "the Phillie Phanatic", "GILBERT & SullIVAN", "a Pringles can", "Blondes", "a Stratocaster", "the anchors", "Romeo", "a camera", "Pamela Anderson", "Trampoline", "(Alan) Vashti McKenzie Eunique Jones", "Jamaica", "Tiger Woods", "dark places", "Elton John", "a Sphinx", "Toy Story", "lump", "density", "hockey", "Heather Locklear", "Pong", "the spokes of the wheels", "the Messiah", "a crone", "John", "whole hog", "Target", "Left Behind", "71 -- 74 \u00b0 C ( 160 -- 165 \u00b0 F )", "the Devastator", "cotton", "Mary Decker", "(Donatello) Maggiore", "1868", "Leafcutter John", "University of Vienna", "Ferraris, a Lamborghini", "80 percent", "Michael Jackson", "north"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8058035714285714}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-16781", "mrqa_searchqa-validation-13276", "mrqa_searchqa-validation-8717", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-136", "mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-16364", "mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-416", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-13668", "mrqa_triviaqa-validation-6743", "mrqa_hotpotqa-validation-537", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-1683"], "SR": 0.71875, "CSR": 0.5434027777777778, "EFR": 0.9444444444444444, "Overall": 0.6888975694444445}, {"timecode": 99, "UKR": 0.689453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1673", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12845", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14027", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6260", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8234", "mrqa_searchqa-validation-8339", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_searchqa-validation-999", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6448", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7028", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-959"], "OKR": 0.83203125, "KG": 0.503125, "before_eval_results": {"predictions": ["file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Bollywood-produced \" Teen Patti\"", "Miami Beach, Florida,", "Kevin Evans", "sixth world title", "Pakistani city of Lahore.", "Air traffic delays began to clear up Tuesday evening after computer problems left travelers across the United States waiting in airports,", "Addis Ababa,", "Saturn", "piano", "the Bronx.", "two years,", "Tug boat owner Roger Rouzier", "$17,000", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Haleigh Cummings,", "The sailboat, named Cynthia Woods,", "saying Chaudhary's death was warning to management.", "violent separatist campaign", "the \" Michoacan Family,\"", "two", "exotic sports", "Bryant Purvis,", "role as a bride in the 2007 movie \"License to Wed\"", "Kurt Cobain", "Department of Homeland Security Secretary Janet Napolitano", "Gary Brooker", "E. coli bacteria", "in July", "engineering and construction", "Sri Lanka,", "a one-shot victory in the Bob Hope Classic", "Dubai", "U.S. Vice President Dick Cheney", "Rwanda", "Tehran,", "The Louvre", "Daytime Emmy Lifetime Achievement Award.", "money or other discreet aid for the effort if it could be made available,", "checkposts and military camps in the Mohmand agency,", "Somalia's piracy problem was fueled by environmental and political events.", "five", "1994", "start a dialogue of peace based on the conversations she had with Americans along the way.", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "President Obama", "The Ministry of Defense", "Wednesday.", "sons", "Michael Arrington,", "BMW 3-Series", "1973", "1975", "1546", "Kaiser Chiefs", "rivers", "Ecuador", "Afghanistan", "Keele University", "Nelson County", "the Running of The Bulls in Pamplona", "Jean-Michel Basquiat", "the U.S. Coast Guard", "silicon oxide"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7318148814012784}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false], "QA-F1": [0.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5555555555555556, 0.0, 1.0, 1.0, 1.0, 0.1, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.7499999999999999, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-2895", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-2968", "mrqa_hotpotqa-validation-3644", "mrqa_searchqa-validation-14651", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-13028"], "SR": 0.609375, "CSR": 0.5440625, "EFR": 0.96, "Overall": 0.7057343749999999}]}