{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_oewc_lr=3e-5_ep=10_lbd=500_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', ewc_gamma=1.0, ewc_lambda=500.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_oewc_lr=3e-5_ep=10_lbd=500_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4130, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Spain", "too much grief", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "underpinning", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "It says \"Adam Trask was born on a farm on the outskirts of a little town which was not far from a big town in Connecticut", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a", "unemployment benefits"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7578004807692308}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.08, 0.16666666666666666, 0.0, 0.0, 0.33333333333333337, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.703125, "CSR": 0.7734375, "EFR": 0.9473684210526315, "Overall": 0.8604029605263157}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "their animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene", "war, famine, and weather", "the western end of the second east-west shipping route", "TLC", "on the south side of the garden", "novel", "friendly and supportive", "Eero Saarinen", "Newton", "41", "that he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "Wednesday", "a Swiss French dish that consists of a big central pot of... Tapas is a very social food because diners typically get a bunch of orders... individual dishes set in the center of the table or floor for all to pick from", "the Green Hornet", "the lynch pin of a rugby team", "Danskin", "Kingston", "sanguine", "New Hampshire", "the Tennessee Valley Authority", "the American Kennel Club", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7825314153439153}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-9310", "mrqa_squad-validation-5525", "mrqa_squad-validation-6393", "mrqa_squad-validation-1529", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.71875, "CSR": 0.7552083333333334, "EFR": 0.9444444444444444, "Overall": 0.8498263888888888}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "technological superiority", "four", "San Joaquin Light & Power Building", "1972", "three", "science fiction", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "issues with technical problems and flight delays", "the United States", "trust God's word", "zeta function", "those who proceed to secondary school or vocational training", "139th", "eight", "kinetic friction force", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "private citizen", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service.", "BBC HD", "Brough Park in Byker", "Genoa", "a circle", "the Chickamauga Lake", "a brown one with gold mane", "a jet test facility, a resonant ultrasound spectroscopy lab, Faraday labs and a... The porous media group", "Gaius Maecenas", "Michael", "Sweden", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "a baseball club", "The Diary of a Young Girl", "Orwell's novel", "The Gleaners", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6909722222222222}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1662", "mrqa_squad-validation-5824", "mrqa_squad-validation-2088", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-412"], "SR": 0.65625, "CSR": 0.73046875, "EFR": 1.0, "Overall": 0.865234375}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "Lord's Prayer", "$5 million", "goxide, superoxide, and singlet oxygen", "2.666 million", "Industry and manufacturing", "violence", "Parish Church of St Andrew", "1262", "New Orleans's Mercedes-Benz Superdome", "April 1523", "radiometric isotopes stop diffusing into and out of the crystal lattice", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Monday", "Miami", "plan the physical proceedings, and to integrate those proceedings with the other parts", "the Autons with the Nestene Consciousness and Daleks in series 1, Cybermen in series 2, the Macra and the Master in series 3, the Sontarans and Davros in series 4", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "standard", "Lucas\u2013Lehmer", "Level 3 Communications", "Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "450 feet", "opera buffa", "Okinawa", "14", "the g grethra", "gated or ground potato, flour and egg", "Basin Street", "Tarsus", "Bloomingdale's", "Woody Allen", "Jane Austen", "President John F. Kennedy", "Treasure Island", "gTSi", "Charles Marion Russell", "a wine liqueur", "white", "Miss You Already", "in the 1960s", "a gilded gver", "Alistair Grant", "they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case"], "metric_results": {"EM": 0.671875, "QA-F1": 0.693029435331825}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-6791", "mrqa_squad-validation-117", "mrqa_squad-validation-4932", "mrqa_squad-validation-10140", "mrqa_squad-validation-7729", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.671875, "CSR": 0.71875, "EFR": 0.9047619047619048, "Overall": 0.8117559523809523}, {"timecode": 5, "before_eval_results": {"predictions": ["ash leaf", "75,000 to 100,000 people", "By the 1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The majority may be powerful but it is not necessarily right", "Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center in San Jose", "about one-eighth the number of French Catholics", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea to reduce its volume and increase its density", "closed", "21 to 11", "The Earth's crustal rock", "The goal of the congress was to formalize a unified front in trade and negotiations with various Indians", "two", "the network and the connected users via leased lines (using the X.121DNIC 2041)", "a separate condenser", "to the North Sea", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element", "39", "The Doctor", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "A\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the wicket", "Donner", "(G) Parker", "the New Netherland Company", "Monrovia", "Umpire", "Taiwan", "Omaha Nation", "Beniamino", "Nez Perce", "Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "(Teri) Myers", "Inchon", "February 29", "(GMAIL.COM", "Alabama", "(Svevo & Tozzi)", "Giorgio Armani", "In Britain followed the rest of the world in decimalising its currency, the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "the District of Columbia National Guard"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5818190197053456}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.5, 1.0, 0.19354838709677422, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-9640", "mrqa_squad-validation-457", "mrqa_squad-validation-2976", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-4829", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.4375, "CSR": 0.671875, "EFR": 0.9444444444444444, "Overall": 0.8081597222222222}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views, prompting Luther to write the hymn \"Ein neues Lied wir heben an\" (\"A new song we raise\")", "the Bible", "water pump", "86.66% (757.7 sq mi or 1,962 km2)", "53% in Botswana to -40% in Bahrain", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger NFL", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "a sample of some of these sculptors' work", "Judith Merril", "The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number, which is different for different packets.", "Von Miller", "weekly screenings of all available classic episodes", "a type III secretion system", "nearly 10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "oxygen concentration is too high", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "the global village", "Sun City", "Freeport, Maine", "the tapir", "auctions", "Liberty Island", "next of kin", "the American Psychiatric Association", "Lenin", "Bill Hickok", "Amtrak", "a log cabin", "The Pianist", "Patty Duke", "the king", "a Macintosh", "Richard Cory", "Homer J. Simpson", "South Africa", "a vodka & 5 oz. of grapefruit juice", "a seasick one of these alliterative creatures", "in the mountains of eastern Nevada", "Trenton", "copper", "different philosophers and statesmen have designed different lists of what they believe to be natural rights", "art", "margarita", "prostate cancer", "DNA's structure", "Pyrenees mountains"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6749539667508417}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.3, 1.0, 1.0, 0.0, 0.8750000000000001, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.29629629629629634, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2395", "mrqa_squad-validation-7473", "mrqa_squad-validation-7449", "mrqa_squad-validation-9334", "mrqa_squad-validation-87", "mrqa_squad-validation-5589", "mrqa_squad-validation-4797", "mrqa_squad-validation-8923", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-6372", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255"], "SR": 0.578125, "CSR": 0.6584821428571428, "EFR": 0.9629629629629629, "Overall": 0.8107225529100528}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "1994 Works Council Directive, which required workforce consultation in businesses, and the 1996 Parental leave Directive", "Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "Pittsburgh Steelers", "McManus", "Gemini program", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-ray", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland, terminating Tesla's relationship with Morgan", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and the Semuren", "highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "holy catholic (or universal) church", "competition", "1516", "decrease in wages", "Prudhoe Bay", "alexandrite", "cigar", "William Godwin", "Lucy Hayes", "ribonucleic acid", "Ma Joad", "Eight Is Enough", "Madrid", "Humphrey Bogart", "Foucault", "Thomas Paine", "a dna molluscs", "Fantastic Four", "G4", "LE CINEMA", "Marcus Junius Brutus", "malaria", "Ann Margret", "Hairspray", "Johann Wolfgang von Goethe", "mask", "a Greek letter society", "Spitfire floatplane", "Sherman Antitrust Act", "Hafnium", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "a last running steam-driven, paddlewheeled overnight passenger boat", "Joe Harn", "he to step down as majority leader"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5822421747979528}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.10810810810810811, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-4101", "mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-1407", "mrqa_squad-validation-1467", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.515625, "CSR": 0.640625, "EFR": 1.0, "Overall": 0.8203125}, {"timecode": 8, "before_eval_results": {"predictions": ["During the 1970s and sometimes later", "Madison Square Garden", "Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "its safaris", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "the deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "him to return to his side", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "an infection caused by corkscrew-shaped bacteria called Leptospira", "(Thor) Marvel", "a nanodevices", "tango", "a horseshoe Bay", "bamboo", "Nevil Shute", "(Septimius) Severus2", "Vlad III", "corn and cattle", "ginseng", "a Name", "Depeche Mode", "(Pepsi Benches Its Drinks)", "a generator or IPG", "Pat Sajak", "a hippopotamus", "1492", "the Madding Crowd", "(M Mikhail) Baryshnikov", "Mars", "the Boston Massacre Trials", "a bee", "a Hardmode gun", "Milan", "the Battle of Puebla", "( Peggy Lee) Lee", "Carl Sagan", "In February 2011, while overseas, she discovered that she was pregnant.", "General Paulus", "John Ford", "CNN", "from a donor molecule to an acceptor molecule.", "Sylvester Stallone", "The Mongol - led Yuan dynasty ( 1271 -- 1368 )"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5769717261904762}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false], "QA-F1": [0.5714285714285715, 0.0, 0.16666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-6815", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6321"], "SR": 0.484375, "CSR": 0.6232638888888888, "EFR": 1.0, "Overall": 0.8116319444444444}, {"timecode": 9, "before_eval_results": {"predictions": ["the Metropolitan Police Authority", "Francis Marion", "all \"trading rules\" that are \"enacted by Member States\"", "the first Block II CSM and LM", "the Tangut relief army", "five", "governmental", "the Great Yuan", "Jordan Norwood", "immune system to mount faster and stronger attacks each time this pathogen is encountered", "more than 70", "movements of nature, movements of free and unequal durations", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "2,100,000 sq mi", "an adjustable spring-loaded valve", "classical position variables", "a science fiction novel", "an alleged robbery", "George Jetson", "Deus", "an arboretum", "pommel horse", "William McKinley", "PSP", "Daphne du Maurier", "Turkish", "antonyms", "a wren", "the American Revolution", "Morrie Schwartz", "the periodic table", "Mercury and Venus", "Tokyo", "an entry-level restaurant job", "a gorillas", "the Pentagon", "oats", "I Love You", "China", "Gone With the Wind", "A Delicate Balance", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "insulin", "the mid-1990s", "Hudson Bay", "Dr Ichak Adizes", "Melpomene", "Boston Bruins", "James Lofton", "can't afford to pay for cable or satellite TV service.", "gunned down four Lakewood, Washington, police officers"], "metric_results": {"EM": 0.5, "QA-F1": 0.587190092383107}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.16666666666666669, 0.9333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4329", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_squad-validation-4402", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-8236", "mrqa_naturalquestions-validation-4124", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.5, "CSR": 0.6109375, "EFR": 1.0, "Overall": 0.80546875}, {"timecode": 10, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.884765625, "KG": 0.46953125, "before_eval_results": {"predictions": ["Mike Figgis", "1.7 billion years ago", "Dordtse Kil", "flight delays", "the fact (Fermat's little theorem) for any n if p is a prime number", "Virgin Media", "he would be killed through overwork.", "Times Square Studios", "Philip Webb and William Morris", "service to the neighbor in the common, daily vocations of this perishing world", "Amtrak San Joaquins", "refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "\"physical control or full-fledged colonial rule\"", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "large protein complexes about 40 nanometers across", "photoelectric", "Peggy", "the plant tissue", "Memoirs of a Geisha", "stability control", "a bolt-action", "Black Death", "silicon", "Sidfodr", "the Cenozoic", "the Horn of Africa", "Reddi-wip", "Jeopardy", "tea", "Larry Fortensky", "the gas surge", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time & 1936", "the Jeffersons", "the Sopranos", "The Crucible", "Muhammad Ali", "Impressionists", "Willa Cather", "Aida", "The Strange & Curious Tale of the Last True Hermit", "the Burgundy wine region", "the Right to Free Expression", "to remove all the stuck", "zero", "Australian & New Zealand", "Maine", "Doug Diemoz", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "Hal Ashby", "John Ford", "119", "the Vigor, Prelude, CR-X, and Quint", "a skilled hacker could disrupt the system and cause a blackout.", "Frank Ricci"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6692640692640692}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-9023", "mrqa_squad-validation-1326", "mrqa_squad-validation-9734", "mrqa_squad-validation-8839", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-3608"], "SR": 0.578125, "CSR": 0.6079545454545454, "EFR": 1.0, "Overall": 0.7397159090909091}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "to connect more than 100 universities and research and engineering institutions via 12 national points of presence with DS-3 (45 Mbit/s)", "allowing the lander spacecraft to be used as a \"lifeboat\"", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79 episodes are missing", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock", "oppidum Ubiorum", "studio 5", "1.7 million", "August 4, 2000", "Abu Zubaydah", "don't have to visit laundromats", "Bob Dole", "1959", "hackers", "three men with suicide vests who were plotting to carry out the attacks", "137", "the green grump", "Opryland", "Asashoryu", "Conway", "How I Met Your Mother", "13 and 15", "the insurgency", "Chinese", "war", "war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "San Simeon", "the women are fighting with each other", "Rev. Alberto Cutie", "blind", "the military commissions", "opium", "Obama's race", "named his company Polo", "antiquities robbers", "Arabic, French and English", "inducted into the Baseball Hall of Fame in July.", "seven", "Roberto Micheletti", "Abu Sayyaf", "four", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings were shown to jurors Thursday in the trial", "Democrats and Republicans", "the 15th century", "1966", "J. S. Bach", "Brainy", "Fitzroya", "Stephanie Plum", "Sweeney", "Andorra", "Uncle Tom's Cabin"], "metric_results": {"EM": 0.5, "QA-F1": 0.5694561795434957}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.11428571428571427, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.625, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.09090909090909091, 1.0, 0.0, 0.058823529411764705, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-7659", "mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-8561"], "SR": 0.5, "CSR": 0.5989583333333333, "EFR": 0.96875, "Overall": 0.7316666666666667}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "prime", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "between Pyongyang and Seoul", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi,", "Suwardi, the village leader of Karas in East Java.", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi,", "U.S. senators", "died peacefully with Mildred and two other females. Breeders are hoping he'll show interest in Lucy, who is about the same age as Mildred, later this year.", "Muslim", "California, Texas and Florida", "Robert De Niro", "Argentina", "Three searches are planned for Monday, said Coast Guard spokesman Ricardo Castrodad.", "creation of an Islamic emirate in Gaza", "near Garacad, Somalia", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "anti-U.S. video, an American al Qaeda member makes reference to his Jewish ancestry for the first time in an official al Qaeda message.", "would try to refile charges against al-Qahtani based on later interrogations that did not have sought to participate, possibly as the \"20th hijacker.\"", "Apple employees", "green-card warriors", "Haiti", "Buster Keaton", "test-launched a rocket capable of carrying a satellite,", "Nieb\u00fcll", "Juan Martin Del Potro.", "would score him even stronger political points, would be to make the tax credits, or the denial of them, qualitative as well as quantitative, and thus support the arts in the most measurable way possible.", "in Seoul,", "John Wayne", "Afghanistan", "seven", "Swedish Prime Minister Fredrik Reinfeldt", "Fix You", "Tom Brady", "Ytterby", "George III", "Philadelphia", "Alien Resurrection", "Fester", "Moscow", "The equestrian program"], "metric_results": {"EM": 0.578125, "QA-F1": 0.634765753467379}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.052631578947368425, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0588235294117647, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.578125, "CSR": 0.5973557692307692, "EFR": 0.9629629629629629, "Overall": 0.7301887464387464}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I,", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation were in error.", "5 nanometers across, arranged in rows 6.4 nanometers apart,", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "City of Edinburgh Council.", "Osama's son,", "Israel", "Hearst Castle", "\"Larry King Live.\"", "Al Gore.", "Quebradillas.", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane,", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "iPhone 4S news,", "Pakistan's largest city of Karachi.", "John McCain", "South Africa", "2006", "Iran's nuclear program.", "North Korea,", "Sunday,", "police car sits outside the Westroads Mall in Omaha, Nebraska,", "Haeftling,", "i report form", "Kurt Cobain", "Nkepile Mabuse", "\"happy ending\" to the case.", "San Diego,", "tie salesman", "At least 40 people in the United States die each year as the result of insect stings,", "$1,500", "25", "137", "suppress the memories and to live as normal a life as possible;", "Coptic Christians and Muslims", "poor", "Tom Hanks,", "ancient Egyptian antiquities in the world,", "27-year-old", "165-room", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Lydia", "Kansas", "September", "modern dance", "Melanie Owen", "Lusitania", "spherical", "Coronation Street", "Turkey, Saudi Arabia, and Pakistan"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5588936237373737}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.3636363636363636, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9333333333333333, 0.6666666666666666, 0.0, 0.0, 0.6, 0.0, 1.0, 1.0, 0.8, 1.0, 0.25, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2009", "mrqa_squad-validation-8869", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_hotpotqa-validation-5850", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.4375, "CSR": 0.5859375, "EFR": 0.9722222222222222, "Overall": 0.7297569444444445}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart,", "between September and November 1946,", "$2.50 per AC horsepower royalty", "1990s", "organic", "Stagg Field", "2010", "Reuben Townroe", "\"it would appear to be some form of the ordinary Eastern or bubonic plague\"", "a water pump", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "Bangladesh's southern Bhola district.", "At least 88", "the U.S.", "Inter Milan", "98", "as soon as 2050, some scientists say.", "not a quota case or (an) affirmative action case.", "The Ski Train", "severe", "The six bodies were found Saturday at about 6:30 p.m.", "Stella McCartney,", "Elspeth Cameron-Ritchie,", "homicide", "\"surge\" strategy he implemented last year.", "the port remains shut down, and desperately needed aid cannot be unloaded quickly.", "voice-assistant software", "Tim O'Connor,", "impeachment", "Kearny, New Jersey", "Thessaloniki and Athens,", "New York-based Human Rights Watch", "Twitter", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces by Sunday,", "genocide", "genocide, crimes against humanity, and war crimes.", "The oldest documented bikinis", "Fullerton, California,", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"Don't Ask, Don't tell\"", "Consumer Reports", "a woman", "Sheikh Abu al-Nour al-Maqdessi,", "the remaining rebel strongholds in the north of Sri Lanka,", "The Everglades,", "six-year veteran of the museum's security staff,", "\"The mysterious disappearance of Flight AF 447 over the Atlantic Ocean has fueled speculation among aviation experts about what caused the state-of-the-art airliner to come down.", "ninth w\u0101", "Magnavox Odyssey", "William Tell", "robin", "Kent Hovind", "The Guest", "\"Longview\"; \"Welcome to Paradise\"; \"Basket Case\"; \"When I Come Around\"", "a skull", "2019", "6 January 793"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5118880965847877}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0588235294117647, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.4375, "CSR": 0.5760416666666667, "EFR": 1.0, "Overall": 0.7333333333333334}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966,", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education.", "proportionally to the number of votes received in the second vote of the ballot using the d'Hondt method.", "North", "Mohammed Mohsen Zayed,", "they are \"still trying to absorb the impact of this week's stunning events,\"", "Lisa Polyak,", "Friday,", "CNN affiliate WFTV.", "The cause of the deaths has not been determined,", "the station", "sculptures", "Atlantic Ocean.", "the 725-mile Veracruz", "200.", "Greece,", "Patrick McGoohan,", "his parents", "$627,", "27-year-old's", "Virgin America", "\"I think the Camry gets a bad rap for being the'microwave oven' of the car industry,\"", "a secret society of two.", "Paktika province in southeastern Afghanistan,", "at my undergrad alma mater, Wake Forest,", "Sporting Lisbon", "tie salesman", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Secretary of State Hillary Clinton,", "the universe formed by analyzing particle collisions.", "10 below in Chicago, Illlinois.", "\"She was focused so much on learning that she didn't notice,\"", "President Obama announced Wednesday that Shah, the 36-year-old administrator of the U.S. Agency for International Development,", "\"Dancing With the Stars.\"", "1 million", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "1.2 million", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "his mother.", "pigs", "Matt Flinders", "Isar", "East of Eden", "Sam Bettley", "14 directly elected members, 12 indirectly elected members representing functional constituencies and 7 members appointed by the chief executive.", "the Sea of Galilee", "honey", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6523256531480216}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.5, 0.7272727272727273, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 0.04761904761904762, 0.33333333333333337, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.10526315789473685, 1.0, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-5573"], "SR": 0.5625, "CSR": 0.5751953125, "EFR": 1.0, "Overall": 0.7331640625}, {"timecode": 16, "before_eval_results": {"predictions": ["np\u2261n (mod p)", "adjustable spring-loaded valve,", "George Low", "Synthetic aperture radar (SAR) and Thematic Mapper (TM)", "A fundamental error", "recant his writings", "diversity", "one can include arbitrarily many instances of 1 in any factorization,", "136,", "union membership", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "Rima Fakih", "apology news conference.", "2nd Lt. Holley Wimunc.", "1918-1919.", "Ben Kingsley", "U.S. Holocaust Memorial Museum,", "Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan Kean,", "William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "software magnate", "U.S. senators who couldn't resist taking the vehicles for a spin.", "Ninety-two percent", "Larry Ellison,", "Taher Nunu", "Obama", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "(Vera Zvonareva of Russia and Austria's Daniel Koellerer", "Caylee Anthony,", "because its facilities are full.", "25 dead", "more than 200.", "a paragraph about the king and crown prince that makes it illegal to defame, insult or threaten the crown.", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South African", "Seoul,", "Haiti", "The United States", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "Daytime Emmy Lifetime Achievement Award.", "Republican", "\" Teen Patti\"", "Eleven people died and 36 were wounded in the Monday terror attack,", "Hugo Chavez", "Four bodies", "translocation Down syndrome", "starch", "the United Kingdom of Great Britain and Northern Ireland", "Diptera", "100th anniversary of the first \"Tour de France\" bicycle race,", "is a reference to the BBC teletext service Ceefax.", "fibrous tissue", "Johannes Brahms", "17th century.", "Orson Welles."], "metric_results": {"EM": 0.53125, "QA-F1": 0.63763999158736}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 0.8, 0.0, 0.6153846153846153, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.06666666666666668, 1.0, 0.5, 1.0, 1.0, 1.0, 0.10526315789473682, 1.0, 0.0, 0.6666666666666666, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3975", "mrqa_squad-validation-4509", "mrqa_squad-validation-2788", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_searchqa-validation-2260", "mrqa_hotpotqa-validation-4478"], "SR": 0.53125, "CSR": 0.5726102941176471, "EFR": 1.0, "Overall": 0.7326470588235294}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "a multi-cultural city", "the father of the house", "John Fox", "US$1,000,000", "Annual Conference", "Colonel Monckton", "thermodynamic", "CNN Moscow Correspondent", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "a remote part of northwestern Montana", "Iran's President Mahmoud Ahmadinejad", "South Africa", "Michael Jackson's father wants a judge to order the pop star's estate to pay him a monthly allowance,", "Sunday", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Lousiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2006,", "the FBI.", "as many as 250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "allegations that a dorm parent mistreated students at the school.", "Pakistan", "Columbia, Illinois,", "\"I never thought any of this was going to be easy,\"", "Pittsburgh", "heavy flooding and scattered debris.", "Oxbow,", "Dolgorsuren Dagvadorj,", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "Alfredo Astiz,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "more than 15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford,", "Ginger Rogers", "five", "Marine Corps", "Garfield", "pickpocket", "seven", "a vigorous deciduous tree", "a transistor"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7848338293650794}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-3601", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425"], "SR": 0.6875, "CSR": 0.5789930555555556, "EFR": 1.0, "Overall": 0.7339236111111112}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline,", "specialty pharmacy", "Doctor of Theology", "God's", "The Prince of P\u0142ock", "multi-stage centrifugal pumps", "Pet Sounds", "40", "Arthur Sarsfield Ward", "Aug 24,", "frax", "a sperm whale", "Tiriac", "Elijah", "Jeffrey Archer", "C N Trueman", "Anne Boleyn", "Golda Meyerson", "a fur hat", "fraxemy\u015bl, Austria-Hungary (today part of Poland)", "Thai", "Parsley", "Japan", "Runic", "a Volkswagen", "Patrick Murray", "blancmange", "fraadina", "frattage", "recorder", "fravelin weigh less,", "Microsoft", "Norway", "Isambard Kingdom Brunel", "Edward Lear", "portugal", "Francis Ford", "Petronas", "Beyonce", "Microsoft", "Otto I", "fraaseodymium", "The Battle of the Three Emperors", "Pacific", "Trimdon,", "Midnight Cowboy", "Surrealist", "FIFA World Cup 2010", "Southwest Airlines", "Afghanistan", "Matt Jones", "Rolf G\u00fcnther and Wilhelm Pfannenstiel", "3 May 1958", "Ewan McGregor", "off Somalia's coast.", "cannibalism", "frax", "Ford Motor Company", "Lake Louise Ski Resort", "a calves"], "metric_results": {"EM": 0.4375, "QA-F1": 0.49739583333333326}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-3824", "mrqa_naturalquestions-validation-4731", "mrqa_hotpotqa-validation-5049", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-16342", "mrqa_searchqa-validation-3267"], "SR": 0.4375, "CSR": 0.571546052631579, "EFR": 1.0, "Overall": 0.7324342105263157}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Wi-Fi hotspot functionality, Power-line and Bluetooth connectivity and a new touch-sensitive remote control", "ash tree", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands,", "Tony Blair", "The Flintstones", "911", "Jonathan Swift", "South Sudan", "Maria Bueno", "bowls", "Frankie Laine", "July 28, 1948", "Thor", "bulgaria", "Preston", "a bear suit", "nucleic acids", "Montr\u00e9al", "Adidas", "slainte", "Rocky & Bullwinkle", "dill waterman", "austerh", "jastarnia", "cavalry Jones", "jediel Sousa", "Hyde Park", "Sydney", "Alabama", "jura", "cavalry", "finger", "a meteoroid", "Norman Brookes", "bobbyjo", "lola", "bodhidharma", "Klaus dolly", "Albert Reynolds", "a hook", "Baltic Sea", "Singapore", "cathead", "yellow", "murray Mix", "Vespa", "Squamish", "an annual income of US $11,770", "Theme Park World", "Cape Cod", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "10", "867-5309", "dill", "dillette", "the small intestine", "thumbini"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5323660714285715}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.4, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.4, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-3139"], "SR": 0.4375, "CSR": 0.56484375, "EFR": 1.0, "Overall": 0.7310937500000001}, {"timecode": 20, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.916015625, "KG": 0.47578125, "before_eval_results": {"predictions": ["red algal derived", "non-specific", "1525\u201332", "a few", "solution", "2011", "random noise", "Wardenclyffe", "aouda", "indiana", "the Washington Post", "honshu", "Steve Biko", "pewter", "a pennsylvanica", "acute", "nellig", "anhydrides", "Beyonce", "Norman Mailer", "Oliver!", "kunsky", "Bolton", "Sandwich", "tsarevitch", "government", "junk", "Hartford", "your Excellency", "George III", "Lincoln", "Severn", "cairn", "Leonard Nimoy", "USVI", "diana humbert", "Jesse Garon Presley", "komando Pasukan Khusus", "lithium", "40", "The Duchess", "Nick Owen", "white", "China", "Salt Lake City,", "Perseus", "Capricorn", "match Rugby", "Sergio Garcia", "meadow brown", "lewitted, bald man", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Amazon.com", "Department of Homeland Security", "cantaloupes", "heartbreak", "leopard", "Wes Craven", "Australian", "King Kelly"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5446969696969697}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.7272727272727273, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2513", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3117", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3822"], "SR": 0.46875, "CSR": 0.5602678571428572, "EFR": 0.9705882352941176, "Overall": 0.738827468487395}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "the Florida legislature", "gold", "Chinese", "Surrey", "tESLAR Satellite", "Restless Leg Syndrome", "Buzz Aldrin", "saint Timothy", "Niger", "Backgammon", "Instagram", "Home alone 2: Lost in New York", "Columbus", "t.S. Eliot", "Venus", "Bob Marley & the Wailers", "Crusades", "topham Chase", "curb-roof", "Angela", "Danae", "tchaikovsky", "Socrates", "uranium", "Stephen King", "horse", "Catskill Mountains", "pumas", "wirings", "fluid", "Jordan", "jerry huggins", "London", "Husqvarna", "poland", "Every Good Boy", "forehead", "pashana Bedhi", "sacrament of Holy Communion", "100 years", "sugar", "Washington, D.C.", "Piccadilly Circus", "crocus", "Melbourne, Victoria, Australia", "meadowbank", "Tangled", "Vincent Motorcycle Company", "Melissa Duck", "inner core", "novella", "The Prodigy", "John Anthony \"Jack\" White", "Michelle Rounds", "21-year-old", "jig", "Daytona Beach", "tony blair", "Mickey's PhilharMagic", "hiphop"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5329861111111112}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-1488", "mrqa_searchqa-validation-13792", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-550"], "SR": 0.46875, "CSR": 0.5561079545454546, "EFR": 1.0, "Overall": 0.7438778409090909}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "drafted into the Austro-Hungarian Army in Smiljan", "63,754", "faith in Christ", "Ticonderoga Point", "seal", "in Season 4", "tywin", "( 1972 -- 81 )", "Dottie West", "October 1980", "Tim Allen", "Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "2018", "Malibu, California", "Gregor Mendel", "Baltimore, Maryland", "first to develop lethal injection as a method of execution", "Battle of Antietam and Lincoln's Emancipation Proclamation", "Paspahegh Indians", "left atrium and ventricle", "Mayflower", "1560s", "Davos", "Prince James, Duke of York and of Albany", "jazz", "2008", "U.S. service members who have died without their remains being identified", "March 16, 2018", "Narendra Modi", "Sohrai", "explosion", "feeling I need to walk with / Tell me why I can't be there where you are", "Annette", "The season was ordered in May 2017", "Hathi Jr", "ABC", "cell nucleus", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "james garner", "between 8.7 % and 9.1 %", "Tip and Ty", "37.7", "Flag Day in 1954", "from 1922 to 1991", "neil helfgott", "jesse", "oromia", "Mountain West Conference", "Sydney", "yasiin Bey", "look at how the universe formed by analyzing particle collisions.", "five female pastors", "returning combat veterans", "The Mill on the Floss", "Greenland", "cherry bombs"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5642926517926518}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.7272727272727272, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.07692307692307693, 1.0, 0.2222222222222222, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1232", "mrqa_squad-validation-2919", "mrqa_squad-validation-2373", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9295", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6295", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-5728"], "SR": 0.46875, "CSR": 0.5523097826086957, "EFR": 1.0, "Overall": 0.7431182065217391}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "vicious and destructive", "60%", "girls", "in the 1980s", "picturebook Shiji no yukikai", "almost 3,000", "Chinese flower shop", "T'Pau", "Bud Light", "comedy web television series", "Universal Pictures and Focus Features", "LED illuminated", "committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "in the eye", "IBM", "Felicity Huffman", "Djokovic", "84", "the United States economy", "Wales and Yorkshire", "In 1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Nalini Negi", "very important", "in the United States", "Jodie Foster", "head of state and the head of government of Zambia", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "meaning", "Massillon, Ohio", "African - Americans", "giant planet", "the RAF", "15,000 BC", "New York City", "Egypt", "20 July 2015", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "ozone", "Pakistan", "Sam Raimi", "7 October 1978", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "ethiopia", "wiki", "gaffer"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6433035714285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.0, 1.0, 0.19999999999999998, 0.8, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9047619047619047, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-79", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.546875, "CSR": 0.5520833333333333, "EFR": 0.9310344827586207, "Overall": 0.7292798132183907}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors", "a three-stanza confession of faith prefiguring Luther's 1529 three-part explanation of the Apostles' Creed in the Small Catechism", "April 20", "Tanzania", "March 29, 2018", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "1928", "the ruling city of the Northern Kingdom of Israel, Samaria", "in northern China", "Missouri River", "Harry", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1950", "May 3, 2005", "David Hemmings as Nigel", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "1997 ( XXXII ), 1998 ( XXXIII ), 2015 ( 50 )", "Cody Fern", "22 November 1970", "Reveille", "2007", "Camping World Stadium", "Aldis Hodge", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "Apostle James", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "a single, very long DNA helix on which thousands of genes are encoded", "in either Tagalog or English", "R.E.M.", "a blend of ground beef and other ingredients", "Juliet", "The United States became strongly opposed to H\u1ed3 Ch\u00ed Minh", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "rocks and minerals", "an diffuse system of small concentrations of lymphoid tissue found in various submucosal membrane sites of the body", "following the 2017 season", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "its vast territory was divided into several successor polities", "in the Tremont neighborhood of Cleveland, Ohio", "a hooker and addict", "Kingsholm Stadium and Sandy Park", "Ahmad Given ( Real ) and Kamal Givens ( Chance )", "a man who could assume the form of a great black bear", "Robert Anthony Plant", "beetle", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Bow River", "41,", "Fareed Zakaria", "Afghan National Security Forces", "John Cotton", "a Welch rabbit", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6051685461715703}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.358974358974359, 0.0, 0.7741935483870968, 1.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_triviaqa-validation-6864", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.484375, "CSR": 0.549375, "EFR": 0.9696969696969697, "Overall": 0.7364706439393939}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "every day", "6.4 nanometers", "1894", "an small portion of the population lives off un earned property income by virtue of ownership titles in capital equipment, financial assets and corporate stock", "Atlanta, Georgia", "Thunder Road", "acidifying particles and gases", "Bette Midler", "gathering money from the public", "the pyloric valve", "chesu", "Julia Ormond", "synovial", "The Satavahanas", "March 16, 2018", "Hathi Jr", "by capillary action", "twice", "Asuka", "when matching regions on matching chromosomes break and then reconnect to the other chromosome", "chesua", "the Lower Mainland in Vancouver", "electronic computers", "notorious Welsh pirate Edward Kenway", "Madison, Wisconsin, United States", "war with the United States", "May 26, 2017", "1981", "USS Chesapeake", "in arcade mode -- an offline single player or local co-op where players can choose which side to play on and which battle to play in", "repudiation, change of mind, repentance, and atonement", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Mad - Eye Moody", "Boris Becker", "without deviating from basic strategy", "Burnham Beeches in Buckinghamshire", "1898", "Clarence Anglin", "April 1st", "12.65 m ( 41.50 ft ) long", "the Northeast Monsoon or Retreating Monsoon", "Michael Crawford", "1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "the 17th episode in the third season of the television series How I Met Your Mother", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "a variety of ancient herding dogs, some dating back to the Roman occupation, which may have included Roman Cattle Dogs, Native Celtic Dogs and Viking Herding Spitzes", "Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley", "Venezuela's", "a national telephone survey of more than 78,000 parents of children ages 3 to 17", "Ohio", "Edward VI", "new Orleans, La"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5268381411704467}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.07142857142857144, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.6666666666666666, 1.0, 0.1290322580645161, 0.5555555555555556, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-3160", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-4972"], "SR": 0.421875, "CSR": 0.5444711538461539, "EFR": 0.972972972972973, "Overall": 0.7361450753638253}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "99", "already-wealthy individuals or entities", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "England", "virtual reality simulator", "the five - year time jump", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "20th Century Fox", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions", "the outside world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "Ben Rosenbaum", "Zilphia Horton", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "President since creation of the office in 1789", "the liver and kidneys", "the lumbar enlargement and the conus medullaris", "tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the 1928 national games", "Geoffrey Zakarian", "Tommy James", "Georgia", "Bonnie Aarons", "March 31, 2018", "Jay Baruchel", "De Waynene Warren", "2004", "rearview mirror", "Portuguese and Spanish - French origins", "2015", "The terrestrial biosphere", "1937", "the 2017 season", "Beijing", "the court from its members", "convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "75", "Famous Players-Lasky Corporation", "Tiffany & Company", "vice-president", "villanelle", "The Arkansas weatherman", "a man's lifeless, naked body", "four months ago", "magnesium", "Captain Christopher Newport", "rotunda"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6022194634068648}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.5283018867924527, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.9, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.4, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.6976744186046512, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7547", "mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.4375, "CSR": 0.5405092592592593, "EFR": 0.9166666666666666, "Overall": 0.7240914351851852}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "Colonel (later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "Seminole", "one out of every 17 children under 3 years old in America", "Tuesday", "Dan Parris, 25, and Rob Lehr, 26,", "the estate with its 18th-century sights, sounds, and scents.", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "\"we have more work to do,\" including on the issue of bullying.", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels", "step up", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "Juliet", "a Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "demolition crews blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "a national telephone survey", "not speak", "Kgalema Motlanthe", "Ankara", "Bill Stanton", "humans", "Herman Thomas", "Bayern", "a lightning strike", "Deputy Treasury Secretary", "St. Louis, Missouri,", "Arizona", "hundreds", "al Qaeda,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "the southern city of Najaf.", "the 11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "two U.S. filmmakers", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Briton Carl Froch", "Abdullah Gul,", "1979", "Heshmatollah Attarzadeh", "Richard Masur", "Jughead Jones", "Sarah Josepha Hale", "1998", "violinist.com", "to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Al Capone", "the Grecian style", "shrimp", "corals, and hydras"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6227336701821198}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.23529411764705882, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.4, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.515625, "CSR": 0.5396205357142857, "EFR": 1.0, "Overall": 0.7405803571428571}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars,", "Nepali", "German, arithmetic, and religion", "President Sheikh Sharif Sheikh Ahmed", "Africa", "two Manchester, England shows", "Illinois Reform Commission", "gasoline", "Union Station in Denver, Colorado.", "Dolgorsuren Dagvadorj", "It does not grant full health-care coverage, which would require an act of Congress,", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain", "Iranian consulate,", "The Casalesi Camorra clan", "President Clinton.", "he regrets describing her as \"wacko.\"", "Adenhart", "people left without loved ones, without homes, without life's belongings.", "education", "environmental", "2009", "problems with the way Britain implements European Union employment directives.", "France's", "More than 15,000", "He won it with an organization that even opponents called brilliant.", "0-0 draw", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam,", "Kim Clijsters.", "his wife,", "Zed,", "to acquire nuclear weapons", "Sharon Bialek", "Kurdish militant group in Turkey", "military veterans", "41,", "millionaire's surtax,", "Sabina Guzzanti", "Booches Billiard Hall,", "15,000", "Nearly eight in 10", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "the presence of correctly oriented P waves on the electrocardiogram ( ECG )", "the Italian pignatta", "early 1974", "rugby", "rabies", "Parkinson's disease", "The 254th episode overall,", "Disha Patani", "Anah\u00ed", "British Labour Party", "The Passing of Arthur,", "witchcraft"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6628397644022643}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.5714285714285715, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.20512820512820515, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1211", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-2678", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.5625, "CSR": 0.5404094827586207, "EFR": 0.9642857142857143, "Overall": 0.733595289408867}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "quarterback Denver Broncos", "teach by rote", "treats as a way to introduce those unfamiliar with a vegan diet to some of the flavorful foods they can eat.", "\"Dance Your Ass Off.\"", "Charles H.W. Bush", "Charles Schrenker exited his small plane before it crashed, and investigators are looking for him, police say.", "Almost all British troops in Iraq are being pulled out because the agreement that allows them to be there expires", "Jacob Zuma,", "Simon Cowell.", "jazz", "\"falling space debris,\"", "Obama's", "killed 10 and wounded 30 in Quetta, the capital of Balochistan province,", "Monday night", "prison inmates.", "Franklin, Tennessee,", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "the coalition", "sexual assault on a child.", "Brian David Mitchell,", "Christmas", "football", "consumer confidence", "Republican", "stuck in Cuba and federal immigration detention centers.", "Dean Martin, Katharine Hepburn and Spencer Tracy", "vitamin injections that promise to improve health and beauty.", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "Paul Ryan", "top designers, such as Stella McCartney,", "about 5:20 p.m.", "think themselves as an \"extermination\" force that works as the armed front \"of the people and for the people.\"", "Darrel Mohler", "Casalesi clan", "Obama and McCain camps", "Sen. Barack Obama", "steep embankment in the Angeles National Forest", "more than 30 Latin American and Caribbean nations", "Empire of the Sun", "30-minute recorded message", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "deployed in major cities aross Italy since the early summer.", "Monday,", "Ghana", "Caylee's", "Charles Shannon announced at a news conference that the U.S. delegation would stay another day.", "killed in a crash in Fullerton, California,", "the coroner's office in Toronto", "they each supported major regional wars known as proxy wars", "annually in late January or early February", "Galileo Galilei", "Zeus", "paper sales company", "Chancellor Christian Kern", "Indianola, Mississippi", "Wayne County, Michigan", "Diff'rent Strokes", "Akihito", "Charles Parker and the Vicious Circle"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5371512112823611}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.14285714285714285, 0.4, 0.0, 0.0, 0.34782608695652173, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.6666666666666666, 0.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444444, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.923076923076923, 0.5454545454545454, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2105263157894737, 0.0, 0.4444444444444444, 1.0, 0.0, 0.5, 0.8, 0.0, 0.8, 0.0, 0.4, 0.28571428571428575]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-2389"], "SR": 0.390625, "CSR": 0.5354166666666667, "EFR": 1.0, "Overall": 0.7397395833333332}, {"timecode": 30, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.861328125, "KG": 0.4953125, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "67.9", "letters between pen-pals", "8 E 3rd St. Wendell,", "Queen Mary II", "Wembley Stadium", "catfish", "Google", "(IE1)", "HIV", "a chela", "BIG 3 (Fox) 1940", "The Last Starfighter", "(to)", "the House of Romanov", "a mirror", "fermentation", "Godot", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Hawaii", "Minnesota", "a bad one", "Han Solo", "Gutzon Borglum", "( Katharine) of Aragon", "Mopsos", "St. Mark", "Oklahoma", "Salman Rushdie", "the United Nations Organisation", "Tycho Brahe", "The Monkees", "Montana", "elephants", "cloister", "\" Mail to the Chief\"", "(D.G.Kunjab)", "Idiot's DOS", "Clue", "Heath", "(Lovely Rita meter) meter", "Ellen Wilson", "dioxins and hexachlorophenol", "tornado", "Omaha, Nebraska", "The Greatest Gift", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Popowo", "Bobby Kennedy", "Mercury", "I Gotta Rash/We Are Thee Goblins", "Niveda Thomas", "1975", "four people believed to be illegal immigrants", "CEO of an engineering and construction company", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5552083333333333}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5951", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-13866", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-4424", "mrqa_hotpotqa-validation-1834", "mrqa_newsqa-validation-1432"], "SR": 0.484375, "CSR": 0.5337701612903225, "EFR": 1.0, "Overall": 0.7230040322580644}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "the quotient", "Carson Palmer", "hail", "Venezuela", "Florida", "the Hippocratic Oath", "Queen Latifah", "Golden Retriever", "Shropshire", "the Aegean Sea", "nails", "the 10th", "Sinclair Lewis", "Crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "conscientious", "The Trans Alaska Pipeline", "trout", "the 13th", "Dixie Chicks", "Carl Bernstein", "a buffalo", "America", "Istanbul", "Crazy Horse", "glare", "Rehab", "the Golden Hind", "Administrative Professionals Week", "New York", "Van Halen", "a black bear", "dams", "Djibouti", "pyrite", "a cyclone", "Ted Morgan", "Cashmere", "Diana", "spilled milk", "grasshopper", "Carat", "Robin Hood", "White Cliffs", "... yang", "September 29, 2017", "almost entirely in Wake County", "July 1790", "Nicolas Sarkozy", "the Republican", "a Breve", "Rabies", "Environmental Protection Agency", "Bob Gibson", "Mogadishu", "45 minutes, five days a week", "400 years"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7801091269841269}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5555555555555556, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5714285714285715, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-171", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-4751", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.703125, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.7240624999999999}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuro immune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "a tornado", "the Taj Mahal", "plantain", "grilling", "John", "London", "The Andy Griffith Show", "the Bahamas", "the Mediterranean", "Celsius", "Janet Reno", "the Spanish American War", "Seinfeld", "corticosteroids", "Atlantic City", "John Galt", "President George W. Bush", "Iraq", "the taro", "\"without worries\"", "Champion", "Chchaikovsky", "Malle Babbe", "the Stone Age", "\"Snowy Landscape\"", "a Champion", "Louis XIII", "it wasn't meaty enough", "Prince Charles", "the Heart", "whiskers", "a lighter", "Elmer", "the Cretaceous", "Peggy Fleming", "Panama", "the metric system", "the United Kingdom", "Castle Rock Entertainment", "fuchsia", "the Sahara", "George W. Bush", "a alcoholic student", "\" Buzz\" Windrip", "Daphne du Maurier", "\"Airplane\"", "King Willem - Alexander", "the New England Patriots", "a cerebral vascular accident ( stroke ), or head trauma ; however, these are not the only possible causes", "Damon Albarn", "the duchy of Mazovia", "Ken Burns", "the Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "between South America and Africa.", "four decades"], "metric_results": {"EM": 0.34375, "QA-F1": 0.43023538961038965}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.7142857142857143, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-16617", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-12745", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-14396", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16407", "mrqa_naturalquestions-validation-3840", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-4806", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.34375, "CSR": 0.5331439393939394, "EFR": 1.0, "Overall": 0.7228787878787879}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual", "echinacea", "poker", "Salmon", "Kenya Airways", "the Bronze Age", "Sulphur Island", "Thomas Merton", "ex-wife", "the (sacred) limb", "Rodeo Drive", "It\\'s A Mad Mad Mad World", "77.7", "Dunkin' Donuts", "basalt", "deor", "German", "volcanoes", "Audrey Hepburn", "Park", "dolomite", "Alaska", "birds", "Columbia University", "Jack O'Lanterns", "Sexuality", "Greece", "the Inca Empire", "contagious", "Toorop", "the \"National Crime Syndicate\"", "New Mexico", "the French Revolution", "a Purple Heart", "Murfreesboro, Arkansas", "the CPU", "Colette", "katana", "(return to Sender)", "Jean Lafitte", "the Komodo dragon", "Italian", "Churchill", "knitting", "Robbie Turner", "receipt", "Damascus", "Kung", "Innsbruck", "the Noachian Deluge", "SeaWorld", "the chest, back, shoulders, torso and / or legs", "Article Two", "Andy Cole", "Genghis Khan", "Roy Rogers", "violet", "the Great Northern Railway", "25 October 1921", "Katarina Witt", "\"The Orchid Thief\"", "\"It seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "died in the Holmby Hills, California, mansion he rented."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6300899621212122}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15542", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-6427", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.53125, "CSR": 0.5330882352941176, "EFR": 0.9, "Overall": 0.7028676470588235}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "Moon River", "King Kong: A Prelude to Skull Island - Toho Kingdom", "William", "the West India Company", "Hans Christian Andersen", "Luffa", "Hershey", "Alsace", "a crossword", "Ali", "a deodorant", "the Supreme Court", "the north magnetic pole", "Putin", "thunderstorms", "Kennebunkport", "a satellite", "Black Death", "Devon", "elia Earhart", "Hoover", "Panty Raid", "French", "cricket", "A Salt Lake Childhood Origins", "the \"NYPD Blue\"", "the Lone Ranger", "a rodent", "white", "getting There.", "a keypunch", "the Athena", "The Fugitive", "Indonesia", "a forge", "Harpers Ferry", "computer vision", "lilac", "a dillfrog", "Tampa", "ductile", "Shakespeare's Men", "Leo", "first anniversary", "nautilus", "salaam", "a Bigfoot", "Juris Doctorate", "buy back the option", "The Thing", "Sebastian Lund ( Rob Kerkovich )", "Stephen Curry", "Kusha", "Mars", "Captain America", "the Great Depression", "South America,", "1998", "Picric acid", "Nineteen", "emergency aid", "Siri"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5753720238095238}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-15530", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-5078", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-7740", "mrqa_newsqa-validation-3365"], "SR": 0.453125, "CSR": 0.5308035714285715, "EFR": 1.0, "Overall": 0.7224107142857144}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "one", "How I Met Your Mother", "the two-state solution", "a welcoming, bright blue-purple", "little blue booties.", "forgery and flying without a valid license,", "Kurdistan Freedom Falcons,", "Lee Myung-Bak", "end of a biology department", "Malawi.", "\"The War Within: A Secret White House History 2006-2008\"", "James Whitehouse,", "shut down buses, subways and trolleys that carry almost a million people daily.", "a Muslim", "a Muslim festival", "Caster Semenya", "Fiona MacKeown", "GospelToday,", "death of cardiac arrest", "it was important to provide alternative work for poor Afghan farmers to encourage them to give up opium production.", "rural Tennessee.", "The BBC", "Plymouth Rock", "$15,413 per venue,", "seven", "Karen Floyd", "Expedia", "Robert Redford", "economic and political engagement", "death squad killings", "hand-painted Swedish wooden clogs", "July for A Country Christmas,", "Castaic", "bossa nova", "Amy Bishop Anderson,", "They decided to use a surrogate", "her landlord", "job training", "a municipal building in Baghdad's Sadr City,", "two years,", "Operation Cast Lead", "Diego Maradona", "21-year-old", "bartering -- trading goods and services without exchanging money", "Rawalpindi", "\"deep sorrow\"", "Leo Frank", "Port-au-Prince", "Buddhism", "Russian bombers", "Bill Clinton", "independently in different parts of the globe, and included a diverse range of taxa", "Sophocles", "a charbagh", "Vito Corleone", "western Caribbean", "Valletta", "the Eisenhower Executive Office Building", "Premier League club Tottenham Hotspur and the England national team", "February 22, 1968", "Palatine", "gasoline", "(12 Years a Slave)"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5247790404040404}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.8, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-1912", "mrqa_hotpotqa-validation-3265", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-5633"], "SR": 0.421875, "CSR": 0.5277777777777778, "EFR": 1.0, "Overall": 0.7218055555555555}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "sustain future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "former", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "Top Gun", "to step up.\"", "Too many glass shards", "one Iraqi soldier,", "Jaipur", "Obama", "April 6, 1994", "the Democratic VP candidate", "Cologne, Germany,", "34", "20,000-capacity O2 Arena.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Immigration Minister Eric Besson", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Kevin Evans", "Some truly mind-blowing structures", "FARC rebels.", "Dan Brown", "The pilot, whose name has not yet been released,", "Paul McCartney and Ringo Starr", "Booches Billiard Hall,", "air support.", "\"She was focused so much on learning that she didn't notice,\"", "in a Starbucks this summer.", "finance", "Friday.", "diagnosed with skin cancer.", "caribbean", "Deutschneudorf,", "more than 5,600", "unable to pass significant restrictions on war funding when they drafted the bill,", "21 percent", "Yoko Ono Lennon,", "at least $20 million to $30 million,", "a vigilante group", "laying out a tournament ladder by arranging slips of paper with the names of players on them", "densely packed in the fovea centralis", "10 years", "Jeffrey Archer", "a peplos", "Jack Nicholson", "Flatbush Zombies", "Crane Wilbur", "Venice", "bagpipe", "Special Boat Teams", "Earvin \"Magic\" Johnson Jr.", "`` Fix You ''"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5881758432539683}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.2222222222222222, 0.33333333333333337, 1.0, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.625, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-383", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1127"], "SR": 0.4375, "CSR": 0.5253378378378378, "EFR": 1.0, "Overall": 0.7213175675675675}, {"timecode": 37, "before_eval_results": {"predictions": ["in all health care settings,", "a resident of la colonia Partido Romero in Ciudad Juarez,", "six", "Sunni Arab and Shiite tribal leaders", "the iconic Hollywood headquarters of Capitol Records,", "African National Congress Deputy President Kgalema Motlanthe,", "ferry", "1994", "Belfast, Northern Ireland.", "Cain", "U.S. filmmakers", "Clarkson", "CEO of an engineering and construction company", "London's", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "breathe through her nose, smell, eat solid foods and drink out of a cup,", "almost 9 million", "the soldiers", "NATO fighters", "low-calorie", "1,500", "Grayback forest-firefighters", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "Bergdahl,", "some of the best stunt ever pulled off", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court", "Virgin America", "The Kirchners", "3,000 kilometers (1,900 miles)", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "nuclear", "Iran's parliament speaker", "No 4,", "\"services to film, theater and the arts and to activism for equal rights for the gay and lesbian community.\"", "cars have chosen their rides based on what their cars say about them.", "10", "artificial intelligence.", "no chance", "10", "April 13,", "Samuel Herr,", "London's", "Obama", "16", "Ralph Lauren", "$10 billion", "35,000", "three", "David Ben - Gurion", "Kiss", "20 years from the filing date", "Ben Affleck", "Noises Off", "piano", "Mauthausen", "Delilah Rene", "Jay Gruden", "Pope John Paul II", "art deco", "Invisible Man", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6937975342258809}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.888888888888889, 0.14285714285714285, 0.4, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.10526315789473684, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 1.0, 0.16666666666666669, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913043, 0.14285714285714288, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-494", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.546875, "CSR": 0.525904605263158, "EFR": 0.9655172413793104, "Overall": 0.7145343693284937}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "\"The U.S. program to assassinate terrorists in Iraq.", "environmental and political events", "the U.S. Holocaust Memorial Museum", "Ireland.", "At least 33 people", "2007", "heavy turbulence", "Liza Murphy", "Opryland", "Brett", "Rod Blagojevich,", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "1980", "Haiti", "The Israeli Navy", "Achmat Dangor, CEO of the Nelson Mandela Foundation,", "84-year-old", "John Kiriakou", "President Bill Clinton", "humans", "Sylt", "chairman of the House Budget Committee,", "Vice's broadband television network.", "President Robert Mugabe's", "he rejected the option of committing more forces for an undefined mission of nation-building without any deadlines.\"", "more than 30", "Lisa Brown", "A total of 133 people in 26 states have been infected, according to the CDC. Additionally, a woman who was pregnant at the time of her illness had a miscarriage.", "it would", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "the Italian Serie A title", "Superman brought down the Ku Klux Klan,", "spent his last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and bus ticket to Johannesburg.", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "President Pervez Musharraf", "two courses", "$1,500 fine", "the MS Columbus", "a murderousic killer who preys on a group of young people at the fictitious Camp Crystal Lake.", "The local Republican Party", "1 October 2006", "1834", "a GTPase responsible for endocytosis in the eukaryotic cell", "jazz", "Scafell Pike", "Alzheimer's disease", "the University College of North Staffordshire", "9,984", "Smithfield, Rhode Island,", "liquid nitrogen", "Donna Rice Hughes", "albatross", "actress"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5613087620063825}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.5, 1.0, 0.14814814814814814, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225808, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.05, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1882", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3468", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185", "mrqa_hotpotqa-validation-3314"], "SR": 0.484375, "CSR": 0.5248397435897436, "EFR": 0.9090909090909091, "Overall": 0.7030361305361306}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "her husband", "Peshawar", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week", "ties", "15 kilometers (9 miles) south of Beirut.", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "Michael Arrington,", "\"It is not acceptable. It is outrageous.\"He said the result is there is no clear picture of how vulnerable utilities are to cyber attacks.", "The Book", "the fact that the teens were charged as adults.", "he would actively engage Arab media.", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "Saturday", "alleviation of their pain", "Robert", "suicides", "Songs penned by Harrison included \"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades. He was extradited from the United States to Israel,", "Tina Constable,", "They're big, strong, and fierce", "over 1,000 pounds", "two satellites", "a time-lapse video showing some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three out of four", "$199", "Lindsey oil refinery", "1,300 meters in the Mediterranean Sea.", "\"He puts more heart and more passion in what he's doing than some of the other dancers,\"", "Pakistan", "Thursday", "\"I will be asking questions,\" he added. \"I am here to seek the truth.\"", "fluoroquinolone", "to ensure that detainees are not drugged unless there is a medical reason to do so.", "Empire of the Sun", "digging", "1000 square meters", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001", "786 -- 802", "31 March 2018", "Muhammad Ali", "the tallest building in the world", "1961", "goalkeeper", "the Secret Intelligence Service", "75 mi", "chef's salad", "grasshopper", "Kneset", "Secretary of the Interior"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5767283657100591}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.17391304347826086, 0.0, 1.0, 0.0, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 0.4799999999999999, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.7142857142857143, 0.0, 0.0, 1.0, 0.21052631578947367, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_naturalquestions-validation-9953", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-6954"], "SR": 0.46875, "CSR": 0.5234375, "EFR": 1.0, "Overall": 0.7209375}, {"timecode": 40, "UKR": 0.67578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.8046875, "KG": 0.4796875, "before_eval_results": {"predictions": ["1985", "a nurse who tried to treat Jackson's insomnia with natural remedies", "eight", "Austin Wuennenberg,", "in a canyon in the path of the blaze", "machine guns and two silencers", "Matthew Fisher", "Barack Obama", "NATO", "Joe Lieberman,", "The meter reader", "the Gulf", "Haiti.", "Iranian consulate,", "Basel", "Pyongyang and Seoul", "\"She was my biggest fan, my best friend. She was with me every step of the way,\"", "Kurt Cobain's", "pulling on the top-knot of an opponent,", "1983", "22-10.", "Egypt.", "Miss USA Rima Fakih", "delivers a big speech", "a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "Justicialist Party, or PJ by its Spanish acronym,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "cell phones", "six", "2004.", "Egypt", "a general, training Afghan police and troops, before trading his uniform for a diplomat's business suit.", "19-year-old", "an odd collection of vehicles on display on Capitol Hill, ranging from a bucket truck used for repairing power lines to something resembling an enclosed golf cart to a pair of hot-looking, two-seater sports cars.", "the 3rd Platoon,", "\"Walk -- Don't Run\" and \"Diamond Head.\"", "melt", "Communist", "the journalists and the flight crew will be freed,", "Haitians", "Sri Lanka's", "saying Chaudhary's death was warning to management.", "summer", "Rev. Alberto Cutie", "since 1983.", "witnesses spotted Caylee since her disappearance.", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "the third generation", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Nicol Williamson,", "29, 2009", "Latin American culture", "Dolly Parton", "People of the Book", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.5, "QA-F1": 0.6459471686223888}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.923076923076923, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3125, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8750000000000001, 0.9411764705882353, 0.4, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.5454545454545454, 1.0, 1.0, 0.9767441860465117, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-1870", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.5, "CSR": 0.5228658536585367, "EFR": 1.0, "Overall": 0.6966044207317073}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "Los Angeles", "\"It seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "KBR managers", "the same drama that pulls in the crowds", "across Greece", "a monthly allowance,", "U.S. Navy", "video cameras", "Marcell Jansen", "The jury at Liverpool Crown Court took a little over an hour to clear Gerrard of charges relating to a fracas in a nightclub bar in the north-western of England city on December 29 of last year.", "the Brundell family", "outside the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago", "Appathurai", "a missing sailor", "The FBI's Baltimore field office", "Tuesday in Los Angeles.", "Honduran", "curfew in Jaipur", "the Sri Lankan", "Robert", "in a park in a residential area of Mexico City,", "16", "iTunes Music Store,", "into the picturesque Gamla Vaster neighborhood", "the Russian air force,", "an Italian and six Africans dead.", "three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist police characterized as \"spectacular.\"", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre sanctuary", "Missouri", "Tibetan exile leaders,", "Ketamine", "Haleigh Cummings,", "at least two and a half hours.", "Bobby Darin,", "New Year's Day", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama on October 23, 2008, shortly before the presidential election.", "an obscure story of flowers", "Schalke", "Kris Allen,", "World Wide Village,", "2 total", "Supplemental oxygen", "between 11000 and 9000 BC", "Harley", "Roy Rogers", "Harriet Tubman", "a leo spelaea", "German", "Forbes", "black magic or of dealings with the devil", "Stroke", "Stockholm", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.375, "QA-F1": 0.49266468626224724}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.4, 0.4878048780487806, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.15384615384615383, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-7589", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-59", "mrqa_naturalquestions-validation-8733"], "SR": 0.375, "CSR": 0.5193452380952381, "EFR": 1.0, "Overall": 0.6959002976190476}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "Mark Andreessen defined the term as follows : `` Product / market fit means being in a good market with a product that can satisfy that market", "Freddie Highmore", "Elvis Presley", "divergent tectonic", "Stefanie Scott", "Tanvi Shah", "Kida", "in Eurasia", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Palmer Williams Jr.", "Chicago metropolitan area", "Coldplay", "$5.4 trillion", "2,050 metres ( 6,730 ft ) at the Urubamba River below the citadel of Macchu Piccu", "Ann Gillespie", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "the opisthodomus", "medical abnormalities, activation level, or recruitment order, or to analyze the biomechanics of human or animal movement", "Albert Einstein", "1994", "Fred E. Ahlert", "Institute of Chartered Accountants of India ( ICAI )", "2018", "Bette Midler", "push the food down the esophagus", "Walter Mondale", "Nick Sager", "Sweden's long - standing policy of neutrality was tested on many occasions during the 1930s", "the 18th century", "Graham McTavish", "1962", "Julie Adams", "Odoacer", "Michael Madhusudan Dutta", "6 - 6", "Bill Patriots", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Lance Robertson U.S., voice dubbed by Ortis Deley in the UK", "January 15, 2007", "John Garfield as Al Schmid", "SURFACE WITHA OF ROOTS IS ENORMOUS", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "geophysicists", "Billy Colman", "360", "November 17, 2017", "Alice Cooper", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay", "1932", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Evey's mother in the Wachowskis", "\"Steamboat Bill, Jr.\"", "supermodel", "\"It didn't matter if you were 60, 40 or 20 like I am.", "a surrogate", "salt", "Rocky Marciano", "consumer confidence"], "metric_results": {"EM": 0.5, "QA-F1": 0.6394945405074839}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.17391304347826084, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.7499999999999999, 1.0, 0.19999999999999998, 0.5, 0.5, 1.0, 0.9523809523809523, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5555555555555556, 0.16, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.15384615384615383, 1.0, 0.5714285714285715, 0.6, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.6666666666666666, 0.0, 1.0, 0.1, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-4225", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.5, "CSR": 0.5188953488372092, "EFR": 0.96875, "Overall": 0.6895603197674418}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational", "A witness", "34", "Miami Beach, Florida,", "\"We very much hope for this special group of patients, there is hope, and one day they will be able to go comfortably from their houses and enjoy the things which we take for granted.\"", "it's impossible to solve the piracy problem without addressing the illegal fishing issue.", "Cash for Clunkers", "Justine Henin", "it has witnessed only normal maritime traffic around Haiti,", "California-based Current TV", "It is I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Tim Clark, Matt Kuchar and Bubba Watson", "Columbia", "Omar Bongo,", "\"active athletes,\"", "mother.", "Madrid's Barajas International Airport", "1940's", "tax", "\"Forty percent of young people abuse drugs in public toilets and playgrounds. That's what our recent data from last year shows,\"", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "up three", "Chinese", "Passers-by", "\"He is more American than German.\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Larry Ellison,", "Mexican military", "Sporting Lisbon", "The Kirchners", "\"Piers Morgan Tonight\"", "July 1999", "CNN's \"Piers Morgan Tonight\"", "\"I hope for the sake of our kids that he gets the psychological help for himself and the safety of others.\"", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "most of those who managed to survive the incident hid in a boiler room and storage closets during the rampage.", "parents", "nearly 28 years", "(3 degrees Fahrenheit),", "Claude Monet pastel drawing of London's Waterloo Bridge", "Princess Diana", "Consumer Reports", "It educates the consumer on how much they are paying for having a low-MPG car and encourages them to get into a more efficient vehicle.", "nine-wicket", "Des Moines, Iowa,", "Plymouth Rock", "was a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Michael Schumacher", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "Medicare", "Rob Tremblay", "line code", "Harry Bailley", "The Muffin Man", "Clovis I", "Roots: The Saga of an American Family", "Almeda Mall", "Greek cheese", "FRAM", "the Ross Ice Shelf", "Bonita Melody Lysette"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5828173922513544}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 0.0, 0.19999999999999998, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.18181818181818185, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.09523809523809525, 1.0, 0.6792452830188679, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-792", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.453125, "CSR": 0.5174005681818181, "EFR": 0.9714285714285714, "Overall": 0.6897970779220779}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000.", "Sunday", "Why he's more American than a German,", "The most visible, most exciting family in America is this beautiful black family and so people are ready and looking for those kinds of images,\"", "Rawalpindi", "poor.", "40 militants and six Pakistan soldiers", "700", "The woman involved -- Mandi Hamlin", "breast cancer.", "Alfredo Astiz,", "$5.5 billion", "Her husband and attorney, James Whitehouse,", "3.5 percent", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27 Awa", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "did not", "repression and dire economic circumstances.", "The question is starting to feel a little old: Whom will Barack Obama pick as his vice president?", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "her mom,", "a federal judge in Mississippi", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "North Korean spies watching him.", "cocaine and 4.5 pounds of heroin,", "3-2", "70,000", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "two weeks after Black History Month", "pictures of you looking smiley.", "Wanda E Elaine Barzee.", "The military fired warning shots into the air and sprayed water cannons to disperse the crowd.", "Kim", "about 3,000 kilometers (1,900 miles), possibly putting U.S. military bases in the Pacific Ocean territory of Guam within striking distance,", "The morphine elixir is widely used by terminal patients in hospital and home hospice care settings and is manufactured by Lehigh Valley Technologies Inc.", "late - September through early January", "the euro", "a member of the family Sturnidae ( starlings and mynas ) native to Asia. An omnivorous open woodland bird with a strong territorial instinct, the myna has adapted extremely well to urban environments", "piscina", "The Bible", "UN Supreme Commander Gen. Douglas MacArthur", "PlayStation 4", "ITV,", "cricket fighting", "The Goonies", "Galileo Galilei", "Carson McCullers", "fearful man, all in coarse gray with a great iron on his leg"], "metric_results": {"EM": 0.53125, "QA-F1": 0.651361986615168}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.8, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.5714285714285714, 0.4, 0.1714285714285714, 0.5, 0.34782608695652173, 0.14634146341463414, 1.0, 1.0, 0.07407407407407407, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-2770", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-5687", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-1685", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10531", "mrqa_triviaqa-validation-3284"], "SR": 0.53125, "CSR": 0.5177083333333333, "EFR": 1.0, "Overall": 0.6955729166666667}, {"timecode": 45, "before_eval_results": {"predictions": ["Queenscliff Music Festival", "0-0", "Aung San Suu Kyi", "grossing $55.7 million during its first weekend,", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab,", "a treadmill", "Bahrain,", "Piers Morgan", "Mary Phagan", "well over two decades.", "100,000", "1981 drowning death,", "more than a million residents", "5-0", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her", "\"A chicken soaked in the rain,\"", "15", "100% of its byproducts", "it really like to be a new member of the world's most powerful legislature?", "foster national reconciliation between religious and ethnic groups.", "The Rosie Show", "helicopters and unmanned aerial vehicles", "racial intolerance.", "\"The Big Three\"", "Rolling Stone", "hooligans and vandals", "Ralph Lauren", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"It would not make sense for Pyongyang to make such a move after going through official channels with its plans,", "\"a striking blow to due process and the rule of law,\"", "surrender.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Lindsey Vonn", "Sunday", "Rwanda", "cancer", "Jose Manuel Zelaya", "around 10:30 p.m. October 3,", "onto the college campus.", "200", "a full garden and pool, a tennis court, or several heli-pads.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "it was split 10-2.", "Sunday", "December 2, 2013, and the third season concluded on October 1, 2017", "North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia ; and 1,578 km ( 981 mi ) north of Puerto Rico", "Christopher Lloyd", "Nero", "Ethiopia", "Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Douglas Fairbanks, Jr.", "Paleomagnetism", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6589864417989418}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8148148148148148, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-2966", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.5625, "CSR": 0.5186820652173914, "EFR": 0.9642857142857143, "Overall": 0.688624805900621}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "an anaphylactic shock,\"", "al Fayed", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "war years,", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight", "around Ciudad Juarez, across the border from El Paso, Texas.", "former U.S. secretary of state.", "Sri Lanka,", "Communist", "Gainsbourg", "U.N.", "Ike", "The military commission decision", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "al Qaeda", "debris", "8,", "new materials -- including ultra-high-strength steel and boron", "a president who understands the world today, the future we seek and the change we need.", "in the neighboring country of Djibouti,", "in the mouth.", "over 1000 square meters in forward deck space,", "Alfredo Astiz,", "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "14 years before", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "The EU naval force", "vice-chairman of Hussein's Revolutionary Command Council.", "Michelle Obama", "a fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan.", "bodies and heads", "Seoul.", "try to make life a little easier for these families", "Muqtada al-Sadr,", "a house party in Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$81,88010.", "Hungary", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Ernest Hemingway", "123", "Ellie Kemper", "President's Volunteer Service Award", "nursery rhyme", "the equatorial plane", "Washington", "Holly", "Lundy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6474736276706385}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727273, 0.9565217391304348, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.8, 0.0, 1.0, 0.5, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-4204", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-1315", "mrqa_searchqa-validation-12477"], "SR": 0.484375, "CSR": 0.5179521276595744, "EFR": 1.0, "Overall": 0.6956216755319149}, {"timecode": 47, "before_eval_results": {"predictions": ["Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.", "\"The Cycle of Life,\"", "\"a striking blow to due process and the rule of law.\"", "make the new truck safer,", "200", "Alexey Pajitnov", "1959.", "catastrophic failure of the aircraft's controls.", "Iron Eyes Cody", "at least 18 federal agents and two soldiers", "$17,000", "these planning processes are urgently needed", "Animal Planet", "Caster Semenya", "a mammoth", "$3 billion,", "Les Bleus", "Samoa", "more than 100.", "the frequency of waterboarding was among the operational details that had not been declassified.", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "the Obama girls from Sen. Ted Kennedy.", "Long Island convenience store", "recanted her allegations,", "Damon Bankston", "Fayetteville, North Carolina,", "clogs", "\"bleaching\" in which algae living in the coral die and leave behind whitened skeletons.", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "Ventures", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "Sovereign Wealth Funds", "an Italian and six Africans", "Damon Bankston", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "The Falklands,", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "wondered what will they do", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "The food, music, culture and language of Latin America", "former Procol Harum bandmate Gary Brooker", "highest ever position", "Tuesday", "she's in love,", "Miguel Cotto", "Zac Efron", "The plane", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game )", "the most recent Super Bowl champions", "Turkey", "czarevitch", "auk", "Portland, OR", "from 1993 to 1996", "Minette Walters", "Zmeiny", "Linda Childers", "photodetector", "March 23, 2018"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6061564303751803}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.9090909090909091, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.125, 0.0, 0.8571428571428571, 1.0, 0.0, 0.19047619047619047, 1.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.8, 0.0, 0.7555555555555554, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-4441", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_searchqa-validation-5955"], "SR": 0.46875, "CSR": 0.5169270833333333, "EFR": 0.9411764705882353, "Overall": 0.6836519607843137}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "the U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future,", "Lindsey Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "gift of lounge music that flourished in 1940's Japan.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "relax the smooth muscle in the gut and relieve cramping", "fake his own death by crashing his private plane into a Florida swamp.", "Kaka", "Aryan Airlines Flight 1625", "ketamine.", "Kris Allen,", "in Fayetteville, North Carolina,", "4-1 Serie A", "Haiti", "organize sanitary arrangements for soldiers at a camp near Yeovil,", "1981,", "in terms of the country's most-wanted list,", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "commander of the current space shuttle mission to upgrade the Hubble Space Telescope.", "Form Design Center.", "it really like to be a new member of the world's most powerful legislature?", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "at three people", "$250,000", "his sixth world title at a different weight by beating Cotto", "Courtney Love,", "Kim Myung-bak,", "North Korea", "54", "Anil Kapoor", "murder in the beating death of a company boss who fired them.", "the African National Congress", "walk", "Russell,", "maintain an \"aesthetic environment\" and ensure public safety,", "30.3 %", "season seven", "BeBe Winans", "Pickwick", "Sean Maddox", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "lethal", "small-town rabbi", "Cheers", "Coleman Hawkins"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6133254664045105}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.7692307692307693, 1.0, 0.75, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.2222222222222222, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.5, 1.0, 0.15384615384615383, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-212", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-7105", "mrqa_searchqa-validation-11020", "mrqa_hotpotqa-validation-864"], "SR": 0.484375, "CSR": 0.5162627551020409, "EFR": 0.9696969696969697, "Overall": 0.689223194959802}, {"timecode": 49, "before_eval_results": {"predictions": ["a delegation of American Muslim and Christian leaders", "\"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.", "35,000.", "curfew in Jaipur", "Muslim revolutionary named Malcolm X", "Four Americans", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "in Japan", "Africa", "Haiti", "current and historic conflict zones,", "cancerous tumor.", "Brett Cummins,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "his former caddy,", "David McKenzie", "\"If we're going to revise our policies here, we need to make it so for all the camps,\"", "Daniel Radcliffe", "\"The Da Vinci Code\"", "exotic sports cars", "secrets of Freemasonry", "the Somali border town of Afmado", "Republican", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "Rwanda declared a cease-fire", "$60 million", "4,000 credit cards", "Alison Sweeney,", "33", "The Carrousel du Louvre mall", "137", "bartering", "Austin Wuennenberg,", "wanted to change the music on the CD player", "\"momentous discovery\"", "\"Hawaii Five-O\"", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher,", "Naples", "\"brain hacking\"", "Saturday", "Both women", "Andy Serkis", "late 1989 and 1990", "Davos", "Malm\u00f6", "Richard Attenborough and wife Sheila Sim", "eclipse", "\"novel with a key\"", "London", "Oklahoma", "Jan Hooks", "Christianity", "Tammy Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6702566964285714}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.08333333333333333, 1.0, 0.5, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.625, 0.0, 1.0, 0.09523809523809523, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.25, 0.16666666666666669, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-6362", "mrqa_searchqa-validation-7640", "mrqa_searchqa-validation-1891"], "SR": 0.546875, "CSR": 0.516875, "EFR": 1.0, "Overall": 0.6954062499999999}, {"timecode": 50, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.85546875, "KG": 0.50703125, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria.", "Two", "in July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Shanghai,", "\"Den of Spies\"", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "Islamabad", "March 8", "female soldier,", "Michoacan state,", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the Little Rock Nine,", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "Arnoldo Rueda Medina.", "the world's two largest movie industries.", "12", "Arabic, French and English,", "40", "Johannesburg", "L'Aquila", "\"Body Works\"", "North Korea,", "at least 27", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Amsterdam,", "burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "\"We are doing our best to dissuade the North Koreans from going forward,", "posting a $1,725 bail,", "Cal Ripken Jr.", "78,000 parents", "Apple Inc.", "London's", "\"fusion teams,\"", "martial arts,", "the couple's", "\"Operation Crank Call,\"", "Orwell", "Guwahati", "the winter solstice", "Frenchman", "sheep", "daisy", "1853", "musicologist", "in 1902,", "the Alaska territory", "\"Twelfth Night\"", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6549937042124543}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.8, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9743589743589743, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.8181818181818181, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-2818", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-1384", "mrqa_triviaqa-validation-7329", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-4112", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.546875, "CSR": 0.5174632352941176, "EFR": 1.0, "Overall": 0.7248207720588236}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan Defense Minister Yusuf Haji", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "more than 1.2 million people.", "Arizona", "Kenyan and Somali governments", "Caylee Anthony", "Diego Maradona", "London", "Oregon State Senior troopers David Petersen after he was able to catch up with six exotic sports cars on a stretch of Highway 18 near Grand Ronde on Thursday,", "rural Tennessee.", "Fakih", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "14", "Former Mobile County Circuit Judge Herman Thomas", "18", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook", "Amado Carrillo Fuentes,", "Asashoryu", "said they would not be making any further comments, citing the investigation.", "41,", "Anil Kapoor.", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "Capt. Richard Phillips,", "the estate", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "the prime minister's handling of the L'Aquila earthquake, which killed nearly 300 people and devastated the city when it struck last year,", "11th year in a row.", "200", "Seminole Tribe", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013, which made up nearly 92 % of coal's contribution to energy supply", "Cecil B. DeMille", "administrative supervision", "El Cid of Castile", "national militia", "Monopoly,", "in the world,", "Kentucky, Virginia,", "1999", "Brazil's", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6583883953188389}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [0.25, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.19354838709677416, 1.0, 1.0, 0.14285714285714288, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4799999999999999, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23076923076923078, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3493", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5512", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-4624", "mrqa_searchqa-validation-12036"], "SR": 0.546875, "CSR": 0.5180288461538461, "EFR": 1.0, "Overall": 0.7249338942307693}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "pregnant", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "unemployment", "$55.7 million", "family friend of a U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "U.S. security coordinator", "Ashley \"A.J. Jewell,", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male.", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "United", "planned attacks in the southern port city of Karachi,", "\"falling space debris,\"", "Seven-time world champion Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Kingman Regional Medical Center,", "Olympic medal", "Long Island", "5,600", "in 2007, only 9 percent of Turks polled by the Pew Research Center held favorable views of America,", "Sharon Bialek", "sniffing the help of generous supporters and an Iraqi humanitarian group", "two", "\"We get a signal prior to violence,\"", "Muslim", "The charges were brought in the Eastern District of New York because al-Moayad allegedly collected terrorist funds at the al-Farooq mosque in Brooklyn.", "Evans", "near the Somali coast", "in the $24,000-30,000 price range.", "2008,", "killing rampage.", "\"Twilight\" book series.", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "Obama and McCain", "The flooding was so fast that the thing flipped over,\"", "five", "The sole survivor of the crash that killed Princess Diana has told a court he still cannot remember the incident but does not support the conspiracy theories surrounding it.", "Dubai", "June 6, 1944,", "\"surge\" strategy he implemented last year.", "home laundromats", "to bring, and \u03bd\u03af\u03ba\u03b7, n\u00edk\u00ea, `` victory ''", "ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Aidan Gallagher", "Rebecca Adlington", "Buckinghamshire", "10", "high-ranking", "2007\u201309", "The entity", "Great Expectations", "1992 American erotic thriller film based on John Lutz's novel SWF Seeks Same.", "launch one ship.", "northern latitudes"], "metric_results": {"EM": 0.4375, "QA-F1": 0.52512627997003}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.3076923076923077, 1.0, 1.0, 0.23999999999999996, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.18181818181818182, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 0.22222222222222224, 1.0, 1.0, 0.3, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1735", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.4375, "CSR": 0.5165094339622642, "EFR": 1.0, "Overall": 0.7246300117924529}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lance Cpl. Maria Lauterbach", "throwing three punches", "Argentina", "Ferraris, a Lamborghini and an Acura NSX", "Laurean killed Lauterbach", "1983", "the simple puzzle video game,", "\"Dancing With the Stars\"", "African National Congress", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "he failed to return home,", "Jiverly Wong,", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday's", "help nations trapped by hunger and extreme poverty,", "$10 billion", "Mokotedi Mpshe,", "April 22.", "Mitt Romney", "twice.", "seeking help", "Mary Phagan", "pesos", "judge", "Herman Cain,", "60 euros", "Barack Obama", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "\"We are a nation of Christians and Muslims, Jews and Hindus -- and nonbelievers.\"", "Sunday", "a share in the royalties for the tune.", "U.S.-Mexico border", "in a canyon in the path of the blaze Thursday.", "additional information regarding actress Natalie Wood's 1981 drowning death,", "Pre-evaluation, strategic planning, operative planning", "Anatomy", "seven", "Henry Higgins", "shoes", "Herbert Lom,", "Battle of Prome", "East 31st Street in the Union Hill section of Kansas City, Missouri", "Thomas Sadoski, Michiel Huisman", "the center of the hex", "the Puerto Rican Winter League", "Tom Osborne", "Kwame Nkrumah,"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7653237048277809}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.3636363636363636, 0.5714285714285715, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037"], "SR": 0.6875, "CSR": 0.5196759259259259, "EFR": 1.0, "Overall": 0.7252633101851853}, {"timecode": 54, "before_eval_results": {"predictions": ["$50", "diabetes and hypertension,", "Jet Republic,", "many different", "at least 27", "last week,", "The Peruvian Supreme Court", "Joan Rivers", "\"Watchmen's\"", "sovereignty over them.", "NATO's Membership Action Plan, or MAP,", "Bangladesh", "250,000", "complicated and deeply flawed man", "Dilshan had Thilan Samaraweera caught in the leg-trap for one just before the tea interval.", "\"novel\"", "\"it is impossible to turn back the tide of globalization.\"", "voluntary manslaughter", "dancing", "South Africa", "The noose incident", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "head injury.", "down a steep embankment in the Angeles National Forest", "Marxist guerrillas", "September 6, 1918,", "Rwanda", "U.N. High Commissioner for Refugees", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "58 minutes.", "see my kids graduate from this school district.", "CNN", "Jobs's", "using recreational drugs", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "Las Vegas", "Vice President Joe Biden", "Tuesday", "jut out from the chariot,", "No", "Al-Shabaab,", "his health", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "2017", "October 23", "quartz", "the Kursk nuclear submarine", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club Manchester United and the England national team", "March", "Eudora Welty's", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5909496753246753}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.1, 0.0, 0.07142857142857142, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.19999999999999998, 0.5, 0.5, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-2095", "mrqa_triviaqa-validation-2064", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3071", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.515625, "CSR": 0.5196022727272727, "EFR": 0.967741935483871, "Overall": 0.7187969666422287}, {"timecode": 55, "before_eval_results": {"predictions": ["a bond hearing", "without the", "Mexico", "London's", "three", "customers are lining up for vitamin injections", "\"Dalmatian syndrome.\"", "writer and starring in 'The Prisoner'", "\"We want to reset our relationship and so we will do it together.\"", "Preah Vihear temple", "general astonishment", "June 6, 1944,", "lightning strike", "twice", "Sen. Barack Obama", "money or other discreet aid", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Steve Williams", "preserved corpses having sex", "mauf Fritzl,", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw", "the 3rd District of Utah.", "Lakes Golf Club in Sydney,", "organizing the distribution of wheelchairs,", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "\"She was focused so much on learning that she didn't notice,\"", "\"We have cameras on board that have been able to image where the Apollo spacecraft landed,", "punish participants in this week's bloody mutiny,", "Somalia's piracy problem was fueled by environmental and political events.", "Alaska or Hawaii.", "Robert Park", "in the neighboring country of Djibouti,", "HPV", "Six", "Bahrain", "will his No. 2 man (or woman) be by his side?", "Facebook and Google,", "Somali", "2006", "five", "March 24,", "The father of Haleigh Cummings,", "a senior at Stetson University studying computer science.", "Saturday,", "NATO fighters", "\"Empire of the Sun\"", "New Zealand", "a model of sustainability.", "The Jewel of the Nile", "winter", "73", "neoclassic", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort in Lake Buena Vista, Florida", "\"Strangers in the Night\"", "mass", "a snout beetle,", "Lord Halifax,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5221354166666666}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.8750000000000001, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.33333333333333337, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2124", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-7008", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.453125, "CSR": 0.5184151785714286, "EFR": 0.9714285714285714, "Overall": 0.7192968750000001}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "those traveling near the Somali coast", "\"To My Mother\"", "billboards", "2.5 million", "almost 100", "137", "1,500", "Worry Free Dinners", "Rod Blagojevich,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees", "the most-wanted man in the world", "Carrousel du Louvre,", "suicide vests", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "approximately 600 square miles of south-central Washington,", "best your own fuel economy achievements,\"", "The remains of Cologne's archive building following the collapse on Tuesday afternoon.", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Milan", "strife in Somalia,", "cancerous tumor.", "increase the flow of water passing through its network of dams.", "Abdullah Gul,", "alcohol and drug abuse,", "11th year in a row.", "the journalists and the flight crew will be freed,", "Gov. Rod Blagojevich", "national telephone", "pliers", "the shootings, handed over the AR-15 and two other rifles and left the cabin.", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "\"wow.\"", "gasoline", "Santaquin City, Utah,", "Swansea Crown Court,", "physicist Steven Chu", "the Dominican Republic", "militants", "Monday", "Paul", "diastema", "to manage the characteristics of the beer's head", "cryogenics", "Cambridge", "mercury", "13 October 1958", "bassline", "Pansexuality, or omnisexuality", "\"Invisibility\"", "Zachary Taylor", "Battlestar Galactica", "Marilyn Monroe"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6369987217643468}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.2222222222222222, 0.19999999999999998, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.125, 0.1904761904761905, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.20000000000000004, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.5, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-10329"], "SR": 0.53125, "CSR": 0.518640350877193, "EFR": 0.9666666666666667, "Overall": 0.7183895285087718}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "\"Mad Men\"", "5,600", "the European Commission", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "American pop star's", "We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "\"Draquila -- Italy Trembles.\"", "al Qaeda,", "U.S. Chamber of Commerce", "Carol Browner", "U.N. Security Council", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline following a disallowed goal for Arsenal.", "a U.S. helicopter crashed in northeastern Baghdad as", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "an acid attack", "Congress", "southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "Petra Nemcova", "South Africa", "Somali", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "the French consulate in the oil-rich city of Port-Gentil, on the country's coast.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "a gym", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "HSH Nordbank Arena", "$40 and a bread.", "tennis", "No. 1 slot at the box office.", "Republican Gov. Jan Brewer.", "Boundary County, Idaho, which borders Canada and abuts the area where the attack took place.", "securities", "$150 billion", "the Berlin School of experimental", "Michael Crawford", "the beginning of the American colonies", "the French 'Chamboule-tout'", "Fenn Street School", "the middle ear", "Australian", "Argentinian", "fibre optic cable", "rap", "inducere,", "Harvard", "129,007,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7005895146520147}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.5833333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3677", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174"], "SR": 0.609375, "CSR": 0.5202047413793103, "EFR": 0.96, "Overall": 0.7173690732758621}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "that the U.S. might use interceptor missiles for offensive purposes.", "Six", "celebrity clientele.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers dead,", "5 season", "Arthur E. Morgan III,", "Jared Polis", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal in the women's figure skating final,", "No 4, the highest ever position", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"a hooligan bereft of any personality as a human being,", "President Obama.", "Jacob Zuma,", "December 7, 1941", "help poor families buy more energy-efficient electrical appliances. The government will set aside 750 million pesos ($55 million) to pay 50 percent of the costs of replacing old appliances.", "18", "the Southeast,", "\"Up,\"", "getting into that Lexus, Lincoln, Infiniti or Ferrari you always wanted, without laying out $70,000 or $80,000 for something you're not actually going to live in.", "fascinating transformation that takes place when carving a pumpkin.", "school, their books burned,", "a motor scooter", "support. We need unfortunately more organization, more of the bureaucratic nitty-gritty that you don't want to do, but you have to,\"", "$249", "J.Crew", "$106.5 million", "Nearly eight in 10", "credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "changed the way the world consumed media,", "\"black box\" label warning", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "education about rainforests.", "Virgin America", "her overriding priority is to protect her children.", "It's so weird. There's two different versions. there's my version of how it went about, and there's the producer's version.", "Kenyan and Somali", "$4 billion,", "1980,", "a man had been stoned to death by an angry mob.", "Africa", "the most-wanted man in the world", "left - sided heart failure", "( 4.09 )", "Devastator, who destroys one of the pyramids to reveal the Sun Harvester inside, before he is killed by a destroyer's railgun called in by Simmons", "Madness", "Jelly Roll Morton", "vice-admiral", "George Lawrence Mikan, Jr.", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy, US", "the United We Stand, Divided We Fall", "professor henry higgins"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6432929131458544}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true], "QA-F1": [0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.0, 0.923076923076923, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.058823529411764705, 0.4, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.972972972972973, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-2951", "mrqa_searchqa-validation-8262"], "SR": 0.546875, "CSR": 0.5206567796610169, "EFR": 1.0, "Overall": 0.7254594809322035}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings", "1913.", "$40 and a loaf of bread.", "9:20 p.m. ET Wednesday.", "U Win Tin,", "543", "Knox's parents, Curt Knox and Edda Mellas,", "11 healthy eggs", "four", "64,", "the mammoth's skull,", "two and a half hours.", "shark River Park in Monmouth County", "improve the environment", "gift to the Obama girls from Sen. Ted Kennedy.", "The park bench facing Lake Washington", "More than 15,000", "\"Teen Patti\" (\"Card Game\")", "Muslim countries,", "CNN's", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty", "sumo wrestling", "10 below", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall notices", "Roy", "VBS.TV", "Kerstin", "Marxist guerrillas", "Greeley, Colorado,", "five", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "the protesters turned on the vendors indiscriminately and confiscating their goods.", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "the Muslim north of Sudan", "at least 18 federal agents and two soldiers have been", "Bahrain", "33", "Kenneth Cole", "the Devastator", "Brazil remains the largest coffee exporting nation", "Theodore Roosevelt", "vice-admiral", "Braves", "the Big Bopper", "Greek-American", "feats of exploration.", "the \"godfather\" of U.S-Mexico border cartels.", "Monarch", "Yale", "Harry S. Truman", "Briton Allan McNish, Dane Tom Kristensen, and Frenchman Lo\u00efc Duval"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6915884462759463}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.8, 0.8, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.3, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2588", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_naturalquestions-validation-5620", "mrqa_triviaqa-validation-105", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.609375, "CSR": 0.5221354166666667, "EFR": 1.0, "Overall": 0.7257552083333334}, {"timecode": 60, "UKR": 0.67578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.8125, "KG": 0.47734375, "before_eval_results": {"predictions": ["183", "Carson", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "Iran's", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "Sixteen", "Obama", "Russian residents and worldwide viewers,", "34", "five victims", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "at the school.", "Russian air company Vertikal-T,", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Michael Brewer,", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "promotes fuel economy and safety while boosted the economy.", "Stuntman: Buster", "Omar Bongo,", "that he wants a \"happy ending\" to the case.", "Obama and McCain camps", "Alabama", "in Fayetteville, North Carolina,", "the only goal of the game", "French", "Honduran President Jose Manuel Zelaya", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "in Somalia.", "response to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Pipeline Express.\"", "Islamabad", "Williams' body", "Conway", "ConAgra Foods plant", "Lalo Schifrin", "April 17, 1982", "Billy Idol", "Illicit", "Theresa May", "every ten years since 1801,", "five months", "\"The Dragon\"", "1994", "Magnolia acuminata", "the same", "Jupiter", "mural"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6877157284513229}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.4210526315789474, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.09523809523809525, 0.0, 1.0, 0.923076923076923, 1.0, 0.0, 0.3333333333333333, 0.888888888888889, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2042", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-1114", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-16357"], "SR": 0.578125, "CSR": 0.5230532786885246, "EFR": 0.9629629629629629, "Overall": 0.6903282483302975}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "at the South Dakota State Penitentiary", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "The leftist guerilla group,", "a baseball bat", "six", "a book.", "Venezuela", "The two parts of her family", "$1.45 billion", "Iranian consulate,", "VoteWoz.com", "Janet Napolitano", "Malawi,", "Daniel Radcliffe", "the attacks", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Shanghai mayor", "in the foyer of the BBC building in Glasgow, Scotland", "reduced their carbon footprint by 132 tons.", "an engineering and construction company", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "outstanding performance by a female actor in a drama series for her role as Deputy Chief Brenda Johnson.", "9:20 p.m. ET Wednesday.", "to be over a kilometer (3,281 feet) high.", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three out of four questioned say that things are going well for them personally.", "The island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "Lavau's accident and the one involving the dead driver are under investigation.", "hiring veterans as well as job training for all service members leaving the military.", "The port won't be back for a while. Roads have been split apart and buckled, fences have fallen over.", "the UK", "\"We need a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.\"", "has a thicker consistency and a deeper flavour than sauce", "skeletal muscle and the brain", "1985", "Dublin", "The character, played by Honor Blackman in the 1964 film version of Goldfinger,", "Lidice", "Columbia", "Wynonna Judd", "most of the youngest publicly documented people to be identified as transgender,", "the Italian occupation", "the great horned type", "Canada", "Bolton"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6345009973686444}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.9333333333333333, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3529411764705882, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 0.33333333333333326, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-431", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-4269", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.546875, "CSR": 0.5234375, "EFR": 1.0, "Overall": 0.6978125000000001}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "sperm and ova", "Michael Buffer", "14", "16,801 students", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "in the 1820s", "the Tigris and Euphrates", "third", "Andrew Garfield", "The Fixx", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "4 in ( 10 cm ) LCD multi-touch Retina display", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison, his former bandmate from the Beatles", "Kristy Swanson", "Chairman of the Monetary Policy Committee", "simulation reproduces the behavior of a system using a mathematical model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "the vas deferens is connected to the epididymis above the point of blockage", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "2 Constant ( C\u03bc and C\u03b4 )", "Universal Pictures,", "Tbilisi", "on dry lake beds northeast of Los Angeles", "autopistas, or tolled ( quota ) highways", "the Vital Records Office of the states, capital district, territories and former territories", "a monocot related to lilies and grasses", "Frank Theodore `` Ted '' Levine", "IIII", "in the absence of a catalyst", "The Maginot Line", "Gustav Bauer", "James Watson and Francis Crick", "Franklin Roosevelt", "card security", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Sondheim", "Tasmania", "Laura Robson", "Afghanistan", "Todd McFarlane,", "Massachusetts", "one", "\"significant skeletal remains\"", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "room for an owners suite and six further double-king sized suites.", "syrup", "the palate", "locoweed", "December 1974"], "metric_results": {"EM": 0.5, "QA-F1": 0.6332212693191311}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.9166666666666666, 0.0, 0.6666666666666666, 0.8571428571428571, 0.6666666666666666, 1.0, 0.33333333333333337, 0.92, 1.0, 0.4, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.625, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.21052631578947367, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699"], "SR": 0.5, "CSR": 0.5230654761904762, "EFR": 1.0, "Overall": 0.6977380952380953}, {"timecode": 63, "before_eval_results": {"predictions": ["Lady Agnes", "the Coriolis force", "1776", "1994", "Roger Dean Stadium", "James Brown", "`` Everywhere ''", "1 mile ( 1.6 km )", "Coldplay", "TC", "Article 1", "Lex Luger", "November 2, 2010", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Mark Lowry", "1877", "a maximum possible strength of 31", "c. 1000 AD", "bow", "Dick Rutan and Jeana Yeager", "near major hotels and in the parking areas of major Chinese supermarkets", "December 1800", "King Saud University", "Hugo Weaving", "Book of Exodus", "a leonine contract, a take - it - or - leave - it contract", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Toby Kebbell", "1078", "James", "Stefanie Scott", "glycine and arginine", "book and architecture", "Stephen A. Douglas", "Dolby Theatre in Hollywood, Los Angeles, California", "The 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII,", "The Republic of Tecala", "during meiosis", "July -- October 2012", "Andy Serkis", "the priests and virgins", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "Berlin", "Marjorie McGinnis", "the Electorate", "U.S. Representative", "Anne Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone:", "The Benchwarmers", "the No Child Left Behind Act", "part of the proceeds"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6198812586353705}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.5, 0.0, 0.0, 0.6153846153846153, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 0.29166666666666663, 1.0, 0.15384615384615383, 0.4, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.21052631578947367, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-7607"], "SR": 0.484375, "CSR": 0.5224609375, "EFR": 0.9393939393939394, "Overall": 0.6854959753787879}, {"timecode": 64, "before_eval_results": {"predictions": ["the winter solstice", "19 July 1990", "senators", "Rex Harrison", "a manufacturing operation", "Turducken", "Patrick Warburton", "the chief priests", "1960", "the President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "the intersection of Mud Mountain Road and Highway 410", "Yuzuru Hanyu", "Tracy McConnell", "Kenny Rogers", "between the stomach and the large intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "Rumplestiltskin", "Sylvester Stallone", "from 35 to 40 hours per week", "Effy", "a body", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "In 2012", "In the mid - to late 1920s", "`` Far Away ''", "Jack McBrayer", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves", "Brenda", "the Battle of Culloden", "Cyanea capillata", "Bonnie Lipton", "2002", "Bill McPherson", "Dawn French", "translator", "Ut\u00f8ya", "125 lb (57 kg)", "Old World fossil representatives", "1992", "pesos", "North Korea", "\"E! News\"", "Carbon", "current congressmen", "The Greatest Show on Earth", "Mary Stuart"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7082651279480323}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 0.125, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4169", "mrqa_hotpotqa-validation-2069", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.640625, "CSR": 0.5242788461538461, "EFR": 1.0, "Overall": 0.6979807692307693}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "`` The Crossing ''", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "lead", "Pebe Sebert and Hugh Moffatt", "Thomas Chisholm", "The interstellar medium", "Lesley Gore", "Paul", "comic book series", "the belligerents", "ingredients", "Charlotte of Mecklenburg - Strelitz", "February 3, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "Amy Winehouse", "the Director of National Intelligence", "Liam Cunningham", "Elliot Scheiner", "a cylinder of glass or plastic that runs along the fiber's length", "Ace", "Goths", "H CO", "StubHub Center in Carson, California", "Mayor Hudnut", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea", "the Norman given name Robert", "the start of the 20th century", "Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "performance marker", "in Super Bowl LII, following the 2017 season, with the Eagles taking their revenge 41 -- 33", "the White River between Enumclaw and Buckley", "Columbia River Gorge in the U.S. states of Oregon and Washington", "Setsuko Thurlow", "John Joseph Patrick Ryan", "1912", "Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13", "Ric Flair", "124 and 800 CE", "continental units", "2009", "Adam Werritty", "the Jets", "\"The Seven Year Itch\"", "Kim Jong-hyun", "King Edward II", "Harrods", "\"Sure to our volunteers, we've been able to fill a million sandbags and place 700,000 around our city,\"", "tax incentives for businesses hiring veterans as", "Arnold Drummond", "Nixon", "Great Expectations", "cathode", "\"Lucky\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.6181236562350692}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428572, 1.0, 1.0, 0.5714285714285715, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.4615384615384615, 0.0, 1.0, 1.0, 0.4, 1.0, 0.34782608695652173, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.06666666666666667, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2200", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_hotpotqa-validation-4316", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1827"], "SR": 0.515625, "CSR": 0.5241477272727273, "EFR": 0.9354838709677419, "Overall": 0.6850513196480938}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "May 1980", "IIII", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "tourneys or slow wheels", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "decreases as the soil becomes saturated", "Kathy Najimy", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "alcohol or smoking", "Richard Crispin Armitage", "Mahalangur Himal sub-range of the Himalayas", "students", "volcanic activity", "In 1837", "late - September through early January", "1991", "Joseph Sherrard Kearns", "The Union's forces", "On 1 September 1939", "a loop ( also called a self - loop or a `` buckle '' )", "Carlos Alan Autry Jr.", "West Egg on prosperous Long Island", "the primary conductor of state - to - state diplomacy", "certified question or proposition of law", "after World War II", "Guwahati", "The chief city, Salamina, lies in the west - facing core of the crescent on Salamis Bay, which opens into the Saronic Gulf", "Cheap trick", "October 29, 2015", "Pir Panjal Range", "16", "~ 3.5 million years old from Idaho, USA", "The federal government", "Tigris and Euphrates rivers", "federalism", "In the year 2026", "Holly Marie Combs", "utopian novels of H.G. Wells", "Sarah Brightman", "password recovery tool for Microsoft Windows", "Indo - Pacific distribution", "Tokyo", "moral", "Lana Del Rey", "NBA", "greyhound, gazelle hound", "Aristotle", "Northwest Mall", "\"Supergirl\"", "Field Marshal Lord Gort", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "gun", "between government soldiers and Taliban militants in the Swat Valley.", "Odysseus", "crawfish", "Boy Scouts of America", "three empty vodka bottles,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5642997729394787}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.5, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 0.25, 1.0, 0.8, 0.23529411764705882, 0.2222222222222222, 0.0, 1.0, 0.25, 1.0, 0.0, 0.6, 0.0, 0.7272727272727273, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.058823529411764705, 0.05555555555555555, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-397", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-4320", "mrqa_newsqa-validation-3067"], "SR": 0.421875, "CSR": 0.5226212686567164, "EFR": 0.972972972972973, "Overall": 0.6922438483259379}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "2010", "Clarence Darrow", "John B. Watson", "Spanish", "Anna Murphy", "a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "ceramic materials", "March 6, 2018", "Erica Rivera", "McFerrin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "Sir Ronald Ross", "Georgia", "Domhnall Gleeson", "Alex Drake", "March 11, 2016", "March 11, 2018", "Thomas Mundy Peterson", "Act 1, Scene 2 of Shakespeare's play Julius Caesar, in which the nobleman Cassius says to Brutus", "`` dead ringer '' and `` graveyard shift", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "in saecula saeculorum in Ephesians 3 : 21", "John Goodman", "mitochondrial inner membrane", "February 25, 2004", "the breast or lower chest of beef or veal", "a nearly - identical `` non-drivers identification card", "Dr. Hartwell Carver", "two", "in Super Bowl LII", "Dadra and Nagar Haveli", "Charles R Ranch, County Road 24", "condemns rural depopulation and the pursuit of excessive wealth", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "Washington metropolitan area", "the euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking song", "tissues of the outer third of the vagina", "Bergen", "John R. Dilworth", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a Mississippi school district and high school in federal court Tuesday over the April 2 prom.", "Hippos & baboons", "the Republic of Belarus", "tommy hilfiger", "a jug"], "metric_results": {"EM": 0.5, "QA-F1": 0.6475354809986196}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.11764705882352941, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.7000000000000001, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.7058823529411764, 0.8421052631578948, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.1818181818181818, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-3449", "mrqa_newsqa-validation-379", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-808"], "SR": 0.5, "CSR": 0.5222886029411764, "EFR": 1.0, "Overall": 0.6975827205882353}, {"timecode": 68, "before_eval_results": {"predictions": ["the 2016 season", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Danny Elfman", "Olivia Olson", "16 May 2007", "Paul Rudd", "Kaitlyn Maher", "4 January 2011", "her brother, Brian", "the ABC adventure fantasy television series Once Upon a Time", "Bindusara", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "NFL", "Davos", "Neil Patrick Harris", "1946", "Joel", "the vascular cambium", "either late 2018 or early 2019", "American rock band R.E.M.", "Jewish audiences", "a chemical element with symbol I and atomic number 53", "the Ark of the Covenant ( the Aron Habrit in Hebrew", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida", "Iran", "the beginning of the Wizarding World shared media franchise", "the inventor Bi Sheng", "the Prince - Electors", "the fourth Anglo - Mysore war", "Kid Creole & The Coconuts", "a god of the Ammonites, as well as Tyrian Melqart", "late - night", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "By 1770 BC", "Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "1825", "Miracle", "Dumfries and Galloway, south-west Scotland", "High Knob,", "President Obama and Britain's Prince Charles", "NATO fighters", "age 19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "lullaby", "E. E. Cummings", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.5, "QA-F1": 0.646936382819377}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.4, 0.8, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.8837209302325582, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.26666666666666666, 1.0, 0.15384615384615385, 1.0, 1.0, 0.8, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-158", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.5, "CSR": 0.5219655797101449, "EFR": 0.96875, "Overall": 0.6912681159420291}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Manchester United Football Club", "the Coercive Acts", "skeletal muscle and the brain", "libretto", "prophets and beloved religious leaders", "2015", "St. Louis Cardinals", "Andy Serkis", "Panning", "September 21, 2017", "to a `` crummy '' hotel in Greenwich Village circa 1964 or 1965", "Virginia Beach is an independent city located on the southeastern coast of the Commonwealth of Virginia in the United States", "Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "eagles", "eleven", "10.5 %", "Roger Dean Stadium", "`` Blood is the New Black ''", "Otis Timson", "four", "colonies of North America", "routing information base ( RIB )", "James Rodr\u00edguez", "AD 95 -- 110", "Johnson", "more than 2,500 locations in all states except Alaska, Hawaii, Connecticut, Maine, New Hampshire, and Vermont", "from the top of the leg to the foot on the posterior aspect", "in awe of Novalee, and had seen her enter the store at closing time, smashes through the window to help deliver her child", "Ashoka", "dermis", "Hodel", "October 27, 2017", "Howard Caine", "to a popular medieval given throughout Europe, coming from the biblical name, Thomas being one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Agamemnon", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the courts", "September 29, 2017", "around 10 : 30am", "Angola", "Norway", "Manley", "December 15, 2017", "Wyatt", "New Year\u2019s Eve", "e pluribus unum", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "to back one side or the other.", "At least 40", "Juan Martin Del Potro.", "the Aral Sea", "Sweden", "photoelectric", "South-West Africa (Deutsch-S\u00fcdwestafrika)"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6506727378225223}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.896551724137931, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-7612", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-8395", "mrqa_triviaqa-validation-5834"], "SR": 0.53125, "CSR": 0.5220982142857142, "EFR": 0.9333333333333333, "Overall": 0.6842113095238096}, {"timecode": 70, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.8125, "KG": 0.47421875, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, another compartment, both at the cell surface ( particularly caveolae internalization ) as well as at the Golgi apparatus", "Lagaan ( English : Taxation", "Super Bowl XXXIX", "close quarters and poor hygiene exhibited at that time Athens became a breeding ground for disease and many citizens died including Pericles, his wife, and his sons Paralus and Xanthippus", "September 2017", "Kanawha River", "12.65 m", "1820s", "the customer's account", "D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "prison", "Supreme Court of Canada", "July 1, 1923", "an earthquake", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "irsten Simone Vangsness", "Frankie Laine's `` I Believe ''", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "the 2002 Tamil film Ramanaa", "RAF Coningsby in Lincolnshire", "President", "de pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Ferrari driver Sebastian Vettel", "Tiger Woods", "2018", "Speaker of the House of Representatives", "the final scene of the fourth season", "Lord's", "mid-size four - wheel drive luxurySU manufactured by Magna Steyr ( formerly Steyr - Daimler - Puch ) in Austria", "Ingrid Bergman", "Malayalam", "Hem Chandra Bose, Azizul Haque", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "The terrestrial biosphere", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "on a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "eyes", "Vietnam", "Jason Voorhees", "Canada", "Robert Jenrick", "Srinagar", "Jewish", "the Dalai Lama's", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "wyatt", "electric currents"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6547286639458154}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9523809523809523, 0.42857142857142855, 0.0, 0.29411764705882354, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.21739130434782608, 0.8363636363636363, 0.6666666666666666, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_triviaqa-validation-949", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.515625, "CSR": 0.5220070422535211, "EFR": 0.7741935483870968, "Overall": 0.6568182431281235}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Justin Timberlake", "the following day", "Conservative Party", "Judi Dench", "a scuffle with the Beast Folk", "six degrees of freedom", "Spanish moss ( Tillandsia usneoides )", "Matt Monro", "1990", "Friedman Billings Ramsey", "in the beta cells of the islets of Langerhans", "Daytona Pole Award winners", "Charles Carroll of Carrollton", "1959", "many forested parts", "Lysander", "in and around an unnamed village", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "V\u1e5bksayurveda", "middle of the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Benzodiazepines", "April 1, 2016", "its absolute temperature", "to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "July 1, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "on the 15th day of the first calendar month", "Phosphorus pentoxide", "a cake", "1886", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the pulmonary arteries", "spacewar", "2016", "1799", "a boy name", "Zachary Taylor", "Oscar Wilde", "S7", "The New Yorker", "Citgo", "school in South Africa", "Jenny Sanford,", "Rolling Stone", "nuggets", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7000389032872585}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 0.4, 0.6666666666666666, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.3333333333333333, 1.0, 0.9090909090909091, 1.0, 0.2, 1.0, 0.19999999999999998, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_newsqa-validation-3376", "mrqa_searchqa-validation-10641"], "SR": 0.578125, "CSR": 0.5227864583333333, "EFR": 0.9629629629629629, "Overall": 0.6947280092592593}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "General George Washington", "Charles Lebrun", "Ed", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "A. Philip Randolph", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "First Lieutenant Israel Greene", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season of the television series How I Met Your Mother and 61st overall", "the Kansas City Chiefs", "Yuzuru Hanyu", "April", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "not restricted to voting for one of the nominated candidates and may vote for any person, even for someone who is not a member of the House at all", "Gloria", "Ali", "Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a forest", "New Jersey Devils", "13 episodes", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Jeff Gillen", "the Korean Empire", "Djokovic", "won gold in the half - pipe", "Judy Collins", "2002", "Georgia Nic Nicholson", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York,", "Hamburger Sport-Verein e.V.", "2", "The Los Angeles Dance Theater", "100 meter", "Sheikh Sharif Sheikh Ahmed", "Brooklyn, New York,", "the suntory distillery", "scotland", "the king of Babylon", "video game"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6584829592103298}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.47619047619047616, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.13793103448275862, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.25, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1335", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-14264", "mrqa_hotpotqa-validation-1074"], "SR": 0.59375, "CSR": 0.5237585616438356, "EFR": 0.9230769230769231, "Overall": 0.6869452219441518}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "A pop and R&B ballad", "October 1986", "Disha Vakani", "the lower motor neurons", "Johannes Gutenberg of Mainz", "Shawn Wayans", "the United States ( USA )", "A regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Brazil", "March 31 to April 8, 2018", "military units from their parent countries of Great Britain and France, as well as by American Indian allies", "the radius R of the turntable", "the United Kingdom ( UK )", "1945", "CeCe Drake", "April 4, 2017", "post translational modification", "1960 Summer Olympics in Rome", "the Naturalization Act of 1790", "September 6, 2019", "Bulgaria", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff", "Werner Ruchti", "Brooklyn, New York", "Chris Rea", "Julie Adams", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "General George Washington", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350", "Uruguay", "to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority ''", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2002", "Gemma Baker", "Cress", "Montr\u00e9al", "Prince Edward, Duke of Kent", "Leslie Lynch King, Jr.", "Bank of China ( Hong Kong) Limited", "Mumbai, Maharashtra", "Corendon Dutch Airlines", "Mark Sanford.", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "King of Africa", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6927596080882719}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0909090909090909, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.28571428571428575, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5, 1.0, 0.7586206896551725, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4545454545454545, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.8, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_newsqa-validation-3372", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.53125, "CSR": 0.5238597972972974, "EFR": 0.9, "Overall": 0.6823500844594594}, {"timecode": 74, "before_eval_results": {"predictions": ["Lgion d'honneur", "Shaft", "a retronym", "berenice", "pharaoh", "Tony Dungy", "The Heats", "melbourneetta", "cayenne pepper", "quizlet", "universal and equal suffrage", "60", "Enigma", "a tornado", "a matinee", "Lord Tennyson", "Laryngitis", "Gentle Ben", "terraces", "a voodoo sorcerer", "Aquiline", "\"The Night Digger\"", "a cozy", "\"Magnificent Inn\" Grand Hotel", "Davenport", "Sammy Sosa", "car", "One billion", "the green-eyed monster", "Mount Olympus", "haematoma", "the four horsemen", "Coral snake", "William Tecumseh Sherman", "Fess Parker", "a duvet", "Baltimore", "the Freshwater", "Japan", "\"Liberty, equality, Fraternity\"", "the African Union", "Vernon Kell,", "Nepal", "USDA", "cat scratch fever", "freezing", "(Diane) Arbus", "the Right to a Fair Trial", "Whatchamacallit", "(Johnny B. Goode) singer/guitarist", "The Plane! Da Plane", "humans", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body", "lord melbourne", "Sororicide", "Saint Aidan", "Sulla", "in the Appenzell Alps", "Parlophone Records", "keyboardist and", "150", "a real person to talk to,\"", "the contestant"], "metric_results": {"EM": 0.484375, "QA-F1": 0.515070564516129}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-7901", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1931", "mrqa_hotpotqa-validation-4525", "mrqa_newsqa-validation-1890", "mrqa_naturalquestions-validation-5636"], "SR": 0.484375, "CSR": 0.5233333333333333, "EFR": 1.0, "Overall": 0.7022447916666666}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "(Luke) Jackson", "Louisiana", "a clapper", "Tombs of Kobol", "The Sound and the Fury", "a sandwich", "six", "Cosmo Kramer", "Poetic Justice", "the guillitine", "the Colossus of Rhodes", "Hugh Jackman", "silver", "Lebanon", "the eagle", "The Communist Party of China", "Larry King", "Hamlet", "Mussolini", "Margot Fonteyn", "Alfred Nobel", "lifejackets", "superlative", "General Mills", "Emmitt Smith", "clay", "a black hole", "Kampala", "Department of the Clerk of the U.S. House", "Heisenberg", "Sin City", "David Hyde Pierce", "the early predecessors of program music,", "the Old North Church", "bones", "Red Bull", "The pirate flag", "the North West Territories", "Alaska", "the Electric Company", "Vienna", "the City of Bridgeport, Connecticut", "Red", "water", "Ellen Wilson", "Esau", "skull", "Agatha Christie", "Ronald Reagan", "Ford Motor Company", "1947", "American actress Moira Kelly", "Zoe Zebra", "Mt Kenya", "Christian Wulff", "Zelle", "Princess Aisha bint Hussein", "French", "England", "Kaka", "133", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.5, "QA-F1": 0.5975861378205127}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-9504", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-11498", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-1497", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.5, "CSR": 0.5230263157894737, "EFR": 1.0, "Overall": 0.7021833881578947}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "the Treasury", "Montserrat", "a cyclone", "the Starland Vocal Band", "gallows", "the ohm", "Paul Newman", "earthquakes", "the Potomac", "Indiana", "Mary", "Hulk Hogan", "air", "Russia", "Adam Sandler", "Paul Newman", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "a sack dress", "Bobby McFerrin", "the Navy", "Capitol Hill", "a glider", "a heart", "Guyana", "a jelly", "camels", "drought", "a vinculo matrimonii", "Jonathan Winters", "Pink", "Rhode Island", "Newton", "the World", "Paul Newman", "Paul Newman", "gold", "Muhammad", "Jamestown", "a coal", "Seymour Cray", "Private Practice", "steroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "a Neptune", "Scotland", "yellow", "Lakeside Shopping Centre", "SBS", "\"Eternal Flame\"", "Tomas Olsson,", "51 percent of the U.S. public consider China a military threat,", "Appathurai", "anxiety"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7390625}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-12072", "mrqa_searchqa-validation-16881", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7732", "mrqa_hotpotqa-validation-1377", "mrqa_hotpotqa-validation-512", "mrqa_newsqa-validation-1313", "mrqa_naturalquestions-validation-4442"], "SR": 0.6875, "CSR": 0.5251623376623377, "EFR": 1.0, "Overall": 0.7026105925324675}, {"timecode": 77, "before_eval_results": {"predictions": ["Charles Darwin", "Inuit", "... Bologna", "Billy the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan of the Apes", "Edmund Tudor", "Lenin", "Belgium", "Wendy Beckett", "1066", "ibuprofen", "flibustier", "Dr. George Washington Carver", "\" Bulldog\" Drummond", "Spooky Salem, MA", "the Indus Valley", "the Baltic Sea", "nolo contendere", "gum", "Abel", "Louis XV", "Keith Gretzky", "Anna Karenina", "Sacramento", "the Andes mountain range", "jury dutyserve", "... Scribd", "pajamas", "Muhammad", "Paul Newman", "Charles H McKenzie 16th Judicial Circuit (Jackson County)", "lalique.com", "Rhode Island", "...The Simple Life", "Laos", "Agent Orange", "the Philippines", "Kellogg's", "...Haircut 100", "Luxor", "Latin", "Venus", "the Hawthorne", "the Congo River", "Charles VII", "Horatio Nelson, 1st Viscount Nelson", "caiman", "Ferrari", "iris", "John Adams", "1886", "Ali", "Tahrir Square", "World War I", "Hedonismbot", "ESPN College Football Friday Primetime", "R&B vocal group", "\"When the Levee Breaks\"", "protective shoes", "Nkepile Mabuse", "officers at a Texas  airport", "silver"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6674107142857143}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-11115", "mrqa_searchqa-validation-7197", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-385"], "SR": 0.515625, "CSR": 0.5250400641025641, "EFR": 1.0, "Overall": 0.7025861378205127}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "Eve", "The Firm", "Messerschmitt", "circumnavigate", "Marilyn Monroe", "Cheddar", "a comet", "wings", "the Enigma", "surface-to-air", "the igloo", "Phobos", "a dermatologist", "Kramer vs. Kramer", "The Tempest", "purple", "Karen", "tire", "Schwarzenegger", "(William) Kosciuszko", "John Bayley", "Ironman", "Kamba", "the National Hockey League", "Chiffon", "(sur un manche)", "Ramses the Great", "Scheherazade", "Scott McClellan", "Jeremiah", "Thomas Edison", "The Chorus Line", "Guadalajara", "Sydney", "flavor", "Dutchman", "Gideon", "the Alamo", "oats", "Zlatan Ibrahimovic", "Pell grants", "Rush", "being buried alive", "Swan", "Kansas", "Helsinki", "kidney", "One Flew", "the Nobel Prize in Literature", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "the Standard Motor Company", "Portugal", "co-", "The Disney Sunday Movie series", "Juan Manuel Mata", "Madeleine L'Engle", "The British troops who are being pulled out include Royal Navy servicemen who have been helping the Iraqis to protect oil fields around the port town of Umm Qasr,", "three", "the Iraq's autonomous region of Kurdistan.", "Tom Ewell (born Samuel Yewell Tompkins, April 29, 1909 \u2013 September 12, 1994)"], "metric_results": {"EM": 0.5, "QA-F1": 0.5915922619047618}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2666666666666667]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-4600", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-11537", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-1169", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-8426", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-5300", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-5904", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010", "mrqa_hotpotqa-validation-4597"], "SR": 0.5, "CSR": 0.5247231012658228, "EFR": 1.0, "Overall": 0.7025227452531645}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "sport", "Peter", "a puppy", "New Zealand", "fontanels", "California", "Augustus", "the Dalmatians", "(Daniel) Day-Lewis", "cotton", "Bridget Fonda", "South Africa", "(Billy & Billie)", "the Mediterranean", "Catherine de' Medici", "potato pancake", "the adder", "a Crossword", "the River Thames", "(chantar)", "Pitcairn", "Adam Sandler", "Mayo", "\" Shut up, just shut up\"", "arrested development", "(the Renaissance)", "German", "Rodeo", "repent", "Denzel Washington", "(Bonn)", "nougat", "(Louise) Davis", "rani", "Louis Comfort Tiffany", "Louise", "conk", "Hillary Clinton", "globalization", "Van Halen", "the French annexation of the County of Nice", "salt", "Samsonite", "chili", "salamu", "Faraday", "necklaces", "Norse", "Niagara Falls", "the Bronx", "the National Football League ( NFL ) for the Atlanta Falcons, the San Francisco 49ers", "Ethel Merman", "Chung", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "615", "over two decades.", "health-care", "14", "8th and 16th"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5959821428571428}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.28571428571428564, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-11493", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-8019", "mrqa_searchqa-validation-2004", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5256", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-3765"], "SR": 0.484375, "CSR": 0.52421875, "EFR": 1.0, "Overall": 0.702421875}, {"timecode": 80, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.85546875, "KG": 0.5, "before_eval_results": {"predictions": ["(George Washington) Washington", "the National Hockey League (NHL)", "blue", "Georgia", "(General William) Devereaux", "pesticides", "the English Channel", "William Shakespeare", "France", "Thornton Wilder", "Baton Rouge", "cupboard", "frittata", "pardon", "Bartholomew", "lymphoma", "Target", "Regrets", "a possum", "Shaun of the Dead", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "vacation", "it is best not to take risks even when it seems boring or difficult", "Canaan", "Yogi Bear", "Idaho", "Georgia O'Keeffe", "a car", "12:30 pm", "Frederick Douglass", "skyscraper", "(William) Bonney", "The Killing Fields", "Oliver Twist", "a landmark", "eggplant", "bread", "Boston", "Martinique", "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb", "the Grand Canal", "Sons of Liberty", "a telescope", "Catholic", "the tuba", "the quarterback throws passes", "a Platonic solid", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II", "eight", "dragonflies", "cranberries", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "2006", "he and Armento, 51, were drinking at a strip club when they decided to go hunt for valium"], "metric_results": {"EM": 0.625, "QA-F1": 0.6763392857142857}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-4633", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-16215", "mrqa_searchqa-validation-16754", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.625, "CSR": 0.525462962962963, "EFR": 1.0, "Overall": 0.7234519675925926}, {"timecode": 81, "before_eval_results": {"predictions": ["order", "Warsaw", "Katrina & the Waves", "the French & Indian War", "Brady", "philosophy", "the American Red Cross", "harm", "Luck Of TheDraw", "As Good as It Gets", "pickles", "a bull", "neurons", "Evian", "\"to steal someone's thunder\"", "The Life and Death of a Man of Character", "the olfactory nerve", "a window", "Isaac Newton", "SpeedMatch", "Alexander Hamilton", "the Colorado River", "Dune", "opera", "YouTube", "heresy", "TV", "Charlie Watts", "a black widow spider", "the portier", "Virginia", "abundant", "Albert Schweitzer", "The hemisphere of the brain", "a dive bomber", "Toulouse-Lautrec", "Helen Hayes", "the Vulgar Tongue", "a chattery chattery", "Herbert George Wells", "\"Sex In Crazy Places\"", "Terry Caster and his wife, Barbara", "the Hippopotamus", "Nietzsche", "\"It\\'s a dog eat dog world, Woody", "Alexander Hamilton", "Israel", "Niagara Falls", "a rudder", "carotenoids", "the Flintstones", "Abanindranath Tagore", "at slightly different times when viewed from different points on Earth", "the trunk", "Carrefour", "Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano", "national telephone", "the Christian Film & Television Commission", "John Carpenter"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6036255411255411}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.060606060606060615, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-6517", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-2805", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-14639", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846", "mrqa_newsqa-validation-175", "mrqa_triviaqa-validation-5750"], "SR": 0.484375, "CSR": 0.5249618902439024, "EFR": 0.9696969696969697, "Overall": 0.7172911469881744}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "The Big Easy,", "the beaver", "Dorothy", "Survivor: Fiji", "Wild Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Eternity", "Marvell", "Quiz Show", "NFL", "acetone", "Donald Trump", "Psycho", "Napoleon", "lullaby", "the capuchins", "Napoleon", "the West", "the reticulated python", "Munich", "digestif", "a straggler", "Pope Benedict XVI", "Los Alamos Scientific Laboratory", "Somerset Maugham", "a sapphire", "Three Coins in the Fountain", "ER", "Goldenrod", "Luke", "the rectum", "a neck warmer or scarf", "frequency", "Grease", "the salamander", "Alexander Solzhenitsyn", "Eyebrows", "The Romaunt", "Guyana", "Charlie Bartlett", "British", "the Big Sky Conference", "Beavers", "Boston", "Michelle Pfeiffer", "a ruckus", "Sweden", "UK Sinha", "the 17th episode in the third season", "94 by 50", "Salix", "the 7th", "the seas around the British Isles", "the University of Kentucky", "Love at First Sting", "1988", "Hollywood", "severe flight delays and some cancellationations along the East Coast.", "$10 billion", "her boyfriend,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7092261904761905}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_newsqa-validation-909"], "SR": 0.609375, "CSR": 0.5259789156626506, "EFR": 1.0, "Overall": 0.7235551581325301}, {"timecode": 83, "before_eval_results": {"predictions": ["the Gulf of Tonkin", "Stitch", "Joe Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafarianism", "cinnamon", "Pirates of Penzance", "make sense", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Asklepios", "troll", "Der Fliegende Hollander", "Dan Quayle", "Ruth", "Answer Who is", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "Kiribati", "Nepal", "Palladio", "God", "Indiana", "Hair", "cicadas", "Asbury Park", "in darkness", "the saguaro", "Zappa", "Hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "a loaf of bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Porridge", "Thermopylae", "Magdalene Laundries", "\"$10,000 Kelly,\"", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7434895833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-3313", "mrqa_searchqa-validation-401", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-1509"], "SR": 0.671875, "CSR": 0.5277157738095238, "EFR": 1.0, "Overall": 0.7239025297619047}, {"timecode": 84, "before_eval_results": {"predictions": ["the typing speed", "a Crescent", "a trident", "Abercrombie & Fitch", "H. L. Hunley", "Standard Oil", "the Crustacea", "Laura Ingalls Wilder", "a carriage", "Monet", "chemicals", "Gerald Ford", "Louis Rukeyser", "Jupiter", "Clinton", "Truisms", "Tin", "Stephen Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia", "The Star-Spangled Banner", "abbreviated", "Heroes", "a large meal", "Kublai Khan", "Lafitte", "the Flushing River", "a relic", "cyclosporine", "the Northern Mockingbird", "a RESTRICTIVE TYPE OF THIS, CLAUSE", "Comedy", "a owl", "the perimeter", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "the seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "the Big Dipper", "1924", "741 weeks", "January 17, 1899", "Douglas MacArthur", "Project Gutenberg", "New Guinea", "Latin American culture", "a farmers' co-op", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million"], "metric_results": {"EM": 0.671875, "QA-F1": 0.71875}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-1304", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_searchqa-validation-6009", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-6220", "mrqa_hotpotqa-validation-3921"], "SR": 0.671875, "CSR": 0.5294117647058824, "EFR": 1.0, "Overall": 0.7242417279411765}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Ellen Holly", "the Prince & Pauper", "Pushing Daisies", "Independence", "the reaping machine", "Five Horizons", "Lent", "apples", "Jesus Sirach", "New Brunswick", "Lake County, Indiana", "Cleopatra", "a northern pike", "Krispy Kreme", "New York", "Luther", "rice", "Frasier", "Kansas City", "arteries", "The Godfather", "improvisation", "Hamlet", "lime", "Antichrist", "alkaline nedir, ne demek, alkaline anlam", "Robert Evans", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "The Eyes Were watching God", "If I Were a Rich man", "Pitcairn Island", "hockey", "etching", "Mars", "a turtle shell", "David", "an extra holiday", "a cookie jar", "Babe Ruth", "a cheesesteak", "Nicky Hilton", "he was unable to wrest", "September 25", "Jessica Simpson", "William Schuman", "the rose bush", "Robert Plant", "Oklahoma", "138,535 people", "Martin Scorsese", "her son has strong values.", "a Burmese python", "The eye of Hurricane Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.5, "QA-F1": 0.6340182387057387}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6539", "mrqa_searchqa-validation-7092", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-995", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13590", "mrqa_searchqa-validation-12814", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.5, "CSR": 0.5290697674418605, "EFR": 1.0, "Overall": 0.7241733284883721}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "the president of Nicaragua", "Chastity", "Frank Sinatra", "Dmitri Mendeleev", "Norman Mailer", "Blitzkrieg", "luminous intensity", "Canty", "the Eurasian Economic Union", "Christina Ricci", "(Paul) Jones", "The Rolling Stones", "(CNN)", "(Andrew) Alito", "kings", "Civic", "Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Rich Girl", "Yogi Berra", "courage", "a jigger", "folate", "a constitution", "the eastern Mediterranean", "virtual reality", "bass", "The Last Remake of Beau Geste", "hot air balloons", "Tarzan & Jane", "the Stanley Cup", "(David) Berkowitz", "oblique", "a fudge", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Shawneetown", "Little Buddha", "the Bolsheviks", "April 17, 1982", "Garden of Gethsemane", "the Vi\u1ec7t Minh and France", "James Cameron", "\"My Sweet Lord\"", "Japan", "( Archie) Peck", "the Kingdom of Dalmatia", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6911611519607843}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.4, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-4824", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-5748", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-16572", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-2007", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4669"], "SR": 0.59375, "CSR": 0.5298132183908046, "EFR": 1.0, "Overall": 0.7243220186781609}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "a hand-powered multiple spinning machine", "onerous", "a Clown", "Fargo", "the Dailies", "fibreboard", "the River Thames", "Napster", "a member of the musical Partridge family", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "dementia", "lightest interchangeable lens full-frame camera", "the Lowest point", "the Golden Fleece", "the kingdom of God", "if you commit a minor crime", "Macaulay Culkin", "the Liverpool & Manchester", "Edwards", "Hawaii", "John F. Kennedy", "the Daniel Boone National Forest", "a minimum", "hemoglobin", "Nancy Sinatra", "ear", "the foxes", "tabby", "Amerigo Vespucci", "Wisconsin", "the Republic of Iraq", "Canada", "bipolar disorder", "a brownie", "a chestnut-tree", "Alexander Calder", "honey", "(Matthew) Broderick", "(Benjamin) Duke of Brittany", "Spider-Man", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "Kansas City, Missouri,", "axiom", "electors", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "the throat", "sea birds", "Meta", "Agent Carter", "the Sasanian Empire", "\"Kill Your Darlings\"", "If the oceans are growing crowded, and governments are increasingly trying to plan their use.", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6417489035087719}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.10526315789473682, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-2933", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-16734", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-6696", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_hotpotqa-validation-172", "mrqa_newsqa-validation-4165"], "SR": 0.578125, "CSR": 0.5303622159090908, "EFR": 1.0, "Overall": 0.7244318181818181}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "high chairs", "Biggie", "(John) Jesus", "John Paul II", "Hillary Clinton", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "James Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "the wagler", "Twister", "Spain", "Scrabble", "the Caspian Sea", "football", "Los Angeles Angels of Anaheim", "Cardiff", "the Ten", "time", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "rowing", "Transamerica", "Xinjiang", "it can be a year of concern", "the Delacorte", "Henry Clay", "the wire loop", "Petsmart", "On the Origin of Species", "Electric Avenue", "a bibliography", "Jerusalem", "Vanna White", "Toyota", "a (cella)", "Istanbul", "Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "Elsa", "ritual harae", "September 24, 2012", "early 1960s", "Taron Egerton", "a linesider", "Henry of Valence", "The Undertones", "Groupe PSA", "Premier Division", "The SoLow Project", "stabbed Tate,", "Herman Cain", "a grizzly bear", "nolte"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5919270833333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5520", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-4664", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714", "mrqa_triviaqa-validation-7327"], "SR": 0.515625, "CSR": 0.5301966292134832, "EFR": 1.0, "Overall": 0.7243987008426966}, {"timecode": 89, "before_eval_results": {"predictions": ["the nivalis", "Finding Nemo", "easel", "the 49 Thumb piano and magnetic tape", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "England", "Denmark", "the saguaro", "Saigon", "kami-no-michi", "\"reshit\"", "Venus", "an iris", "Chanel Iman", "Armistice", "toilet paper", "the Panama Canal", "Cesare Borgia", "May", "brandy", "Hangman", "Charles Dickens", "October", "Camptown Races", "henrik Ibsen", "Linkin Park", "a doggy", "storm surge", "the lungs", "gravity", "Elizabeth Cook", "Robert Bruce", "Marlon Brando", "the 17th President of the United States", "Lana Turner", "a nock", "Othello", "Emiliano Zapata", "Bone Thugs-N-Harmony", "zebras", "Helio Castroneves", "Richard III", "Hugh Grant", "waiting for Godot", "voyeurism", "the Articles of Confederation", "Pavlov", "a hull", "Doll", "England, Northern Ireland, Scotland and Wales", "James Madison", "The Firm", "Harriet Tubman", "the Hebrew alphabet", "\" Finding Nemo\"", "his superhero roles", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6805059523809524}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6]}}, "before_error_ids": ["mrqa_searchqa-validation-11505", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2849", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-12554", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-13729", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-2737", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099", "mrqa_naturalquestions-validation-1848"], "SR": 0.609375, "CSR": 0.5310763888888889, "EFR": 1.0, "Overall": 0.7245746527777778}, {"timecode": 90, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.814453125, "KG": 0.503125, "before_eval_results": {"predictions": ["Wisconsin", "Gonzo", "a stagecoach", "Henry Winkler", "faction & action", "Hasta la vista", "Indiana", "croissant", "bat", "Tunisia", "a plexus", "a rattlesnake", "(Nicholas) Massie", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "\"AA\"", "Catherine of Aragon", "Hawaii", "Ravi Shankar", "Bangkok", "Spain", "archery", "slanting", "( Joe) Torre", "meatballs", "Kennedy Space Center", "a Rosetta Stone", "Pilate", "the United States", "Marco Polo", "an adder", "paddy", "Matt Leinart", "Alabama", "ayahuasca", "Queen Anne", "the banjo", "second feature", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime minister", "Fi", "Motown / Stax", "in AD 95 -- 110", "pepsinogen", "Valentino Rossi", "1919", "Paris", "Point Place,", "11", "National Aviation Hall of Fame", "Thursday", "78,000 parents of children ages 3 to 17.", "prisoners at the South Dakota State Penitentiary", "Anne Boleyn"], "metric_results": {"EM": 0.75, "QA-F1": 0.7926897321428572}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-10419", "mrqa_triviaqa-validation-3485", "mrqa_newsqa-validation-3194"], "SR": 0.75, "CSR": 0.5334821428571428, "EFR": 0.9375, "Overall": 0.7026339285714286}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a Chile Relleno", "Oliver Twist", "Vampire Slayer", "the Vistula", "Coriolanus", "Fort Worth", "an aide", "an oblique fracture", "Roman Polanski", "Court TV", "the Horn of Africa", "Jake La Motta", "Mastering the Art of French Cooking", "Pan Am", "Athens", "Holiday Inn", "Buffalo Bills", "Bret Harte", "Islam", "(Madeleine) Albright", "Mount Everest", "the Renaissance", "Calamity Jane", "John Lennon", "(Richard) Branson", "a catcher", "lights", "Tarzan", "Once", "Warren G. Harding", "Berrigan", "Marilyn Monroe", "Daedalus", "Flanders Field", "London", "Bonnie Raitt", "Man Friday", "Lord North", "Doublemint", "a shilling coin", "a narwhal", "the wall", "John Marshall", "Wyatt Earp", "Punjabi", "Athens", "Department of Agriculture", "heels", "Frottage", "a complementary angle", "1999", "cheated", "2017", "Oskar Schindler", "harrison ford", "Tallinn", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million", "Charlotte Gainsbourg", "improve the environment by taking on greenhouse gas emissions.", "Audrey Roberts"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7153645833333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-5282", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-14537", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.640625, "CSR": 0.5346467391304348, "EFR": 1.0, "Overall": 0.7153668478260868}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Fiddler on the Roof", "Muhammad Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Sinclair Lewis", "Captains Courageous", "the handles", "Central Park", "the nave", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "Monty Python's Flying Circus", "Al Gore", "an asteroid", "first base", "cork", "Ichabod Crane", "the king", "Chinatown", "a butterfly", "Lolita", "Nacre", "tango", "Wesley Clark", "ponnon", "a saint", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "The Museum of Modern Art", "canals", "the Galatians", "Lewis Carroll", "parking meters", "ribs", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "Edouard Manet", "sons", "The Hairy Ape", "Jason Flemyng", "eight", "UK or overseas in the British Armed Forces or with Her Majesty's Government", "Nicholas Garland", "John Quincy Adams", "dordogne Valley of France", "1968", "Vinnie Jones, Scot Williams, and Vytautas \u0160apranauskas", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides", "1957"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7342905405405405}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5945945945945945, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-14833", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-2156", "mrqa_triviaqa-validation-6104", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236"], "SR": 0.6875, "CSR": 0.5362903225806452, "EFR": 0.9, "Overall": 0.695695564516129}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet On The Western Front", "the Rhine & the Main", "Kingston", "one for the Road", "Indiana", "Walt Kelly", "a kidney", "Paris", "\"A Few Notes on The Martian Chronicles\"", "China", "Maine", "Gertrude Stein", "Hemingway", "a dishwasher", "Da Vinci Code", "cricket", "Death", "the Kuche", "Rouen", "United Airlines", "Notre Dame", "Tiberius Nero", "Jupiter", "loverly", "rugby", "the Falkland Islands", "Broadway", "Iceland", "Tintin", "chess", "a porter", "Jonathan Swift", "Miracle on 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "fuel for cooking, central heating and to water heating", "the Mesozoic", "Dwight D. Eisenhower", "\"For What It\\'s Worth\"", "the fourteen points", "\"Seven Seas of Rhye\"", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "( Wiley) Post", "the Misty Mountains", "a cantaloupe", "London", "Carl Sandburg", "a federal republic", "The Enchantress", "James Earl Jones", "the medical profession", "kenry", "the Treaty of Waitangi", "Jessica Lange", "Heinkel He 178", "Kenan & Kel", "304,000", "one", "around 8 p.m. local time Thursday", "work between fires"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6494791666666666}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-11614", "mrqa_searchqa-validation-1263", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-15431", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-13738", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-5792", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4060"], "SR": 0.59375, "CSR": 0.5369015957446808, "EFR": 1.0, "Overall": 0.715817819148936}, {"timecode": 94, "before_eval_results": {"predictions": ["(E.B. White)", "Logan's Run", "Ricardo Sanchez Robert Gates", "a zoos", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Doolittle", "a crowd protesting against another group, a government policy,", "Lon Chaney", "New York", "the Nathan Coen", "Sicily", "Boston Celtics", "sugar", "Enron", "the fulcrum", "the Central African Republic", "Rudolf Hess", "one's dissatisfaction", "the hippopotamus", "an eye", "Bech", "Reagan & Mondale", "Washington Irving", "the White Mountains of California", "the Romans", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "(Jerry) Mathers", "9 to 5", "the U.S. Department of Housing and Urban Development", "the extradition", "the head", "Eddie Murphy", "Michael Collins", "The Sopranos", "The Sound And The Fury", "the mother-aughters dyad", "Brazil", "obsessive-compulsive", "Timothy Hutton", "oatmeal", "the arteries", "1773", "a joule", "Justice", "20 November 1989", "about 8 : 20 p.m. on 25 September 2007", "the forces of Andrew Moray and William Wallace defeated the combined English forces of John de Warenne, 6th Earl of Surrey, and Hugh de Cressingham near Stirling, on the River Forth", "Nafea Faa Ipoipo?", "a window", "St. Augustine", "commentary on Isaac Newton's book \"Principia\"", "PETE", "SKUM", "backbreaking labor", "Joan Rivers", "second", "Billy Ray ( Harry Dean Stanton )"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6489206259426847}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.9411764705882353, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-5262", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-10541", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-1950", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-2638", "mrqa_naturalquestions-validation-6460"], "SR": 0.515625, "CSR": 0.5366776315789474, "EFR": 1.0, "Overall": 0.7157730263157894}, {"timecode": 95, "before_eval_results": {"predictions": ["Leonid Kuchma", "London Broils", "the Communist Party", "The Goonies", "Velvet Revolver", "Haunted Mansion", "the Continental Congress", "Robert Johnson", "Mahlemuts", "a shank", "fish", "a parens", "Casablanca", "The Dutchess", "the Detroit River", "George Sand", "Caliber", "Kilimanjaro", "Nebuchadnezzar", "a flip", "a komodo dragon", "Mordecai Richler", "The Simpsons", "The West Wing", "curry powder", "ravens", "prika", "Beckren", "Pocahontas", "viruses", "Hersey", "Patricia Arquette", "Ernie Banks", "a Grotto", "Prince Harry", "Elizabeth Barrett Browning", "Hades", "Whigs", "Capone", "Maria Callas", "Beckame", "the Medieval Times", "Ptolemy", "Tennyson", "National Geographic", "From Walt Disney", "Jerusalem", "the nativity scene", "the Edict of Nantes", "Achilles", "Omega", "a punctuation mark written before the first letter of an interrogative sentence or clause to indicate that a question follows", "six doctors from Seattle Grace Mercy West Hospital", "since 3, 1, and 4 are the first three significant digits of \u03c0", "paul esterh\u00e1zy", "exponentiation", "w Worcestershire", "1754", "49", "Lowe's", "Snow,", "Fernando Gonzalez", "Chester Arthur Stiles,", "a wasps"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6413352272727273}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-16080", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-7180", "mrqa_hotpotqa-validation-5354"], "SR": 0.59375, "CSR": 0.5372721354166667, "EFR": 1.0, "Overall": 0.7158919270833334}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a cartwheel", "assemble", "hot air balloons", "personification", "Nomar Garciaparra", "John Glenn", "a heron", "Apollo 1", "the White Company", "New Balance", "\"S.F.\"", "Joan of Arc", "finale", "mollusca", "Camille Claudel", "the East River", "caricaturist", "the Seven Years' War", "\"Pride and Prejudice\"", "The Wizard of Oz", "madding", "tribes", "Zayn", "Argentina", "Woodrow Wilson", "the Osmonds", "LAUGH", "Tribbles", "\"Just the Way You Are\"", "Wyoming", "Tigger", "Geneva", "Frank Sinatra", "kelp", "an Islamic leadership position", "backstroke", "629", "Sydney", "dermatology", "Solomon", "Look Who\\'s Talking", "Chirac", "20/40", "snowmobilers", "My ntonia", "Surinam", "kelp", "the Czech Republic", "the Corinthians", "dilithium", "vinyl", "in 1976", "2010", "1215", "wurst", "President of the United States of America", "Indira Gandhi International Airport", "Robert Gibson", "11", "skeletal dysplasia,", "\"GoldenEye\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5551835317460317}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-1445", "mrqa_searchqa-validation-12162", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-7670", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-3859"], "SR": 0.515625, "CSR": 0.5370489690721649, "EFR": 1.0, "Overall": 0.715847293814433}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomadic", "Washington", "tribbles", "the Death Valley", "The Two Gentlemen of Verona", "Cobb", "Hydra", "Gulliver's Travels", "the Distant Early Warning Line", "\"The Song of Norway\"", "ice cream", "Xinjiang-Uygur Autonomous Region", "sonic boom", "Fergie", "Sacramento", "emerald", "\"Swiss Cheese\"", "Ernest Hemingway", "Blue Mountain", "Annika Sorenstam", "stars", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "(Henry) Shrapnel", "the Andes", "Penelopeia", "Oklahoma City", "the Amazon", "Bob Fosse", "the West African manatees", "rain", "1880", "the French & Indian War", "a checkerboard", "Waterloo", "a waterbed", "monkey", "a bagel", "propeller", "a bonnet", "an acre", "( Alexander) Calder", "a cruller", "helium", "Tokyo", "cheese", "Charles Perrault", "Bali, Indonesia", "c. 1000 AD", "Tony Blair", "bacteria", "\"Big Dipper\"", "Sofia the First", "Africa", "Eric Morecambe & Wise Show", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6911764705882353}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-906", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-773", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_naturalquestions-validation-8823", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.640625, "CSR": 0.5381058673469388, "EFR": 1.0, "Overall": 0.7160586734693878}, {"timecode": 98, "before_eval_results": {"predictions": ["Marley", "magnum", "Ottoman Empire", "Helen of Troy", "a whale", "New York", "Himalayas", "Wayne\\'s World", "Poland", "Kwanzaa", "nuclear submarine", "Russell Crowe", "Edna Grazer", "a Shelby GT350", "tears", "roulette", "W. Somerset Maugham", "Christo", "Matisse", "the sea", "All Quiet On The Western Front", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "indonesia", "Ford", "Sidney Sheldon", "surround", "Faraday", "a pastries", "Krispy Kreme", "the Spanish dignitary", "Stanton Avery", "Death Valley", "the Cumberland Gap", "geolu", "Secretary of Defense", "a place without privacy", "a brown rat", "Cleveland", "Poe", "Belgium", "Chirac", "proclamation", "Destiny's Child", "Luxor", "Spain", "The Beatles", "coconut", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "a finger", "James I", "Macbeth", "Carol Ann Duffy", "Ravenna", "travel", "prevent and difficult to deal with when they occur.", "many Marines we talked to in this coastal, scrub pine-covered North Carolina base are more than excited to go, despite the dangers that await them.", "Ayatollah Bahrami", "make life a little easier"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6441907051282052}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-7831", "mrqa_searchqa-validation-4971", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-5066", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.546875, "CSR": 0.5381944444444444, "EFR": 1.0, "Overall": 0.7160763888888889}, {"timecode": 99, "UKR": 0.677734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.822265625, "KG": 0.4890625, "before_eval_results": {"predictions": ["the Hundred Years' War", "the vertebral column", "Alfred Binet", "Venial", "a caveat", "\"There's no place like home\"", "shrimp", "the Spanish Republic", "Vanessa Hudgens", "King Kong", "Wizard", "Japan", "Rhiannon", "Scotland", "\"The Beaver\"", "Kurdish", "Ann Richards", "half-staff", "France", "Langston Hughes", "Coke", "The Color Purple", "THX surround sound", "Macbeth", "El Greco", "General Motors", "Daily Mail", "a shark", "Frankie Valli", "a blade", "a Backpacking Route", "pineapple", "Buffalo nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "fondue", "a cable TV network", "Schwarzenegger", "AT&T", "Animal Crackers", "Oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Denmark", "Students for a Democratic Society", "All the King\\'s Men", "Charles Gounod", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 14, 2017", "James Mason", "a slide trumpet", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale, North West England", "Matamoros, Mexico,", "Florida", "Capitol Hill.", "775"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7145833333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-15432", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-1302", "mrqa_triviaqa-validation-2452", "mrqa_hotpotqa-validation-1618", "mrqa_newsqa-validation-1996"], "SR": 0.671875, "CSR": 0.53953125, "EFR": 0.9047619047619048, "Overall": 0.686671130952381}]}