{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 7900, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["April 20", "Paul Whiteman", "2005", "Islamism", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "the center of the curving path", "eight", "28", "global", "Fresno", "Amazonia: Man and Culture in a Counterfeit Paradise", "88%", "broken wing and leg", "Emmy Awards", "silt up the lake", "The TEU specifically excludes certain regions, for example the Faroe Islands, from the jurisdiction of European Union law", "legitimate medical purpose", "week 7", "Kromme Rijn", "Robert Boyle", "five", "Paul Samuelson", "The First Doctor encounters himself in the story The Space Museum", "until the age of 16", "80%", "Five", "threatened \"Old Briton\" with severe consequences", "less than 200,000", "Anheuser-Busch InBev", "Charles River", "Veni redemptor gentium", "northwestern Canada", "intracellular pathogenesis", "1998", "seven months old", "The Service Module was discarded", "July", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "Article 17(3)", "Doctor Who \u2013 The Ultimate Adventure", "Eric Roberts", "electricity could be used to locate submarines", "1910\u20131940", "the owner", "2.666 million", "September 1969", "Apollo Program Director", "dreams", "Charles I", "more than 1,100 tree species", "complete the modules to earn Chartered Teacher Status", "true larvae", "apicomplexan-related diseases", "Rev. Paul T. Stallsworth", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "2009", "Daniel Diermeier", "PNU and ODM camps", "136", "12 to 15 million", "flax", "February 1, 2016", "2001", "lipophilic alkaloid toxins"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8867121848739495}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6614", "mrqa_squad-validation-4170", "mrqa_squad-validation-6426", "mrqa_squad-validation-8037", "mrqa_squad-validation-7774", "mrqa_squad-validation-3885", "mrqa_squad-validation-1492", "mrqa_squad-validation-5788", "mrqa_squad-validation-4343"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["Carolina Panthers", "1530", "Gallifrey", "ca. 2 million", "intractable problems", "effective planning", "a nucleomorph", "Canterbury", "1873", "A bridge", "clergyman", "Commission v Italy", "sports tourism", "G", "eating both fish larvae and small crustaceans", "slow to complete division", "Astra's satellites", "T. T. Tsui Gallery", "1905", "theatres", "those who proceed to secondary school or vocational training", "1521", "Search the Collections", "CD8", "3 January 1521", "a bill", "the University of Aberdeen", "2014", "Missy", "US$10 a week", "Super Bowl City", "Dudley Simpson", "esoteric", "temperature and light", "4000 years", "a new entrance building", "Levi's Stadium", "tutor", "the application of electricity", "2007", "Los Angeles", "zeta function", "adviser to churches in new territories", "over $40 million", "Sunday Service of the Methodists in North America", "Lead fusible plugs", "occupational stress", "Synthetic aperture radar", "European Parliament and the Council of the European Union", "the coronation of Queen Elizabeth II", "time O(n2)", "the Main Quadrangles", "35", "quadratic time", "antisemitic", "over-fishing and long-term environmental changes", "the Onon River and the Burkhan Khaldun mountain", "Hassan al-Turabi", "robbery", "Arlen Specter", "it is Oprah's daughters", "Venus Williams", "prisoners at the South Dakota State Penitentiary", "a globose pome"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7407606792717087}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 0.2857142857142857, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3958", "mrqa_squad-validation-4648", "mrqa_squad-validation-8864", "mrqa_squad-validation-9454", "mrqa_squad-validation-7818", "mrqa_squad-validation-1272", "mrqa_squad-validation-2122", "mrqa_squad-validation-2481", "mrqa_squad-validation-9977", "mrqa_squad-validation-1719", "mrqa_squad-validation-1818", "mrqa_squad-validation-2525", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-1148", "mrqa_triviaqa-validation-5325"], "SR": 0.703125, "CSR": 0.78125, "retrieved_ids": ["mrqa_squad-train-83671", "mrqa_squad-train-27388", "mrqa_squad-train-19698", "mrqa_squad-train-33532", "mrqa_squad-train-29419", "mrqa_squad-train-18128", "mrqa_squad-train-58114", "mrqa_squad-train-3840", "mrqa_squad-train-85500", "mrqa_squad-train-51739", "mrqa_squad-train-4473", "mrqa_squad-train-53338", "mrqa_squad-train-54430", "mrqa_squad-train-77552", "mrqa_squad-train-29548", "mrqa_squad-train-49594", "mrqa_squad-validation-4343", "mrqa_squad-validation-3885", "mrqa_squad-validation-6614", "mrqa_squad-validation-6426", "mrqa_squad-validation-5788", "mrqa_squad-validation-1492", "mrqa_squad-validation-7774", "mrqa_squad-validation-4170", "mrqa_squad-validation-8037"], "EFR": 1.0, "Overall": 0.890625}, {"timecode": 2, "before_eval_results": {"predictions": ["advanced research and education networking in the United States", "best-known legend", "Egyptians", "quantity surveyor", "\"missile gap\"", "Tanaghrisson", "1852", "1564", "Concentrated O2", "adenosine triphosphate", "300 men", "11.5 inches", "1964", "infected corpses", "Lck", "Lutheran and Reformed states", "Chinggis Khaan International Airport", "modern buildings", "May through September", "NBC", "Three", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Gymnosperms", "to punish Christians by God", "Word and Image", "Nafzger", "MPEG-4", "BSkyB", "chromalveolates", "embroidery", "three hundred years", "26", "Duran Duran", "warmest regions with hot winds blowing from nearby semi-deserts", "long distance services", "in the chloroplasts of C4 plants", "the violence that subsequently engulfed the country", "primality", "divergent boundaries", "Chancel Chapel", "Kurt H. Debus", "the construction of military roads to the area by Braddock and Forbes", "in bays where they occur in very high numbers", "Dallas", "Ted Heath", "14th-century", "high-voltage", "contract", "Arabic numerals", "critical pamphlets on Islam", "draftsman", "1993\u201394", "$10 billion", "France", "20", "Iran", "two", "the results by a chaplain about 1:45 p.m., per jail policy", "56", "school", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday", "AbdulMutallab", "coca wine", "Dissection"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7833265692640693}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_squad-validation-3480", "mrqa_squad-validation-8905", "mrqa_squad-validation-6548", "mrqa_squad-validation-3270", "mrqa_squad-validation-1909", "mrqa_squad-validation-2565", "mrqa_squad-validation-2906", "mrqa_squad-validation-2899", "mrqa_squad-validation-4322", "mrqa_squad-validation-5600", "mrqa_squad-validation-2291", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1201"], "SR": 0.71875, "CSR": 0.7604166666666666, "retrieved_ids": ["mrqa_squad-train-55074", "mrqa_squad-train-22083", "mrqa_squad-train-27769", "mrqa_squad-train-75548", "mrqa_squad-train-80697", "mrqa_squad-train-59494", "mrqa_squad-train-60518", "mrqa_squad-train-65685", "mrqa_squad-train-1394", "mrqa_squad-train-53669", "mrqa_squad-train-9996", "mrqa_squad-train-64880", "mrqa_squad-train-75234", "mrqa_squad-train-26964", "mrqa_squad-train-46486", "mrqa_squad-train-31711", "mrqa_squad-validation-7774", "mrqa_squad-validation-8864", "mrqa_squad-validation-6426", "mrqa_squad-validation-1719", "mrqa_triviaqa-validation-5325", "mrqa_squad-validation-4170", "mrqa_squad-validation-8037", "mrqa_squad-validation-9977", "mrqa_newsqa-validation-1465", "mrqa_squad-validation-2525", "mrqa_newsqa-validation-1148", "mrqa_squad-validation-1818", "mrqa_squad-validation-5788", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-808"], "EFR": 1.0, "Overall": 0.8802083333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["\"vanguard of change and Islamic reform\"", "less than a year", "dampening the fire", "187 feet", "Zagreus", "Swynnerton Plan", "extra-legal", "Harvey Martin", "December 1895", "lion, leopard, buffalo, rhinoceros, and elephant", "Establishing \"natural borders\"", "drama series", "17", "sold", "income inequality", "pastor", "1534", "mesoglea", "20%", "Isaac Newton", "Miocene", "citizenship", "13", "friction", "Super Bowl City", "three", "Georgia", "Fort Caroline", "Horace Walpole", "Afrikaans", "adaptive immune system", "24", "applications such as on-line betting, financial applications", "United Kingdom", "issues related to the substance of the statement", "2007", "Luther's anti-Jewish works", "short-tempered and even harsher", "the Scottish Government", "two catechisms", "orientalism and tropicality", "Robert Stephenson", "Super Bowl 50", "Beryl", "prima scriptura", "the Mongol Empire", "LOVE Radio", "in the 19th and early 20th centuries,", "Robinson's Brewery", "weather  events", "cutis anserina", "Bogota", "Artemis", "malaxis paludosa", "Chile's second-largest city", "singer, songwriter, musician, airline pilot, author and broadcaster", "three", "South Africa", "9", "Dorset", "The Daily Mirror", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Paul Lynde", "adenosine diphosphate"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7679029304029305}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-3139", "mrqa_squad-validation-2486", "mrqa_squad-validation-9540", "mrqa_squad-validation-433", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-5104"], "SR": 0.71875, "CSR": 0.75, "retrieved_ids": ["mrqa_squad-train-78832", "mrqa_squad-train-79793", "mrqa_squad-train-85852", "mrqa_squad-train-72459", "mrqa_squad-train-64157", "mrqa_squad-train-40286", "mrqa_squad-train-49976", "mrqa_squad-train-64653", "mrqa_squad-train-28878", "mrqa_squad-train-54968", "mrqa_squad-train-44936", "mrqa_squad-train-5698", "mrqa_squad-train-85278", "mrqa_squad-train-13254", "mrqa_squad-train-58055", "mrqa_squad-train-60039", "mrqa_squad-validation-3270", "mrqa_squad-validation-4322", "mrqa_squad-validation-2525", "mrqa_squad-validation-4343", "mrqa_squad-validation-9977", "mrqa_squad-validation-6548", "mrqa_newsqa-validation-1148", "mrqa_squad-validation-6614", "mrqa_squad-validation-2899", "mrqa_squad-validation-2291", "mrqa_squad-validation-4170", "mrqa_triviaqa-validation-5325", "mrqa_squad-validation-2481", "mrqa_squad-validation-3088", "mrqa_squad-validation-3958", "mrqa_newsqa-validation-2834"], "EFR": 1.0, "Overall": 0.875}, {"timecode": 4, "before_eval_results": {"predictions": ["four days", "events and festivals", "Nuda", "private individuals", "21", "The Christmas Invasion", "early vertebrates", "Miocene", "Robert Underwood Johnson", "Cuba", "the Horniman Museum", "the ability to pursue valued goals", "University of Erfurt", "curved", "Johann Sebastian Bach", "1524\u201325", "Britain's position as the dominant colonial power in eastern North America", "by using net wealth (adding up assets and subtracting debts)", "2009", "British", "2005", "Germany and Austria", "self-starting", "two tumen", "international metropolitan region", "Henry Laurens", "projects sponsored by the National Science Foundation (NSF) beginning in 1985 to promote advanced research and education networking in the United States", "Arriva", "entertainers", "second-largest", "biologist", "upper sixth", "arrested", "Pakistan", "1185", "Central business districts", "Hisao Yamada", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "Mongolian", "fossils", "co-NP", "between 1.4 and 5.8 \u00b0C", "In 2007, Charlie Crist took over for Jeb Bush in the Terri Schiavo case", "Parts of the film were shot in Brownsville, Oregon", "szerepl(k)", "cornwall", "cornwall", "cornwall", "santa", "moulding", "in the car with her mother, Mrs. Har-... crease in milk sales in the college", "cornwall", "santa duchamp", "in 1908, he was shot to death by a New Mexico rancher in a land dispute", "cornwall", "the Killers", "agdel's Lost Letter", "Russell Crowe", "santa christie", "cornwall", "Skat", "music business law", "Christopher Nolan", "gun"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6029017857142858}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-680", "mrqa_squad-validation-6981", "mrqa_squad-validation-10145", "mrqa_squad-validation-7554", "mrqa_squad-validation-6046", "mrqa_squad-validation-4848", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-6279", "mrqa_squad-validation-5178", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-9913", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-4510", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-2062", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-13775", "mrqa_triviaqa-validation-1816"], "SR": 0.578125, "CSR": 0.715625, "retrieved_ids": ["mrqa_squad-train-4635", "mrqa_squad-train-5202", "mrqa_squad-train-27295", "mrqa_squad-train-74387", "mrqa_squad-train-69934", "mrqa_squad-train-37148", "mrqa_squad-train-82497", "mrqa_squad-train-4812", "mrqa_squad-train-59291", "mrqa_squad-train-56656", "mrqa_squad-train-20774", "mrqa_squad-train-78845", "mrqa_squad-train-79876", "mrqa_squad-train-45868", "mrqa_squad-train-69407", "mrqa_squad-train-18412", "mrqa_squad-validation-1272", "mrqa_squad-validation-8864", "mrqa_triviaqa-validation-4928", "mrqa_squad-validation-1492", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-3802", "mrqa_squad-validation-6426", "mrqa_triviaqa-validation-1523", "mrqa_squad-validation-2122", "mrqa_squad-validation-3270", "mrqa_squad-validation-3088", "mrqa_squad-validation-7774", "mrqa_squad-validation-6614", "mrqa_squad-validation-1909", "mrqa_squad-validation-2565"], "EFR": 1.0, "Overall": 0.8578125}, {"timecode": 5, "before_eval_results": {"predictions": ["wedding banquet", "respiration", "deficit", "$216,000", "City of Malindi", "Apollo Applications Program", "trial division", "1965 to 1991", "An attorney", "eight", "The Book of Discipline", "Olivier Messiaen", "1759-60", "The Rankine cycle", "deadly explosives", "first half of the eighteenth century", "5,560", "Tricia Marwick", "the main contractor", "ctenophores", "third most abundant chemical element", "adjustable spring-loaded valve", "agriculture", "metamorphosed", "David Suzuki", "if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "June 1979", "6.04 milliliters ( mL)", "December 12", "second half of the 20th Century", "Ten", "edge railed rack and pinion Middleton Railway", "Denver's Executive Vice President of Football Operations and General Manager", "1598", "The Eleventh Doctor", "Tugh Temur", "War of Currents", "dampening the fire", "eight", "the Dalai Lama", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "corruption", "the Dalai Lama", "lethal", "2-0", "Chesley \"Sully\" Sullenberger", "Stanford", "issued his first military orders", "July", "veterans and their families", "Gen. Stanley McChrystal", "the Bronx", "Shanghai", "Fernando Gonzalez", "sportswear", "Dr. Jennifer Arnold and husband Bill Klein", "U.N. aid agency", "sins of the members of the church", "humans", "twelve", "George Best", "August 9, 2017", "Mahler Symphonies", "March 19, 2017"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7785590277777779}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7831", "mrqa_squad-validation-4730", "mrqa_squad-validation-3559", "mrqa_squad-validation-3555", "mrqa_squad-validation-3179", "mrqa_squad-validation-383", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-539", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-6817"], "SR": 0.734375, "CSR": 0.71875, "retrieved_ids": ["mrqa_squad-train-22491", "mrqa_squad-train-42959", "mrqa_squad-train-65761", "mrqa_squad-train-30868", "mrqa_squad-train-51465", "mrqa_squad-train-80933", "mrqa_squad-train-77400", "mrqa_squad-train-58057", "mrqa_squad-train-17609", "mrqa_squad-train-14911", "mrqa_squad-train-39951", "mrqa_squad-train-41362", "mrqa_squad-train-53368", "mrqa_squad-train-69289", "mrqa_squad-train-14468", "mrqa_squad-train-44344", "mrqa_squad-validation-1492", "mrqa_searchqa-validation-2062", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-7269", "mrqa_searchqa-validation-8114", "mrqa_triviaqa-validation-1498", "mrqa_squad-validation-5788", "mrqa_squad-validation-1719", "mrqa_triviaqa-validation-4130", "mrqa_newsqa-validation-2998", "mrqa_squad-validation-1272", "mrqa_squad-validation-6614", "mrqa_newsqa-validation-1201", "mrqa_searchqa-validation-7241", "mrqa_triviaqa-validation-6739", "mrqa_squad-validation-5318"], "EFR": 0.9411764705882353, "Overall": 0.8299632352941176}, {"timecode": 6, "before_eval_results": {"predictions": ["John Mayow", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "1 July 1851", "1562", "32.9%", "architect's client and the main contractor", "2009", "Katy\u0144 Museum", "Derek Wolfe", "silicates", "methotrexate or azathioprine", "over 100,000", "3.55 inches", "Bukhara", "Beyonc\u00e9", "safaris", "Huntington Boulevard", "The Northern Pride Festival", "eight", "Henry Laurens", "Roone Arledge", "Levi's Stadium", "autoimmune", "Diarmaid MacCulloch", "permafrost", "University Athletic Association", "idolatry", "Protestant clergy to marry", "a suite of network protocols", "German", "21 to 11", "eight", "\"Guilt implies wrong-doing", "1130", "15", "50", "November 25, 2002", "northernmost point on the Earth", "in the fovea centralis", "Moloch", "Victor Dhar", "MGM Resorts International", "Bobby Darin", "Gene MacLellan", "Matt Monro", "between the Eastern Ghats and the Bay of Bengal", "2013", "Wisconsin", "Jonathan Breck", "Neil Young", "cutting surfaces", "oxygen", "de jure racial segregation was ruled a violation of the Equal Protection Clause of the Fourteenth Amendment of the United States Constitution", "hot summers and mild winters", "Sir Alex Ferguson", "Bachendri Pal", "2014", "1979", "Manet", "Justin Spitzer", "CNN", "Resting on or touching the ground or bottom", "Chronic Obstructive Pulmonary disease", "Jim Inhofe"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7127632783882785}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.5, 1.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1166", "mrqa_squad-validation-291", "mrqa_squad-validation-8400", "mrqa_squad-validation-170", "mrqa_squad-validation-6223", "mrqa_squad-validation-4673", "mrqa_squad-validation-2416", "mrqa_squad-validation-978", "mrqa_squad-validation-6913", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9961", "mrqa_searchqa-validation-10794", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-9370"], "SR": 0.65625, "CSR": 0.7098214285714286, "retrieved_ids": ["mrqa_squad-train-36503", "mrqa_squad-train-72091", "mrqa_squad-train-63188", "mrqa_squad-train-10010", "mrqa_squad-train-59076", "mrqa_squad-train-40774", "mrqa_squad-train-13394", "mrqa_squad-train-29890", "mrqa_squad-train-38774", "mrqa_squad-train-79060", "mrqa_squad-train-12509", "mrqa_squad-train-85872", "mrqa_squad-train-15214", "mrqa_squad-train-59154", "mrqa_squad-train-35870", "mrqa_squad-train-77563", "mrqa_squad-validation-4343", "mrqa_triviaqa-validation-387", "mrqa_newsqa-validation-3727", "mrqa_squad-validation-4170", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-2778", "mrqa_squad-validation-3559", "mrqa_squad-validation-2906", "mrqa_squad-validation-2122", "mrqa_squad-validation-3088", "mrqa_squad-validation-8037", "mrqa_squad-validation-1492", "mrqa_triviaqa-validation-2216", "mrqa_squad-validation-6981", "mrqa_searchqa-validation-8114", "mrqa_squad-validation-4848"], "EFR": 1.0, "Overall": 0.8549107142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["The Hoppings", "Chicago", "a service if I killed my father when he is hunting, made an alliance with Sultan Muhammad, brought this land to life and gave assistance and support to the Muslims", "Inherited wealth", "\"There is a world of difference between his belief in salvation and a racial ideology", "StubHub Center", "WMO Executive Council and UNEP Governing Council", "trial division", "1999", "Gian Lorenzo Bernini", "housing stock", "dendritic cells, keratinocytes and macrophages", "Newcastle Mela", "ambiguity", "1665", "The Day of the Doctor", "punts", "1.6 kilometres", "\"winds up\" the debate", "2005", "100,000", "thermal expansion", "John Sutcliffe", "Owen Daniels", "incitement to terrorism", "Brookhaven", "A construction project", "Roy", "17 February 1546", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people)", "September 30, 1960", "Brian Steele", "the status line", "the defendant's negligence was gross, that is, it showed such a disregard for the life and safety of others", "callable bonds", "left coronary artery", "five", "Havana Harbor", "four", "1932", "Walter Mondale", "Mitch Murray", "Marie Fredriksson ( vocals ) and Per Gessle ( vocals and guitar )", "December 1, 2009", "virtual reality simulator", "Noah Schnapp", "Antonio Banderas", "a pair of compasses", "Kanawha River", "Julie Stichbury", "in consistency and content", "William J. Bell", "Spanish explorers", "September 2017", "Krypton", "1990", "Norway", "a member of the subfamily Satyrinae in the family Nymphalidae", "Murcia", "India", "Wolfgang Amadeus Mozart", "Armin Meiwes", "Mot\u00f6rhead", "Bill Clinton"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6960180108628817}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.7317073170731707, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6294", "mrqa_squad-validation-2608", "mrqa_squad-validation-8910", "mrqa_squad-validation-5189", "mrqa_squad-validation-6402", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-2674", "mrqa_searchqa-validation-5845"], "SR": 0.65625, "CSR": 0.703125, "retrieved_ids": ["mrqa_squad-train-79676", "mrqa_squad-train-1442", "mrqa_squad-train-41056", "mrqa_squad-train-78940", "mrqa_squad-train-1156", "mrqa_squad-train-75448", "mrqa_squad-train-287", "mrqa_squad-train-45333", "mrqa_squad-train-30935", "mrqa_squad-train-43259", "mrqa_squad-train-12228", "mrqa_squad-train-2277", "mrqa_squad-train-19437", "mrqa_squad-train-72653", "mrqa_squad-train-12837", "mrqa_squad-train-71783", "mrqa_newsqa-validation-1148", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5325", "mrqa_searchqa-validation-4510", "mrqa_squad-validation-383", "mrqa_searchqa-validation-10794", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-2216", "mrqa_squad-validation-2899", "mrqa_triviaqa-validation-7269", "mrqa_squad-validation-8037", "mrqa_searchqa-validation-13775", "mrqa_searchqa-validation-3563", "mrqa_squad-validation-3270", "mrqa_squad-validation-680", "mrqa_newsqa-validation-2281"], "EFR": 0.9545454545454546, "Overall": 0.8288352272727273}, {"timecode": 8, "before_eval_results": {"predictions": ["Prague", "extremely high humidity", "Manning", "Sports Night", "2nd century BCE", "taxation", "destroy the antichrist", "a job where there are few able or willing workers (low supply) competing for a job that few require (low demand)", "the Ilkhanate", "a second Gleichschaltung", "1864", "NL and NC", "fifty", "Albert Einstein", "discarded", "state or government schools", "\"ash tree\"", "Commission v France", "without destroying historical legitimacy", "1652", "adaptive immune system", "case law by the Court of Justice", "Von Miller", "an archetypal \"mad scientist\"", "Dendritic cells", "Allston Science Complex", "writing a five volume book in his native Greek \u03a0\u03b5\u03c1\u03af \u03cd\u03bb\u03b7\u03c2 \u03b9\u03b1\u03c4\u03c1\u03b9\u03ba\u03ae\u03c2", "wars", "in soils", "enforcing racially separated educational facilities", "Hirschman", "June 9", "Sergeant Himmelstoss", "four", "bohrium", "a major fall in stock prices", "Buddhism", "April 8, 2018", "Blind carbon copy to tertiary recipients who receive the message", "to start fires, hunt, and bury their dead", "Ray Charles", "while studying All My Sons by Arthur Miller", "a white one", "Rose Stagg", "A continuing resolution continues the pre-existing appropriations at the same levels as the previous fiscal year", "Arnold Schoenberg", "Tiffany Adams Coyne", "Trace Adkins", "Evermoist   Whiskey Shivers", "Southampton", "March 10, 2017", "Auburn Tigers football team", "Katherine Kiernan Maria", "1943", "Rumplestilskin", "Kryptonite", "to convert RNA to DNA for use in molecular cloning, RNA sequencing, polymerase chain reaction ( PCR ), or genome analysis", "Santiago", "Japan", "Lance Cpl. Maria Lauterbach", "Johnny Depp", "californium", "the orchestra", "isosceles"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6131677362249583}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.8, 1.0, 1.0, 0.8421052631578948, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.0, 0.5, 0.5454545454545454, 1.0, 0.32558139534883723, 0.0, 0.0, 0.6451612903225806, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 1.0, 0.2127659574468085, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7409", "mrqa_squad-validation-4634", "mrqa_squad-validation-3113", "mrqa_squad-validation-6678", "mrqa_squad-validation-3946", "mrqa_squad-validation-1248", "mrqa_squad-validation-6310", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-2351", "mrqa_newsqa-validation-2518", "mrqa_searchqa-validation-10924", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-2789"], "SR": 0.515625, "CSR": 0.6822916666666667, "retrieved_ids": ["mrqa_squad-train-57307", "mrqa_squad-train-41620", "mrqa_squad-train-74455", "mrqa_squad-train-76416", "mrqa_squad-train-6606", "mrqa_squad-train-34241", "mrqa_squad-train-50172", "mrqa_squad-train-64157", "mrqa_squad-train-4031", "mrqa_squad-train-46350", "mrqa_squad-train-69434", "mrqa_squad-train-3702", "mrqa_squad-train-59848", "mrqa_squad-train-27302", "mrqa_squad-train-52352", "mrqa_squad-train-19237", "mrqa_searchqa-validation-7241", "mrqa_naturalquestions-validation-8075", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-3352", "mrqa_squad-validation-4730", "mrqa_naturalquestions-validation-2044", "mrqa_squad-validation-5600", "mrqa_squad-validation-8864", "mrqa_squad-validation-6402", "mrqa_squad-validation-3885", "mrqa_squad-validation-2291", "mrqa_squad-validation-5178", "mrqa_squad-validation-3480", "mrqa_squad-validation-7774", "mrqa_squad-validation-6073", "mrqa_squad-validation-3555"], "EFR": 0.9354838709677419, "Overall": 0.8088877688172043}, {"timecode": 9, "before_eval_results": {"predictions": ["gold", "War of Currents", "113", "Antoine Lavoisier", "United Kingdom, Australia, Canada and the United States", "well before Braddock's departure for North America", "Philip Webb and William Morris", "forming a 'A National Gallery of British Art',", "Henry's dominance", "Scottish Parliament", "exceeds any given number", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)", "the Channel Islands", "additional warming of the Earth's surface", "they were nomads", "private southern Chinese manufacturers and merchants", "Owen Jones", "the Eleventh Doctor", "the BBC National Orchestra of Wales", "bilaterians", "patrimonial feudalism", "August 1992", "NBC", "France's claim to the region was superior to that of the British", "New South Wales", "November 17, 2017", "Daren Maxwell Kagasoff", "the Royal Air Force ( RAF )", "the Joudeh Al - Goudia family", "62 mi", "to form a higher alkane", "Paradise, Nevada", "James Watson and Francis Crick", "September 29, 2017", "during the period of rest ( day )", "the Anglo - Norman French waleis", "204,408", "Lake Michigan", "orbit", "7000301604928199000", "wavelength \u03bb, or photon energy E.", "Spanish explorers", "roofs of the choir side - aisles", "the Indian Civil Service", "different parts of the globe", "statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "5,534", "Louis XVIII", "the spinal cord reaches its permanent position at the level of L1 or L2 ( closer to the head )", "T'Pau", "the London Symphony Orchestra and London Philharmonic", "Julia Ormond", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "George Strait", "Karina Smirnoff", "the fallopian tube", "insects", "1994", "Dana Andrews", "Japan", "Hakeemullah Mehsud", "Jimi Hendrix", "a mythical half-human and half-eagle creature"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6607142857142857}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 0.14285714285714285, 0.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.4, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7888", "mrqa_squad-validation-3585", "mrqa_squad-validation-7771", "mrqa_squad-validation-5733", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10283", "mrqa_hotpotqa-validation-1398", "mrqa_newsqa-validation-3354", "mrqa_searchqa-validation-16618"], "SR": 0.578125, "CSR": 0.671875, "retrieved_ids": ["mrqa_squad-train-74420", "mrqa_squad-train-73870", "mrqa_squad-train-18943", "mrqa_squad-train-9502", "mrqa_squad-train-53113", "mrqa_squad-train-32329", "mrqa_squad-train-10057", "mrqa_squad-train-66343", "mrqa_squad-train-39713", "mrqa_squad-train-51956", "mrqa_squad-train-57981", "mrqa_squad-train-62835", "mrqa_squad-train-72203", "mrqa_squad-train-74673", "mrqa_squad-train-66211", "mrqa_squad-train-26679", "mrqa_newsqa-validation-3855", "mrqa_squad-validation-7831", "mrqa_naturalquestions-validation-3208", "mrqa_searchqa-validation-5845", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-2062", "mrqa_naturalquestions-validation-5104", "mrqa_squad-validation-3958", "mrqa_squad-validation-7818", "mrqa_newsqa-validation-3802", "mrqa_naturalquestions-validation-10257", "mrqa_squad-validation-8910", "mrqa_squad-validation-6046", "mrqa_newsqa-validation-3790", "mrqa_squad-validation-1909"], "EFR": 1.0, "Overall": 0.8359375}, {"timecode": 10, "UKR": 0.76953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-940", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8859", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-996", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-299", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-13775", "mrqa_searchqa-validation-16618", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10035", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10145", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10301", "mrqa_squad-validation-10359", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-10415", "mrqa_squad-validation-10466", "mrqa_squad-validation-1082", "mrqa_squad-validation-1109", "mrqa_squad-validation-1131", "mrqa_squad-validation-1151", "mrqa_squad-validation-1166", "mrqa_squad-validation-1180", "mrqa_squad-validation-1180", "mrqa_squad-validation-1195", "mrqa_squad-validation-121", "mrqa_squad-validation-1217", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1395", "mrqa_squad-validation-1468", "mrqa_squad-validation-1488", "mrqa_squad-validation-1492", "mrqa_squad-validation-1621", "mrqa_squad-validation-1630", "mrqa_squad-validation-1645", "mrqa_squad-validation-170", "mrqa_squad-validation-1719", "mrqa_squad-validation-1732", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1962", "mrqa_squad-validation-1971", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2082", "mrqa_squad-validation-2122", "mrqa_squad-validation-2159", "mrqa_squad-validation-217", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2361", "mrqa_squad-validation-2412", "mrqa_squad-validation-2416", "mrqa_squad-validation-2418", "mrqa_squad-validation-2457", "mrqa_squad-validation-2486", "mrqa_squad-validation-2548", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2608", "mrqa_squad-validation-2633", "mrqa_squad-validation-2667", "mrqa_squad-validation-2678", "mrqa_squad-validation-2843", "mrqa_squad-validation-2861", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2899", "mrqa_squad-validation-291", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-2985", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3088", "mrqa_squad-validation-3104", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3119", "mrqa_squad-validation-3162", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3209", "mrqa_squad-validation-3215", "mrqa_squad-validation-3241", "mrqa_squad-validation-3307", "mrqa_squad-validation-3355", "mrqa_squad-validation-3362", "mrqa_squad-validation-3407", "mrqa_squad-validation-3411", "mrqa_squad-validation-3413", "mrqa_squad-validation-3451", "mrqa_squad-validation-348", "mrqa_squad-validation-349", "mrqa_squad-validation-3522", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3559", "mrqa_squad-validation-3569", "mrqa_squad-validation-3585", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3822", "mrqa_squad-validation-383", "mrqa_squad-validation-3830", "mrqa_squad-validation-3841", "mrqa_squad-validation-3855", "mrqa_squad-validation-3882", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-3946", "mrqa_squad-validation-4008", "mrqa_squad-validation-4028", "mrqa_squad-validation-4046", "mrqa_squad-validation-4121", "mrqa_squad-validation-4172", "mrqa_squad-validation-423", "mrqa_squad-validation-4296", "mrqa_squad-validation-433", "mrqa_squad-validation-4331", "mrqa_squad-validation-4376", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4418", "mrqa_squad-validation-4430", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-4463", "mrqa_squad-validation-4495", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4648", "mrqa_squad-validation-4673", "mrqa_squad-validation-4704", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-477", "mrqa_squad-validation-4772", "mrqa_squad-validation-4803", "mrqa_squad-validation-4807", "mrqa_squad-validation-4841", "mrqa_squad-validation-4848", "mrqa_squad-validation-4936", "mrqa_squad-validation-4983", "mrqa_squad-validation-5023", "mrqa_squad-validation-5063", "mrqa_squad-validation-5083", "mrqa_squad-validation-513", "mrqa_squad-validation-513", "mrqa_squad-validation-5136", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5247", "mrqa_squad-validation-5250", "mrqa_squad-validation-5254", "mrqa_squad-validation-5265", "mrqa_squad-validation-5295", "mrqa_squad-validation-5307", "mrqa_squad-validation-5318", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5418", "mrqa_squad-validation-5445", "mrqa_squad-validation-5485", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5563", "mrqa_squad-validation-5564", "mrqa_squad-validation-5566", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5608", "mrqa_squad-validation-5653", "mrqa_squad-validation-5664", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5766", "mrqa_squad-validation-5782", "mrqa_squad-validation-5788", "mrqa_squad-validation-5821", "mrqa_squad-validation-5843", "mrqa_squad-validation-5852", "mrqa_squad-validation-5951", "mrqa_squad-validation-6046", "mrqa_squad-validation-6049", "mrqa_squad-validation-6067", "mrqa_squad-validation-6073", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6294", "mrqa_squad-validation-6310", "mrqa_squad-validation-638", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-653", "mrqa_squad-validation-6531", "mrqa_squad-validation-6535", "mrqa_squad-validation-6548", "mrqa_squad-validation-6567", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-6678", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6840", "mrqa_squad-validation-6841", "mrqa_squad-validation-6868", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6917", "mrqa_squad-validation-6959", "mrqa_squad-validation-6962", "mrqa_squad-validation-6981", "mrqa_squad-validation-703", "mrqa_squad-validation-7030", "mrqa_squad-validation-7142", "mrqa_squad-validation-7175", "mrqa_squad-validation-7211", "mrqa_squad-validation-7252", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-743", "mrqa_squad-validation-7435", "mrqa_squad-validation-7456", "mrqa_squad-validation-7554", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7632", "mrqa_squad-validation-7633", "mrqa_squad-validation-7678", "mrqa_squad-validation-7699", "mrqa_squad-validation-7704", "mrqa_squad-validation-7709", "mrqa_squad-validation-7716", "mrqa_squad-validation-7747", "mrqa_squad-validation-7766", "mrqa_squad-validation-7771", "mrqa_squad-validation-7774", "mrqa_squad-validation-7775", "mrqa_squad-validation-7800", "mrqa_squad-validation-7818", "mrqa_squad-validation-7831", "mrqa_squad-validation-7846", "mrqa_squad-validation-7863", "mrqa_squad-validation-7888", "mrqa_squad-validation-7899", "mrqa_squad-validation-795", "mrqa_squad-validation-7965", "mrqa_squad-validation-797", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8071", "mrqa_squad-validation-8163", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8365", "mrqa_squad-validation-8372", "mrqa_squad-validation-8410", "mrqa_squad-validation-8414", "mrqa_squad-validation-8441", "mrqa_squad-validation-8486", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8589", "mrqa_squad-validation-859", "mrqa_squad-validation-8598", "mrqa_squad-validation-8600", "mrqa_squad-validation-8666", "mrqa_squad-validation-8670", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8907", "mrqa_squad-validation-9020", "mrqa_squad-validation-9030", "mrqa_squad-validation-9050", "mrqa_squad-validation-9116", "mrqa_squad-validation-9121", "mrqa_squad-validation-9151", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9379", "mrqa_squad-validation-9401", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9454", "mrqa_squad-validation-9465", "mrqa_squad-validation-9484", "mrqa_squad-validation-9540", "mrqa_squad-validation-9590", "mrqa_squad-validation-9608", "mrqa_squad-validation-9689", "mrqa_squad-validation-9738", "mrqa_squad-validation-976", "mrqa_squad-validation-9760", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9954", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2579", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6711"], "OKR": 0.921875, "KG": 0.46015625, "before_eval_results": {"predictions": ["2009", "Huntington Boulevard", "regularly since 1979", "William Hartnell and Patrick Troughton", "Distributed Adaptive Message Block Switching", "1331", "Confucianism and promoting Chinese cultural values", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "pseudo- Sciencesences", "a military coup d'\u00e9tat", "the A1", "proteolysis", "a form of starch", "Eisleben", "break off the cathode, pass out of the tube, and physically strike him", "late night talk shows", "13\u20137", "around half", "the comprehensive institutions of the Great Yuan", "linear", "Chuck Howley", "with observations", "Annette", "Tim McGraw and Kenny Chesney", "Beorn", "a house edge of between 0.5 % and 1 %, placing blackjack among the cheapest casino table games", "fertilization", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "a sociological perspective which developed around the middle of the twentieth century and that continues to be influential in some areas of the discipline", "6 January 793", "Colon Street", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Jane Fonda", "Robert Irsay", "Edgar Lungu", "UNESCO / ILO Recommendation concerning the Status of Teachers", "five", "Achal Kumar Jyoti", "the President", "Luther Ingram", "The management team", "British R&B girl group Eternal", "Montreal Montreal", "When the others arrive", "six", "Yuzuru Hanyu", "regulatory", "William Whewell", "Everywhere", "31 October 1972", "December 2, 1942", "September 8, 2017", "the stage", "Dolph Lundgren", "the Gaget, Gauthier & Co. workshop", "Renhe Sports Management Ltd", "Tangled", "smartphones and similar devices to establish radio communication with each other by touching them together or bringing them into close proximity, usually no more than a few centimetres", "Lawton Chiles", "they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\"", "Scotland", "\"She wasn't the best \"coach,\"", "Compromise of 1850", "Peyton Place"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6187553709123002}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.15384615384615383, 0.782608695652174, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.4444444444444445, 1.0, 1.0, 0.09523809523809523, 0.0, 1.0, 0.375, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.07142857142857142, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7832", "mrqa_squad-validation-8160", "mrqa_squad-validation-4849", "mrqa_squad-validation-9831", "mrqa_squad-validation-5357", "mrqa_squad-validation-1388", "mrqa_squad-validation-434", "mrqa_squad-validation-625", "mrqa_squad-validation-8747", "mrqa_squad-validation-8530", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-2277", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3223", "mrqa_searchqa-validation-15757"], "SR": 0.515625, "CSR": 0.6576704545454546, "retrieved_ids": ["mrqa_squad-train-37209", "mrqa_squad-train-17181", "mrqa_squad-train-29823", "mrqa_squad-train-39418", "mrqa_squad-train-13877", "mrqa_squad-train-67552", "mrqa_squad-train-58959", "mrqa_squad-train-36427", "mrqa_squad-train-24215", "mrqa_squad-train-55465", "mrqa_squad-train-20088", "mrqa_squad-train-82335", "mrqa_squad-train-66600", "mrqa_squad-train-14530", "mrqa_squad-train-72718", "mrqa_squad-train-26488", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-8119", "mrqa_squad-validation-9977", "mrqa_naturalquestions-validation-5599", "mrqa_squad-validation-1719", "mrqa_searchqa-validation-9370", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-3163", "mrqa_squad-validation-8864", "mrqa_squad-validation-433", "mrqa_naturalquestions-validation-8794", "mrqa_newsqa-validation-3790", "mrqa_squad-validation-6073", "mrqa_triviaqa-validation-4928", "mrqa_newsqa-validation-3855", "mrqa_triviaqa-validation-5342"], "EFR": 0.967741935483871, "Overall": 0.7553949780058651}, {"timecode": 11, "before_eval_results": {"predictions": ["The availability of the Bible in vernacular languages", "detention", "Western Xia", "by qualified majority", "carbon related", "co-chair", "Katharina", "three", "August 10, 1948", "the holy catholic (or universal) church", "30\u201360%", "Louis Paul Cailletet", "Class II MHC molecules", "crust and lithosphere", "orange", "electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons", "stroke", "13 June 1525", "the type of reduction being used", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "Tom Alweendo", "September 1947", "Villa de Bejar", "Western Australia", "Stan and Cartman accidentally destroy a dam, causing the town of Beaverton to be destroyed", "April 10, 2018", "October 1, 2015", "number of games where the player played, in whole or in part", "cat", "f\u0254n", "Tom Brady", "the country was called by the name of the dynasty", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Kida", "1960", "Richard Parker", "Part 2", "a barrier that runs across a river or stream to control the flow of water", "De Wayne Warren", "lacteals", "1970", "ancient Etruscan root autu - and has within it connotations of the passing of the year", "Tom Brady", "Lady Olenna Tyrell", "the Isle of Sheppey in England", "Lake Powell", "Clarence L. Tinker", "Hotel barge   Bed and breakfast   Botel   Boutique hotel   Bunkhouse", "Trace Adkins", "Garbi\u00f1e Muguruza", "Mick Jagger", "Hercules", "June 12, 2018", "Bart Howard", "111", "Leonard Bernstein", "The Hague", "historic buildings, arts, and published works", "Lisa", "Arnoldo Rueda Medina.", "tax incentives", "Robert Fellmeth", "Florida", "CNN"], "metric_results": {"EM": 0.53125, "QA-F1": 0.610149628761761}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.4, 1.0, 0.0, 0.5555555555555556, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4074", "mrqa_squad-validation-8581", "mrqa_squad-validation-3474", "mrqa_squad-validation-4952", "mrqa_squad-validation-10483", "mrqa_squad-validation-3385", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-386", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1549", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-13057"], "SR": 0.53125, "CSR": 0.6471354166666667, "retrieved_ids": ["mrqa_squad-train-78422", "mrqa_squad-train-46329", "mrqa_squad-train-22796", "mrqa_squad-train-5954", "mrqa_squad-train-20496", "mrqa_squad-train-83792", "mrqa_squad-train-65644", "mrqa_squad-train-42139", "mrqa_squad-train-52350", "mrqa_squad-train-52441", "mrqa_squad-train-33041", "mrqa_squad-train-3417", "mrqa_squad-train-16793", "mrqa_squad-train-58517", "mrqa_squad-train-79168", "mrqa_squad-train-10637", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2208", "mrqa_squad-validation-6913", "mrqa_squad-validation-1166", "mrqa_squad-validation-3559", "mrqa_squad-validation-2481", "mrqa_naturalquestions-validation-10090", "mrqa_squad-validation-4322", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-81", "mrqa_searchqa-validation-10794", "mrqa_naturalquestions-validation-801", "mrqa_triviaqa-validation-6620", "mrqa_searchqa-validation-8114", "mrqa_naturalquestions-validation-7837", "mrqa_triviaqa-validation-2216"], "EFR": 0.9666666666666667, "Overall": 0.7530729166666666}, {"timecode": 12, "before_eval_results": {"predictions": ["savanna or desert", "Downtown San Bernardino", "Allan Bloom, ''The Good War\" author Studs Terkel, American writer, essayist, filmmaker, teacher, and political activist Susan Sontag", "three", "Graz, Austria", "Sky Digital", "until 1796", "Galileo Galilei and Sir Isaac Newton", "Gary Kubiak", "soy farmers", "The Sinclair Broadcast Group", "mercuric oxide (HgO)", "Guglielmo Marconi", "education", "wages and profits", "central, western, and southern Europe", "The judicial branch", "biostratigraphers", "2001", "Miami Heat of the National Basketball Association ( NBA )", "The visible part of the human nose", "a warrior, Mage, or rogue coming from an elven, human, or Dwarven background", "Bob Dylan", "1799", "interphase", "$2 million in 2011", "a tree species ( that generally grows in the elevation range of 3,000 to 4,200 metres ( 9,800 to 13,800 ft ) in the Himalayas )", "Roger Federer", "James Corden", "Pasek & Paul", "October 2", "1956", "Richard T. Jones", "Gwendoline Christie", "Orangeville, Ontario, Canada", "Guy Pemberton", "random - access memory ( RAM )", "Afghanistan", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Walter Brennan", "James Rodr\u00edguez", "8ft", "various submucosal membrane sites", "1985, 2016, 2018", "Russia", "April 17, 1982", "acidifying particles and gases", "April 26, 2005", "a stem", "Thespis", "Ra\u00fal Eduardo Esparza", "By functions", "Athens", "constitutional monarchy", "a birth certificate from a local authority is commonly provided to the federal government to obtain a U.S. passport", "True or false", "the forefoot", "southwestern", "Planet Terror", "Atlantic Ocean", "one", "Easter to West", "The snowflake curve", "the ruble"], "metric_results": {"EM": 0.59375, "QA-F1": 0.637854619186856}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.4, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 0.0, 0.29629629629629634, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2644", "mrqa_squad-validation-8032", "mrqa_squad-validation-10341", "mrqa_squad-validation-9895", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2644", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3160", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-6998", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-559", "mrqa_newsqa-validation-2782", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-10635"], "SR": 0.59375, "CSR": 0.6430288461538461, "retrieved_ids": ["mrqa_squad-train-52004", "mrqa_squad-train-54987", "mrqa_squad-train-7174", "mrqa_squad-train-67306", "mrqa_squad-train-63118", "mrqa_squad-train-73789", "mrqa_squad-train-33221", "mrqa_squad-train-51700", "mrqa_squad-train-14576", "mrqa_squad-train-65349", "mrqa_squad-train-41180", "mrqa_squad-train-58068", "mrqa_squad-train-70492", "mrqa_squad-train-26415", "mrqa_squad-train-2064", "mrqa_squad-train-21909", "mrqa_squad-validation-3958", "mrqa_triviaqa-validation-6711", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-1882", "mrqa_hotpotqa-validation-1398", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-2169", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-1745", "mrqa_newsqa-validation-2281", "mrqa_naturalquestions-validation-10533", "mrqa_triviaqa-validation-5342", "mrqa_squad-validation-7818", "mrqa_triviaqa-validation-4130", "mrqa_squad-validation-4634"], "EFR": 0.9615384615384616, "Overall": 0.7512259615384614}, {"timecode": 13, "before_eval_results": {"predictions": ["return home", "socialist realism", "to spearhead the regeneration of the North-East", "sleep deprivation", "1977", "Anderson", "quantum electrodynamics (or QED)", "provisional elder/deacon", "antigenic variation", "kinematic measurements", "worker, capitalist/business owner, landlord", "the communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Licensed Local Pastor", "driving them in front of the army", "a corruption of the Kamba version", "24 March 1879", "Rob Van Winkle", "electrons.", "Edie Falco", "Leonard Bernstein", "Empire of the Sun", "Young Hickory", "Nicaragua", "Afghanistan", "Cambodia", "the Aladdin", "Uncle Henry", "Lady and the Tramp", "volatile phase in bipolar disorder", "China", "mirthless", "Hanoi", "Scaramouche", "Alexander Ulyanov", "Beatrix Potter", "Louvre", "James Buchanan", "Volvic", "Andrea del Sarto", "Colorado is of Spanish origin, meaning \"colored red.\"", "Christopher Columbus", "the Balfour Declaration", "Contingency fees", "a tortoise", "zucchetto", "Shinto", "The Simpsons", "haemophilia", "\"Beauty is truth, truth beauty\"", "silk cord", "Bad Boys", "Philadelphia", "Robert Downey Jr.", "The Fresh Prince of Bel-Air", "her abusive husband", "US labor law and constitutional law", "Sylvester J. Pussycat, Sr.", "Daniel Defoe", "Atlantic Ocean", "Battle of Britain and the Battle of Malta", "U.S. Consulate in Rio de Janeiro", "the club's board has yet to make it so for all the camps", "three", "in the clubs of Hollywood"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6810568086883877}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.07017543859649122, 0.4, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-8065", "mrqa_squad-validation-8258", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-708", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-4663", "mrqa_searchqa-validation-4983", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-12856", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-11170", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-4717", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-2199"], "SR": 0.640625, "CSR": 0.6428571428571428, "retrieved_ids": ["mrqa_squad-train-37680", "mrqa_squad-train-29415", "mrqa_squad-train-14161", "mrqa_squad-train-23882", "mrqa_squad-train-80401", "mrqa_squad-train-36197", "mrqa_squad-train-17068", "mrqa_squad-train-30334", "mrqa_squad-train-75019", "mrqa_squad-train-16299", "mrqa_squad-train-39759", "mrqa_squad-train-14458", "mrqa_squad-train-60626", "mrqa_squad-train-39459", "mrqa_squad-train-52262", "mrqa_squad-train-10327", "mrqa_squad-validation-9895", "mrqa_squad-validation-2525", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8633", "mrqa_searchqa-validation-10924", "mrqa_squad-validation-10483", "mrqa_squad-validation-6279", "mrqa_searchqa-validation-10635", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-2847", "mrqa_searchqa-validation-3563", "mrqa_newsqa-validation-2834", "mrqa_naturalquestions-validation-1974", "mrqa_squad-validation-3946", "mrqa_squad-validation-3139", "mrqa_naturalquestions-validation-9992"], "EFR": 0.9565217391304348, "Overall": 0.7501882763975155}, {"timecode": 14, "before_eval_results": {"predictions": ["Huguenots", "1281", "Tower Theatre", "Mi'kmaq and the Abenaki", "Eastern bloc countries", "Palestine", "colloblasts, sticky cells", "the convenience of the railroad and worried about flooding", "Ireland", "18 April 1521", "the Archangel Michael", "Warszawa", "the Migration period", "2012", "35", "Donald Trump", "Jeopardy", "Blue Nile", "Solomon", "Betsey Johnson", "Eragon", "Rawhide", "p puppies", "Judy Garland", "Kevin Freibott", "Chiffon", "Hot Stuff", "Fantastic Four", "John Mahoney", "Murder By Death", "Dave Brubeck", "The Dalton Gang", "Washington", "Humphrey Bogart", "Can we get along?", "Franklin D. Roosevelt", "rice straw", "udon", "Sirhan Sirhan", "Moon", "winter", "b", "Mountain Dew", "Omaha", "Van Halen", "actress", "the Erie Canal", "The ____ Family", "Baltic Sea", "senex", "Magic Johnson", "The Duomo Florence", "hoo-hoo", "a syllable", "Comancheria, consisted of present - day eastern New Mexico, southeastern Colorado, southwestern Kansas, western Oklahoma, and most of northwest Texas and northern Chihuahua", "Renishaw Hall, Derbyshire, England, UK", "Imola Circuit", "sheep", "provides its services in the Japanese market", "Berea College", "signed an amnesty lifting corruption charges.", "Sharon Bialek", "European Council", "Dominican Republic"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5934895833333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-2943", "mrqa_squad-validation-901", "mrqa_squad-validation-4621", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-7242", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-9432", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-534", "mrqa_naturalquestions-validation-129", "mrqa_triviaqa-validation-1936", "mrqa_hotpotqa-validation-5184", "mrqa_newsqa-validation-846", "mrqa_triviaqa-validation-2317"], "SR": 0.515625, "CSR": 0.634375, "retrieved_ids": ["mrqa_squad-train-45157", "mrqa_squad-train-46522", "mrqa_squad-train-29650", "mrqa_squad-train-72497", "mrqa_squad-train-69368", "mrqa_squad-train-160", "mrqa_squad-train-10993", "mrqa_squad-train-7441", "mrqa_squad-train-30310", "mrqa_squad-train-60090", "mrqa_squad-train-22530", "mrqa_squad-train-23131", "mrqa_squad-train-86143", "mrqa_squad-train-41693", "mrqa_squad-train-80066", "mrqa_squad-train-15760", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-1426", "mrqa_squad-validation-8864", "mrqa_searchqa-validation-4054", "mrqa_triviaqa-validation-6739", "mrqa_naturalquestions-validation-10533", "mrqa_newsqa-validation-3223", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-2169", "mrqa_triviaqa-validation-6711", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-307", "mrqa_searchqa-validation-4216", "mrqa_squad-validation-8530", "mrqa_triviaqa-validation-1523"], "EFR": 1.0, "Overall": 0.7571874999999999}, {"timecode": 15, "before_eval_results": {"predictions": ["more effectively", "arrested", "Francis Marion", "the death of Elisabeth Sladen in early 2011", "3.6%", "nearly 42,000", "14", "the United States Census Bureau", "Asia", "19 April 1943", "actions-oriented", "rapidly evolve and adapt", "present-day Upstate New York and the Ohio Country", "the Roman Republic", "Debbie Abrahams", "South Africa", "Will Carling", "the Chatham House Rule", "phase changes", "America Online Inc.", "Aramis", "bees", "The Firm", "The Streets", "violin", "a blazer", "breaking the sound barrier", "the Titanic", "inguinal", "gluteus minimus", "Mario", "Georgia", "Massachusetts", "London theatre district, ballet and circus", "La Boh\u00e8me", "John Quincy Adams", "oliban", "Ethel Skinner", "Queen Victoria", "My Fair Lady", "Austria", "cellulose", "Adolphe Adam", "bitts", "Barcelona", "coelacanth", "mono", "cats", "Love Never Dies", "Elizabeth I", "jazz", "George Costanza", "the ISS", "the Marcy Brothers", "1770 BC", "A costume party ( American English )", "exploitation of natural resources particularly North Sea oil", "Sydney", "German Chancellor Angela Merkel", "Al Gore", "Mike Wallace", "the Professor", "Burundi", "The Howard Stern Show"], "metric_results": {"EM": 0.5, "QA-F1": 0.5788487554112554}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false], "QA-F1": [0.3333333333333333, 0.0, 0.0, 0.7272727272727273, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1830", "mrqa_squad-validation-6702", "mrqa_squad-validation-3118", "mrqa_squad-validation-7872", "mrqa_squad-validation-10108", "mrqa_squad-validation-5893", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-871", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-7024", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3616", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8359", "mrqa_hotpotqa-validation-1537", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-8487"], "SR": 0.5, "CSR": 0.6259765625, "retrieved_ids": ["mrqa_squad-train-82767", "mrqa_squad-train-55039", "mrqa_squad-train-77562", "mrqa_squad-train-75973", "mrqa_squad-train-18273", "mrqa_squad-train-83123", "mrqa_squad-train-26080", "mrqa_squad-train-67606", "mrqa_squad-train-53358", "mrqa_squad-train-42383", "mrqa_squad-train-79412", "mrqa_squad-train-28227", "mrqa_squad-train-82341", "mrqa_squad-train-66247", "mrqa_squad-train-19042", "mrqa_squad-train-79473", "mrqa_naturalquestions-validation-3163", "mrqa_squad-validation-4952", "mrqa_squad-validation-1248", "mrqa_naturalquestions-validation-10090", "mrqa_squad-validation-4634", "mrqa_newsqa-validation-3223", "mrqa_searchqa-validation-708", "mrqa_squad-validation-10483", "mrqa_naturalquestions-validation-8794", "mrqa_squad-validation-5357", "mrqa_triviaqa-validation-2674", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-6392", "mrqa_squad-validation-3474", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-3855"], "EFR": 0.96875, "Overall": 0.7492578125}, {"timecode": 16, "before_eval_results": {"predictions": ["computational complexity theory", "\"apostate\" leaders of Muslim states", "tensions over slavery and the power of bishops in the denomination", "mannerist architecture", "489", "Grainger Town area", "the Great Fire of London", "Theory of the Earth", "Ward", "Newton", "bilaterians", "Antigone, one of the daughters of former King of Thebes, Oedipus", "the forces of Andrew Moray and William Wallace", "the Gulf of California opened and lowered the river's base level ( its lowest point )", "between two and 30 eggs", "in 2018", "Arthur Chung", "18", "9 February 2018", "Yuzuru Hanyu", "Marley & Me", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff / \ufef5 26.617 \u00b0 N 81.617", "the 2013 non-fiction book of the same name by David Finkel", "Donald Fauntleroy Duck", "McKim Marriott", "Hanna Alstr\u00f6m", "31", "Camp Green Lake", "postero - medially", "the Twelvers, and Seven pillars", "vasoconstriction of most blood vessels", "extra material from chromosome 21 attached to another chromosome", "Rockwell", "All Hallows", "Sylvester Stallone", "In Time", "Rodney Crowell", "`` 0 '' trunk code", "eukaryotic cells", "Melissa Disney", "Darren McGavin", "UNESCO / ILO", "Thaddeus Rowe Luckinbill", "ummat al - Islamiyah", "William DeVaughn", "a Spanish surname", "Brevet Colonel Robert E. Lee", "The Annunciation", "Santiago Ram\u00f3n y Cajal", "preteen boys", "seven", "novella", "southeast of the city", "21 June 2007", "four", "California", "England", "Roc Me Out", "\"Dear John,\"", "root out terrorists within its borders", "Rudolf Nureyev", "35", "the Kurdish Workers' Party, or Kurdish group that has been attacking Turkey from inside northern Iraq", "Mohamed Alanssi"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6493538673443859}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.823529411764706, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5348", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-3649", "mrqa_searchqa-validation-15479", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-814"], "SR": 0.578125, "CSR": 0.6231617647058824, "retrieved_ids": ["mrqa_squad-train-13843", "mrqa_squad-train-76410", "mrqa_squad-train-19311", "mrqa_squad-train-58182", "mrqa_squad-train-22867", "mrqa_squad-train-35685", "mrqa_squad-train-26353", "mrqa_squad-train-58080", "mrqa_squad-train-81732", "mrqa_squad-train-52342", "mrqa_squad-train-21027", "mrqa_squad-train-69072", "mrqa_squad-train-18744", "mrqa_squad-train-77115", "mrqa_squad-train-80981", "mrqa_squad-train-52458", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-16618", "mrqa_naturalquestions-validation-1315", "mrqa_squad-validation-6310", "mrqa_naturalquestions-validation-307", "mrqa_squad-validation-3088", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-10199", "mrqa_newsqa-validation-539", "mrqa_hotpotqa-validation-4906", "mrqa_squad-validation-5733", "mrqa_naturalquestions-validation-9773", "mrqa_searchqa-validation-534", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-4717", "mrqa_squad-validation-2525"], "EFR": 0.9629629629629629, "Overall": 0.747537445533769}, {"timecode": 17, "before_eval_results": {"predictions": ["Sydney", "Doritos", "Hans Vredeman de Vries", "$20.4 billion", "\u00dcberlingen", "Finsteraarhorn", "seven", "The Master is the Doctor's archenemy", "Lippe", "the American Revolutionary War", "25 per cent", "Chuck Noland", "Donna Mills", "March 31, 2013", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "before the first year begins", "Kansas", "the president", "UNESCO / ILO", "asexually", "South American country", "2008", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Travis Tritt", "March 16, 2018", "Cee - Lo", "The Sun", "prokaryotic cell ( or organelle )", "1966", "Charlotte Hornets", "Julie Adams", "April 29, 2009", "J. Presper Eckert and John William Mauchly's ENIAC", "201", "1983", "October 12, 2017", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T )", "MacFarlane", "regulate the employment and working conditions of civil servants", "Hercules", "Angel Island ( California )", "Rufus and Chaka Khan", "moist temperate climates", "before they kill him", "7.6 mm", "1971 album, What's Going On", "1955", "B.F. Skinner", "2020", "continues the pre-existing appropriations at the same levels as the previous fiscal year ( or with minor modifications ) for a set amount of time", "during World War II", "the Gospels of Matthew, Mark, Luke and John", "Orangeville, Ontario, Canada", "a conic sections", "Switzerland", "Andrew Johnson", "St Augustine's Abbey", "Russia denied any link between that announcement and the conflict in Georgia, although Russia has criticized U.S. support for Georgia.", "the insurgency", "Joe DiMaggio", "a FBI agent", "NBA 2K16", "Baltimore", "September 25, 2017"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6386155453342953}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.72, 0.5714285714285715, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4444444444444445, 1.0, 0.0, 0.18181818181818182, 0.25, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5925925925925926, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9285", "mrqa_squad-validation-7700", "mrqa_squad-validation-7288", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-7892", "mrqa_triviaqa-validation-2922", "mrqa_newsqa-validation-3489", "mrqa_searchqa-validation-10032", "mrqa_hotpotqa-validation-4735"], "SR": 0.546875, "CSR": 0.6189236111111112, "retrieved_ids": ["mrqa_squad-train-70724", "mrqa_squad-train-78970", "mrqa_squad-train-19900", "mrqa_squad-train-75829", "mrqa_squad-train-70595", "mrqa_squad-train-78505", "mrqa_squad-train-26688", "mrqa_squad-train-69100", "mrqa_squad-train-77282", "mrqa_squad-train-65873", "mrqa_squad-train-54163", "mrqa_squad-train-22438", "mrqa_squad-train-68788", "mrqa_squad-train-7890", "mrqa_squad-train-46643", "mrqa_squad-train-1717", "mrqa_hotpotqa-validation-4906", "mrqa_searchqa-validation-12469", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8075", "mrqa_squad-validation-2291", "mrqa_naturalquestions-validation-8659", "mrqa_newsqa-validation-2782", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-794", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-10103", "mrqa_squad-validation-6702", "mrqa_newsqa-validation-3790", "mrqa_triviaqa-validation-670", "mrqa_squad-validation-8530"], "EFR": 0.9655172413793104, "Overall": 0.7472006704980843}, {"timecode": 18, "before_eval_results": {"predictions": ["1904", "Cardinal Cajetan Luther stated that he did not consider the papacy part of the biblical Church", "Michael Heckenberger and colleagues of the University of Florida", "declined significantly", "quantized", "prices", "Miller", "Super Bowl XX", "gold-themed initiatives", "p", "2017", "to collect menstrual flow", "Sets heart in mediastinum and limits its motion", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "12 to 36 months old", "fresh water and fresh water", "her abusive husband", "A request line", "the south coast of eastern New Guinea", "2009", "December 2, 1942", "John Cooper Clarke", "1792", "Amitabh Bachchan, Akshay Kumar, Bobby Deol, Divya Khosla Kumar, Sandali Sinha and Nagma", "Annette Strean", "the Bee Gees", "September 25, 1987", "the English counties in the Twenty20 Cup", "the Executive branch and the Senate", "Andrew Lincoln", "Tommy James and the Shondells", "14 : 46 JST ( 05 : 46 UTC )", "the right to peaceably assemble, or to petition for a governmental redress of grievances", "2010", "James Corden", "development of electronic computers in the 1950s", "the internal reproductive anatomy ( such as the uterus in females )", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "two tablets", "its population", "beta decay", "Rachel Sarah Bilson", "the NIRA", "March 5, 2014", "Haikou on the Hainan Island", "19 June 2018", "stromal connective tissue", "Sunday night", "Stephen Curry", "Ben Willis", "Rufus and Chaka Khan", "Jodie Foster", "Rory McIlroy", "Aristotle", "Baseball", "1919", "Lithuania", "Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "the Impeccable", "treasury bail bonds", "internal organ systems in adults", "Pete Seeger", "Kiss Me, Kate", "Wigan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6615833226656342}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5106382978723404, 1.0, 1.0, 0.0, 0.2222222222222222, 0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.888888888888889, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9152542372881356, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2268", "mrqa_squad-validation-845", "mrqa_squad-validation-5", "mrqa_squad-validation-9213", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-1930", "mrqa_hotpotqa-validation-4927", "mrqa_newsqa-validation-686", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-10336"], "SR": 0.546875, "CSR": 0.6151315789473684, "retrieved_ids": ["mrqa_squad-train-14598", "mrqa_squad-train-81358", "mrqa_squad-train-71095", "mrqa_squad-train-22694", "mrqa_squad-train-73925", "mrqa_squad-train-4284", "mrqa_squad-train-63931", "mrqa_squad-train-318", "mrqa_squad-train-15084", "mrqa_squad-train-30007", "mrqa_squad-train-25853", "mrqa_squad-train-41310", "mrqa_squad-train-55156", "mrqa_squad-train-75004", "mrqa_squad-train-71925", "mrqa_squad-train-42127", "mrqa_triviaqa-validation-4928", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-8359", "mrqa_squad-validation-4673", "mrqa_searchqa-validation-13057", "mrqa_squad-validation-2608", "mrqa_triviaqa-validation-6466", "mrqa_searchqa-validation-16618", "mrqa_naturalquestions-validation-10113", "mrqa_squad-validation-4322", "mrqa_naturalquestions-validation-5476", "mrqa_squad-validation-5600", "mrqa_newsqa-validation-3418", "mrqa_squad-validation-3585", "mrqa_squad-validation-3118", "mrqa_naturalquestions-validation-1023"], "EFR": 0.896551724137931, "Overall": 0.7326491606170599}, {"timecode": 19, "before_eval_results": {"predictions": ["phagosomal", "well before Braddock's departure for North America", "General Hospital", "destroyed", "night", "assertive", "antagonistic", "whether or not to plead guilty", "parallelogram", "Valley Falls", "23 December 2014", "Sam Raimi", "Saddle Rock Elementary School", "political thriller", "James Ellison", "Robert \"Bobby\" Germaine, Sr.", "1995\u201396", "the NYPD's 83rd Precinct", "Azeroth", "October 5, 1969", "1999", "musical research", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.", "Michael Sheen", "humorous", "the Mayor of the City of New York", "841", "My Boss, My Hero", "Roy Spencer", "Bardney", "tragedy", "Everglades", "Saint Louis County", "1891", "Eielson Air Force Base", "Dzyha Vertov", "Germany", "eight", "Carson City", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "2008", "October 12, 1962", "Eli Roth", "Paige O'Hara", "Northern Ireland", "St. Louis, Missouri", "the first hole of a sudden-death playoff", "100 million", "every aspect of public and private life", "Burnley", "Russian", "between 11 or 13 and 18", "the onset and progression of Alzheimer's disease", "15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least 28 vowel forms", "7 June 2005", "Imola", "Sufjan Stevens", "producing rock music with a country influence", "planned training exercise designed to help the prince learn to fly in combat situations", "a donkey", "Irving Berlin", "$1.5 million", "Sen. Barack Obama", "Tutsi and Hutu"], "metric_results": {"EM": 0.609375, "QA-F1": 0.716395757020757}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.8461538461538461, 0.0, 1.0, 1.0, 0.0, 0.8666666666666666, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-6024", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-5485", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2629", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-1283", "mrqa_searchqa-validation-7828", "mrqa_newsqa-validation-3659"], "SR": 0.609375, "CSR": 0.61484375, "retrieved_ids": ["mrqa_squad-train-66370", "mrqa_squad-train-44717", "mrqa_squad-train-60417", "mrqa_squad-train-73005", "mrqa_squad-train-6335", "mrqa_squad-train-22153", "mrqa_squad-train-28017", "mrqa_squad-train-62826", "mrqa_squad-train-24663", "mrqa_squad-train-84105", "mrqa_squad-train-24794", "mrqa_squad-train-38736", "mrqa_squad-train-73243", "mrqa_squad-train-9044", "mrqa_squad-train-81538", "mrqa_squad-train-69465", "mrqa_squad-validation-2608", "mrqa_naturalquestions-validation-4664", "mrqa_triviaqa-validation-3716", "mrqa_searchqa-validation-11170", "mrqa_newsqa-validation-3223", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-5465", "mrqa_searchqa-validation-12856", "mrqa_naturalquestions-validation-1745", "mrqa_searchqa-validation-11166", "mrqa_newsqa-validation-2199", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1315", "mrqa_searchqa-validation-10103", "mrqa_naturalquestions-validation-10049"], "EFR": 1.0, "Overall": 0.7532812499999999}, {"timecode": 20, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10019", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1939", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2918", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-386", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4867", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15836", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-346", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-4917", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5845", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8487", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9432", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10151", "mrqa_squad-validation-10192", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10381", "mrqa_squad-validation-10388", "mrqa_squad-validation-10399", "mrqa_squad-validation-10408", "mrqa_squad-validation-1062", "mrqa_squad-validation-1082", "mrqa_squad-validation-1131", "mrqa_squad-validation-1180", "mrqa_squad-validation-1248", "mrqa_squad-validation-1272", "mrqa_squad-validation-1334", "mrqa_squad-validation-1348", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-150", "mrqa_squad-validation-1530", "mrqa_squad-validation-1551", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1939", "mrqa_squad-validation-2003", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-217", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2361", "mrqa_squad-validation-2416", "mrqa_squad-validation-2481", "mrqa_squad-validation-2511", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2667", "mrqa_squad-validation-2772", "mrqa_squad-validation-2861", "mrqa_squad-validation-2881", "mrqa_squad-validation-2906", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3148", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3270", "mrqa_squad-validation-3355", "mrqa_squad-validation-3385", "mrqa_squad-validation-3411", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3830", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4159", "mrqa_squad-validation-4186", "mrqa_squad-validation-423", "mrqa_squad-validation-4331", "mrqa_squad-validation-434", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4430", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4681", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-501", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5198", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5265", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-552", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5640", "mrqa_squad-validation-5653", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5849", "mrqa_squad-validation-5893", "mrqa_squad-validation-5986", "mrqa_squad-validation-6024", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6294", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-6728", "mrqa_squad-validation-680", "mrqa_squad-validation-6841", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6917", "mrqa_squad-validation-7029", "mrqa_squad-validation-7164", "mrqa_squad-validation-7175", "mrqa_squad-validation-7302", "mrqa_squad-validation-7362", "mrqa_squad-validation-7366", "mrqa_squad-validation-7435", "mrqa_squad-validation-744", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7709", "mrqa_squad-validation-7740", "mrqa_squad-validation-7766", "mrqa_squad-validation-7775", "mrqa_squad-validation-7818", "mrqa_squad-validation-7821", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8144", "mrqa_squad-validation-8163", "mrqa_squad-validation-828", "mrqa_squad-validation-8336", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8372", "mrqa_squad-validation-848", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8600", "mrqa_squad-validation-8687", "mrqa_squad-validation-8747", "mrqa_squad-validation-8759", "mrqa_squad-validation-8807", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-9050", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9348", "mrqa_squad-validation-9401", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9830", "mrqa_squad-validation-9831", "mrqa_squad-validation-9893", "mrqa_squad-validation-9930", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4825", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7542", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-871"], "OKR": 0.876953125, "KG": 0.46484375, "before_eval_results": {"predictions": ["one way streets", "since at least the mid-14th century", "Marlee Matlin", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight", "Classic", "88", "826", "leishmaniasis", "Romulus, My Father", "Theodore Roosevelt Mason", "1996", "1964", "Kinnairdy Castle", "James Edward Franco", "Thai Air Asia X", "December 24, 1973", "Freiburg im Breisgau", "James Dean", "Rob Reiner", "Kansas City", "Westfield Tea Tree Plaza", "26,000", "Alemannic", "1992", "Darci Kistler", "the EN World web site", "1985", "Pigman's Bar-B- Que", "Columbus, Ohio", "Emilia-Romagna", "Miami Gardens, Florida", "Chaplain to the Forces", "John Sullivan", "churros", "Franklin, Indiana", "Ub Iwerks", "mentalfloss.com", "Omega SA", "Flashback", "The Walter Reed Army Medical Center (WRAMC)", "Kiernan Brennan Shipka", "1993", "Democratic Republic of the Congo", "Chris DeStefano", "The pronghorn", "Walcha", "French", "FBI", "BMW X6", "David Michael Bautista Jr.", "the Cherokee River", "15 mi", "Taylor Swift", "for control purposes", "the 2017 / 18 Divisional Round game against the New Orleans Saints", "Robin", "Melbourne, Victoria, Australia", "Felipe Massa", "The Washington Post", "Constantine XI Palaiologos", "Viva Las Vegas", "bats", "Democritus", "Tennessee"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5910183566433567}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3636363636363636, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1459", "mrqa_squad-validation-6653", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-238", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4802", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3052", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3402", "mrqa_searchqa-validation-15983"], "SR": 0.453125, "CSR": 0.6071428571428572, "retrieved_ids": ["mrqa_squad-train-78994", "mrqa_squad-train-21820", "mrqa_squad-train-32948", "mrqa_squad-train-48230", "mrqa_squad-train-46824", "mrqa_squad-train-30379", "mrqa_squad-train-35042", "mrqa_squad-train-1416", "mrqa_squad-train-72455", "mrqa_squad-train-34590", "mrqa_squad-train-68811", "mrqa_squad-train-14403", "mrqa_squad-train-60331", "mrqa_squad-train-38557", "mrqa_squad-train-22693", "mrqa_squad-train-31578", "mrqa_squad-validation-7818", "mrqa_squad-validation-3118", "mrqa_newsqa-validation-1148", "mrqa_searchqa-validation-15757", "mrqa_squad-validation-3113", "mrqa_triviaqa-validation-3087", "mrqa_naturalquestions-validation-9576", "mrqa_searchqa-validation-14441", "mrqa_squad-validation-680", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-7241", "mrqa_naturalquestions-validation-10113", "mrqa_squad-validation-6402", "mrqa_squad-validation-7774", "mrqa_naturalquestions-validation-5039", "mrqa_triviaqa-validation-6739"], "EFR": 1.0, "Overall": 0.7425223214285714}, {"timecode": 21, "before_eval_results": {"predictions": ["between 1000 and 1900", "Imperialism", "student-teacher relationships", "2%", "Ted Ginn Jr.", "Harrods", "Coptic Cathedral", "Philadelphia", "Disco", "MMA", "Kentucky River", "Tiberius", "1995", "public", "Lewis Madison Terman", "Robert FitzRoy", "Beno\u00eet Jacquot", "Nanna Popham Britton", "Sada Carolyn Thompson", "S. F. Newcombe", "Nelson Mandela", "Dissection", "Iranian-German", "The A41", "New South Wales", "Bronwyn Kathleen Bishop", "The Timekeeper", "Jena Malone", "water", "Hong Kong First Division League", "Four Weddings and a Funeral", "from 1993 to 1996", "July 16, 1971", "influenced by the music genres of electronic rock, electropop and R&B", "Ted Nugent", "Lovejoy", "Geet or The Song", "October 5, 1930", "Pamelyn Ferdin", "The Ninth Gate", "Comeng and Clyde Engineering", "Martin Noel Galgani Fitzpatrick", "Saint Motel", "200", "War Is the Answer", "Eleanor of Aquitaine", "Anandapala", "Amy Poehler", "The Division of Cook", "Peter Townsend", "Blue Origin", "Taylor Swift", "18 December 1975", "a convergent plate boundary", "its vast territory was divided into several successor polities", "Kiri Te Kanawa", "Christchurch", "30-minute", "the gunfight continued late into the night.", "Robert Wightman", "tsuzumi", "`` how now '' is a greeting, short for `` how say you now ''", "24 hours", "Nigel Lythgoe, Mia Michaels, and Adam Shankman"], "metric_results": {"EM": 0.53125, "QA-F1": 0.632062728937729}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.4444444444444445]}}, "before_error_ids": ["mrqa_squad-validation-9904", "mrqa_squad-validation-1873", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-4466", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-1219", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4172", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-5447", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-7852"], "SR": 0.53125, "CSR": 0.6036931818181819, "retrieved_ids": ["mrqa_squad-train-4059", "mrqa_squad-train-73823", "mrqa_squad-train-46879", "mrqa_squad-train-86355", "mrqa_squad-train-8004", "mrqa_squad-train-44522", "mrqa_squad-train-77280", "mrqa_squad-train-10508", "mrqa_squad-train-70856", "mrqa_squad-train-69442", "mrqa_squad-train-9768", "mrqa_squad-train-22642", "mrqa_squad-train-50458", "mrqa_squad-train-45886", "mrqa_squad-train-28841", "mrqa_squad-train-34435", "mrqa_hotpotqa-validation-2436", "mrqa_searchqa-validation-11166", "mrqa_squad-validation-625", "mrqa_searchqa-validation-16574", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-10271", "mrqa_squad-validation-4730", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-3918", "mrqa_squad-validation-3559", "mrqa_naturalquestions-validation-716", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-4664", "mrqa_triviaqa-validation-6466", "mrqa_naturalquestions-validation-8359"], "EFR": 1.0, "Overall": 0.7418323863636364}, {"timecode": 22, "before_eval_results": {"predictions": ["Zorro", "Zagreus", "January 1985", "Computational complexity theory", "to plan the physical proceedings, and to integrate those proceedings with the other parts", "post-World War I", "Esteban Ocon", "New Orleans Saints", "Clarence Nash", "The Dressmaker", "1,800", "August 14, 1848", "Urijah Faber", "Mary-Kay Wilmers", "baeocystin", "was first published under William Shakespeare's name in the 1601 with no named author", "Vaisakhi List", "Ars Nova Theater", "Monticello", "a large portion of rural Maine, published six days per week in Bangor, Maine", "Menace II Society", "16 March 1987", "Holden Calais (VF)", "Louis \"Louie\" Zamperini", "1968", "PlayStation 4", "The Andes or Andean Mountains", "the Knight Company", "Parlophone Records", "Nicolas Winding Refn", "South America", "Morocco", "Gerard Marenghi", "1958", "126,202", "31 July 1975", "orange", "Roseann O'Donnell", "The Case for Hillary Clinton", "SAVE", "29,000", "the extraterrestrial hypothesis ( ETH)", "Larnelle Steward Harris", "4,613", "a mixture of ice, orange juice, sweetener, milk, powdered egg whites and vanilla flavoring", "Ramzan Kadyrov", "Albert", "Tetrahydrogestrinone", "5,042", "Captain B.J. Hunnicutt", "Victoria", "The Keeping Hours", "the Gilbert building", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Isabel Maru", "in reality there\u2019s a whole world (literally) of great pizza out there", "The Archers", "the raven", "intravenous vitamin \"drips\"", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Daniel Radcliffe", "Milla Jovovich", "Excalibur", "the brat"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6745963874680307}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6956", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-3156", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-460", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-2495", "mrqa_triviaqa-validation-1447", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-11933"], "SR": 0.59375, "CSR": 0.6032608695652174, "retrieved_ids": ["mrqa_squad-train-75214", "mrqa_squad-train-60054", "mrqa_squad-train-73657", "mrqa_squad-train-16762", "mrqa_squad-train-34499", "mrqa_squad-train-56242", "mrqa_squad-train-47467", "mrqa_squad-train-65336", "mrqa_squad-train-7916", "mrqa_squad-train-5244", "mrqa_squad-train-4618", "mrqa_squad-train-46850", "mrqa_squad-train-17548", "mrqa_squad-train-1755", "mrqa_squad-train-34603", "mrqa_squad-train-15942", "mrqa_hotpotqa-validation-2419", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-2351", "mrqa_squad-validation-5318", "mrqa_naturalquestions-validation-7514", "mrqa_squad-validation-6073", "mrqa_hotpotqa-validation-2720", "mrqa_naturalquestions-validation-379", "mrqa_squad-validation-3885", "mrqa_naturalquestions-validation-4863", "mrqa_hotpotqa-validation-10", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-1201", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-13057"], "EFR": 1.0, "Overall": 0.7417459239130435}, {"timecode": 23, "before_eval_results": {"predictions": ["mass production", "Edmonton, Canada", "Ollie Treiz", "Western Xia", "Richard Trevithick", "coughing and sneezing", "9", "the outdoors", "The Backstreet Boys", "November of that year", "Hawaii", "Tool", "1919", "May 5, 2015", "Arthur Miller", "Edmonton, Alberta", "Brent Robert Barry", "1991", "The LA Galaxy", "Nicholas Kristof", "film and short novels", "Flushed Away", "first baseman and third baseman", "The River Welland", "The Process", "200,167", "Bohemia", "\"Jawbreaker\"", "Armin Meiwes", "1933", "Flula Borg", "Carl Michael Edwards", "In a Better World", "the 45th Infantry Division", "Iron Man 3", "Julie 2", "Conservatorio Verdi in Milan", "6'5\"", "a terrible date", "June 26, 1970", "Woking, England", "7", "About a Boy", "1982", "1978", "A black cat", "Denmark", "1999", "The Ryukyuan", "Peter Thiel", "Summerlin", "Cersei Jaimeister", "Kida", "the disk", "Mahinda Rajapaksa", "iron", "naples", "Alias Smith and Jones", "this is not a project for commercial gain.", "\"underwear bomber\"", "2.5 million", "1,000,000", "Sockeye", "the Watergate scandal"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6984809027777779}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.125, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-3634", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-2293", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-865", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-10123"], "SR": 0.640625, "CSR": 0.6048177083333333, "retrieved_ids": ["mrqa_squad-train-81907", "mrqa_squad-train-79059", "mrqa_squad-train-22814", "mrqa_squad-train-51357", "mrqa_squad-train-81164", "mrqa_squad-train-11218", "mrqa_squad-train-52995", "mrqa_squad-train-30954", "mrqa_squad-train-22961", "mrqa_squad-train-12132", "mrqa_squad-train-79779", "mrqa_squad-train-40260", "mrqa_squad-train-69494", "mrqa_squad-train-62303", "mrqa_squad-train-13678", "mrqa_squad-train-14246", "mrqa_searchqa-validation-7241", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-4863", "mrqa_squad-validation-9285", "mrqa_searchqa-validation-16574", "mrqa_squad-validation-4673", "mrqa_searchqa-validation-9477", "mrqa_squad-validation-4848", "mrqa_searchqa-validation-15479", "mrqa_naturalquestions-validation-6052", "mrqa_hotpotqa-validation-5250", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-1284", "mrqa_squad-validation-6614", "mrqa_naturalquestions-validation-2876"], "EFR": 1.0, "Overall": 0.7420572916666666}, {"timecode": 24, "before_eval_results": {"predictions": ["1965", "low demand", "A tundra", "article 30", "The mermaid", "Helmand province, Afghanistan", "it was eventually ratified by the World Sailing Speed Record Council", "Troy Livesay", "Kurt Cobain", "seven", "two years", "to come together and \"set a long-term goal for reducing\" greenhouse emissions.\"", "piano", "Charlotte Gainsbourg and Willem Dafoe", "The Charlie Daniels Band", "Turkey not only can and will continue to help provide safety and physical security, but also could further assist with the reconstruction projects", "curfew", "Missouri", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards", "five victims by helicopter", "30-minute", "San Diego", "Iran", "cancer", "a baseball bat", "the Navy", "$199", "the directive is signed by Gen. Stanley McChrystal, the top NATO commander in Afghanistan,", "sportswear", "is president and CEO of Ripken Baseball,", "eight-day", "the FAA received no reports from pilots in the air of any sightings", "December 7, 1941", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "NATO fighters", "Jaime Andrade", "Somali President Sheikh Sharif Sheikh Ahmed", "that I've been feeling better every single day since surgery and this weekend my doctors gave me the green light to get back to work.\"", "that the majority of commentators have settled on the idea that the Richmond students did nothing because of the \"bystander effect\"", "Orbiting Carbon Observatory", "German Chancellor Angela Merkel", "Rivers", "Ryder Russell", "Samson D'Souza", "the FBI", "Michael Jackson's mother, Katherine Jackson, his three children and undisclosed charities.", "Asashoryu", "the District of Columbia National Guard", "the Interior Ministry", "Anil Kapoor", "it is estimated at $3 billion, with further foreign direct investment exceeding $40 billion during the operations phase.", "question people if there's reason to suspect they're in the United States illegally", "a young girl", "Elizabeth Dean Lail", "1926", "a squall", "Wyoming", "Kenny Everett", "Japan", "Richa Sharma", "The Frost Place Advanced Seminar", "it's still one of my favorite shows", "Valentina Tereshkova", "Baccarat"], "metric_results": {"EM": 0.53125, "QA-F1": 0.638396586052836}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.16666666666666669, 0.0, 0.5, 1.0, 1.0, 0.09999999999999999, 1.0, 0.0, 1.0, 0.05555555555555555, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.25, 1.0, 0.26666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.8, 0.13333333333333333, 0.24242424242424243, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.888888888888889, 0.0, 0.0, 0.3, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3010", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6214", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-733", "mrqa_searchqa-validation-14187"], "SR": 0.53125, "CSR": 0.6018749999999999, "retrieved_ids": ["mrqa_squad-train-80655", "mrqa_squad-train-37205", "mrqa_squad-train-1161", "mrqa_squad-train-19359", "mrqa_squad-train-9096", "mrqa_squad-train-15526", "mrqa_squad-train-45297", "mrqa_squad-train-997", "mrqa_squad-train-27919", "mrqa_squad-train-32437", "mrqa_squad-train-70160", "mrqa_squad-train-29789", "mrqa_squad-train-74713", "mrqa_squad-train-70117", "mrqa_squad-train-66708", "mrqa_squad-train-43433", "mrqa_naturalquestions-validation-10357", "mrqa_squad-validation-8258", "mrqa_naturalquestions-validation-1165", "mrqa_squad-validation-10145", "mrqa_naturalquestions-validation-7351", "mrqa_newsqa-validation-3790", "mrqa_naturalquestions-validation-2945", "mrqa_squad-validation-5357", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-438", "mrqa_squad-validation-9213", "mrqa_triviaqa-validation-5590", "mrqa_squad-validation-4322", "mrqa_triviaqa-validation-1816", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5821"], "EFR": 0.9666666666666667, "Overall": 0.7348020833333333}, {"timecode": 25, "before_eval_results": {"predictions": ["phycobilisomes", "WatchESPN", "full independent prescribing authority", "10,000", "it infringed on democratic freedoms", "Osama bin Laden's sons", "a house party in Crandon, Wisconsin", "Paul Ryan", "Amstetten,", "propofol,", "his comments", "U.S.", "Polo", "an accidental death in which no law was broken or criminal negligence involved.", "Nkepile M abuse", "a Royal Air Force helicopter", "Samuel Herr, 26, and Juri Kibuishi, 23", "Unibail-Rodamco", "the administration's progress, while we also encourage him to 'evolve faster' on supporting full marriage equality,\"", "1981", "Jewish", "al Fayed's", "Teresa Hairston", "Dan Parris, 25, and Rob Lehr, 26,", "18", "July", "\"G gossip Girl\"", "Jason Chaffetz is a freshman Republican congressman representing the 3rd District of Utah.", "debris", "a woman who accuse him of sexual harassment", "Hansa (Malmborgsgatan 6)", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "21", "in the heart of an urban center like Los Angeles.", "they would not be making any further comments, citing the investigation.", "North Korean leader Kim Jong Il seems to be \"testing the new administration.\"", "3rd District of Utah", "House-passed bill that eliminates the 3% withholding requirement for government contractors", "\"perezagruzka,\"", "one", "Arsene Wenger can expect an apology from Premier League referees chief Keith Hackett following his dismissal in the closing seconds of Saturday's 2-1 English Premier League defeat to Manchester United.", "1998", "23", "American", "\"we take this issue seriously,\"", "Monday.", "Frank Ricci", "Old Trafford", "Guinea, Myanmar, Sudan and Venezuela.", "the Louvre.", "homicide by undetermined means", "Les Bleus", "Madison's", "Gibraltar", "of their need to repent in time", "John McCarthy", "\"One true Friend,\"", "dolphins", "Atlas ICBM", "Lionel Eugene Hollins", "Duchess Eleanor of Aquitaine", "John Irving", "diabetes", "the University of Vienna"], "metric_results": {"EM": 0.375, "QA-F1": 0.45124322674874146}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.28571428571428575, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.1818181818181818, 0.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5833333333333334, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-8392", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-981", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-3726", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2673", "mrqa_triviaqa-validation-3799", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-5238", "mrqa_searchqa-validation-11369"], "SR": 0.375, "CSR": 0.5931490384615384, "retrieved_ids": ["mrqa_squad-train-15562", "mrqa_squad-train-74994", "mrqa_squad-train-16834", "mrqa_squad-train-10133", "mrqa_squad-train-46703", "mrqa_squad-train-8788", "mrqa_squad-train-83001", "mrqa_squad-train-80439", "mrqa_squad-train-734", "mrqa_squad-train-67150", "mrqa_squad-train-2994", "mrqa_squad-train-86296", "mrqa_squad-train-46133", "mrqa_squad-train-73561", "mrqa_squad-train-741", "mrqa_squad-train-49540", "mrqa_squad-validation-383", "mrqa_naturalquestions-validation-9895", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-9454", "mrqa_naturalquestions-validation-4664", "mrqa_triviaqa-validation-4987", "mrqa_naturalquestions-validation-5599", "mrqa_squad-validation-6913", "mrqa_newsqa-validation-3802", "mrqa_squad-validation-3585", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-3160", "mrqa_triviaqa-validation-1498", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-5821"], "EFR": 1.0, "Overall": 0.7397235576923077}, {"timecode": 26, "before_eval_results": {"predictions": ["Eero Saarinen", "according to a multiple access scheme", "composite", "Climate fluctuations during the last 34 million years", "Chicago Cubs", "The Salopian", "the slide", "Norman Hartnell", "canoeing", "Millbank", "Poland", "Adam Ant", "sea levels", "New Democracy", "Missouri", "six", "sea-fruiting", "Turkey", "1979", "Pooh and his friends", "London Borough of Walford", "Sacred Theology", "Laurence Kerr Olivier", "four", "Leonard Bernstein", "in writing, whereby all writings whatsoever may be engrossed in paper or parchment so that the said machine or method may be of great use in settlements", "11-1", "braille is a system of reading and writing for blind people,", "blood", "aircraft", "passion fruit", "a jumper", "The Italian Job", "caridean shrimp", "Yemen", "Orson Welles", "George IV", "Barry Briggs", "Joseph Smith, Jr.", "Cornwall, Durham, East Riding of Yorkshire, Shropshire", "Kievan Rus", "the heart", "Beaujolais Nouveau", "rowing", "$100,000", "1970", "Harry (Mick)", "'Lord Nelson'", "ancient Testament", "Argentina", "a lion", "Sinclair Lewis", "Phosphorus pentoxide", "Kalinga Ashoka ( son of Bindusara )", "Norwegian", "the County of York", "Cheshire County, New Hampshire", "Atomic Kitten", "Phillip A. Myers.", "11th year in a row", "Opry Mills,", "Heroes", "Marian Anderson", "Alzheimer's disease"], "metric_results": {"EM": 0.5, "QA-F1": 0.5953125}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-8918", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-1872", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-2496", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-6073", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-459", "mrqa_naturalquestions-validation-946", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-347", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-1554"], "SR": 0.5, "CSR": 0.5896990740740741, "retrieved_ids": ["mrqa_squad-train-78914", "mrqa_squad-train-5034", "mrqa_squad-train-21613", "mrqa_squad-train-37612", "mrqa_squad-train-61838", "mrqa_squad-train-74669", "mrqa_squad-train-35693", "mrqa_squad-train-13688", "mrqa_squad-train-76652", "mrqa_squad-train-47653", "mrqa_squad-train-15246", "mrqa_squad-train-27643", "mrqa_squad-train-22402", "mrqa_squad-train-77596", "mrqa_squad-train-14602", "mrqa_squad-train-51624", "mrqa_triviaqa-validation-2317", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-4454", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-5857", "mrqa_searchqa-validation-11366", "mrqa_hotpotqa-validation-3175", "mrqa_squad-validation-1459", "mrqa_naturalquestions-validation-5611", "mrqa_searchqa-validation-1643", "mrqa_naturalquestions-validation-9005", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-7627", "mrqa_newsqa-validation-1453", "mrqa_naturalquestions-validation-5719"], "EFR": 1.0, "Overall": 0.7390335648148147}, {"timecode": 27, "before_eval_results": {"predictions": ["water-cooled", "The Genghis Khan Mausoleum", "the most cost efficient bidder", "February 1, 2016", "Stormi Webster", "53", "Valinor ( Land of the Valar )", "1993", "Elizabeth Dean Lail", "Lee Diamond", "Peter Finch", "Jerry Ekandjo", "the inmates have been detained indefinitely without trial and several detainees have alleged torture,", "775", "north", "23 %", "Joie de vivre", "American Horror Story : Roanoke", "a flash music video", "Saxony, Thuringia and Saxony - Anhalt, or a smaller part of this region, such as the metropolitan area of Leipzig and Halle", "King Saud University", "Sherwood Forest", "on the southeastern coast of the Commonwealth of Virginia in the United States", "Camping World Stadium in Orlando", "prospective studies", "Kyrie Irving", "a brain region that in humans is located in the dorsal portion of the frontal lobe", "Paul Hogan", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "1996", "Bill Condon", "capillaries", "September 6, 2019", "two", "Scar", "August 6", "3000 BC", "Isthmus of Corinth", "1976", "on the genitals floor", "October 2008", "Broken Hill and Sydney", "Latin liberalia studia", "The Hunger Games : Mockingjay -- Part 2 ( 2015 )", "1773", "Escherichia coli", "5 : 7 -- 8", "radioisotope thermoelectric generator", "Carol Worthington", "when viewed from different points on Earth", "interstitial and intravascular", "2014", "the baggage car of the train", "Dorset", "pressure", "Frederick Martin \"Fred\" Mac Murray", "Franz Ferdinand Carl Ludwig Joseph Maria", "authoritarian", "(the Democratic VP candidate delivers a big speech next Wednesday)", "Jeanne Tripplehorn", "How I Met Your Mother", "Portugal", "grow old along with me", "a Holstein cow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6110140039558143}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.06896551724137931, 0.4, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 0.5454545454545454, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-8625", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-1180", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-5286", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-435", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-7864"], "SR": 0.515625, "CSR": 0.5870535714285714, "retrieved_ids": ["mrqa_squad-train-74814", "mrqa_squad-train-33292", "mrqa_squad-train-71812", "mrqa_squad-train-57512", "mrqa_squad-train-35144", "mrqa_squad-train-11020", "mrqa_squad-train-30875", "mrqa_squad-train-68682", "mrqa_squad-train-61131", "mrqa_squad-train-37565", "mrqa_squad-train-20216", "mrqa_squad-train-63446", "mrqa_squad-train-16067", "mrqa_squad-train-30854", "mrqa_squad-train-49433", "mrqa_squad-train-82066", "mrqa_newsqa-validation-808", "mrqa_naturalquestions-validation-2452", "mrqa_newsqa-validation-1457", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8352", "mrqa_triviaqa-validation-4928", "mrqa_naturalquestions-validation-3182", "mrqa_newsqa-validation-1811", "mrqa_naturalquestions-validation-81", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-943", "mrqa_newsqa-validation-1234", "mrqa_searchqa-validation-16574", "mrqa_hotpotqa-validation-4112", "mrqa_newsqa-validation-2954", "mrqa_naturalquestions-validation-7827"], "EFR": 0.8709677419354839, "Overall": 0.7126980126728111}, {"timecode": 28, "before_eval_results": {"predictions": ["the father of the house when in his home", "Welsh", "data link", "blue", "white wine", "Vietnam", "John Peel", "Moscow", "insects and their relationship to humans, other organisms, and the environment", "Soviet Union", "St. Petersburg", "malaria", "1976", "Kent", "Arthur, Prince of Wales", "Israel", "butterflies", "New Jersey", "the Philippines", "political violence", "the number thirteen", "Aquae Sulis", "Eric Coates", "diggory Venn", "to make a furrow or furrows in (land)", "long and experimental", "southern North America", "Aberystwyth", "Eric Morley", "Saskatchewan", "Mickey Spillane", "Erik Aunapuu", "Frank Sinatra", "Galen Rupp", "Washington Post", "Niger", "The Lone Gunmen", "David Nixon", "piano", "triton", "Addis Ababa", "pascal", "heart", "Nova Scotia", "Francesso Strazzari", "James Lillywhite", "Nigeria", "the Dead Sea", "40", "the isthmus", "the first inflight movie was shown", "penultima", "Gerald Ford", "Spanish missionaries, ranchers and troops", "William Wyler", "Nardwuar the Human Serviette", "1887", "Portal", "at checkposts and military camps in the Mohmand agency,", "a plaque", "Phillip A. Myers", "Priscilla Ann Beaulieu", "Cold Mountain", "samurai"], "metric_results": {"EM": 0.5, "QA-F1": 0.5314867424242424}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.6, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2318", "mrqa_squad-validation-4968", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-836", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-3536", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-3924", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-388", "mrqa_newsqa-validation-2885", "mrqa_searchqa-validation-1846", "mrqa_searchqa-validation-2066"], "SR": 0.5, "CSR": 0.584051724137931, "retrieved_ids": ["mrqa_squad-train-85706", "mrqa_squad-train-28911", "mrqa_squad-train-5390", "mrqa_squad-train-33053", "mrqa_squad-train-1967", "mrqa_squad-train-30554", "mrqa_squad-train-38761", "mrqa_squad-train-18443", "mrqa_squad-train-17587", "mrqa_squad-train-9654", "mrqa_squad-train-44185", "mrqa_squad-train-85367", "mrqa_squad-train-82830", "mrqa_squad-train-75598", "mrqa_squad-train-3544", "mrqa_squad-train-18498", "mrqa_triviaqa-validation-4130", "mrqa_squad-validation-6024", "mrqa_triviaqa-validation-1855", "mrqa_hotpotqa-validation-5098", "mrqa_triviaqa-validation-7269", "mrqa_newsqa-validation-3198", "mrqa_naturalquestions-validation-4762", "mrqa_searchqa-validation-10635", "mrqa_hotpotqa-validation-943", "mrqa_triviaqa-validation-3616", "mrqa_squad-validation-2906", "mrqa_squad-validation-7409", "mrqa_naturalquestions-validation-9576", "mrqa_squad-validation-8747", "mrqa_triviaqa-validation-2183", "mrqa_newsqa-validation-3175"], "EFR": 1.0, "Overall": 0.7379040948275862}, {"timecode": 29, "before_eval_results": {"predictions": ["Albert C. Outler", "Annan and his UN-backed panel and African Union chairman Jakaya Kikwete", "Drogo", "arthur ltd", "Bennet", "Australia", "chipmunk", "Amnesty International", "the Scud", "Australia", "Labrador Retriever", "Rio Grande", "horror", "ecclesiastical communities", "jurata", "times", "Brooklyn", "Augustus", "the Bible", "ltd", "PJ Harvey", "Gryffindor", "archer", "The French Connection", "Thundercats", "arthur", "Rapa Nui", "labrador", "baku", "piero della Francesca", "Oliver", "Hindu", "sole", "Alanis Morissette", "high sewing", "Herbert Henry Asquith,", "sugar", "index fingers", "plants and animals", "tennis", "times", "9", "function", "soprano voice", "purple rain", "Fenn Street School", "estonia w", "alfa", "mike lmao", "sartre", "temperature", "couscous", "March 31, 2017", "1987", "reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "Dutch", "1981", "Humberside", "to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "were injected with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "many sequels, theme park attractions and an extensive line of merchandise", "jesu the Virginian", "tap", "microwave"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5149999999999999}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.16, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7292", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-2997", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2618", "mrqa_searchqa-validation-10920"], "SR": 0.484375, "CSR": 0.5807291666666667, "retrieved_ids": ["mrqa_squad-train-38219", "mrqa_squad-train-36374", "mrqa_squad-train-60392", "mrqa_squad-train-31130", "mrqa_squad-train-39908", "mrqa_squad-train-41353", "mrqa_squad-train-18296", "mrqa_squad-train-8745", "mrqa_squad-train-44363", "mrqa_squad-train-36852", "mrqa_squad-train-46368", "mrqa_squad-train-51490", "mrqa_squad-train-12037", "mrqa_squad-train-27956", "mrqa_squad-train-69422", "mrqa_squad-train-2856", "mrqa_squad-validation-2565", "mrqa_naturalquestions-validation-4280", "mrqa_searchqa-validation-8896", "mrqa_squad-validation-5178", "mrqa_triviaqa-validation-3351", "mrqa_hotpotqa-validation-1204", "mrqa_squad-validation-4343", "mrqa_squad-validation-7771", "mrqa_hotpotqa-validation-10", "mrqa_squad-validation-434", "mrqa_searchqa-validation-3563", "mrqa_triviaqa-validation-7613", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-10049", "mrqa_triviaqa-validation-3087", "mrqa_naturalquestions-validation-2100"], "EFR": 1.0, "Overall": 0.7372395833333334}, {"timecode": 30, "UKR": 0.79296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1132", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-2642", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3117", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-394", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-478", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-5368", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-998", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2364", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-913", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5986", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-9310", "mrqa_squad-validation-10141", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-1131", "mrqa_squad-validation-1272", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-1551", "mrqa_squad-validation-1639", "mrqa_squad-validation-1641", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1732", "mrqa_squad-validation-1830", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2849", "mrqa_squad-validation-2881", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3205", "mrqa_squad-validation-3270", "mrqa_squad-validation-3385", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3841", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4166", "mrqa_squad-validation-423", "mrqa_squad-validation-4385", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4655", "mrqa_squad-validation-4673", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4806", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5198", "mrqa_squad-validation-5234", "mrqa_squad-validation-5265", "mrqa_squad-validation-5331", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5590", "mrqa_squad-validation-5640", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5986", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6614", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-668", "mrqa_squad-validation-6727", "mrqa_squad-validation-6894", "mrqa_squad-validation-6956", "mrqa_squad-validation-7029", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-7435", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7740", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8144", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-839", "mrqa_squad-validation-848", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8600", "mrqa_squad-validation-8646", "mrqa_squad-validation-8656", "mrqa_squad-validation-8687", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-915", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-939", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9813", "mrqa_squad-validation-9878", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1021", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2789", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3512", "mrqa_triviaqa-validation-3595", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4717", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4858", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-884"], "OKR": 0.8828125, "KG": 0.475, "before_eval_results": {"predictions": ["cellular respiration", "The centre-left Australian Labor Party", "the most cost efficient bidder", "Prussian", "1,500 ft", "Armani, Esprit and Volvo", "the Forbes billionaires list from 2017", "Hanoi", "Stephen Mangan", "saloon-keeper", "Bill McCutcheon", "Vixen", "Adult Swim", "Venice", "Tropical Storm Ana", "1974", "tokamak", "March 17, 2015", "American", "1958", "China Airlines", "Wayne County, Michigan", "holder of the Great Seal of Scotland", "Richard Nixon", "Greek-American", "Honey Irani", "2000", "Beauty and the Beast", "137th", "Nobel Prize", "Nayvadius DeMun Wilburn", "17 April 1986", "Francis Nethersole", "hiphop", "47,818", "Salisbury", "Lakshmibai", "Tony Aloupis", "sarod", "Anne Arundel County", "Austria Wien", "the midnight sun", "Robert A. Iger", "the Netherlands", "biochemist and academic Dr. Alberto Taquini", "2 March 1972", "Terry the Tomboy", "Gracie Mansion", "Parlophone Records", "The R-8 Human Rhythm Composer", "Obergruppenf\u00fchrer", "World War I", "714", "Sandy Knox and Billy Stritch", "the Sunni Muslim family", "mona Lisa", "eight", "the Cevennes", "an open window", "staff sergeant", "\"The detention of terror suspects should be tried by a U.S. military commision, has refused to refer the case of Mohammed al-Qahtani to prosecutors", "trans fats", "Anna Mary Robertson", "Albert Einstein"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6170253357753358}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.07692307692307693, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2885", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-3524", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-1370", "mrqa_hotpotqa-validation-4594", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-2529", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-3672", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-4709", "mrqa_newsqa-validation-3819", "mrqa_searchqa-validation-12835"], "SR": 0.515625, "CSR": 0.5786290322580645, "retrieved_ids": ["mrqa_squad-train-46165", "mrqa_squad-train-75662", "mrqa_squad-train-84224", "mrqa_squad-train-19463", "mrqa_squad-train-58386", "mrqa_squad-train-82369", "mrqa_squad-train-64221", "mrqa_squad-train-33986", "mrqa_squad-train-39579", "mrqa_squad-train-79019", "mrqa_squad-train-36327", "mrqa_squad-train-85036", "mrqa_squad-train-50736", "mrqa_squad-train-63851", "mrqa_squad-train-65445", "mrqa_squad-train-26472", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4928", "mrqa_naturalquestions-validation-7827", "mrqa_triviaqa-validation-1622", "mrqa_squad-validation-1388", "mrqa_hotpotqa-validation-4086", "mrqa_triviaqa-validation-1023", "mrqa_naturalquestions-validation-10357", "mrqa_hotpotqa-validation-4463", "mrqa_naturalquestions-validation-2945", "mrqa_searchqa-validation-5845", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-6930", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-1796", "mrqa_searchqa-validation-711"], "EFR": 1.0, "Overall": 0.7458820564516129}, {"timecode": 31, "before_eval_results": {"predictions": ["medical services", "phagocytes", "Finland", "Gulf of Aden", "Natty Bumppo", "stanley", "Amsterdam", "cellulose", "a dragon", "Ryan O\u2019 Neal", "Sicily", "Howard Keel", "cobra Bubbles", "Charlie Sheen", "Sweet Home Alabama", "alopecia universalis", "Percy Sledge", "porcelain", "wales", "Man V Food", "eighteenth-century", "Kajagoogoo", "George Fox", "Croatian", "mrs henderson", "Exile", "heineken", "Esau", "South Africa", "fidelio", "ABBA", "Some Like It Hot", "mercury", "Cleopatra", "fumage", "Enrico Caruso", "Hitler right to invade Russia in 1941", "hydrogen", "nitric acid", "Tasmania", "three thousand ducats", "Mille Miglia", "tiger", "rhododendron", "Uranus", "Utrecht", "polente del Arzobispo", "a wedge of hard cheese", "stenographer", "caliper", "arts", "Adolf Hitler", "dealer sets the cards face - down on the table near the player designated to make the cut, typically the player to the dealer's right", "2020", "In the mountains outside City 17", "4,613", "Swiss Super League", "Benj Pasek and Justin Paul", "state's first lady,", "witty skits or doom-ladened eco-horror scenarios", "innovative, exciting skyscrapers", "chicago", "a shipwreck", "cleaning products"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5920758928571429}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4166666666666667, 0.2857142857142857, 1.0, 1.0, 0.8571428571428571, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4462", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-3257", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-5359", "mrqa_triviaqa-validation-7201", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-2298", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-7145", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-5649", "mrqa_hotpotqa-validation-1690", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3", "mrqa_searchqa-validation-199", "mrqa_searchqa-validation-10166"], "SR": 0.53125, "CSR": 0.5771484375, "retrieved_ids": ["mrqa_squad-train-41762", "mrqa_squad-train-34280", "mrqa_squad-train-85704", "mrqa_squad-train-16129", "mrqa_squad-train-60085", "mrqa_squad-train-53616", "mrqa_squad-train-66419", "mrqa_squad-train-39486", "mrqa_squad-train-16865", "mrqa_squad-train-2903", "mrqa_squad-train-6532", "mrqa_squad-train-40793", "mrqa_squad-train-51901", "mrqa_squad-train-75510", "mrqa_squad-train-8784", "mrqa_squad-train-19746", "mrqa_naturalquestions-validation-9818", "mrqa_hotpotqa-validation-2112", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-3517", "mrqa_hotpotqa-validation-1474", "mrqa_naturalquestions-validation-1368", "mrqa_hotpotqa-validation-1204", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3611", "mrqa_hotpotqa-validation-1350", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-3559", "mrqa_naturalquestions-validation-8530", "mrqa_newsqa-validation-2865", "mrqa_searchqa-validation-7136", "mrqa_naturalquestions-validation-4505"], "EFR": 0.9666666666666667, "Overall": 0.7389192708333334}, {"timecode": 32, "before_eval_results": {"predictions": ["Torchwood", "Thomas Sowell", "wannabe", "toledle human growth hormones", "toledo", "October", "Wyoming", "Leicester", "rattlesnakes", "toledo", "Lisieux", "1929", "hypopituitarism", "February", "piano", "Gloucestershire", "Jupiter Mining Corporation", "Harold Bierman, Jr.", "Scooby snack", "Amal Clooney", "the houseboat", "the Adriatic Sea", "hitler", "fife", "Goran Ivanisevic", "Francis Drake", "Wikipedia", "Baku", "Truro", "The Five Most Famous Terriers", "france", "Madness", "Barings", "Anne Boleyn", "Boddy", "Ken Norton", "Yann Martel", "cabbage", "John", "Fleet Street", "Claire Goose", "one-third", "monaco", "ali Kaplinsky", "sorrento", "Edward III", "Bill Bryson", "toasting", "Triumph and Disaster", "monaco", "Norman Mailer", "a \"major science finding from the agency's ongoing exploration of Mars.\"", "between 1923 and 1925", "member states", "capillaries, alveoli, glomeruli", "sarod", "Laban Movement Analysis", "James I of England", "\"There were no reports of ground strikes or interference with aircraft in flight,", "on the bench", "Bright Automotive", "Will & Grace", "prehensile", "the English Channel"], "metric_results": {"EM": 0.484375, "QA-F1": 0.563923611111111}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.4444444444444445, 0.5, 0.0, 1.0, 0.6666666666666666, 0.08, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_triviaqa-validation-1725", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-952", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-7073", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-1770", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-2133", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-3994"], "SR": 0.484375, "CSR": 0.5743371212121212, "retrieved_ids": ["mrqa_squad-train-43879", "mrqa_squad-train-74824", "mrqa_squad-train-59502", "mrqa_squad-train-29560", "mrqa_squad-train-66238", "mrqa_squad-train-26460", "mrqa_squad-train-31540", "mrqa_squad-train-86100", "mrqa_squad-train-51944", "mrqa_squad-train-48452", "mrqa_squad-train-64365", "mrqa_squad-train-80330", "mrqa_squad-train-52891", "mrqa_squad-train-64677", "mrqa_squad-train-81252", "mrqa_squad-train-26087", "mrqa_hotpotqa-validation-4466", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3418", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5036", "mrqa_hotpotqa-validation-3324", "mrqa_newsqa-validation-981", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-1816", "mrqa_hotpotqa-validation-1831", "mrqa_searchqa-validation-9535", "mrqa_hotpotqa-validation-1398", "mrqa_squad-validation-7832", "mrqa_hotpotqa-validation-3081", "mrqa_newsqa-validation-808"], "EFR": 1.0, "Overall": 0.7450236742424242}, {"timecode": 33, "before_eval_results": {"predictions": ["Theory of the Earth to the Royal Society of Edinburgh", "the 1580s", "scoque", "France", "Ted Heath", "wuthering Heights", "Portugal", "Everton", "vice-admiral", "Sweeney Todd", "baby corvids", "Benfica", "six", "2002", "11", "Buzz Aldrin", "MC Hammer", "Archie Shuttleworth", "venus williams", "The IT Crowd", "The Cream of Manchester", "bovidae", "turtle", "Cold Comfort Farm", "Isar", "Ruth Rendell", "Australia", "Rose-Marie", "nicholas", "Beaujolais", "John Constable", "sheep", "linseed", "Carmen Miranda", "nottingham", "jump jump", "1882", "Jessica Simpson", "diolchwch", "sashimi", "sea otter", "dot-com", "Tunisia", "hunny", "earache", "scar", "Vladimir Putin", "croquet", "Southwest Airlines", "The Shard", "the best value diamond for your money", "wigan Warriors", "Taittiriya Samhita", "`` Product / market fit means being in a good market with a product that can satisfy that market", "The United States Secretary of State", "Jos\u00e9 Bispo Clementino dos Santos", "the Big East Conference", "Albert II, Prince of Monaco, Umberto II of Italy", "Spc. Megan Lynn Touma,", "suppress the memories and to live as normal a life as possible", "40", "ID", "The Tonight Show", "copper"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5613613816738816}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true], "QA-F1": [0.11111111111111112, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.6666666666666665, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-7271", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-7609", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-1809", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-4146", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-11614"], "SR": 0.515625, "CSR": 0.5726102941176471, "retrieved_ids": ["mrqa_squad-train-85845", "mrqa_squad-train-8930", "mrqa_squad-train-55329", "mrqa_squad-train-84121", "mrqa_squad-train-82416", "mrqa_squad-train-26180", "mrqa_squad-train-14256", "mrqa_squad-train-67981", "mrqa_squad-train-31912", "mrqa_squad-train-29622", "mrqa_squad-train-6807", "mrqa_squad-train-43654", "mrqa_squad-train-19113", "mrqa_squad-train-35477", "mrqa_squad-train-11721", "mrqa_squad-train-63591", "mrqa_naturalquestions-validation-9773", "mrqa_hotpotqa-validation-4133", "mrqa_triviaqa-validation-6930", "mrqa_hotpotqa-validation-738", "mrqa_naturalquestions-validation-678", "mrqa_triviaqa-validation-2365", "mrqa_squad-validation-3270", "mrqa_squad-validation-2608", "mrqa_naturalquestions-validation-836", "mrqa_newsqa-validation-2199", "mrqa_naturalquestions-validation-3160", "mrqa_naturalquestions-validation-8161", "mrqa_squad-validation-2481", "mrqa_searchqa-validation-5447", "mrqa_newsqa-validation-2998", "mrqa_hotpotqa-validation-3970"], "EFR": 1.0, "Overall": 0.7446783088235295}, {"timecode": 34, "before_eval_results": {"predictions": ["Central business districts", "1973", "Brittany", "March 19", "Kenya", "September", "Japan", "edward Kipling", "james Garner", "Martin Luther King, Jr.", "petticoat", "the Indus River", "Puerto Rico", "180", "Charles Taylor", "conchita wurst", "canary", "the Savoy", "niki lauda", "Finland", "india", "japan", "Massachusetts", "Boutros Ghali", "PETER FRAMPTON", "Uranus", "mole", "Aleister Crowley", "Greek", "sheep", "Mumbai", "kitsunes", "frottage", "collage", "herbaceous", "the first European ones to ever have touched North American soil", "Captain Henry Appleton", "Angus Deayton", "Dean Martin", "Emily Davison", "Loki Laufeyiarson", "penny", "james Khan", "ethel Skinner", "jann haworth", "ghee", "Marcus Antonius", "Ragman Rolls", "commitment", "Trina Gulliver", "S\u00e8vres", "Procol Harum", "Victory gardens", "artes liberales", "Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4", "AT&T", "1998", "Dachshunds", "\"totaled,\"", "Iran of trying to build nuclear bombs", "\"It was quite surprising to learn of the request,\"", "Tennessee", "\"reshit\"", "anemia"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5833333333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-805", "mrqa_triviaqa-validation-7088", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-1087", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-3914", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-7019", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-2249", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3770", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-16252"], "SR": 0.5625, "CSR": 0.5723214285714286, "retrieved_ids": ["mrqa_squad-train-31401", "mrqa_squad-train-29748", "mrqa_squad-train-11520", "mrqa_squad-train-27312", "mrqa_squad-train-8450", "mrqa_squad-train-42048", "mrqa_squad-train-53110", "mrqa_squad-train-39424", "mrqa_squad-train-25403", "mrqa_squad-train-37471", "mrqa_squad-train-21218", "mrqa_squad-train-41267", "mrqa_squad-train-37285", "mrqa_squad-train-13540", "mrqa_squad-train-70817", "mrqa_squad-train-81580", "mrqa_squad-validation-5893", "mrqa_triviaqa-validation-2496", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6930", "mrqa_searchqa-validation-4216", "mrqa_newsqa-validation-616", "mrqa_triviaqa-validation-6140", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-1068", "mrqa_triviaqa-validation-1523", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-2800", "mrqa_squad-validation-5733", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-2373"], "EFR": 1.0, "Overall": 0.7446205357142858}, {"timecode": 35, "before_eval_results": {"predictions": ["Seven Days to the River Rhine", "the Seerhein", "2,579", "to collect menstrual flow", "New Jersey Devils of the National Hockey League ( NHL )", "as an extension to this procedure", "on Saturday", "Emily Blunt", "Hook", "Brian Steele", "nearby objects show a larger parallax than farther objects", "four", "Leonard Bernstein", "Prince Henry", "9 February 2018", "the 1970s", "2001", "the 18th century", "her abusive husband", "The Canterbury Tales", "two - third of the total members present", "a federal republic", "July 14, 1969", "Frank Langella", "Tennessee Titan", "the Italian pignatta", "April 26, 2005", "Castleford", "Action Jackson", "New England Patriots", "the world's first collected descriptions of what builds nations'wealth", "Patrick Warburton", "when the cell is undergoing the metaphase of cell division", "`` One Son ''", "Mara Jade", "revenge and karma", "Kyla Coleman", "Nathan Hale", "Rachel Kelly Tucker", "far lesser degree by blood capillaries", "during World War II", "Joe Pizzulo and Leeza Miller", "on an inward spiral where it would eventually cross the event horizon", "David Tennant", "Kelly Reno", "Brooke Wexler", "$1.84 billion", "a patronymic surname", "Leonard Nimoy", "Billy Hill", "2005", "Pangaea or Pangea", "Kent", "petula Clark", "elbow", "Tufts University", "2002", "May 4, 2004", "to work together to stabilize Somalia and cooperate in security and military operations.", "Communist Party of Nepal", "Robert Barnett", "mac Gyver", "Daniel Boone", "chicago"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7011508137330507}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.21052631578947367, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8338", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6190", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-296"], "SR": 0.609375, "CSR": 0.5733506944444444, "retrieved_ids": ["mrqa_squad-train-42053", "mrqa_squad-train-48206", "mrqa_squad-train-8946", "mrqa_squad-train-57696", "mrqa_squad-train-51020", "mrqa_squad-train-8966", "mrqa_squad-train-86487", "mrqa_squad-train-28747", "mrqa_squad-train-75056", "mrqa_squad-train-31682", "mrqa_squad-train-21707", "mrqa_squad-train-17064", "mrqa_squad-train-46538", "mrqa_squad-train-7102", "mrqa_squad-train-76065", "mrqa_squad-train-82470", "mrqa_triviaqa-validation-2183", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-1166", "mrqa_naturalquestions-validation-9410", "mrqa_newsqa-validation-2315", "mrqa_hotpotqa-validation-4802", "mrqa_triviaqa-validation-274", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-2502", "mrqa_squad-validation-6310", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-9285", "mrqa_newsqa-validation-3034", "mrqa_triviaqa-validation-2684"], "EFR": 0.92, "Overall": 0.728826388888889}, {"timecode": 36, "before_eval_results": {"predictions": ["$5,000,000", "The Handmaid's Tale", "Austrian", "Archbishop of Canterbury", "1912", "102,984", "Emmanuel Ofosu Yeboah", "Clarence Nash", "Dutch", "Macomb County", "Dusty Dvoracek", "politician and environmentalist", "1972", "Disney California Adventure", "Indiana", "Travis County", "The 1996 PGA Championship", "orange", "Regionalliga Nord", "15-lap race was won by BRM driver Graham Hill after he started from second position", "Wragby", "life", "Ukrainian", "Cape Cod", "actress and model", "BBC Focus", "George Clooney", "Charles Joseph Scarborough", "Tottenham ( ) or Spurs", "the attack on Pearl Harbor", "Alemannic", "Vienna", "Jesper Myrfors", "Paper", "Amway", "Ogallala Aquifer", "The Heirs", "Robert Thomson", "Vigorish", "12", "Rockbridge County", "sexual activity", "English", "Gatwick", "Carlos Santana", "two Nobel Peace Prizes", "Dutch", "campaign setting", "2013 Cannes Film Festival", "Aiden English", "actor", "Bill Belichick", "Achal Kumar Jyoti", "a set of connected behaviors, rights, obligations, beliefs, and norms as conceptualized by people in a social situation", "piano", "17", "plac\u0113b\u014d", "a federal judge in Mississippi", "when times get tough", "\"It was perfect work, ready to go for the stimulus funds,\"", "wounds", "walruses", "comoros", "Harry S. Truman"], "metric_results": {"EM": 0.5, "QA-F1": 0.6240413232600732}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.3076923076923077, 0.8, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.1904761904761905, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3145", "mrqa_triviaqa-validation-2776", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2449", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-13135"], "SR": 0.5, "CSR": 0.5713682432432432, "retrieved_ids": ["mrqa_squad-train-33635", "mrqa_squad-train-65324", "mrqa_squad-train-82910", "mrqa_squad-train-44553", "mrqa_squad-train-72049", "mrqa_squad-train-12024", "mrqa_squad-train-85181", "mrqa_squad-train-66362", "mrqa_squad-train-15693", "mrqa_squad-train-50230", "mrqa_squad-train-2441", "mrqa_squad-train-72455", "mrqa_squad-train-17890", "mrqa_squad-train-70971", "mrqa_squad-train-59912", "mrqa_squad-train-55605", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-871", "mrqa_naturalquestions-validation-3848", "mrqa_hotpotqa-validation-3634", "mrqa_squad-validation-3958", "mrqa_squad-validation-1248", "mrqa_searchqa-validation-534", "mrqa_naturalquestions-validation-8657", "mrqa_searchqa-validation-15983", "mrqa_naturalquestions-validation-9195", "mrqa_squad-validation-4849", "mrqa_triviaqa-validation-7613", "mrqa_searchqa-validation-10032", "mrqa_hotpotqa-validation-2419", "mrqa_naturalquestions-validation-4103"], "EFR": 1.0, "Overall": 0.7444298986486486}, {"timecode": 37, "before_eval_results": {"predictions": ["Robert Lane and Benjamin Vail", "ave", "20", "cannibalism", "College of William and Mary", "Chinese", "diamond", "Biggie Smalls", "Marsha Hunt", "coho", "tsar", "Sarah Hughes", "Sonnets", "Caesar", "blade protector", "David Berkowitz", "jeopardy/2762_Qs.txt at master", "Taos Pueblo", "a 3500 lb. sculpture", "licorice stick", "Eugene O'Neill", "Donovan", "a blue rectangle", "Dublin", "mathematical", "George II", "Suzuki Grand Vitara", "Yogi Bear", "Emile Lahoud", "Judas Iscariot", "Christopher Reeve", "Stripes", "Little Red Riding Hood", "violet", "Daryl Hall and John Oates", "Cherokee", "The cause of the French people", "Gettysburg", "a wonderful lamp", "Jackie Kennedy", "Grover Cleveland", "Orange County", "Dorian Gray", "Arkansas", "Aphrodite", "a claim brought by an individual", "Faust", "Aaron Burr", "violins", "ethanol", "Adam Smith", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "the church at Philippi", "poor hygiene", "red", "Cole Porter", "a non-speaking character", "Washington, D.C.", "Westminster", "Marilyn Martin", "nuclear", "Egypt", "Jackson sitting in Renaissance-era clothes and holding a book", "boxing"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5073931277056277}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.06666666666666667]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_searchqa-validation-12628", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-11446", "mrqa_searchqa-validation-1092", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-12401", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-11221", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-11435", "mrqa_searchqa-validation-2181", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-8404", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-10312", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-14370", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-2068", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-822", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-10156", "mrqa_triviaqa-validation-7414", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-4021", "mrqa_naturalquestions-validation-9459"], "SR": 0.421875, "CSR": 0.5674342105263157, "retrieved_ids": ["mrqa_squad-train-44743", "mrqa_squad-train-71904", "mrqa_squad-train-3459", "mrqa_squad-train-73280", "mrqa_squad-train-37944", "mrqa_squad-train-72307", "mrqa_squad-train-50675", "mrqa_squad-train-77927", "mrqa_squad-train-51494", "mrqa_squad-train-11057", "mrqa_squad-train-9385", "mrqa_squad-train-2624", "mrqa_squad-train-51067", "mrqa_squad-train-270", "mrqa_squad-train-11024", "mrqa_squad-train-73413", "mrqa_naturalquestions-validation-1193", "mrqa_hotpotqa-validation-3731", "mrqa_naturalquestions-validation-4711", "mrqa_newsqa-validation-1453", "mrqa_squad-validation-10483", "mrqa_newsqa-validation-1950", "mrqa_hotpotqa-validation-4195", "mrqa_squad-validation-2122", "mrqa_squad-validation-2481", "mrqa_squad-validation-6046", "mrqa_newsqa-validation-710", "mrqa_triviaqa-validation-1623", "mrqa_naturalquestions-validation-8136", "mrqa_searchqa-validation-8487", "mrqa_hotpotqa-validation-520", "mrqa_triviaqa-validation-7613"], "EFR": 0.972972972972973, "Overall": 0.7382376866998578}, {"timecode": 38, "before_eval_results": {"predictions": ["1954", "Monument Valley", "cloves", "The Apartment", "Abraham Lincoln", "Allston Depot", "a physician or surgeon", "Greenpeace", "a golden eagle", "the Ice Cream sundae", "a bog", "a canton", "Caracas", "Giza", "Breakfast at Tiffany's", "horseplayers", "Faneuil Hall", "Babe", "shrimp", "balsa", "Menachem Begin", "Sonagachi", "an anteater", "The Saints", "Princess Diana", "Tasmania", "the Taj Mahal", "iron", "Louisa May Alcott", "the improvidence of society itself", "Spider-Man 3", "grease", "glucose", "a fracas", "Sony", "Van Helsing", "Hugh Grant", "the Great Wall", "hand", "Hormel Foods Corporation", "UTC", "Clara Barton", "Kauai", "esophagus", "Joseph", "Otsego County", "Sanford", "Venezuela", "Thomas Paine", "Venezuela", "canticle", "Donna", "outside the playing season, or before November 1", "AIM", "Apollon", "Chicago", "Andrew Lloyd Webber", "1776", "Tim Howard", "Princess Jessica", "Hutus and Tutsis", "her father", "Monday and Tuesday", "management of balance of payments difficulties and international financial crises"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5518830128205128}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615383]}}, "before_error_ids": ["mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-7187", "mrqa_searchqa-validation-8722", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-6833", "mrqa_searchqa-validation-1238", "mrqa_searchqa-validation-9939", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-3861", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-12021", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-1716", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1534", "mrqa_searchqa-validation-14173", "mrqa_searchqa-validation-4196", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-6886", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-4264", "mrqa_hotpotqa-validation-5676", "mrqa_newsqa-validation-3774", "mrqa_naturalquestions-validation-4021"], "SR": 0.484375, "CSR": 0.5653044871794872, "retrieved_ids": ["mrqa_squad-train-51785", "mrqa_squad-train-59706", "mrqa_squad-train-72848", "mrqa_squad-train-20639", "mrqa_squad-train-36456", "mrqa_squad-train-40874", "mrqa_squad-train-13214", "mrqa_squad-train-64644", "mrqa_squad-train-24791", "mrqa_squad-train-68272", "mrqa_squad-train-48255", "mrqa_squad-train-32757", "mrqa_squad-train-13726", "mrqa_squad-train-12492", "mrqa_squad-train-76824", "mrqa_squad-train-81067", "mrqa_searchqa-validation-6817", "mrqa_squad-validation-7554", "mrqa_searchqa-validation-8114", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-7019", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-9568", "mrqa_newsqa-validation-3726", "mrqa_triviaqa-validation-5738", "mrqa_squad-validation-2608", "mrqa_newsqa-validation-2199", "mrqa_searchqa-validation-708", "mrqa_squad-validation-291", "mrqa_hotpotqa-validation-2244", "mrqa_searchqa-validation-4461", "mrqa_naturalquestions-validation-8338"], "EFR": 1.0, "Overall": 0.7432171474358975}, {"timecode": 39, "before_eval_results": {"predictions": ["movements of nature", "retirement", "Hill Street Blues", "bacteria", "Ross Perot", "(Little Caesar) was the only known son of Julius Caesar", "achilles", "fracture", "Lance Armstrong", "Kung Fu", "tennis elbow", "raw sienna", "Hindustan", "The Village Voice", "Nacho Libre", "\"Strawberry Fields Forever\"", "Cygnus", "lowlands", "nougat", "a Scotch egg", "Manhattan Project", "the Eiffel tower", "Roger Federer", "sculpere", "cholesterol", "aheddar", "Iran", "Florida", "The Virgin Spring", "(the three sons)", "the Nome", "(Alfred) Strutt", "Atlanta", "achilles", "Zorro", "assume", "Old Mother Hubbard", "offbeat", "uranium", "Ismene", "life's Refinements", "( Christopher Paolini)", "Petruchio", "(George) Sand", "seattle", "Dick Gephardt", "a Harvard Alum", "Lord Louis Mountbatten", "Master of Fine Arts", "American Idol", "Robert Peary", "down to the ground", "Hellenism", "James W. Marshall", "Kratos", "willow", "The Sixth Sense", "Wojtek", "Montreal, Quebec, Canada", "the Firth of Forth", "\"I think the Camry gets a bad rap for being the'microwave oven'", "58 minutes", "\"What she's doing is putting a personal and human face on the issue... there's nothing more crucial,\"", "$1,500"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5776041666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-2577", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-4568", "mrqa_searchqa-validation-12788", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-7469", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-9039", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-262", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-4826", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-1888", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-6258", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-31", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-3934"], "SR": 0.53125, "CSR": 0.564453125, "retrieved_ids": ["mrqa_squad-train-4295", "mrqa_squad-train-60642", "mrqa_squad-train-67752", "mrqa_squad-train-56925", "mrqa_squad-train-34264", "mrqa_squad-train-70261", "mrqa_squad-train-55207", "mrqa_squad-train-70878", "mrqa_squad-train-24507", "mrqa_squad-train-53366", "mrqa_squad-train-22283", "mrqa_squad-train-49481", "mrqa_squad-train-32257", "mrqa_squad-train-84093", "mrqa_squad-train-52634", "mrqa_squad-train-71785", "mrqa_newsqa-validation-2217", "mrqa_triviaqa-validation-255", "mrqa_hotpotqa-validation-5483", "mrqa_squad-validation-7831", "mrqa_searchqa-validation-3014", "mrqa_naturalquestions-validation-2644", "mrqa_searchqa-validation-11170", "mrqa_triviaqa-validation-4620", "mrqa_searchqa-validation-7807", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2800", "mrqa_naturalquestions-validation-8075", "mrqa_hotpotqa-validation-1010", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-1742", "mrqa_squad-validation-5178"], "EFR": 1.0, "Overall": 0.743046875}, {"timecode": 40, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2920", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3586", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-514", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1316", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-13508", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-2066", "mrqa_searchqa-validation-222", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4829", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9908", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10088", "mrqa_squad-validation-10388", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1248", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2608", "mrqa_squad-validation-2831", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3331", "mrqa_squad-validation-349", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4046", "mrqa_squad-validation-4155", "mrqa_squad-validation-4331", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4634", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5248", "mrqa_squad-validation-5307", "mrqa_squad-validation-5389", "mrqa_squad-validation-5469", "mrqa_squad-validation-5506", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5728", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6024", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6224", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6702", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7211", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-801", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-9140", "mrqa_squad-validation-915", "mrqa_squad-validation-9285", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9525", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-1425", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.869140625, "KG": 0.49609375, "before_eval_results": {"predictions": ["the center of the curving path", "a random number generator", "Hinduism", "a turtle", "Donkey", "Jane Eyre", "Carrie Bradshaw", "All's Well That Ends Well", "France", "Montmartre", "The Dying Swan", "Elvis Presley", "a protractor", "'s", "The Kite Runner", "white granite", "Islamabad", "horseshoe", "Stephen Crane", "trespass", "Jack Dempsey", "beheading", "Val Kilmer", "Pakistan", "Milwaukee", "a kiwi", "Pop-Tarts", "sugar", "Enrico Fermi", "Tiger Woods", "the Madding Crowd", "local broadcasters", "Grace Kelly", "a monkey's tail", "Bilbo", "Oliver Wendell Holmes", "the Constitution of the United States", "proverbs", "a photon", "Maria Montessori", "orchid", "orchid", "the Sun", "Michelangelo", "'s", "ale", "Superman", "'s", "Brazil", "Puget Sound", "phylum", "Yahya Khan", "Prince Bao", "1038", "Bubba Watson, Jr.", "Easter Parade", "Thom Yorke", "beer", "Keeper of the Privy Seal of Scotland", "Martin O'Malley", "1983", "The cause of the child's death will be listed as homicide by undetermined means,", "five", "Kineton, near Banbury, in Oxfordshire"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7308779761904762}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-7659", "mrqa_searchqa-validation-12159", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-7222", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-4248", "mrqa_naturalquestions-validation-3485", "mrqa_triviaqa-validation-5547", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-2627", "mrqa_triviaqa-validation-5693"], "SR": 0.671875, "CSR": 0.5670731707317074, "retrieved_ids": ["mrqa_squad-train-1266", "mrqa_squad-train-66508", "mrqa_squad-train-3388", "mrqa_squad-train-68584", "mrqa_squad-train-39368", "mrqa_squad-train-60172", "mrqa_squad-train-57500", "mrqa_squad-train-35024", "mrqa_squad-train-53700", "mrqa_squad-train-4960", "mrqa_squad-train-59413", "mrqa_squad-train-50632", "mrqa_squad-train-30150", "mrqa_squad-train-22245", "mrqa_squad-train-52069", "mrqa_squad-train-64769", "mrqa_newsqa-validation-2217", "mrqa_hotpotqa-validation-4445", "mrqa_naturalquestions-validation-7892", "mrqa_searchqa-validation-708", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1015", "mrqa_searchqa-validation-4983", "mrqa_naturalquestions-validation-10310", "mrqa_squad-validation-7626", "mrqa_searchqa-validation-5845", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-1327", "mrqa_triviaqa-validation-3257", "mrqa_newsqa-validation-3198", "mrqa_hotpotqa-validation-2880"], "EFR": 1.0, "Overall": 0.7434927591463415}, {"timecode": 41, "before_eval_results": {"predictions": ["metals", "2004", "to capitalize on her publicity", "Karen Gillan", "Niketa Calame", "Nick Kroll", "Albert Einstein", "Miami Heat", "Poems : Series 1", "transceivers", "the east African coast", "200 to 500 mg up to 7 mg", "James Madison", "Tom Brady", "asphyxia", "1947", "Thomas Edison", "in the genome", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "1954", "Erica Rivera", "Afghanistan", "Gettysburg College", "the Geography of Oklahoma", "thia Weil", "into the gastrointestinal tract", "an oxidant, usually atmospheric oxygen", "exceeds 1 mile ( 1.6 km )", "1871", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "2018", "solids", "Presley Smith", "a combination of genetics and the male hormone dihydrotestosterone", "Dylan Massett", "the types of instruments that are used in data collection", "Tami Lynn", "Christianity", "radians", "1931", "the Beatles", "May 2010", "24", "August 8, 1945", "Phil Gallagher", "16 December 1908", "a U.S. federal statute intended to increase consistency in United States federal sentencing", "the meridian", "Stephen Stills", "the times sign or the dimension sign", "after the title page, copyright notices, and, in technical journals, the abstract", "Angevin", "transportation", "Mar del Sur", "Julie Taymor", "an organ", "Tony Aloupis", "American Civil Liberties Union", "Chevron", "attempted burglary", "Pancho Gonzales", "William Henry Harrison", "the orchids", "Britain"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6296135461760461}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.24000000000000002, 0.0, 0.16666666666666669, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6060606060606061, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-2671", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-9237", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-7595", "mrqa_newsqa-validation-3427", "mrqa_newsqa-validation-2835", "mrqa_searchqa-validation-2827"], "SR": 0.546875, "CSR": 0.5665922619047619, "retrieved_ids": ["mrqa_squad-train-22781", "mrqa_squad-train-52933", "mrqa_squad-train-58799", "mrqa_squad-train-45270", "mrqa_squad-train-80233", "mrqa_squad-train-81390", "mrqa_squad-train-21368", "mrqa_squad-train-16073", "mrqa_squad-train-50227", "mrqa_squad-train-40190", "mrqa_squad-train-31077", "mrqa_squad-train-4835", "mrqa_squad-train-45023", "mrqa_squad-train-61906", "mrqa_squad-train-66125", "mrqa_squad-train-84494", "mrqa_naturalquestions-validation-2100", "mrqa_newsqa-validation-1546", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-3269", "mrqa_searchqa-validation-262", "mrqa_squad-validation-10341", "mrqa_squad-validation-6548", "mrqa_naturalquestions-validation-4021", "mrqa_triviaqa-validation-2816", "mrqa_hotpotqa-validation-4133", "mrqa_triviaqa-validation-4515", "mrqa_naturalquestions-validation-7685", "mrqa_triviaqa-validation-200", "mrqa_hotpotqa-validation-5164", "mrqa_triviaqa-validation-7073", "mrqa_hotpotqa-validation-3145"], "EFR": 0.9310344827586207, "Overall": 0.7296034739326764}, {"timecode": 42, "before_eval_results": {"predictions": ["high voltage", "water buffalo", "Texas", "Song of Solomon", "Zohan", "Sweden", "Battlestar Galactica", "Sainte-Marie", "sheep", "Mary I of England", "Scotland", "a blackbird", "Patty Duke", "Superman", "Judas Iscariot", "3,000th", "grow old", "savanna mosaic", "Luciano", "longton", "Kellogg's", "The Fall Guy", "cape", "Elizabeth Barrett Browning", "Judy Garland", "crocodilians", "Greece", "Bosnia and Herzegovina", "Baby Moses", "Morris West", "outta nowhere", "El burlador de Sevilla", "Hawaii", "the Empire State Building", "the League of Nations", "Sally Ride", "bullet proof", "Queen Elizabeth I", "75% percentile grade", "patchouli", "the Civil War", "a moon", "Greek Meatballs", "Holden Caulfield", "the Caucasus mountains", "The Firebird", "14", "Lecompton", "Midnight Cowboy", "the Rosetta Stone", "Louisiana", "Malayalam", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "breaks down and decides to share his alcohol with the Beast Folk", "Arabian Gulf", "Ross Kemp", "Marine One", "Juan Manuel Mata Garc\u00eda", "Vietnam War", "Taylor Swift", "Herman,", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "David Beckham", "the killing of a 15-year-old boy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5925080128205128}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-14410", "mrqa_searchqa-validation-9124", "mrqa_searchqa-validation-15998", "mrqa_searchqa-validation-8140", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-8429", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-6562", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-11902", "mrqa_searchqa-validation-1970", "mrqa_searchqa-validation-9663", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-448", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-1406", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-1411", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2870"], "SR": 0.484375, "CSR": 0.5646802325581395, "retrieved_ids": ["mrqa_squad-train-35983", "mrqa_squad-train-49389", "mrqa_squad-train-14019", "mrqa_squad-train-6346", "mrqa_squad-train-18932", "mrqa_squad-train-28420", "mrqa_squad-train-58451", "mrqa_squad-train-38513", "mrqa_squad-train-40184", "mrqa_squad-train-72204", "mrqa_squad-train-30694", "mrqa_squad-train-8184", "mrqa_squad-train-57599", "mrqa_squad-train-75970", "mrqa_squad-train-71385", "mrqa_squad-train-67244", "mrqa_newsqa-validation-652", "mrqa_triviaqa-validation-5547", "mrqa_naturalquestions-validation-4609", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-4664", "mrqa_hotpotqa-validation-2837", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2715", "mrqa_squad-validation-1459", "mrqa_naturalquestions-validation-5104", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-459", "mrqa_hotpotqa-validation-840", "mrqa_triviaqa-validation-4117", "mrqa_squad-validation-170"], "EFR": 1.0, "Overall": 0.7430141715116279}, {"timecode": 43, "before_eval_results": {"predictions": ["Southeastern U.S.", "lemon juice", "Leicester", "the mile run", "the Jets", "scurvy", "Japan", "\"Razor\"", "falconry", "Cameroon", "Norman Brookes", "bicarbonate", "Jaipur", "venus williams", "36", "venus hossain", "Spain", "Henry Hudson", "bridge", "a raven", "Much Ado About Nothing", "Felix Leiter", "Louis XV", "Mrs Mainwaring", "Australia", "Robert A. Heinlein", "the USS Constitution", "Aug. 24, 1572", "Stage 1", "Decoupage", "Massachusetts", "Sherlock Holmes", "Delaware", "Olivia Smith", "Costa Concordia", "spiral (S), elliptical (E), and lenticular (S0)", "orange juice", "Jim Morrison", "mona Lisa", "clay", "Bash Street Kids", "Mercury", "Ireland", "Gandalf", "Moses Sithole", "Bogart", "Minder", "a neutron star", "a turkey", "Eva Marie", "eeyore", "Saint Alphonsa", "Ku - Klip", "March 15, 1945", "Portsea, Victoria", "1861", "McLaren", "romantic e-mails", "Diego Milito", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Zyrtec", "Cockney lids", "diameter", "Leo Frank,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.609375}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-954", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-10", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-3871", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7617", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-5446", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-2751", "mrqa_searchqa-validation-14791"], "SR": 0.5625, "CSR": 0.5646306818181819, "retrieved_ids": ["mrqa_squad-train-7141", "mrqa_squad-train-28187", "mrqa_squad-train-4059", "mrqa_squad-train-33990", "mrqa_squad-train-11932", "mrqa_squad-train-64137", "mrqa_squad-train-107", "mrqa_squad-train-69459", "mrqa_squad-train-17118", "mrqa_squad-train-24745", "mrqa_squad-train-46883", "mrqa_squad-train-19806", "mrqa_squad-train-33159", "mrqa_squad-train-59925", "mrqa_squad-train-76042", "mrqa_squad-train-57682", "mrqa_hotpotqa-validation-208", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-14845", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5104", "mrqa_triviaqa-validation-559", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-7878", "mrqa_hotpotqa-validation-3304", "mrqa_searchqa-validation-11170", "mrqa_naturalquestions-validation-10049", "mrqa_squad-validation-2486", "mrqa_hotpotqa-validation-3156", "mrqa_naturalquestions-validation-6524", "mrqa_triviaqa-validation-3716", "mrqa_naturalquestions-validation-4664"], "EFR": 1.0, "Overall": 0.7430042613636363}, {"timecode": 44, "before_eval_results": {"predictions": ["Hoek van Holland", "Tim McGraw", "from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "Marcus Atilius Regulus", "the state in which both reactants and products are present in concentrations which have no further tendency to change with time", "Lucknow", "Kristy Swanson", "Hendersonville, North Carolina", "Pure water is neutral, at pH 7 ( 25 \u00b0 C )", "Abraham", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "1773", "the final episode of the series", "appears to refer to a god of the Ammonites, as well as Tyrian Melqart", "the Ming", "the Super Bowl", "the 1980s", "31 October 1972", "The Italian Agostino Bassi", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "2014", "Real Madrid", "LED illuminated", "BC Jean", "about 24 hours", "can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "2015", "UVA", "Tommy Shaw", "November 2016", "Thomas Alva Edison", "B.R. Ambedkar", "the Indian Hockey Federation ( IHF )", "Niles", "Stephen Stills'former girlfriend", "the Dust Bowl", "Grand Inquisition", "British pop band T'Pau", "Angel Island Immigration Station", "Johannes Gutenberg", "Terry Reid", "the name of a work gang", "Johannes Gutenberg", "Domhnall Gleeson", "Russia", "the Pandavas", "2020 National Football League ( NFL ) season", "the 1994 season", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "the church at Philippi", "Chuck Noland", "The Green Mile", "Cal Ripken, Jr.", "1905", "Workers' Party", "Revolver", "140 million", "cell phones", "the International Space Station", "estimated 750", "springsteen", "nantucket", "Microsoft", "Love Never Dies"], "metric_results": {"EM": 0.5, "QA-F1": 0.6271418685481185}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 0.19999999999999998, 0.5714285714285715, 1.0, 0.10714285714285714, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4592", "mrqa_triviaqa-validation-7676", "mrqa_newsqa-validation-1345", "mrqa_newsqa-validation-755", "mrqa_searchqa-validation-7952", "mrqa_searchqa-validation-3760"], "SR": 0.5, "CSR": 0.5631944444444444, "retrieved_ids": ["mrqa_squad-train-82395", "mrqa_squad-train-28621", "mrqa_squad-train-42385", "mrqa_squad-train-41321", "mrqa_squad-train-77275", "mrqa_squad-train-55471", "mrqa_squad-train-38183", "mrqa_squad-train-14257", "mrqa_squad-train-42299", "mrqa_squad-train-46258", "mrqa_squad-train-15129", "mrqa_squad-train-24001", "mrqa_squad-train-35296", "mrqa_squad-train-43", "mrqa_squad-train-75343", "mrqa_squad-train-41902", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-952", "mrqa_searchqa-validation-9432", "mrqa_searchqa-validation-11749", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-9773", "mrqa_searchqa-validation-4792", "mrqa_newsqa-validation-2352", "mrqa_naturalquestions-validation-6968", "mrqa_searchqa-validation-6855", "mrqa_newsqa-validation-1283", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-7473", "mrqa_naturalquestions-validation-3241", "mrqa_triviaqa-validation-4905"], "EFR": 0.875, "Overall": 0.7177170138888889}, {"timecode": 45, "before_eval_results": {"predictions": ["westward", "Gulf of Aden.", "Felipe Massa.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Tibetans", "\"handful\" of domestic disturbance calls to police since 2000", "Mugabe and Tsvangirai", "prisoners", "Sunday", "Vice President Joe Biden", "about 1,300 meters in the Mediterranean Sea", "only one", "raping and murdering", "Ryder Russell", "4 months", "his father", "the club's board", "insect stings", "it is provocative action", "peter henderson", "Fullerton, California", "Chinese and international laws", "40 lash after he was convicted of drinking alcohol in Sudan where he plays for first division side Al-Merreikh of Omdurman.", "Rev. Alberto Cutie", "its main priority", "Chris Robinson", "269,000", "surrounding areas of the bustling capital", "Dharamsala, India.", "the state's attorney", "delivered three machine guns and two silencers to the hip-hop star", "\"deep sorrow\"", "two", "Melbourne", "\"It was a wrong thing to say, something that we both acknowledge,\"", "theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg, a major art museum in Zurich, said.", "Oxbow", "motor scooter", "Joan Rivers", "6,000", "g gossip Girl", "the U.S. Food and Drug Administration", "Diversity", "650", "Yemeni port city of Aden", "South Africa", "Daniel Radcliffe", "President Sheikh Sharif Sheikh Ahmed", "cities throughout Canada.", "Florida's Everglades", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "king's Landing", "June 8, 2009", "'Q'", "wolf", "polio", "mathematics", "mountain-climbing", "Leon Schlesinger Productions (later Warner Bros.  Cartoons)", "giving mouth", "Colorado", "CIA", "Billy Bob Thornton"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5218693430929684}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.5454545454545454, 0.4, 0.0, 0.09523809523809525, 1.0, 0.2857142857142857, 0.0, 0.3333333333333333, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 1.0, 0.08695652173913042, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.16666666666666669, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-5370", "mrqa_triviaqa-validation-3086", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-2694", "mrqa_searchqa-validation-6261"], "SR": 0.4375, "CSR": 0.5604619565217391, "retrieved_ids": ["mrqa_squad-train-68995", "mrqa_squad-train-70871", "mrqa_squad-train-65569", "mrqa_squad-train-22350", "mrqa_squad-train-86541", "mrqa_squad-train-11445", "mrqa_squad-train-10970", "mrqa_squad-train-30265", "mrqa_squad-train-53684", "mrqa_squad-train-71929", "mrqa_squad-train-49236", "mrqa_squad-train-48762", "mrqa_squad-train-219", "mrqa_squad-train-8518", "mrqa_squad-train-44315", "mrqa_squad-train-64953", "mrqa_searchqa-validation-15991", "mrqa_hotpotqa-validation-4133", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3924", "mrqa_newsqa-validation-1465", "mrqa_triviaqa-validation-7623", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-5857", "mrqa_naturalquestions-validation-6886", "mrqa_newsqa-validation-3315", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-2328", "mrqa_searchqa-validation-296", "mrqa_triviaqa-validation-1342", "mrqa_squad-validation-8581", "mrqa_squad-validation-8392"], "EFR": 1.0, "Overall": 0.7421705163043478}, {"timecode": 46, "before_eval_results": {"predictions": ["every four years", "a stray wandering the streets of Moscow", "to ordain presbyters / bishops", "duodenum by enterocytes of the duodenal lining", "warmth", "a limited period of time", "during meiosis", "the federal government", "dental alveoli", "Katharine Hepburn", "Norway", "multiple origins", "England, Northern Ireland, Scotland and Wales", "Gladys Knight & the Pips", "to solve its problem of lack of food self - sufficiency", "January 2004", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "276 episodes", "The Internet protocol suite ( UDP / IP ) was developed by Robert E. Kahn and Vint Cerf in the 1970s and became the standard networking protocol on the ARPANET", "when each of the variables is a perfect monotone function of the other", "the Kansas City Chiefs", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "in Western cultures", "Husrev Pasha", "2001", "in Squamish, British Columbia, Canada", "Mankombu Sambasivan Swaminathan", "can change an annuity ticket to cash should they be eligible for a jackpot share", "1962", "prophets and beloved religious leaders", "Best Picture, Best Director for Fincher, Best Actor for Pitt and Best Supporting Actress for Taraji P. Henson", "the Federal Communications Commission ( FCC )", "Patris et Filii et Spiritus Sancti", "William Chatterton Dix", "1987", "Stefanie Scott", "Karen Gillan", "in the retina of mammalian eyes ( e.g. the human eye )", "Ephesus", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah", "George H.W. Bush", "seven", "Uralic languages", "when an individual noticing that the person in the photograph is attractive, well groomed, and properly attired, assumes, using a mental heuristic, that the people in the photographs is a good person based upon the rules of that", "manga is said to originate from scrolls dating back to the 12th century, and it is believed they represent the basis for the right - to - left reading style", "British and French Canadian fur traders", "Lori McKenna", "2017", "October 27, 1904", "in the mid - to late 1920s", "Major Robert Smith of the British Indian Army", "Runic", "home board and outer board", "Malcolm Bradbury", "Wade Boggs", "Sam Cassell  Samuel James Cassell Sr. (born November 18, 1969) is a former professional basketball player and current assistant coach of the Los Angeles Dodgers", "Ludvig Holberg, Baron of Holberg (3 December 1684 \u2013 28 January 1754) was a writer, essayist, philosopher, historian and playwright born in Bergen, Norway", "Pakistan", "Malcolm X,", "February 12", "the Hearth", "the Capulets & the Montagues", "plutonium-238", "24"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6004267084282204}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.923076923076923, 0.6666666666666666, 0.30769230769230765, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.9743589743589743, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.41666666666666663, 0.0, 0.7499999999999999, 0.09999999999999999, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.8749999999999999, 0.3870967741935484, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.23076923076923078, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10509", "mrqa_triviaqa-validation-1055", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-1692", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-5895"], "SR": 0.4375, "CSR": 0.5578457446808511, "retrieved_ids": ["mrqa_squad-train-61465", "mrqa_squad-train-9302", "mrqa_squad-train-6300", "mrqa_squad-train-72727", "mrqa_squad-train-82162", "mrqa_squad-train-74343", "mrqa_squad-train-61786", "mrqa_squad-train-56910", "mrqa_squad-train-65779", "mrqa_squad-train-63882", "mrqa_squad-train-55728", "mrqa_squad-train-40989", "mrqa_squad-train-14996", "mrqa_squad-train-27354", "mrqa_squad-train-12939", "mrqa_squad-train-13912", "mrqa_newsqa-validation-814", "mrqa_naturalquestions-validation-2387", "mrqa_triviaqa-validation-1033", "mrqa_searchqa-validation-3570", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-1130", "mrqa_newsqa-validation-3489", "mrqa_searchqa-validation-1643", "mrqa_hotpotqa-validation-3139", "mrqa_squad-validation-8400", "mrqa_hotpotqa-validation-5864", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-8555", "mrqa_hotpotqa-validation-4802", "mrqa_newsqa-validation-1212", "mrqa_naturalquestions-validation-8290"], "EFR": 0.9166666666666666, "Overall": 0.7249806072695035}, {"timecode": 47, "before_eval_results": {"predictions": ["Max Martin", "Mitsubishi Eclipse", "2005", "Lewis Carroll", "the government - owned corporation of Puerto Rico responsible for electricity generation, power distribution, and power transmission on the island", "Austria - Hungary", "Lizzy Greene", "libretto", "Theodore Roosevelt", "Harrys", "1800", "in the dress shop", "As of January 17, 2018, 201 episodes", "Millerlite", "Instagram's own account", "Experimental neuropsychology", "30 years after Return of the Wars", "Selena Gomez", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W", "Dido", "during initial entry training", "Eddie Murphy", "1997", "Bonnie Aarons", "1960", "1979", "Part XI of the Indian constitution", "the Constitution of India came into effect on 26 January 1950", "The oldest known recording of the song, under the title `` Rising Sun Blues ''", "halogenated paraffin hydrocarbons", "the 1980s", "MacFarlane", "18 by Frankie Laine's `` I Believe '' in 1953", "1898", "enabled business applications to be developed with Flash", "Majandra Delfino", "the members of the actual club with the parading permit as well as the brass band", "The Confederate States Army ( C.S.A. )", "four", "Olivia", "Zachary John Quinto", "Sanchez Navarro", "`` Have I Told You Lately ''", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "John Travolta", "Scheria", "1 October 2006", "Auburn Tigers", "Michael Kopelow", "Michael Clarke Duncan", "Origination Clause of the United States Constitution", "Gestapo", "muggings", "heartburn", "Baltimore and Ohio Railroad", "Vanessa Hudgens", "two", "Johannesburg", "onstage demos.", "hardship for terminally ill patients and their caregivers", "The Odyssey", "The Fly", "Prego", "Marillion"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6689373364797371}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.8, 0.0, 0.08695652173913042, 0.2727272727272727, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.8, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.8333333333333334, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.5, 0.761904761904762, 0.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.48275862068965514, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-10381", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-89", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-2606", "mrqa_newsqa-validation-886", "mrqa_searchqa-validation-3520"], "SR": 0.546875, "CSR": 0.5576171875, "retrieved_ids": ["mrqa_squad-train-85808", "mrqa_squad-train-79367", "mrqa_squad-train-29362", "mrqa_squad-train-76143", "mrqa_squad-train-69598", "mrqa_squad-train-8698", "mrqa_squad-train-68530", "mrqa_squad-train-76583", "mrqa_squad-train-23640", "mrqa_squad-train-13876", "mrqa_squad-train-61206", "mrqa_squad-train-66623", "mrqa_squad-train-84672", "mrqa_squad-train-61542", "mrqa_squad-train-81945", "mrqa_squad-train-45801", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-2585", "mrqa_newsqa-validation-3056", "mrqa_searchqa-validation-14366", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-7857", "mrqa_searchqa-validation-13490", "mrqa_squad-validation-3118", "mrqa_squad-validation-2291", "mrqa_squad-validation-7774", "mrqa_newsqa-validation-2399", "mrqa_triviaqa-validation-7723", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-4280", "mrqa_hotpotqa-validation-1219", "mrqa_naturalquestions-validation-10093"], "EFR": 0.896551724137931, "Overall": 0.7209119073275863}, {"timecode": 48, "before_eval_results": {"predictions": ["Daylight Saving Time", "Cygnus", "Heraclius", "Henri de Toulouse-Lautrec", "Nigeria", "Hawaii", "Jeremiah", "Paddock", "Don Knotts", "cinnamon Life", "Kbenhavn", "Porgy and Bess", "Dutchman", "Phobos", "Cheers", "Robert Frost", "Geena Davis", "Underground Chapel", "the sky", "Laila Ali", "Fiji Islands", "THE LARGEST in ISA", "San DiegoTijuana", "The Hustler", "nestle", "Led Zeppelin", "the Book of Kells", "great great grandfather", "nuclear power", "a Heisman", "Gulf of Tonkin", "Stephen", "coelacanth", "Prague", "the Federal Reserve", "coal-fired power plant", "Afghanistan", "Cheetah", "Ambrose Bierce", "the American Lung Association", "lilo croquet", "Aphrodite", "You get your house back, your wife comes back to life", "Chico Rodriguez", "Budapest", "John Mahoney", "reticulated python", "Nit-A-Nee", "Charles de Gaulle", "Beverly Cleary", "Afghanistan", "2004", "a recognized group of people who jointly oversee the activities of an organization", "since 3, 1, and 4 are the first three significant digits of \u03c0", "82", "the YMCA", "lutwidge", "direct scattering and inverse scattering", "43rd", "South America", "energy-efficient", "$150 billion", "fill a million sandbags", "Angel Cabrera"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5963541666666666}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8333333333333333, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-4404", "mrqa_searchqa-validation-12001", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-5114", "mrqa_searchqa-validation-7598", "mrqa_searchqa-validation-8359", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-8727", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-6550", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-12165", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-3066", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-4738", "mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-1316", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-3842"], "SR": 0.484375, "CSR": 0.5561224489795918, "retrieved_ids": ["mrqa_squad-train-11524", "mrqa_squad-train-70652", "mrqa_squad-train-392", "mrqa_squad-train-75280", "mrqa_squad-train-60963", "mrqa_squad-train-10335", "mrqa_squad-train-73587", "mrqa_squad-train-57408", "mrqa_squad-train-31021", "mrqa_squad-train-68160", "mrqa_squad-train-47955", "mrqa_squad-train-25687", "mrqa_squad-train-70955", "mrqa_squad-train-53996", "mrqa_squad-train-35315", "mrqa_squad-train-83593", "mrqa_triviaqa-validation-6462", "mrqa_naturalquestions-validation-2206", "mrqa_squad-validation-7288", "mrqa_searchqa-validation-12577", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-836", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-3485", "mrqa_searchqa-validation-4248", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2365", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-2783", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-1680"], "EFR": 1.0, "Overall": 0.7413026147959184}, {"timecode": 49, "before_eval_results": {"predictions": ["William Wyler", "various locations in Redford's adopted home state of Utah", "pelvic floor", "Kanawha Rivers", "amino acids glycine and arginine", "Brian Steele", "1937", "1920s", "during the united monarchy of Israel and Judah", "2005", "New York City", "Beorn", "Jonathan Cheban", "1992", "Montreal", "18", "Kristy Swanson", "143,000,000 mi", "Orlando", "the Bactrian", "sorrow regarding the environment", "the New York Yankees", "President since Woodrow Wilson", "Phosphorus pentoxide", "Brooklyn", "Secretary of Commerce Herbert Hoover", "the Mishnah", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "Tom Brady", "Around 1200", "Walter Egan", "season seven finale", "it has spiritual meaning `` The person who has existence in two paradise ''", "a list of island countries", "origin and synthesis of new strands", "in the late 1930s in southern California, where people raced modified cars on dry lake beds northeast of Los Angeles under the rules of the Southern California Timing Association ( SCTA ), among other groups", "13 to 22 June 2012", "Jackie Robinson", "Coton in the Elms", "the American Civil War", "Pradyumna", "1979", "James Madison", "UMBC", "Marin \u010cili\u0107", "17 - year -", "Cathy Dennis and Rob Davis", "5,534", "semilunar pulmonary valve", "electron donors", "A rear - view mirror", "\"Rarely is the question asked,", "the USS Constitution", "Lucas McCain", "Martin Scorsese", "2014", "June 6, 1959", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "Listeria monocytogenes bacteria", "United States, NATO member states, Russia and India", "Sagamore Hill", "hyperthyroidism", "the Bunsen burner", "UVB rays"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6740518484537188}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.19999999999999998, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.25, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5714285714285715, 1.0, 0.6976744186046512, 1.0, 0.0, 1.0, 0.0, 0.631578947368421, 0.0, 0.1818181818181818, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7213", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-9403", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-616", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2074", "mrqa_searchqa-validation-14922", "mrqa_searchqa-validation-10011", "mrqa_triviaqa-validation-7608"], "SR": 0.5625, "CSR": 0.55625, "retrieved_ids": ["mrqa_squad-train-7508", "mrqa_squad-train-4080", "mrqa_squad-train-19597", "mrqa_squad-train-81849", "mrqa_squad-train-59920", "mrqa_squad-train-72140", "mrqa_squad-train-15608", "mrqa_squad-train-82806", "mrqa_squad-train-19478", "mrqa_squad-train-81496", "mrqa_squad-train-21978", "mrqa_squad-train-64764", "mrqa_squad-train-47070", "mrqa_squad-train-78507", "mrqa_squad-train-15424", "mrqa_squad-train-58156", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-12469", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-6392", "mrqa_hotpotqa-validation-2092", "mrqa_searchqa-validation-14187", "mrqa_newsqa-validation-796", "mrqa_naturalquestions-validation-2044", "mrqa_hotpotqa-validation-1316", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-8794", "mrqa_triviaqa-validation-1781", "mrqa_hotpotqa-validation-550", "mrqa_searchqa-validation-8359", "mrqa_newsqa-validation-1003"], "EFR": 0.9642857142857143, "Overall": 0.7341852678571429}, {"timecode": 50, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-561", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9724", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3331", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.8515625, "KG": 0.48359375, "before_eval_results": {"predictions": ["a cappella", "caterpillar", "Zeus", "9", "Lady Gaga", "John Alec Entwistle", "kray Brothers", "a child", "seine", "December 18, 1958", "boudin", "Gloucestershire", "Queen Victoria and Prince Albert", "car ferry", "mrs henderson", "emirate", "Madagascar", "persian gulf", "Miss Havisham", "oxygen", "Macbeth", "Gentlemen Prefer Blondes", "Khrushchev", "Netherlands", "hose", "Lake Erie", "John Key", "Subway", "geodesy(Noun)", "Dylan Thomas", "Fred Astaire", "the Tower of London", "Bridge", "quarter", "cirrus uncinus", "mrs henderson", "Harry patch", "raclette cheese", "Klaus dolls", "Argentina", "Colin Montgomerie", "james Valentine", "the Count Basie Orchestra", "the Behemoth", "arthur henderson", "maverick", "elbow", "Hitler", "isohyet", "neapolitan", "Denmark", "July 14, 1969", "1923", "Massillon, Ohio", "1937", "LA Galaxy", "2017", "British charities for aid to Gaza", "Frank Ricci,", "Dean Martin, Katharine Hepburn and Spencer Tracy", "sweatshirt", "Susan B. Anthony dollar", "ivory", "in the lawless southern provinces and especially in the Taliban stronghold of Helmand,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5420386904761905}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6534", "mrqa_triviaqa-validation-6964", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-4700", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5001", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1447", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2793", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-4112", "mrqa_searchqa-validation-2495"], "SR": 0.484375, "CSR": 0.5548406862745099, "retrieved_ids": ["mrqa_squad-train-13344", "mrqa_squad-train-8108", "mrqa_squad-train-30098", "mrqa_squad-train-25380", "mrqa_squad-train-21249", "mrqa_squad-train-74815", "mrqa_squad-train-64089", "mrqa_squad-train-60922", "mrqa_squad-train-4709", "mrqa_squad-train-1521", "mrqa_squad-train-1389", "mrqa_squad-train-59231", "mrqa_squad-train-11265", "mrqa_squad-train-73776", "mrqa_squad-train-50451", "mrqa_squad-train-39312", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-2922", "mrqa_searchqa-validation-13135", "mrqa_naturalquestions-validation-9005", "mrqa_searchqa-validation-12788", "mrqa_squad-validation-6678", "mrqa_searchqa-validation-10920", "mrqa_triviaqa-validation-111", "mrqa_naturalquestions-validation-3242", "mrqa_newsqa-validation-715", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-435", "mrqa_triviaqa-validation-4717", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-12367"], "EFR": 0.9696969696969697, "Overall": 0.7250637811942958}, {"timecode": 51, "before_eval_results": {"predictions": ["Samson", "Basil Feldman", "Passenger Pigeon", "catalyst", "Sir Edwin Landseer", "Hitler", "Albert Einstein", "bristow", "scales", "Judy Garland", "David Walliams", "tepuis", "one of the Vikings nine realms", "lyle", "horse", "hypopituitarism", "Delaware", "bees", "Treaty of Amiens", "limestone", "six month", "stanley", "france", "mantle", "rugs", "algae", "shine", "Algeria", "Churchill Downs", "The United States", "Leonard Bernstein", "Vladimir Putin", "gansbaai", "Nahuatl", "20", "Ken Platt", "Jane Krakowski", "Red Rock", "Norway", "Kim Smith", "Muppet Christmas Carol", "wiziwig", "jump", "creme anglaise", "an electric chair", "jr", "10", "Vancouver Island", "Dublin", "Babylonian Empire", "the Continental Marines", "Joe Spano", "Gary Grimes", "skeletal muscle", "CBS News", "Herman's Hermits", "United States House of Representatives", "\"revolution of values\"", "Haeftling range.", "2-0 lead.", "Brunei", "Pringles can", "a peanut butter cup", "1983"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6171875}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7762", "mrqa_naturalquestions-validation-692", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-4091", "mrqa_searchqa-validation-3853"], "SR": 0.546875, "CSR": 0.5546875, "retrieved_ids": ["mrqa_squad-train-26673", "mrqa_squad-train-67418", "mrqa_squad-train-44591", "mrqa_squad-train-56279", "mrqa_squad-train-22784", "mrqa_squad-train-53074", "mrqa_squad-train-47436", "mrqa_squad-train-10111", "mrqa_squad-train-22551", "mrqa_squad-train-43143", "mrqa_squad-train-52933", "mrqa_squad-train-41530", "mrqa_squad-train-41202", "mrqa_squad-train-73597", "mrqa_squad-train-53494", "mrqa_squad-train-23365", "mrqa_newsqa-validation-1242", "mrqa_hotpotqa-validation-3634", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-1180", "mrqa_searchqa-validation-3861", "mrqa_squad-validation-6294", "mrqa_naturalquestions-validation-4454", "mrqa_squad-validation-6640", "mrqa_hotpotqa-validation-4146", "mrqa_newsqa-validation-3842", "mrqa_searchqa-validation-12401", "mrqa_triviaqa-validation-1661", "mrqa_hotpotqa-validation-96", "mrqa_naturalquestions-validation-9101", "mrqa_triviaqa-validation-3325"], "EFR": 1.0, "Overall": 0.73109375}, {"timecode": 52, "before_eval_results": {"predictions": ["horse", "spanish", "spanish", "Surfer Darlings", "Tony Manero", "Snarks", "Prussian", "silversmith", "Joshua Tree National Park", "Carpathia", "Superman", "Letchworth", "velvet", "5", "nightmare", "Crackerjack", "white Ferns", "Utah", "carburetors", "as You Like It", "Labrador Retriever", "dreamt", "Delaware", "crimes", "Clara Josephine Wieck", "Squeeze", "Apocalypse Now", "Boojum", "st Moritz", "dave hockney", "Scafell Pike", "Edgar Allan Poe", "Chris Moyles", "Tony Blackburn", "Donna Summer", "united Kingdom", "kiki", "wolf", "the Titanic", "jennifer purdy", "trumpet", "Andrew Lloyd Webber", "Malawi", "australia", "eric barker", "mrs henderson Baryshnikov", "carol", "Ruth Rendell", "The Smiths", "ontario", "Cleveland", "Miami Heat", "Gwen West", "1961", "1902", "FCI Danbury", "9 February 1971", "\"It feels great to be back at work,\"", "Caster Semenya", "a \" happy ending\" to the case.", "universal and equal suffrage", "Tom Cruise", "Joe Montana", "Colonel"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6386135057471265}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.13793103448275862, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-4558", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2767", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-1377", "mrqa_naturalquestions-validation-3074", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-75", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-11787"], "SR": 0.59375, "CSR": 0.5554245283018868, "retrieved_ids": ["mrqa_squad-train-14161", "mrqa_squad-train-4814", "mrqa_squad-train-54044", "mrqa_squad-train-43468", "mrqa_squad-train-36805", "mrqa_squad-train-29671", "mrqa_squad-train-65821", "mrqa_squad-train-83968", "mrqa_squad-train-1807", "mrqa_squad-train-33179", "mrqa_squad-train-75272", "mrqa_squad-train-20543", "mrqa_squad-train-34698", "mrqa_squad-train-56905", "mrqa_squad-train-43390", "mrqa_squad-train-13507", "mrqa_triviaqa-validation-1978", "mrqa_searchqa-validation-1837", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-2176", "mrqa_searchqa-validation-16574", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-4495", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-7598", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-5102", "mrqa_naturalquestions-validation-6383", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-5660"], "EFR": 1.0, "Overall": 0.7312411556603774}, {"timecode": 53, "before_eval_results": {"predictions": ["albinism", "Jack Ruby", "france", "hugh Dowding", "squash", "noel", "Jim Smith", "morphine", "David Bowie", "hemp", "canoeing", "mocambique", "Christian Louboutin", "Ironside", "caber", "James Dean", "Mars", "once", "About Eve", "chicken Marengo", "George Orwell", "hemingal Cocoa", "homeless", "Derbyshire", "Cubism", "mrs hebrides", "france", "lothbrok", "Charlie Brown", "Ruth Rendell", "nottingham", "Morticia", "four Weddings", "sweden", "1921", "Iberian", "The Clash", "Praseodymium", "Bruce Alexander", "clamp", "the brain", "azor", "Arthur Hailey", "hedgehog", "Madonna", "Little arrows", "nerve cell cluster", "christopher heminger", "buttermere", "27", "orchids", "104 colonists and Discovery", "aiding the war effort", "anterograde amnesia and dissociation", "Cesar Millan", "2004 Paris Motor Show", "Brazilian Jiu-Jitsu", "Hundreds of contraband cell phones were found behind bars or in transit to Texas inmates in 2008.", "heming tumhare", "drug cartels", "Guinea-Bissau", "steel", "4", "2012"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5564144736842105}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-3401", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-3136", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-31", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-3119", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-294", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4525", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4442", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-3515", "mrqa_searchqa-validation-14449"], "SR": 0.546875, "CSR": 0.5552662037037037, "retrieved_ids": ["mrqa_squad-train-45163", "mrqa_squad-train-79906", "mrqa_squad-train-24896", "mrqa_squad-train-67774", "mrqa_squad-train-84430", "mrqa_squad-train-30829", "mrqa_squad-train-41915", "mrqa_squad-train-6628", "mrqa_squad-train-74856", "mrqa_squad-train-5667", "mrqa_squad-train-4190", "mrqa_squad-train-34355", "mrqa_squad-train-60918", "mrqa_squad-train-75945", "mrqa_squad-train-31358", "mrqa_squad-train-68948", "mrqa_triviaqa-validation-5902", "mrqa_hotpotqa-validation-2319", "mrqa_triviaqa-validation-2527", "mrqa_squad-validation-3585", "mrqa_newsqa-validation-3072", "mrqa_naturalquestions-validation-3918", "mrqa_triviaqa-validation-1930", "mrqa_naturalquestions-validation-808", "mrqa_hotpotqa-validation-2112", "mrqa_triviaqa-validation-952", "mrqa_hotpotqa-validation-1867", "mrqa_naturalquestions-validation-2495", "mrqa_newsqa-validation-2748", "mrqa_searchqa-validation-9173", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-10483"], "EFR": 0.9310344827586207, "Overall": 0.7174163872924648}, {"timecode": 54, "before_eval_results": {"predictions": ["Big Mamie", "dancer", "Canadian", "Sunyani West District", "six", "Big 12 Conference", "Adelaide", "Alfred Preis", "Forest of Bowland", "Pakistan Aeronautical Complex (PAC)", "Tom Rob Smith", "Switzerland", "The Keeping Hours", "Jeffrey Adam \"Duff\" Goldman", "Ang Lee", "Mineola", "67,038", "Kentucky Wildcats", "Eileen Atkins", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "The Kingkiller Chronicle series", "the Haitian Revolution", "alcoholic drinks", "Patti Smith", "John Meston", "York County", "Cleveland Cavaliers", "The Joshua Tree", "Hillsborough County", "Cartoon Network Too", "Bing Crosby", "the fictional city of Quahog, Rhode Island", "French", "October 4, 1970", "George Raft", "As One", "1944", "the British Army", "311", "Jennifer Taylor", "John R. Dilworth", "Columbia Records", "British", "6,241", "October 17, 2017", "Leslie James \"Les\" Clark", "Phil Collins", "Hawaii", "Gregg Popovich", "before the majority of their members quit and formed Faith No More", "John Malkovich", "Lauren Tom", "~ 55 - 75", "Spektor", "Maine", "Richmond, Va.", "vanilla", "Rihanna", "Teen Patti", "The Louvre", "with this surname", "hemming", "will", "Romulus and Remus"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6387061403508771}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2105263157894737, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-3161", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-4449", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-1310", "mrqa_naturalquestions-validation-8962", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5383", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2609", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-9920"], "SR": 0.53125, "CSR": 0.5548295454545454, "retrieved_ids": ["mrqa_squad-train-19796", "mrqa_squad-train-40484", "mrqa_squad-train-34922", "mrqa_squad-train-72939", "mrqa_squad-train-26899", "mrqa_squad-train-33728", "mrqa_squad-train-1837", "mrqa_squad-train-67664", "mrqa_squad-train-36539", "mrqa_squad-train-57549", "mrqa_squad-train-29144", "mrqa_squad-train-1946", "mrqa_squad-train-4110", "mrqa_squad-train-11929", "mrqa_squad-train-61049", "mrqa_squad-train-23699", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-6524", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-7694", "mrqa_naturalquestions-validation-9235", "mrqa_hotpotqa-validation-4091", "mrqa_naturalquestions-validation-681", "mrqa_triviaqa-validation-4313", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-1534", "mrqa_newsqa-validation-1143", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-398", "mrqa_searchqa-validation-6960", "mrqa_squad-validation-901", "mrqa_hotpotqa-validation-3149"], "EFR": 0.9666666666666667, "Overall": 0.7244554924242423}, {"timecode": 55, "before_eval_results": {"predictions": ["2016 United States Senate election in Nevada", "1952", "private Roman Catholic university", "voice-work", "400 MW", "Kerry Marie Butler", "Esp\u00edrito Santo Financial Group", "September 5, 2017", "November 23, 2011", "Javan leopard", "Salman Rushdie", "Tom Rob", "Big Bad Wolf", "Rockland County", "Personal History", "Australian-American", "Holston River", "1943", "1996", "Tamil Nadu", "Tallahassee City Commission", "Markov Random Field", "three different covers", "Hechingen in Swabia", "Floyd Mutrux and Colin Escott", "A Little Princess", "\"\", and the 2013 Marvel One- Shot short film of the same name", "2008", "from July 2, 1967 to August 21, 1995", "Kennedy Road", "Mark Neveldine and Brian Taylor", "the fictional city of Quahog, Rhode Island", "Spiro Theodore \"Ted\" Agnew", "Dr. Ralph Stanley", "three years", "Noel Gallagher", "Thorgan Ganael Francis Hazard", "1692", "9", "the twelfth title in the mainline \" Final Fantasy\" series", "Turkmenistan", "26,000", "Northampton Town", "Gujarat", "General Manager", "punk rock", "Matt Lucas", "Duke", "Bill Clinton", "John D Rockefeller's Standard Oil Company", "D", "pigs", "dystopian science fiction action thriller", "The flag of Vietnam", "pulsar", "dogger Bank", "the first eight seasons", "\"Even, there are many accidents that produce structural damage such that the vehicle's frame is bent, even though the exterior of the car might even look drivable,\"", "Steven Gerrard", "Ketamine", "the Caspian tern", "Antarctica", "Diebold", "Andr\u00e9s Iniesta"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6869115259740259}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true], "QA-F1": [0.8333333333333333, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.04545454545454545, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-5615", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3340", "mrqa_hotpotqa-validation-5280", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3512", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-5228", "mrqa_naturalquestions-validation-5394", "mrqa_triviaqa-validation-3487", "mrqa_newsqa-validation-456", "mrqa_searchqa-validation-15908"], "SR": 0.59375, "CSR": 0.5555245535714286, "retrieved_ids": ["mrqa_squad-train-79128", "mrqa_squad-train-25625", "mrqa_squad-train-17338", "mrqa_squad-train-25853", "mrqa_squad-train-67730", "mrqa_squad-train-49016", "mrqa_squad-train-80814", "mrqa_squad-train-52081", "mrqa_squad-train-8861", "mrqa_squad-train-38880", "mrqa_squad-train-33597", "mrqa_squad-train-33800", "mrqa_squad-train-63749", "mrqa_squad-train-7736", "mrqa_squad-train-48173", "mrqa_squad-train-28996", "mrqa_squad-validation-3480", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-7145", "mrqa_squad-validation-7872", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-5907", "mrqa_squad-validation-433", "mrqa_hotpotqa-validation-2336", "mrqa_naturalquestions-validation-522", "mrqa_newsqa-validation-846", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-3989", "mrqa_triviaqa-validation-2601", "mrqa_searchqa-validation-12070"], "EFR": 1.0, "Overall": 0.7312611607142857}, {"timecode": 56, "before_eval_results": {"predictions": ["35,000", "Vishal Bhardwaj", "Macau", "Russian film industry", "no. 3", "The Government of Ireland", "Washington Street", "Eielson Air Force Base", "Galo (], \"Rooster\"", "Tom Shadyac", "Michael Phelps", "Richard Strauss", "Claire Fraser", "Iynx", "David Starkey", "Edward of Angoul\u00eame", "Anne Perry", "Ready to Die", "Humphrey Goodman", "\"Darconville\u2019s Cat\"", "An invoice, bill or tab", "October 16, 2015", "Dan Brandon Bilzerian", "The Andes", "Srinagar", "STS-51-C.", "Gregg Harper", "The Ones Who Walk Away from Omelas", "8th congressional district", "(Polish: \"Trzy kolory\", French: \" Trois couleurs\"", "November 27, 2002", "Henry Albert \"Hank\" Azaria", "UFC Fight Pass", "The Sun", "Peter Yarrow", "Monty Python's Flying Circus", "Ready Player One", "1,925", "7 October 1978", "June 2, 2008", "Ravenna", "John Meston", "the Battelle Energy Alliance", "a co-op of grape growers", "The Spiderwick Chronicles", "goalkeeper", "George Orwell", "Croatian", "Labour Party", "Fortunino", "Warsaw, Poland", "Texhoma", "blood plasma and lymph in the `` intravascular compartment ''", "September 2017", "Ken Norton", "india", "Suchet", "the UK", "a tanker", "5 1/2-year-old", "cobalt", "Hollaback Girl", "the bassoon", "time in exchange for detailed public disclosure of an invention"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6661439255189254}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3953", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-7526", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-6632"], "SR": 0.578125, "CSR": 0.555921052631579, "retrieved_ids": ["mrqa_squad-train-42078", "mrqa_squad-train-76490", "mrqa_squad-train-62388", "mrqa_squad-train-29135", "mrqa_squad-train-31559", "mrqa_squad-train-31063", "mrqa_squad-train-57468", "mrqa_squad-train-30198", "mrqa_squad-train-47494", "mrqa_squad-train-17182", "mrqa_squad-train-7353", "mrqa_squad-train-43956", "mrqa_squad-train-36324", "mrqa_squad-train-37065", "mrqa_squad-train-14433", "mrqa_squad-train-76049", "mrqa_newsqa-validation-1212", "mrqa_triviaqa-validation-670", "mrqa_newsqa-validation-4112", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-1618", "mrqa_searchqa-validation-902", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-7286", "mrqa_triviaqa-validation-4117", "mrqa_naturalquestions-validation-9492", "mrqa_newsqa-validation-2627", "mrqa_naturalquestions-validation-9467", "mrqa_newsqa-validation-1466", "mrqa_triviaqa-validation-6214", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-9717"], "EFR": 0.9259259259259259, "Overall": 0.7165256457115009}, {"timecode": 57, "before_eval_results": {"predictions": ["1961", "Ballon d'Or", "Elsie Audrey Mossom", "trans-Pacific flight", "House of Hohenstaufen", "BraveStarr", "White Knights of the Ku Klux Klan", "Francisco Rafael Arellano F\u00e9lix", "lion", "Flower Drum Song", "Lebensraum", "China", "Esperanza Spalding", "1854", "Art Deco-style", "City of Starachowice", "Debbie Harry", "England", "Mineola", "Kagoshima Airport", "Melbourne's City Centre", "Abbey Road", "bioelectromagnetics", "Germany", "casting, job opportunities, and career advice", "ten episodes", "2012", "Ford Falcon", "Igor Stravinsky, Carl Orff, Paul Hindemith", "Euripides", "the third", "Clitheroe Football Club", "Peter Wooldridge Townsend", "Paper", "November 27, 2002", "Campbellsville", "1837", "2006", "al-Qaeda", "Humberside Airport", "3,500,000", "a successful racehorse breeder", "Nia Kay", "11 November 1918", "1891", "Bank of China Tower", "Lerotholi Polytechnic", "a private liberal arts college", "the twelfth and thirteenth centuries", "(born 25 May 1945), known professionally as Dave Lee Travis", "Richard Street", "latitude 90 \u00b0 North", "a radius 1.5 times the Schwarzschild radius", "photodiode", "Japan", "Apollo", "leeds", "bragging about his sex life", "I think that people are going to look at the content of the speech,", "Fernando Torres", "Alicia Keys", "Days Inns", "Napoleon", "Sonja Henie"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7303571428571428}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-1862", "mrqa_hotpotqa-validation-4341", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4436", "mrqa_naturalquestions-validation-3499", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2325", "mrqa_searchqa-validation-4002"], "SR": 0.578125, "CSR": 0.5563038793103448, "retrieved_ids": ["mrqa_squad-train-52308", "mrqa_squad-train-37473", "mrqa_squad-train-14505", "mrqa_squad-train-11072", "mrqa_squad-train-2112", "mrqa_squad-train-60411", "mrqa_squad-train-49063", "mrqa_squad-train-16580", "mrqa_squad-train-79738", "mrqa_squad-train-48808", "mrqa_squad-train-36206", "mrqa_squad-train-30892", "mrqa_squad-train-85083", "mrqa_squad-train-43156", "mrqa_squad-train-45854", "mrqa_squad-train-6977", "mrqa_searchqa-validation-8046", "mrqa_triviaqa-validation-1876", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-6524", "mrqa_newsqa-validation-2589", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-1023", "mrqa_searchqa-validation-7469", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-5245", "mrqa_searchqa-validation-4751", "mrqa_triviaqa-validation-439", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-7952", "mrqa_squad-validation-7774"], "EFR": 0.9629629629629629, "Overall": 0.7240096184546615}, {"timecode": 58, "before_eval_results": {"predictions": ["Kr\u00f8yer", "an open window", "King Gyanendra,", "Susan Atkins,", "a brown coffin containing the remains of Israeli soldiers killed during the 2006 war.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "menstrual cycle.", "his hip on the left side and tore through his abdomen.", "The iconic Abbey Road music studios made famous by the Beatles are not for sale,", "10 municipal police officers", "his past and his future", "Empire of the Sun", "The plan aims to put Americans to work updating the country's infrastructure, making public buildings more energy-efficient and implementing environmentally friendly technologies, including alternative energy sources.", "uncomfortable,\"", "Haeftling,", "55-year-old", "capital murder and three counts of attempted murder", "shock, quickly followed by speculation about what was going to happen next,\"", "eight people", "his first grand Slam,", "curfew", "\"Nude, Green Leaves and Bust\"", "making her comeback last year after giving birth to baby daughter Jada,", "Glasgow, Scotland", "an antihistamine and an epinephrine auto-injector", "Damon Bankston", "Afghanistan's", "an \"aesthetic environment\" and ensure public safety,", "we believe that leadership is an indispensable ingredient. Nothing is more central to preventing genocide and mass atrocities,", "Women in Somalia's third-largest city, Baidoa, have been ordered to wear Islamic dress starting this week or face jail time,", "humans", "The torch for the 2010 Vancouver Olympics was lit in a ceremony at the ancient Greek site of Olympia on Thursday,", "immediately releasing all civilians and laying down arms,\"", "two years ago.", "is a businessman, team owner, radio-show host and author.", "Sen. Barack Obama", "social issues like homelessness and AIDS.", "parachuted to the ground.", "his first task was to remedy the situation of America wielding a big stick for the last eight years.", "one-of-a-kind navy dress with red lining", "The Rev. Alberto Cutie -- sometimes called \"Father Oprah\" because of the advice he gave on Spanish-language media --", "citizenship", "housing, business and infrastructure repairs,", "The nation's foremost concert producer, Charles Jubert, died.", "a mammoth", "\"She had a smile on her face, like she always does when she comes in here,\"", "the creation of an Islamic emirate in Gaza,", "the north and west of the country,", "The 19-year-old woman", "1-1.", "Somalia's piracy problem was fueled by environmental and political events.", "Francisco Pizarro", "Owen Vaccaro", "2003", "Renault", "cheese", "Alan Freed", "Woolsthorpe-by-Colsterworth", "War Is the Answer", "arts manager", "ice sheets", "Ms.", "Your Dreams", "freezing"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5110145351437921}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.058823529411764705, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7368421052631579, 0.0, 0.4, 0.0, 1.0, 0.923076923076923, 0.5833333333333334, 0.07407407407407408, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8571428571428571, 0.0, 0.33333333333333337, 1.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 0.25000000000000006, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3981", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2472", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-4517", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-430"], "SR": 0.40625, "CSR": 0.553760593220339, "retrieved_ids": ["mrqa_squad-train-12794", "mrqa_squad-train-5233", "mrqa_squad-train-34458", "mrqa_squad-train-46271", "mrqa_squad-train-51975", "mrqa_squad-train-36674", "mrqa_squad-train-80650", "mrqa_squad-train-15067", "mrqa_squad-train-71278", "mrqa_squad-train-35170", "mrqa_squad-train-62659", "mrqa_squad-train-62012", "mrqa_squad-train-62082", "mrqa_squad-train-26638", "mrqa_squad-train-20771", "mrqa_squad-train-65559", "mrqa_squad-validation-4170", "mrqa_newsqa-validation-2076", "mrqa_squad-validation-3118", "mrqa_hotpotqa-validation-4618", "mrqa_searchqa-validation-14922", "mrqa_squad-validation-433", "mrqa_squad-validation-7626", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-238", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-6383", "mrqa_hotpotqa-validation-2720", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-5036", "mrqa_hotpotqa-validation-206", "mrqa_naturalquestions-validation-4983"], "EFR": 1.0, "Overall": 0.7309083686440678}, {"timecode": 59, "before_eval_results": {"predictions": ["a female soldier,", "Sunday", "relatives of the five suspects,", "A witness", "U.N. agencies", "Democratic VP candidate", "Ken Choi", "second child", "on the bench", "leftist Workers' Party.", "Robert Park", "Christiane Amanpour", "Narayanthi Royal", "Mexico", "18th", "a city of romance, of incredible architecture and history.", "a song about freedom of speech.", "Keating Holland.", "Asashoryu,", "Mutassim,", "Dr. Jennifer Arnold and husband Bill Klein,", "Lindsey oil refinery in eastern England.", "Sunday.", "future relations between the Middle East and Washington.", "prostate cancer,", "the southern city of Naples", "at least 300", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "2002", "pregnant soldier", "how preachy and awkward cancer movies can get.", "Some truly mind-blowing structures are being planned for the Middle East.", "strife in Somalia,", "Miami Beach, Florida,", "Charles Darwin", "it was unjustifiable \"for a project which does nothing more than perpetuate misconceptions about the state and its citizens.\"", "President Obama.", "KVBC,", "some of the most gigantic pumpkins in the world,", "baseball bat", "his native Philippines", "OneLegacy,", "Jaime Andrade", "Alfredo Astiz,", "ties", "The station", "Ronald Cummings,", "Amir Zaki is back at his Premier League club side Wigan Athletic in northern England.", "$8.8 million", "late Tuesday night,", "Alan Graham", "Canada south of the Arctic", "Glenn Close", "Joseph M. Scriven", "Richard Krajicek", "Manchester", "Robert Plant", "Big Kenny", "Cannes Film Festival", "Mario Winans", "three", "Tom Chaney", "Seoul", "1936"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5996775793650794}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.8, 0.3, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3095", "mrqa_naturalquestions-validation-1872", "mrqa_triviaqa-validation-5522", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-53", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-11013"], "SR": 0.515625, "CSR": 0.553125, "retrieved_ids": ["mrqa_squad-train-74706", "mrqa_squad-train-62298", "mrqa_squad-train-19768", "mrqa_squad-train-82286", "mrqa_squad-train-63237", "mrqa_squad-train-49964", "mrqa_squad-train-389", "mrqa_squad-train-84003", "mrqa_squad-train-21743", "mrqa_squad-train-11896", "mrqa_squad-train-50852", "mrqa_squad-train-46355", "mrqa_squad-train-29536", "mrqa_squad-train-72761", "mrqa_squad-train-75847", "mrqa_squad-train-38459", "mrqa_hotpotqa-validation-1370", "mrqa_newsqa-validation-2820", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-9063", "mrqa_triviaqa-validation-6496", "mrqa_newsqa-validation-1745", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-5036", "mrqa_newsqa-validation-4171", "mrqa_hotpotqa-validation-3145", "mrqa_triviaqa-validation-2277", "mrqa_newsqa-validation-1143", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-3086", "mrqa_hotpotqa-validation-1140", "mrqa_naturalquestions-validation-5739"], "EFR": 1.0, "Overall": 0.7307812499999999}, {"timecode": 60, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1899", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-382", "mrqa_hotpotqa-validation-4012", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-892", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-266", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2558", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1143", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5687", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6157", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7783", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.86328125, "KG": 0.5015625, "before_eval_results": {"predictions": ["2017", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "Thomas Edison", "whether to die or to feed on human blood", "Master Christopher Jones", "Juliet", "the status line", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "The Maginot Line", "1937", "Fix You", "Hirschman", "Bobby Eli", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "11 %", "Guy Berryman", "Austria - Hungary", "Sharyans Resources", "Babe Ruth", "Seattle, Washington", "420", "Wylie Draper", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province )", "DNA was a repeating set of identical nucleotides", "Magnavox Odyssey", "the ball is fed into the gap between the two forward packs", "Sunday night", "tolled ( quota ) highways", "close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "the retina", "Saturday", "Glenn Close", "reservoirs at high altitudes", "aiding the war effort", "into smaller pulmonary arteries that spread throughout the lungs", "Nepal", "the symbol \u00d7", "Ace", "Phillip Paley", "10.5 %", "the liver and kidneys", "Lee County, Florida, United States", "tenderness of meat", "Detective Superintendent Dave Kelly", "Florida", "2010", "the Archies", "infection, irritation, or allergies", "ABC", "August 1991", "The Maidstone Studios in Maidstone, Kent", "three", "helium", "caliber", "Edward James Olmos", "Days of Our Lives", "Hilux", "Omar bin Laden,", "a man's lifeless, naked body", "jazz", "Falkland Islands", "cement pond", "Seventy-six Trombones", "Fifty Shades of Grey"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6092222587195413}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.5, 0.0, 0.3076923076923077, 0.0, 0.2666666666666667, 0.8695652173913044, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 0.08, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-8998", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-6052", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-1584", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-15624", "mrqa_searchqa-validation-893"], "SR": 0.515625, "CSR": 0.5525102459016393, "retrieved_ids": ["mrqa_squad-train-16717", "mrqa_squad-train-1929", "mrqa_squad-train-44366", "mrqa_squad-train-26297", "mrqa_squad-train-65587", "mrqa_squad-train-46655", "mrqa_squad-train-46978", "mrqa_squad-train-32057", "mrqa_squad-train-38616", "mrqa_squad-train-61980", "mrqa_squad-train-22071", "mrqa_squad-train-75314", "mrqa_squad-train-26675", "mrqa_squad-train-48908", "mrqa_squad-train-7715", "mrqa_squad-train-37409", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-1927", "mrqa_searchqa-validation-12835", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2389", "mrqa_triviaqa-validation-5406", "mrqa_newsqa-validation-3489", "mrqa_triviaqa-validation-2492", "mrqa_searchqa-validation-4802", "mrqa_hotpotqa-validation-1316", "mrqa_hotpotqa-validation-831", "mrqa_naturalquestions-validation-8245", "mrqa_searchqa-validation-15517", "mrqa_triviaqa-validation-2073", "mrqa_newsqa-validation-501"], "EFR": 0.967741935483871, "Overall": 0.7324879362771021}, {"timecode": 61, "before_eval_results": {"predictions": ["17 - year - old", "The procedure can be performed at any level in the spine ( cervical, thoracic, or lumbar ) and prevents any movement between the fused vertebrae", "Dido", "international aid", "an economy of at least US $2 trillion by GDP in nominal or PPP terms", "January 15, 2007", "Gorakhpur railway station", "Roger Federer", "Battle of Antietam", "1933", "about the hardships of growing older and has no relationship to drug - taking", "the person compelled to pay for reformist programs", "John Travolta", "August 5, 1937", "Rory McIlroy", "Jonathan Breck", "the 4th century", "Smith Jerrod", "`` Acid rain ''", "2018 NCAA Division I Men's Basketball Tournament", "Mulberry Street", "Pete Seeger", "sinoatrial node", "2009", "Manuel `` Manny '' Heffley is Greg and Rodrick's younger brother", "10 May 1940", "Phillipa Soo", "Kansas City Chiefs", "Judith Aline Keppel", "Vincent Price", "the customer's account", "Redwood National Park ( established 1968 ) and California's Del Norte Coast, Jedediah Smith, and Prairie Creek Redwoods State Parks ( dating from the 1920s )", "to fit in", "metaphase", "Sally Field", "Eric Clapton", "E-1 through E-3", "Lagaan", "Colman", "Karen Gillan", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) or Kozunak", "Beijing, China", "5 lakh of rupees", "Havana Harbor", "1824", "2017", "Qutab Ud - Din - Aibak", "The Portuguese", "the inferior thoracic border", "United States customary system", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "floating ribs", "The picture-perfect property with its own private island sits on the banks of the River Kennet in the village of Padworth", "Romania", "The Gold Coast", "Kathryn Jean Martin \"Kathy\" Sullivan AM", "25 million", "to see my kids graduate from this school district.", "use of torture and indefinite detention", "Tuesday's iPhone 4S news,", "Chagas disease", "Jacob Marley", "John Hersey", "22 September 2015"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6740496618308771}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.12903225806451613, 1.0, 0.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.08, 0.5, 1.0, 1.0, 1.0, 0.6, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.75, 0.2758620689655173, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10172", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8382", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6596", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-451", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2249", "mrqa_searchqa-validation-4844", "mrqa_searchqa-validation-14622"], "SR": 0.53125, "CSR": 0.5521673387096775, "retrieved_ids": ["mrqa_squad-train-28389", "mrqa_squad-train-17429", "mrqa_squad-train-33654", "mrqa_squad-train-58162", "mrqa_squad-train-44001", "mrqa_squad-train-76241", "mrqa_squad-train-71043", "mrqa_squad-train-5051", "mrqa_squad-train-39420", "mrqa_squad-train-55671", "mrqa_squad-train-38969", "mrqa_squad-train-84184", "mrqa_squad-train-42552", "mrqa_squad-train-49116", "mrqa_squad-train-52166", "mrqa_squad-train-71115", "mrqa_triviaqa-validation-4896", "mrqa_squad-validation-4673", "mrqa_searchqa-validation-13135", "mrqa_newsqa-validation-3223", "mrqa_triviaqa-validation-2816", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-358", "mrqa_hotpotqa-validation-460", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-10461", "mrqa_hotpotqa-validation-3145", "mrqa_triviaqa-validation-2482", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-808", "mrqa_searchqa-validation-12856"], "EFR": 0.9666666666666667, "Overall": 0.7322043010752688}, {"timecode": 62, "before_eval_results": {"predictions": ["Christian Dior", "cranberry", "the Gateway Arch", "fibula", "a hyena", "a beached whale", "the Mississippi", "Kevin Costner", "Lil Jon", "the Boer War", "the Theater of Pompey", "Goldeneye", "meat", "tarmes", "Winston Churchill", "Scampton, Lincolnshire", "the Lincoln Tunnel", "Pinta", "Louisiana", "Billy the Kid", "Rembrandt", "Canada", "Tony Banks", "Benedict XVI", "a 300 million barrel strategic petroleum reserve", "Prague", "Hanging Gardens", "electricity", "Two and a Half Men", "the Golden Compass", "Nacho Libre", "William and Mary", "Rocky", "clarinet", "Montpelier", "Halo 3", "Pound of flesh", "Bahrain", "Best Picture", "Hamlet is trying to protect Ophelia", "Heroes", "Russia", "mead", "the Hawks", "Carnival", "wood", "Samson", "Dustin Hoffman", "Nittany", "Sicily", "Socrates", "Wingstop Restaurants, Inc.", "the large area needed for effective gas exchange", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "jupiter", "Honolulu", "William Randolph Hearst", "Esperanza Spalding", "2000", "Sargent Shriver", "Daytime Emmy Lifetime Achievement Award", "Impeccable", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "par 4 third hole"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6679415931132178}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6, 0.8695652173913043, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5263157894736842, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-13095", "mrqa_searchqa-validation-4677", "mrqa_searchqa-validation-13118", "mrqa_searchqa-validation-1906", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-5976", "mrqa_searchqa-validation-1751", "mrqa_searchqa-validation-6371", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-16438", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-14898", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-8062", "mrqa_searchqa-validation-546", "mrqa_searchqa-validation-15398", "mrqa_naturalquestions-validation-6169", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6028", "mrqa_hotpotqa-validation-5895", "mrqa_newsqa-validation-3961", "mrqa_triviaqa-validation-1787"], "SR": 0.546875, "CSR": 0.5520833333333333, "retrieved_ids": ["mrqa_squad-train-8551", "mrqa_squad-train-6367", "mrqa_squad-train-26006", "mrqa_squad-train-43412", "mrqa_squad-train-39315", "mrqa_squad-train-79462", "mrqa_squad-train-16350", "mrqa_squad-train-66865", "mrqa_squad-train-17963", "mrqa_squad-train-84493", "mrqa_squad-train-15157", "mrqa_squad-train-41946", "mrqa_squad-train-10294", "mrqa_squad-train-9602", "mrqa_squad-train-77601", "mrqa_squad-train-62524", "mrqa_naturalquestions-validation-2967", "mrqa_newsqa-validation-11", "mrqa_naturalquestions-validation-2245", "mrqa_searchqa-validation-4568", "mrqa_newsqa-validation-205", "mrqa_triviaqa-validation-5590", "mrqa_searchqa-validation-9717", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3402", "mrqa_naturalquestions-validation-5631", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2907", "mrqa_triviaqa-validation-635", "mrqa_searchqa-validation-16618", "mrqa_hotpotqa-validation-1316", "mrqa_naturalquestions-validation-688"], "EFR": 0.9655172413793104, "Overall": 0.7319576149425286}, {"timecode": 63, "before_eval_results": {"predictions": ["the Voting Rights Act", "handguns", "disabilities", "Harry Potter", "President Kennedy", "a soap", "the Bronze Age", "the People's Party", "the Tower of London", "Daniel", "the Jinx", "largest city", "California", "Cosmopolitan", "David Beckham", "Minoan", "Japan", "rodeo", "a door", "Vietnam", "Stanford", "minor", "a prism schism", "Alaska", "Wallis Warfield Simpson", "a centipede", "Greek", "The Sisters Rosensweig", "Brooklyn", "Penn State", "Easter Island", "Nasser", "the Hell Is Hell", "Stephen Hawking", "Labor Day", "Mozambique", "landfills", "The Silence of the Lambs", "sea otter", "Capone", "Paul McCartney", "Anne Murray", "Tennessee", "Buenos", "contagious", "Jane Austen", "rye", "baking soda", "peanuts", "philosophy", "Byzantines", "The Paris Sisters", "waiting tables at the Moondance Diner", "Miami Heat", "gluten", "embroidered cloth", "Friday", "2004", "Pacific War", "Tsavo East National Park", "Casa de Campo International Airport in the Dominican Republic", "almost 100", "incentive and a method to reach car owners who haven't complied fully with recalls", "left fielder"], "metric_results": {"EM": 0.625, "QA-F1": 0.6956597222222223}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-9089", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-3380", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-2172", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-6957", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-14995", "mrqa_searchqa-validation-16029", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-7085", "mrqa_hotpotqa-validation-4005", "mrqa_newsqa-validation-2362", "mrqa_hotpotqa-validation-798"], "SR": 0.625, "CSR": 0.55322265625, "retrieved_ids": ["mrqa_squad-train-10981", "mrqa_squad-train-51601", "mrqa_squad-train-36165", "mrqa_squad-train-12043", "mrqa_squad-train-55682", "mrqa_squad-train-76825", "mrqa_squad-train-33856", "mrqa_squad-train-82431", "mrqa_squad-train-3366", "mrqa_squad-train-50986", "mrqa_squad-train-49920", "mrqa_squad-train-33775", "mrqa_squad-train-74572", "mrqa_squad-train-81186", "mrqa_squad-train-56726", "mrqa_squad-train-86192", "mrqa_searchqa-validation-2068", "mrqa_hotpotqa-validation-2286", "mrqa_triviaqa-validation-274", "mrqa_naturalquestions-validation-9141", "mrqa_squad-validation-2943", "mrqa_triviaqa-validation-1411", "mrqa_naturalquestions-validation-4804", "mrqa_searchqa-validation-1837", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-936", "mrqa_hotpotqa-validation-238", "mrqa_searchqa-validation-15870", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-2207", "mrqa_naturalquestions-validation-10614", "mrqa_triviaqa-validation-768"], "EFR": 1.0, "Overall": 0.7390820312499999}, {"timecode": 64, "before_eval_results": {"predictions": ["Malawi", "Ethiopia", "knife hones", "echidna and the duck-billed platypus", "British Airways", "super luigi", "1929", "Blades", "the Kelly Gang", "Dante", "repechage", "uranium", "Wildcats", "d'Artagnan", "Abraham Lincoln", "The Merchant of Venice", "pint", "Paul Rudd", "Tanzania", "Julian", "Christian Louboutin", "clarinet", "fort boyardem", "September", "Lincolnshire", "Muriel Spark", "Turkey", "kvetch", "the Dutch", "Lome", "what", "Christian Dior", "Pat Houston", "King William IV", "Dutch", "Jack Lemmon", "Trainspotting", "Australia", "the north-west corner of the central business district", "Diptera", "India and Pakistan", "obi", "ccoli", "heisenberg", "1976", "Cyclopes", "phrenology", "Full Metal jacket", "Tokyo", "California", "Windermere", "third", "at the fictional elite conservative Vermont boarding school Welton Academy", "Norman occupational surname ( meaning tailor ) in France", "Daniel Richard \" Danny\" Green, Jr.", "between 11 or 13 and 18", "Archduke Franz Ferdinand of Austria", "Chile", "the single-engine Cessna 206 went down,", "helping on the sandbags lines as the community raced to fill 1 million of them.", "Rachel Carson", "laundry", "Toni Morrison", "Copts"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6540274064171123}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.9411764705882353, 0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.3529411764705882, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-2678", "mrqa_triviaqa-validation-4208", "mrqa_triviaqa-validation-1543", "mrqa_triviaqa-validation-7398", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6308", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1819", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-8858", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3827", "mrqa_newsqa-validation-1926", "mrqa_searchqa-validation-4044", "mrqa_newsqa-validation-2435"], "SR": 0.578125, "CSR": 0.5536057692307692, "retrieved_ids": ["mrqa_squad-train-45679", "mrqa_squad-train-69779", "mrqa_squad-train-43961", "mrqa_squad-train-78182", "mrqa_squad-train-77826", "mrqa_squad-train-19071", "mrqa_squad-train-80487", "mrqa_squad-train-46027", "mrqa_squad-train-60414", "mrqa_squad-train-55635", "mrqa_squad-train-74736", "mrqa_squad-train-80587", "mrqa_squad-train-13326", "mrqa_squad-train-31579", "mrqa_squad-train-42500", "mrqa_squad-train-26133", "mrqa_hotpotqa-validation-1690", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-3760", "mrqa_triviaqa-validation-1815", "mrqa_searchqa-validation-3451", "mrqa_squad-validation-2565", "mrqa_naturalquestions-validation-222", "mrqa_hotpotqa-validation-3324", "mrqa_triviaqa-validation-4019", "mrqa_naturalquestions-validation-5093", "mrqa_searchqa-validation-8407", "mrqa_naturalquestions-validation-950", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-4652", "mrqa_hotpotqa-validation-2880", "mrqa_newsqa-validation-1003"], "EFR": 1.0, "Overall": 0.7391586538461538}, {"timecode": 65, "before_eval_results": {"predictions": ["amanda barrie", "drums", "plump", "Tina Turner", "dreamsgirls", "jennifer stobart", "1986", "right-hand side", "commercial", "billy williams", "Uganda", "well cared for", "temperature", "satirical cartoons", "Alaska", "iron", "Sven Goran Eriksson", "c\u00e9vennes", "bagram", "jupiter", "wellington", "The Pillow Book", "isabella", "Iran", "john denver", "smack", "Massachusetts", "Coriolis", "brothers in arms", "The Apprentice", "The Altamont Speedway Free Festival", "jennifer williams", "brixham", "The Marcy Brothers", "Ghana", "alla capella", "Nelson Mandela", "wellington", "Illinois", "sailing", "a rain hat", "2/6", "netherlands", "CBS", "noah beery jr", "Nebraska\u2013Lincoln", "Sarajevo", "netherlands", "Beyonce", "Carl Johan", "MI5", "Games played", "Something to Sing About", "petition for a writ of certiorari", "William T. Anderson", "American Way", "Arab", "fluoroquinolone", "Alina Cho", "eight.\"", "Batman", "Passover", "Johann Strauss Jr", "John Lennon and George Harrison,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.58203125}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3599", "mrqa_triviaqa-validation-533", "mrqa_triviaqa-validation-2184", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-2918", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-5878", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-6095", "mrqa_naturalquestions-validation-7950", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-150", "mrqa_newsqa-validation-1804", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-14825"], "SR": 0.53125, "CSR": 0.5532670454545454, "retrieved_ids": ["mrqa_squad-train-72058", "mrqa_squad-train-77163", "mrqa_squad-train-41048", "mrqa_squad-train-63191", "mrqa_squad-train-61798", "mrqa_squad-train-14135", "mrqa_squad-train-79185", "mrqa_squad-train-4706", "mrqa_squad-train-3856", "mrqa_squad-train-2987", "mrqa_squad-train-12633", "mrqa_squad-train-17377", "mrqa_squad-train-58403", "mrqa_squad-train-34181", "mrqa_squad-train-53307", "mrqa_squad-train-39418", "mrqa_triviaqa-validation-1809", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4863", "mrqa_newsqa-validation-3143", "mrqa_searchqa-validation-9089", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-174", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-3112", "mrqa_hotpotqa-validation-4689", "mrqa_naturalquestions-validation-2847", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-6833", "mrqa_naturalquestions-validation-8159", "mrqa_searchqa-validation-708"], "EFR": 0.9666666666666667, "Overall": 0.7324242424242424}, {"timecode": 66, "before_eval_results": {"predictions": ["sleepless in seattle", "Margaret Beckett", "littoral", "typhoid fever", "Sheryl Crow", "Melvil Dewey", "korea", "Pancho Villa", "the Wild Bunch", "Mikhail Gorbachev", "South Korea", "Princess bride", "tiptoe through the Tulips", "landerstra\u00dfe", "barber of seville\u00ef\u00bf\u00bd", "Vengeance", "c\u00e9vennes", "Les Invalides", "1861", "a snowman", "d.\u00a0W. Griffith", "nine letters", "Samuel", "aslan", "26", "the narwhal", "wishbone", "photography", "Charlie Chan", "taekwondo", "phosphorus", "Hogwarts School of Witchcraft and Wizardry", "plutonium", "Mercury", "tanks", "porter", "Antonio Vivaldi", "Groucho Marx", "lacrosse", "Queen Elizabeth I", "one small step for man", "1807", "l Lithuania", "canmore", "mad hatter", "Bette Davis", "cooking", "Chrysler", "what", "Congo", "london", "1994", "U.S. Bank Stadium", "depending on the country", "The Liberties (Irish: Na Saoirs\u00ed or occasionally Na Libirt\u00ed)", "the Knight Company", "north Miami-Dade County", "World leaders", "January 24, 2006.", "after giving birth to baby daughter Jada,", "this", "Vibin", "a dream", "Latin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5625}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4125", "mrqa_triviaqa-validation-4413", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-458", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-1577", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-1834", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-1794", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-4523", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-1030", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-7374", "mrqa_searchqa-validation-8081"], "SR": 0.5625, "CSR": 0.5534048507462687, "retrieved_ids": ["mrqa_squad-train-56259", "mrqa_squad-train-43930", "mrqa_squad-train-20691", "mrqa_squad-train-60320", "mrqa_squad-train-55862", "mrqa_squad-train-67731", "mrqa_squad-train-50804", "mrqa_squad-train-77731", "mrqa_squad-train-63613", "mrqa_squad-train-20468", "mrqa_squad-train-5064", "mrqa_squad-train-22429", "mrqa_squad-train-85018", "mrqa_squad-train-83779", "mrqa_squad-train-44172", "mrqa_squad-train-84415", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-3918", "mrqa_searchqa-validation-11369", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-1799", "mrqa_searchqa-validation-14791", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-4183", "mrqa_naturalquestions-validation-186", "mrqa_searchqa-validation-9277", "mrqa_naturalquestions-validation-3041", "mrqa_squad-validation-7626", "mrqa_triviaqa-validation-5878"], "EFR": 1.0, "Overall": 0.7391184701492537}, {"timecode": 67, "before_eval_results": {"predictions": ["Anunnaki", "Mahatma Gandhi", "Battlestar Galactica", "a sperm whale", "South Africa", "Brett Favre", "Perilis", "Texas", "Angioplasty", "anxiety", "a phaser", "Mary Pickford", "Rivera", "Sayonara", "a cat", "India", "the Lone Ranger", "Mars", "the Battle of Verdun", "a statistic", "Andes", "India", "Barnato", "Houston Rockets", "shank", "South Africa", "Boston", "Red Dead Redemption", "Van Helsing", "Thomas Hobson", "pesos", "Shop", "Bligh", "Urban Outfitters", "Baskin-Robbins", "Andrew Wyeth", "smallpox", "jimmy", "Al Jolson", "Risk", "Peril", "Don Quixote", "Richmond", "midnight", "The Age of Innocence", "Students for a Democratic Society", "the Bering Sea", "the Caucasus", "Coretta Scott King", "C-E", "Yellow Submarine", "Brooklyn, New York", "After World War II", "initial components in a phrase or a word", "brighton", "Microsoft", "Fleet Street", "1966", "Juilliard School", "City of Starachowice", "success as a recording artist", "Mombasa, Kenya,", "Thaksin Shinawatra,", "the Peloponnese"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6635416666666667}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3288", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-15118", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4010", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-5960", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-14069", "mrqa_searchqa-validation-10385", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-3274", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-4308", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2674", "mrqa_naturalquestions-validation-2064"], "SR": 0.59375, "CSR": 0.5539981617647058, "retrieved_ids": ["mrqa_squad-train-37480", "mrqa_squad-train-41606", "mrqa_squad-train-67270", "mrqa_squad-train-45633", "mrqa_squad-train-69048", "mrqa_squad-train-81764", "mrqa_squad-train-61340", "mrqa_squad-train-20840", "mrqa_squad-train-42231", "mrqa_squad-train-17081", "mrqa_squad-train-80637", "mrqa_squad-train-9909", "mrqa_squad-train-66600", "mrqa_squad-train-41841", "mrqa_squad-train-39222", "mrqa_squad-train-63794", "mrqa_naturalquestions-validation-9897", "mrqa_searchqa-validation-16252", "mrqa_squad-validation-5374", "mrqa_newsqa-validation-1796", "mrqa_naturalquestions-validation-5039", "mrqa_hotpotqa-validation-4805", "mrqa_squad-validation-8160", "mrqa_newsqa-validation-435", "mrqa_naturalquestions-validation-4103", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-4364", "mrqa_searchqa-validation-5377", "mrqa_hotpotqa-validation-2403", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-825", "mrqa_newsqa-validation-3726"], "EFR": 0.9615384615384616, "Overall": 0.7315448246606334}, {"timecode": 68, "before_eval_results": {"predictions": ["Davenport", "soup", "Ulysses S. Grant", "gonads", "the High Plains", "the gap", "Saturday Night Fever", "North Dakota", "motor neuron", "John Fogerty", "a megaton", "king", "electrons", "the Communist Party of China", "( Alfred) Binet", "( Geoffrey) Chaucer", "the Mediterranean Sea", "Billboard", "James Buchanan", "Hinduism", "the Miasa French Countryside Ruby", "( Wayne) Brady", "Frida Kahlo", "Arkansas", "embryos", "Cuba", "the Internet", "airplanes", "Surgeons", "Thomas Nast", "3", "freezing", "Picabo Street", "National Security Agency", "Kiss Me Kate", "Honey Nut", "Hercules", "the Gallic War", "Rod Laver", "rice", "Ivy Dickens", "Selma ( Alabama)", "vote", "Herbert Hoover", "Joe Hill", "IHOP", "Miples (1948)", "amyotrophic lateral sclerosis", "Samuel Beckett", "Gordon Brown", "Independence Day", "Daya Jethalal Gada", "Kevin Sumlin", "Charles Carson", "Albert Einstein", "Orion", "dodo", "1986", "James Gandolfini", "Mandarin Airlines", "at least two and a half hours.", "six-year veteran", "Buenos Aires.", "Citizens for a Sound Economy"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6864583333333334}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-30", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-15032", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-5152", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-7549", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3663", "mrqa_searchqa-validation-9373", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-8996", "mrqa_searchqa-validation-2684", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14015", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_newsqa-validation-2945"], "SR": 0.59375, "CSR": 0.5545742753623188, "retrieved_ids": ["mrqa_squad-train-85979", "mrqa_squad-train-10746", "mrqa_squad-train-57961", "mrqa_squad-train-50392", "mrqa_squad-train-27147", "mrqa_squad-train-45246", "mrqa_squad-train-33438", "mrqa_squad-train-59257", "mrqa_squad-train-38286", "mrqa_squad-train-76089", "mrqa_squad-train-69082", "mrqa_squad-train-40558", "mrqa_squad-train-10035", "mrqa_squad-train-63965", "mrqa_squad-train-33073", "mrqa_squad-train-422", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-5447", "mrqa_hotpotqa-validation-1681", "mrqa_naturalquestions-validation-1431", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7406", "mrqa_hotpotqa-validation-738", "mrqa_naturalquestions-validation-894", "mrqa_searchqa-validation-6730", "mrqa_newsqa-validation-3119", "mrqa_searchqa-validation-13095", "mrqa_naturalquestions-validation-8075", "mrqa_searchqa-validation-2181", "mrqa_naturalquestions-validation-1284", "mrqa_searchqa-validation-5304", "mrqa_hotpotqa-validation-5467"], "EFR": 1.0, "Overall": 0.7393523550724638}, {"timecode": 69, "before_eval_results": {"predictions": ["Manitoba", "Santa Monica, California", "60 Minutes", "a fruitcake", "midnight", "Socrates", "Al Gore", "the Louvre", "the Kings and Queens of England", "the Desert Fox", "Baton Rouge", "Langston Hughes", "manicula", "Green Bay Phoenix", "Cleveland", "slave", "Davenport", "Stevie Wonder", "Typhoid Mary", "an inch", "an enormous bite force", "Tokyo", "Tennessee Williams", "the United Arab Emirates", "Lurch", "a chancellor", "Big Ben", "Old Yeller", "Proverbs", "2001: A Space Odyssey", "the Big Bang to Black Holes", "Prince", "Abraham", "Duncan", "Yitzhak Rabin", "Redcliffe", "Jason Bourne", "the Holocaust", "the largest lakes and rivers", "Iowa", "Mephistopheles", "Samsonite L luggage", "the Marine Corps", "Vietnam", "the Swiss Canton of Geneva", "Punxsutawney, Pennsylvania", "Sports Illustrated", "Venus and Adonis", "Shia LaBeouf", "Annapolis", "Princess Anne", "The flag of Hungary", "January 15, 2007", "Introverted Feeling ( Fi ) and Extroverted Intuition ( Ne )", "ethelbald I", "Trainspotting", "Stella McCartney", "1898", "Anne of Green Gables", "Daniil Borisovich Shafran", "a Columbian mammoth", "Walt Disney Studios", "cope in prison.", "then-Sen. Obama"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6486742424242424}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6, 0.0, 1.0, 1.0, 0.0, 0.4, 0.8, 0.0, 0.0, 0.18181818181818182, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-14167", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-3164", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-13697", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-6016", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-7011", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-13437", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-6872", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-1411", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3695"], "SR": 0.5625, "CSR": 0.5546875, "retrieved_ids": ["mrqa_squad-train-83466", "mrqa_squad-train-67149", "mrqa_squad-train-64506", "mrqa_squad-train-46112", "mrqa_squad-train-81290", "mrqa_squad-train-65142", "mrqa_squad-train-74808", "mrqa_squad-train-37684", "mrqa_squad-train-52999", "mrqa_squad-train-14524", "mrqa_squad-train-23384", "mrqa_squad-train-79735", "mrqa_squad-train-23366", "mrqa_squad-train-42698", "mrqa_squad-train-15245", "mrqa_squad-train-71480", "mrqa_squad-validation-680", "mrqa_triviaqa-validation-1523", "mrqa_squad-validation-8918", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-246", "mrqa_naturalquestions-validation-8794", "mrqa_searchqa-validation-2068", "mrqa_newsqa-validation-2778", "mrqa_squad-validation-2416", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-4980", "mrqa_searchqa-validation-1554", "mrqa_naturalquestions-validation-8382", "mrqa_triviaqa-validation-7750", "mrqa_searchqa-validation-10920"], "EFR": 1.0, "Overall": 0.739375}, {"timecode": 70, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4260", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5757", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1840", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10156", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-1971", "mrqa_squad-validation-2082", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3215", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4299", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-513", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6067", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6917", "mrqa_squad-validation-6960", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9401", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.912109375, "KG": 0.53984375, "before_eval_results": {"predictions": ["Doc Holliday", "Fritos", "the pale", "fowls", "a fruitcake", "Alicia", "Watson", "Christa McAuliffe", "Kilimanjaro", "Misbegotten", "a pumpkin", "Jumbo", "Stoke-on-Trent", "Rocky Down Mexico Way", "Adolf Hitler", "Portland", "imagism", "Diesel", "a palace", "the brig", "the Spanish Republic", "Ruth", "nuts", "Cheaper by the Dozen", "Hans Christian Andersen", "San Francisco", "the Velvet", "Wanted", "the bugle", "BORE", "Emma Peel", "the Homestead Act", "Liberty", "Max Factor", "plantain", "Marie Curie", "the Indo-European Language Association", "Aladdin", "rain", "Peter the Great", "Toy Story", "the Boston Tea Party", "George Sand", "Jim Jarmusch", "Afghanistan", "Minos", "salamanders", "Charles", "Rosehips", "Led Zeppelin", "Germenicia", "Michigan", "Texas A&M Aggies", "Alex Burrall, Jason Weaver and Wylie Draper", "Leroy", "The Kentucky Derby", "Iwo Jima", "Charles L. Clifford", "Vince Staples", "Sam Bettley", "Al-Shabaab", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Lucky Dube,", "Florida"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6858966503267974}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.7000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4747", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-2455", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-6077", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-8455", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-1531", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-585"], "SR": 0.640625, "CSR": 0.5558978873239437, "retrieved_ids": ["mrqa_squad-train-18823", "mrqa_squad-train-65236", "mrqa_squad-train-75503", "mrqa_squad-train-63249", "mrqa_squad-train-26044", "mrqa_squad-train-31626", "mrqa_squad-train-63394", "mrqa_squad-train-33522", "mrqa_squad-train-6856", "mrqa_squad-train-45929", "mrqa_squad-train-34137", "mrqa_squad-train-23794", "mrqa_squad-train-46712", "mrqa_squad-train-77898", "mrqa_squad-train-61334", "mrqa_squad-train-63421", "mrqa_hotpotqa-validation-5821", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-3999", "mrqa_naturalquestions-validation-3989", "mrqa_newsqa-validation-3592", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-616", "mrqa_naturalquestions-validation-3112", "mrqa_triviaqa-validation-2627", "mrqa_hotpotqa-validation-4177", "mrqa_searchqa-validation-4844", "mrqa_hotpotqa-validation-4594", "mrqa_newsqa-validation-2281", "mrqa_searchqa-validation-394", "mrqa_triviaqa-validation-5888", "mrqa_naturalquestions-validation-801"], "EFR": 0.9565217391304348, "Overall": 0.7522495502908757}, {"timecode": 71, "before_eval_results": {"predictions": ["The Beatles", "The Andy Griffith Show", "impractical", "the Pocono Mountains", "trip", "watermelon", "a kart", "tanks", "Simon & Garfunkel", "Albert Pujols", "the Andean bear", "ordinal numbers", "nebulae", "Scott McClellan", "Eastwick", "The Who", "Cy Young", "Austin Powers", "lawyers", "conga drums", "a redwood tree", "Nellie Bly", "IBM", "Pizza", "Athol Fugard", "Ricky Martin", "debts", "cloven", "Nicole Kidman", "Aristophanes", "Wimbledon", "Prince Siddhartha Gautama", "RESTRICTIVE", "a monkey", "turquoise", "Papua New Guinea", "Rooster Cogburn", "Halo 3", "Boz", "Sayonara", "touch", "the crescent", "The Moment of Truth", "aardvark", "Harpy", "Henry Fielding", "James A. Garfield", "a duck", "Howie Mandel", "a rocket", "Henry Cavendish", "three", "Olivia", "May 31, 2012", "Syria", "Ida Noddack", "nor\u00f0rvegr", "World War II", "Peter Kay's Car Share", "Oneida Limited", "Zac Efron", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "vicious brutality", "mantle"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7197516025641025}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11101", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-1282", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-2234", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-10201", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-2850", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-5808", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-2382"], "SR": 0.703125, "CSR": 0.5579427083333333, "retrieved_ids": ["mrqa_squad-train-49814", "mrqa_squad-train-41114", "mrqa_squad-train-32063", "mrqa_squad-train-53980", "mrqa_squad-train-7721", "mrqa_squad-train-5934", "mrqa_squad-train-45009", "mrqa_squad-train-32588", "mrqa_squad-train-46278", "mrqa_squad-train-4237", "mrqa_squad-train-67613", "mrqa_squad-train-21788", "mrqa_squad-train-45553", "mrqa_squad-train-32413", "mrqa_squad-train-27868", "mrqa_squad-train-85276", "mrqa_triviaqa-validation-1661", "mrqa_naturalquestions-validation-1439", "mrqa_squad-validation-8910", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2855", "mrqa_searchqa-validation-9173", "mrqa_newsqa-validation-2834", "mrqa_hotpotqa-validation-2793", "mrqa_newsqa-validation-886", "mrqa_triviaqa-validation-398", "mrqa_naturalquestions-validation-1226", "mrqa_squad-validation-1492", "mrqa_newsqa-validation-3873", "mrqa_triviaqa-validation-1936", "mrqa_searchqa-validation-11314", "mrqa_squad-validation-3885"], "EFR": 1.0, "Overall": 0.7613541666666667}, {"timecode": 72, "before_eval_results": {"predictions": ["133", "the United States", "Comoros Islands", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "$2 billion.", "\"Slumdog Millionaire\"", "17", "London's O2 arena", "Joel \"Taz\" DiGregorio,", "second child", "in her home", "forgery and flying without a valid license,", "U.S. 93 in White Hills, Arizona,", "a progressive neurological disease", "64,", "Muslim festival", "figure out a way that was practical to get a drum set on a plane,", "Republicans, for the most part, have stuck with Bush on the war.", "ordered the immediate release", "a satellite.", "President Thabo Mbeki", "cross-country skiers", "Jeff Klein", "Saturn's Christmas parade", "the reality he has seen is \"terrifying.\"", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "Haeftling,", "severe flooding", "Consumer Reports", "Now Zad in Helmand province, Afghanistan.", "he won it with a clear strategy that was stuck to with remarkably little internal drama.", "second", "through Saturday,", "Sen. Barack Obama", "South Africa's president", "the abduction of minors.", "three", "Fernando Caceres", "Joan Rivers.", "the L'Aquila earthquake,", "David Russ,", "country directors", "Tehran, Iran.", "We must eliminate the perceived stigma, shame and dishonor of asking for help,\"", "When the economy turns unfriendly,", "Filippo Inzaghi", "BMW", "the Indian embassy in Kabul", "the bombers", "Mom.", "Dolgorsuren Dagvadorj,", "king", "Session Initiation Protocol", "Ant & Dec", "Robert A. Heinlein", "the ostrich", "friends", "IT", "St. Louis Cardinals", "south-west", "Popcorn", "3", "white elephant", "Lord Robert Cecil"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6473756712037961}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1373", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3088", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-2653", "mrqa_searchqa-validation-8732", "mrqa_hotpotqa-validation-364"], "SR": 0.5625, "CSR": 0.5580051369863014, "retrieved_ids": ["mrqa_squad-train-66690", "mrqa_squad-train-6310", "mrqa_squad-train-60672", "mrqa_squad-train-1365", "mrqa_squad-train-46660", "mrqa_squad-train-72360", "mrqa_squad-train-40208", "mrqa_squad-train-50916", "mrqa_squad-train-77481", "mrqa_squad-train-85621", "mrqa_squad-train-75172", "mrqa_squad-train-59273", "mrqa_squad-train-31177", "mrqa_squad-train-78164", "mrqa_squad-train-12906", "mrqa_squad-train-84594", "mrqa_searchqa-validation-14995", "mrqa_searchqa-validation-2141", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3149", "mrqa_squad-validation-7872", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2272", "mrqa_naturalquestions-validation-7212", "mrqa_newsqa-validation-1950", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5105", "mrqa_newsqa-validation-1118", "mrqa_hotpotqa-validation-5878", "mrqa_searchqa-validation-11056"], "EFR": 1.0, "Overall": 0.7613666523972602}, {"timecode": 73, "before_eval_results": {"predictions": ["$40 and a Bread of bread.", "Justicialist Party,", "\"Vaughn,\"", "Efraim Kam,", "11,", "\"The Sopranos,\"", "25 percent", "March 22,", "President Robert Mugabe intends to rig next week's elections in his favor,", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "OneLegacy,", "upper respiratory infection,\"", "around 10:30 p.m. October 3,", "Arabic, French and English,", "U.N. drug chief.", "\"Maude\" and the sardonic Dorothy on \"The Golden Girls,\"", "Cal Ripken Jr.", "the several dogs that played him.", "Jewish tradition", "The Da Vinci Code", "J.G. Ballard,", "protective shoes", "17", "2008.", "12 hours", "Now Zad in Helmand province, Afghanistan.", "Russia and some European countries have expressed concerns about the missile defense system. While Poland and the Czech Republic have agreed to host parts of the system, others in Europe share Russian concerns that the defensive shield could be used for offensive aims.", "American", "the Los Alamitos Joint Forces Training Base", "Al-Shabaab,", "safer surroundings.", "A McCain spokesman attacked Obama's plan, saying the Democrat's \"agenda to raise taxes and isolate America from foreign markets will not get our economy back on track or create new jobs.\"", "upper respiratory infection,\"", "Ronald F. Ferguson", "Sri Lanka", "his efforts to help male veterans struggling with homelessness and addiction.", "1,073 immigration detainees", "nearly $162 billion in war funding", "scientific reasons.", "innovative, exciting skyscrapers", "concentration camps,", "Pakistani territory", "hanged in 1979 for the murder of a political opponent two years after he was ousted as prime minister in a military coup.", "Pew Research Center", "self-styled revolutionary Symbionese Liberation Army -- perhaps best known for kidnapping Patricia Hearst --", "38,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help,\"", "speed attempts", "Leo Frank,", "At least 38", "Iran", "in Egypt", "Philadelphia, which is Greek for brotherly love", "September 2017", "Dr John Sentamu", "Tony Cozier", "Sheffield", "Dulwich", "New York Giants", "South America", "Vulcan", "Beloved", "a flapjack", "Hungarian"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6373681594673442}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false], "QA-F1": [0.8000000000000002, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.44000000000000006, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.4, 1.0, 0.25, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2763", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-6111", "mrqa_searchqa-validation-1383"], "SR": 0.53125, "CSR": 0.5576435810810811, "retrieved_ids": ["mrqa_squad-train-14785", "mrqa_squad-train-26787", "mrqa_squad-train-74523", "mrqa_squad-train-71861", "mrqa_squad-train-23115", "mrqa_squad-train-80882", "mrqa_squad-train-46402", "mrqa_squad-train-81540", "mrqa_squad-train-4609", "mrqa_squad-train-33444", "mrqa_squad-train-55524", "mrqa_squad-train-9987", "mrqa_squad-train-52077", "mrqa_squad-train-80510", "mrqa_squad-train-50697", "mrqa_squad-train-74615", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-7382", "mrqa_triviaqa-validation-6249", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-4019", "mrqa_squad-validation-8400", "mrqa_naturalquestions-validation-7837", "mrqa_hotpotqa-validation-733", "mrqa_triviaqa-validation-2789", "mrqa_triviaqa-validation-285", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-6207", "mrqa_squad-validation-8065", "mrqa_naturalquestions-validation-3358", "mrqa_squad-validation-3270", "mrqa_triviaqa-validation-5834"], "EFR": 0.9666666666666667, "Overall": 0.7546276745495495}, {"timecode": 74, "before_eval_results": {"predictions": ["South Korea's", "the couple's surrogate", "16", "the program was made with the parents' full consent.", "five", "Marie-Therese Walter.", "Sixteen", "1979", "promised federal help for those affected by the fires.", "consumer confidence", "$81,88010", "Climate Care,", "a Florida girl", "state senators", "August 19, 2007.", "Ghana", "6,000", "1-0", "Brian Smith.", "Barack Obama sent a message that fight against terror will respect America's values.", "to provide security as needed.", "Sunday", "a city of romance, of incredible architecture and history. Some people call it the \"golden city,\"", "1,500 Marines", "hopes the journalists and the flight crew will be freed,", "Bill Haas", "Silicon Valley.", "people switched from the very bad category to the pretty bad category,", "five", "British troops", "Cash for Clunkers", "Yemen,", "the Dalai Lama", "800,000", "carving", "Russia", "one", "Democratic VP candidate", "former U.S. soldier Steven Green", "Cologne's archive building", "President George H.W. Bush", "Kurdish militant group in Turkey", "6-4 loss,", "Karen Floyd", "St Petersburg and Moscow,", "flights affected", "the defending champions were held to a 1-1 draw at Stoke City.", "Bahrain's", "Alaska or Hawaii.", "30", "repeal of the military's \"don't ask, don't tell\" policy", "Dollree Mapp", "Robert Hooke", "the American Civil War", "giraffe", "climbing", "World Health Organization", "Dallas/Fort Worth Metroplex", "Robert Matthew Hurley", "Ashanti Region", "Michael Kors", "Katherine Heigl", "Lenin", "italy"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6359172077922077}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-258", "mrqa_triviaqa-validation-280", "mrqa_triviaqa-validation-2282", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-5300", "mrqa_triviaqa-validation-6077"], "SR": 0.546875, "CSR": 0.5575, "retrieved_ids": ["mrqa_squad-train-42584", "mrqa_squad-train-1109", "mrqa_squad-train-39453", "mrqa_squad-train-41170", "mrqa_squad-train-76756", "mrqa_squad-train-74354", "mrqa_squad-train-24201", "mrqa_squad-train-16147", "mrqa_squad-train-28796", "mrqa_squad-train-602", "mrqa_squad-train-55270", "mrqa_squad-train-57826", "mrqa_squad-train-77011", "mrqa_squad-train-51017", "mrqa_squad-train-30180", "mrqa_squad-train-18972", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-8350", "mrqa_searchqa-validation-15830", "mrqa_naturalquestions-validation-2190", "mrqa_hotpotqa-validation-893", "mrqa_searchqa-validation-2858", "mrqa_naturalquestions-validation-2692", "mrqa_triviaqa-validation-7083", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2472", "mrqa_triviaqa-validation-458", "mrqa_hotpotqa-validation-831", "mrqa_naturalquestions-validation-2462", "mrqa_hotpotqa-validation-203"], "EFR": 0.9310344827586207, "Overall": 0.7474725215517241}, {"timecode": 75, "before_eval_results": {"predictions": ["bermBRIDGE-BORN Bear Grylls", "Reichsmark", "Adam Faith", "the Elbe", "New South Wales", "jennifer pierce", "Stockholm", "Leo", "Henry VIII", "a power outage", "jodie Foster", "Pluto", "Deep Blue", "Sienna", "13 March", "whole fish, salt, and water", "the duke of Monmouth", "Isaac", "shekel", "serbia", "The Lost Weekend", "althorp", "the sun", "June", "Persuasion", "Robert Taylor", "the AllStars", "Hannibal", "Norman Mailer", "fishes", "floating", "Florence", "gold hallmarks", "pascal", "baroudeur", "carole king", "11", "Israel", "football", "The Lady of the Lake", "bajan", "Gentlemen Prefer Blondes", "puppies", "Kenya", "Conrad Murray", "rocket", "elia Earhart", "spinach", "John Gorman", "Benedict XVI", "cirrocumulus", "Ariana Clarice Richards", "Turner Layton", "The Parlement de Bretagne", "Campbellsville University", "Daniel Craig", "Vanilla Air", "The cervical cancer vaccine, approved in 2006,", "\"This is not something that anybody can reasonably anticipate,\"", "21 percent suggesting that", "bottle", "Kansas", "Parkinson\\'s", "John Gotti"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6125}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5621", "mrqa_triviaqa-validation-5929", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-771", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-4874", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7059", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-3591", "mrqa_naturalquestions-validation-7021", "mrqa_hotpotqa-validation-684", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-10356", "mrqa_searchqa-validation-3069"], "SR": 0.546875, "CSR": 0.557360197368421, "retrieved_ids": ["mrqa_squad-train-66979", "mrqa_squad-train-50771", "mrqa_squad-train-20944", "mrqa_squad-train-57566", "mrqa_squad-train-77080", "mrqa_squad-train-27149", "mrqa_squad-train-83153", "mrqa_squad-train-11613", "mrqa_squad-train-55913", "mrqa_squad-train-80093", "mrqa_squad-train-31703", "mrqa_squad-train-6753", "mrqa_squad-train-65964", "mrqa_squad-train-11171", "mrqa_squad-train-75240", "mrqa_squad-train-39266", "mrqa_newsqa-validation-1549", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-2315", "mrqa_naturalquestions-validation-1882", "mrqa_triviaqa-validation-280", "mrqa_newsqa-validation-652", "mrqa_naturalquestions-validation-8350", "mrqa_hotpotqa-validation-2933", "mrqa_naturalquestions-validation-321", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-484", "mrqa_triviaqa-validation-10", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-6562", "mrqa_naturalquestions-validation-4365", "mrqa_triviaqa-validation-2456"], "EFR": 0.9655172413793104, "Overall": 0.7543411127495462}, {"timecode": 76, "before_eval_results": {"predictions": ["Steve Jobs", "Les Bleus", "Islamic", "Cpl. Richard Findley,", "two women", "seven", "Vicente Carrillo Leyva,", "suicide car bombing", "Illinois Reform Commission", "the \"People of Palestine\" and calls on them to fight back against Israel in Gaza.", "high tide", "The recent violence -- which has included attacks on pipelines and hostage-taking -- has limited shipment of crude oil supplies out of Nigeria,", "183 people, including 137 children,", "80,", "Lisa Brown", "Bob Bogle,", "back at work", "Mexico", "The Rev. Alberto Cutie", "California, Texas and Florida,", "3-3 draw", "Iran of trying to build nuclear bombs,", "\"There is no imminent threat of the government being overthrown, but the Taliban has gained momentum,\"", "dancing", "this week's Australian Open,", "Jenny Sanford,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Sheikh Sharif Sheikh Ahmed", "8,", "that Toyota vehicles are safe,\"", "bipartisan", "Haiti", "Mashhad", "10 percent", "Buenos Aires.", "U.S. Navy", "Dennis Davern,", "Oaxacan", "American third seed Venus Williams", "Aravane Rezai", "7.0-magnitude earthquake sent a quarter-mile pier crumbling into the sea along with two of his trucks.", "\"the most dangerous precedent in this country, violating all of our due process rights.\"", "15,000", "the wife of Gov. Mark Sanford,", "on vacation", "The father of Haleigh Cummings,", "\"I am sick of life -- what can I say to you?\"", "E. coli", "southwestern Mexico,", "1,500", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "Jyotirindra Basu", "60 by West All - Stars", "Homer Banks, Carl Hampton and Raymond Jackson", "leeds", "ethelbald", "seattle", "strings", "Xherdan Shaqiri", "1958", "William Tecumseh Sherman", "Lincoln", "air pressure", "eucritta"], "metric_results": {"EM": 0.5, "QA-F1": 0.63025357681268}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.8, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.08333333333333333, 1.0, 0.0, 1.0, 0.625, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.88, 0.08695652173913043, 0.5, 0.28571428571428575, 1.0, 0.3333333333333333, 0.3076923076923077, 1.0, 0.8, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-990", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-780", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-4796", "mrqa_triviaqa-validation-5828", "mrqa_hotpotqa-validation-1902", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-12792", "mrqa_triviaqa-validation-1454"], "SR": 0.5, "CSR": 0.5566152597402597, "retrieved_ids": ["mrqa_squad-train-6259", "mrqa_squad-train-78250", "mrqa_squad-train-12315", "mrqa_squad-train-52444", "mrqa_squad-train-26994", "mrqa_squad-train-84418", "mrqa_squad-train-52375", "mrqa_squad-train-59374", "mrqa_squad-train-84257", "mrqa_squad-train-56927", "mrqa_squad-train-29017", "mrqa_squad-train-64917", "mrqa_squad-train-34070", "mrqa_squad-train-13335", "mrqa_squad-train-42461", "mrqa_squad-train-81180", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-4999", "mrqa_newsqa-validation-808", "mrqa_naturalquestions-validation-7827", "mrqa_newsqa-validation-686", "mrqa_searchqa-validation-7242", "mrqa_naturalquestions-validation-4454", "mrqa_triviaqa-validation-3136", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-284", "mrqa_searchqa-validation-8487", "mrqa_triviaqa-validation-5342", "mrqa_hotpotqa-validation-5799", "mrqa_squad-validation-8918", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-413"], "EFR": 0.96875, "Overall": 0.754838676948052}, {"timecode": 77, "before_eval_results": {"predictions": ["Bill Haas", "a team of eight surgeons", "more than 200.", "the forward's lawyer", "American Civil Liberties Union", "Misty Cummings, then known asMisty Croslin,", "Dr. Death in Germany", "Ralph Lauren", "Charlie Chaplin", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "Columbia", "one", "Jenny Sanford,", "London and Buenos Aires", "hundreds", "more than 78,000 parents", "St. Francis De Sales Catholic Church", "The Israeli Navy", "Buddhism", "in Seoul,", "President Obama", "the United States", "strangulation and asphyxiation and had two broken bones in his neck,", "John and Elizabeth Calvert", "education", "terrorize", "Latvia", "Michael Jackson", "Brazil", "Pixar's", "Pixar's", "1,500", "racially-tinged remark made by his former caddy,", "Nechirvan Barzani,", "fascinating transformation that takes place when carving a pumpkin.", "2-0", "Transport Workers Union leaders", "Amanda Knox's aunt", "in the Muslim north of Sudan", "Melbourne", "cancer", "unspecified threat to disrupt the inauguration,", "the Impeccable,", "338", "finance", "Haeftling,", "bribing other wrestlers to lose bouts,", "Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "outfit", "a slave labor camp", "\"Taz\" DiGregorio,", "active absorption", "Robin", "17 -- 15", "bible school", "the Sulu Archipelago", "Egyptian", "The satirical", "Valhalla Highlands Historic District", "boxer", "push", "rain", "Marsalis", "argon"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5366009424603174}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.125, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2633", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3310", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-699", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-4358"], "SR": 0.453125, "CSR": 0.5552884615384616, "retrieved_ids": ["mrqa_squad-train-82778", "mrqa_squad-train-36149", "mrqa_squad-train-64868", "mrqa_squad-train-37609", "mrqa_squad-train-12892", "mrqa_squad-train-34795", "mrqa_squad-train-27112", "mrqa_squad-train-83117", "mrqa_squad-train-6044", "mrqa_squad-train-85480", "mrqa_squad-train-36393", "mrqa_squad-train-73096", "mrqa_squad-train-46146", "mrqa_squad-train-75583", "mrqa_squad-train-49303", "mrqa_squad-train-35746", "mrqa_triviaqa-validation-5250", "mrqa_naturalquestions-validation-716", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-4112", "mrqa_searchqa-validation-7187", "mrqa_hotpotqa-validation-502", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-3897", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-7624", "mrqa_hotpotqa-validation-203", "mrqa_searchqa-validation-793", "mrqa_newsqa-validation-2325", "mrqa_searchqa-validation-3274", "mrqa_searchqa-validation-10920", "mrqa_triviaqa-validation-58"], "EFR": 0.9714285714285714, "Overall": 0.7551090315934067}, {"timecode": 78, "before_eval_results": {"predictions": ["3-2", "Aung San Suu Kyi", "1957", "North Carolina.", "emergency plans", "South Africa", "Al-Shabaab,", "black is beautiful,\"", "assassination of President Mohamed Anwar al-Sadat at the hands of four military officers", "Diego Milito's", "11 countries,", "Robert Barnett,", "More than 22 million", "a senior at Stetson University studying computer science.", "volatile and dangerous.", "did not", "200 human bodies at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "Phillip A. Myers.", "Congress", "bipartisan", "not showing up at work with no notice.", "1912,", "Steven Green", "an Airbus A320-214,", "Casey Anthony,", "Lifeway's 100-plus stores nationwide", "clothes that are consistent and accessible.", "some deaths", "Jeff Burlingame,", "Christina Romete,", "Siri.", "\"The Real Housewives of Atlanta\"", "independent homeland", "\"The situation is pretty much resolved,\"", "Iowa,", "Barbara Streisand's", "the death of a pregnant soldier", "raping", "five", "Asashoryu,", "Bob Johnson", "\u00a341.1 million", "the sanctuary", "natural disasters", "Joe Jackson,", "red", "the inspector-general", "the peace with Israel that Sadat worked steadily to achieve may be at the brink of collapse.", "John Demjanjuk,", "following in Arizona's footsteps would take states in the wrong direction.", "seven", "a couple broken apart by the Iraq War", "antimeridian", "instructions", "undertones", "Johnny Mercer", "Fenn Street School", "acting", "the Vietnam War", "1967", "Ukraine", "capitals", "Scott Fitzgerald", "genome"], "metric_results": {"EM": 0.546875, "QA-F1": 0.634084145021645}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.8, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0909090909090909, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2118", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9852", "mrqa_triviaqa-validation-6200", "mrqa_hotpotqa-validation-5128", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-10570"], "SR": 0.546875, "CSR": 0.5551819620253164, "retrieved_ids": ["mrqa_squad-train-37067", "mrqa_squad-train-43796", "mrqa_squad-train-64293", "mrqa_squad-train-73840", "mrqa_squad-train-40299", "mrqa_squad-train-19015", "mrqa_squad-train-69758", "mrqa_squad-train-23717", "mrqa_squad-train-80415", "mrqa_squad-train-68391", "mrqa_squad-train-10688", "mrqa_squad-train-26585", "mrqa_squad-train-5937", "mrqa_squad-train-19958", "mrqa_squad-train-27382", "mrqa_squad-train-75422", "mrqa_newsqa-validation-990", "mrqa_triviaqa-validation-3086", "mrqa_searchqa-validation-7374", "mrqa_naturalquestions-validation-1186", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-7708", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-9410", "mrqa_squad-validation-7700", "mrqa_searchqa-validation-14580", "mrqa_squad-validation-8918", "mrqa_naturalquestions-validation-6166", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-11194", "mrqa_triviaqa-validation-4208"], "EFR": 0.9310344827586207, "Overall": 0.7470089139567874}, {"timecode": 79, "before_eval_results": {"predictions": ["Dick Rutan and Jeana Yeager", "20 years from the filing date", "The eighth and final season of the fantasy drama television series Game of Thrones", "Rigg", "March 2016", "`` Glory '' is one of the most common words in scripture", "the name announcement of Kylie Jenner's first child", "2003", "small orange collection boxes", "seven", "February 9, 2018", "Coroebus of Elis", "Jonathan Breck", "George Harrison", "Virgil Tibbs", "Ancy Lostoma duodenale", "January 17, 1899", "UTC \u2212 09 : 00", "Thespis", "biological taxonomy", "Wednesday, September 21, 2016", "Robber baron", "Eight full seasons", "Tbilisi, Georgia", "off the rez", "1895", "King Harold Godwinson", "Sir Ronald Ross", "May 29, 2018 on NBC", "1979", "fourth season", "ancient Athens", "Russia", "observing the magnetic stripe `` anomalies '' on the ocean floor", "United Nations", "2018", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "Real Madrid", "Angel Benitez", "capillary action", "1830", "Louis Mountbatten", "foreign investors", "Bhupendranath Dutt", "1932", "775 rooms", "By mid-1988", "senators", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Paul Lynde", "electric potential generated by muscle cells when these cells are electrically or neurologically activated", "the treaty of Waitangi", "James Garfield", "gambia", "England", "Sverdlovsk", "47,818", "seven", "Samoa", "to close their shops during daily prayers,", "Versailles", "Westminster College", "Bob Dylan", "Mount Pelee"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7311537369901341}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [0.5714285714285715, 0.9090909090909091, 0.2857142857142857, 1.0, 1.0, 0.0, 0.7999999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.5, 0.5, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-10640", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-7848", "mrqa_triviaqa-validation-5704", "mrqa_searchqa-validation-13149", "mrqa_searchqa-validation-13773"], "SR": 0.59375, "CSR": 0.5556640625, "retrieved_ids": ["mrqa_squad-train-69840", "mrqa_squad-train-56241", "mrqa_squad-train-66516", "mrqa_squad-train-4429", "mrqa_squad-train-75120", "mrqa_squad-train-69954", "mrqa_squad-train-20985", "mrqa_squad-train-46245", "mrqa_squad-train-31536", "mrqa_squad-train-21903", "mrqa_squad-train-2563", "mrqa_squad-train-86435", "mrqa_squad-train-75237", "mrqa_squad-train-60284", "mrqa_squad-train-20799", "mrqa_squad-train-26866", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-2472", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-847", "mrqa_squad-validation-8864", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-3425", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-3164", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-3072", "mrqa_searchqa-validation-9123", "mrqa_squad-validation-1272", "mrqa_hotpotqa-validation-733", "mrqa_hotpotqa-validation-4054", "mrqa_triviaqa-validation-726"], "EFR": 0.8461538461538461, "Overall": 0.7301292067307692}, {"timecode": 80, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11088", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.87890625, "KG": 0.49453125, "before_eval_results": {"predictions": ["the 12th century", "14 December 1972 UTC", "China", "1919", "Abraham Gottlob Werner", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "near the Afghan - Pakistan border", "As of September, 2016", "July 2014", "an enzyme that breaks down peroxides may be called peroxidase", "at a given temperature", "Skat", "Claudia Grace Wells", "Bill Russell", "the British East India company", "the Reverse - Flash", "in cell - mediated, cytotoxic innate immunity )", "Christopher Lloyd", "four", "1923", "San Francisco, California", "2018", "Lilian Bellamy", "the Kansas City Chiefs", "The Vamps", "1997", "William Wyler", "Ray Harroun", "one person", "the NIRA", "Santa Fe, New Mexico", "Kaley Christine Cuoco", "Marshall Sahlins", "Easter", "the United Kingdom", "the Gupta Empire", "the oculus, or `` eye point ''", "James Madison", "Matt Monro", "Coton in the Elms", "Angola", "June 1992", "prokaryotic", "hosted by Brazil", "the English", "Inti, the sun god of the Inca religion", "Jason Flemyng", "blue", "an expression of unknown origin", "the NW North American Plate", "1960", "Kaiser Chiefs", "Donna Jo Napoli", "Claire goose", "Stephen Richards Covey", "jazz homeland section of New Orleans and on that part of the South in particular", "Earvin \"Magic\" Johnson Jr.", "President Sheikh Sharif Sheikh Ahmed", "a Burmese python", "posting a $1,725 bail,", "Yitzhak Rabin", "spinal stenosis", "1936", "carbon"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6832251082251082}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5714285714285715, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.6666666666666666, 0.38095238095238093, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-5711", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-5995", "mrqa_triviaqa-validation-4432", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2802", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-1713", "mrqa_searchqa-validation-11202"], "SR": 0.609375, "CSR": 0.5563271604938271, "retrieved_ids": ["mrqa_squad-train-31035", "mrqa_squad-train-47512", "mrqa_squad-train-62610", "mrqa_squad-train-8179", "mrqa_squad-train-61222", "mrqa_squad-train-44921", "mrqa_squad-train-63845", "mrqa_squad-train-58815", "mrqa_squad-train-3015", "mrqa_squad-train-36554", "mrqa_squad-train-28963", "mrqa_squad-train-54619", "mrqa_squad-train-78609", "mrqa_squad-train-13395", "mrqa_squad-train-52347", "mrqa_squad-train-84947", "mrqa_naturalquestions-validation-4740", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-6998", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2351", "mrqa_searchqa-validation-12533", "mrqa_naturalquestions-validation-5007", "mrqa_newsqa-validation-1159", "mrqa_triviaqa-validation-2789", "mrqa_newsqa-validation-2179", "mrqa_triviaqa-validation-31", "mrqa_newsqa-validation-2693", "mrqa_naturalquestions-validation-2452", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-1242"], "EFR": 1.0, "Overall": 0.7429841820987655}, {"timecode": 81, "before_eval_results": {"predictions": ["William the Conqueror", "1963", "Sachin Tendulkar and Kumar Sangakkara", "Frederick Chiluba, Levy Mwanawasa, Rupiah Banda, Michael Sata, and current President Edgar Lungu", "Ray Charles", "in AD 95 -- 110", "HTML", "in the 1970s", "Dan Stevens", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "Idaho", "Tatsumi", "1969", "Ethiopia and Liberia", "1937", "artes liberales", "Paul Lynde", "Mediterranean Shipping Company S.A.", "2020", "2007", "Hodel", "the coffee shop Monk's", "Siddharth Arora / Vibhav Roy", "1991", "From 1900", "under the National September 11 Memorial plaza", "Munich, Bavaria", "Steve Russell", "October 2, 2017", "Paul Lynde", "to regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Frank Oz", "Sam Waterston", "15 May 2004", "Johnny Darrell", "in the 7th century at Rendlesham in East Anglia", "2018", "Ed Roland", "Ben Rosenbaum", "2013", "two", "Chris Martin", "Potter Potter and the Chamber of Secrets", "Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluated", "Jackie Robinson", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "Brazil, Turkey and Uzbekistan", "New York City west to Lincoln Park in San Francisco", "two parallel planes", "The pia mater", "United Nations Peacekeeping Operations", "the kinks", "cornwall", "the Indus Valley civilization", "Interstate 95", "lawology", "infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "violating anti-trust laws.", "Lindsey Vonn", "$55.7 million", "Hipparchus", "Dixie", "San Salvador", "Snickers candy bars"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6590748889268626}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.8571428571428571, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.7999999999999999, 0.4, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.4210526315789474, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-5862", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-1861", "mrqa_newsqa-validation-3156", "mrqa_searchqa-validation-10515", "mrqa_triviaqa-validation-7778"], "SR": 0.484375, "CSR": 0.5554496951219512, "retrieved_ids": ["mrqa_squad-train-81003", "mrqa_squad-train-36603", "mrqa_squad-train-10910", "mrqa_squad-train-81258", "mrqa_squad-train-23706", "mrqa_squad-train-75699", "mrqa_squad-train-11389", "mrqa_squad-train-63277", "mrqa_squad-train-27835", "mrqa_squad-train-2919", "mrqa_squad-train-360", "mrqa_squad-train-4714", "mrqa_squad-train-36816", "mrqa_squad-train-55291", "mrqa_squad-train-86526", "mrqa_squad-train-13144", "mrqa_naturalquestions-validation-2666", "mrqa_hotpotqa-validation-4054", "mrqa_naturalquestions-validation-10140", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-805", "mrqa_naturalquestions-validation-8350", "mrqa_hotpotqa-validation-364", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-4767", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-3419", "mrqa_triviaqa-validation-575", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-6261", "mrqa_hotpotqa-validation-5360", "mrqa_searchqa-validation-5547"], "EFR": 0.9393939393939394, "Overall": 0.7306874769031781}, {"timecode": 82, "before_eval_results": {"predictions": ["Four Seasons", "Kathleen Danielle Walsh", "1603", "Stephen Graham", "in 2010", "Tulsa, Oklahoma", "beta decay", "ZZ Top", "1998", "Thomas Jefferson", "Ford", "Grisha Alekandrovich Nikolaev", "Christian Bale", "manet", "July 2, 1928", "Dottie West", "northern China", "Blood is the New Black", "1977", "arm", "Professor Eobard Thawne", "Jack Nicklaus", "arm regions", "St. John's, Newfoundland and Labrador", "by the early - to - mid fourth century", "a young husband and wife", "May 29, 2018", "Church of England", "September 27, 2017", "April 10, 2018", "Daniel Suarez", "six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "January 15, 2007", "they each supported major regional wars known as proxy wars", "optic chiasma", "Empire of Japan", "The Bangladesh -- India border", "Seven nations", "Jenny Slate", "2013", "Prince William, Duke of Cambridge, the Prince of Wales's elder son", "Nicklaus", "1904", "Peter Greene", "Matt Monro", "a little warmth", "ADP and P", "November 28, 1973", "when energy from light is absorbed by proteins called reaction centres", "in 336", "September 30", "Charles Darwin", "Jon Stewart", "Fiat", "five", "Wanda", "April 24, 1934", "40", "49,", "$2 billion", "Miranda Hobbes", "coffee", "Bon Jovi", "oldham"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7252036888550046}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727272, 0.9090909090909091, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9473684210526316, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7692307692307692, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9687", "mrqa_hotpotqa-validation-1812", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2444", "mrqa_searchqa-validation-13906", "mrqa_searchqa-validation-12962"], "SR": 0.5625, "CSR": 0.5555346385542168, "retrieved_ids": ["mrqa_squad-train-55160", "mrqa_squad-train-34144", "mrqa_squad-train-12313", "mrqa_squad-train-40219", "mrqa_squad-train-31486", "mrqa_squad-train-56705", "mrqa_squad-train-10878", "mrqa_squad-train-28986", "mrqa_squad-train-38760", "mrqa_squad-train-40457", "mrqa_squad-train-9770", "mrqa_squad-train-39970", "mrqa_squad-train-72952", "mrqa_squad-train-41467", "mrqa_squad-train-42497", "mrqa_squad-train-46640", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-2438", "mrqa_triviaqa-validation-2293", "mrqa_naturalquestions-validation-686", "mrqa_squad-validation-1909", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-10640", "mrqa_squad-validation-6046", "mrqa_triviaqa-validation-7694", "mrqa_naturalquestions-validation-5711", "mrqa_newsqa-validation-1072", "mrqa_hotpotqa-validation-5098", "mrqa_triviaqa-validation-4767", "mrqa_searchqa-validation-4216", "mrqa_triviaqa-validation-2091", "mrqa_searchqa-validation-448"], "EFR": 0.8571428571428571, "Overall": 0.7142542491394148}, {"timecode": 83, "before_eval_results": {"predictions": ["American 3D computer-animated comedy film", "1964", "Hindi", "eight", "Dirk Werner Nowitzki", "Oklahoma", "Swiss", "(foaled February 1, 1999)", "Congo River", "4,613", "George Washington Bridge", "H. R. Haldeman", "Guardians of the Galaxy Vol.  2", "2.1 million", "6'5\" and 190 pounds", "526", "North West England", "casting, job opportunities, and career advice", "Saint Elgiva", "Scottish", "2006", "Carrefour", "infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "Stravinsky's \"The Rite of Spring\"", "Manchester United", "in the second half of the third season", "Johnson & Johnson", "novelty songs, comedy, and strange or unusual recordings", "Brown Mountain Lights", "Backstreet Boys", "45th", "Foxborough, Massachusetts", "Wichita", "animation", "Sir John Major", "sandstone", "Reginald Engelbach", "Dutch", "BC Dz\u016bkija", "Anne", "Sir Derek George Jacobi", "Rebirth", "Big Mamie", "was one of the founding members of The Pogues", "Maxwell Atoms", "1903", "King of France", "Alemannic", "Larry Alphonso Johnson Jr.", "Matthieu Vaxivi\u00e8re", "Taylor Swift", "October 27, 2016", "meat technology", "the Tigris and Euphrates rivers", "Mozambique Channel", "Astor", "Tashkent", "Afghanistan,", "police", "a hunter", "Speed Racer", "Yogi Bear", "the cherry", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6763762507594745}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4210526315789474, 0.0, 0.0, 0.9090909090909091, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.888888888888889]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-138", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1512", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-4696", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2680", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-3680", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5496", "mrqa_naturalquestions-validation-7393"], "SR": 0.578125, "CSR": 0.5558035714285714, "retrieved_ids": ["mrqa_squad-train-7296", "mrqa_squad-train-64875", "mrqa_squad-train-81465", "mrqa_squad-train-66634", "mrqa_squad-train-36769", "mrqa_squad-train-56531", "mrqa_squad-train-71196", "mrqa_squad-train-21581", "mrqa_squad-train-52185", "mrqa_squad-train-24865", "mrqa_squad-train-31328", "mrqa_squad-train-13212", "mrqa_squad-train-17853", "mrqa_squad-train-56293", "mrqa_squad-train-24648", "mrqa_squad-train-9336", "mrqa_searchqa-validation-8081", "mrqa_naturalquestions-validation-5476", "mrqa_squad-validation-4848", "mrqa_triviaqa-validation-5769", "mrqa_squad-validation-7888", "mrqa_squad-validation-3958", "mrqa_searchqa-validation-1406", "mrqa_searchqa-validation-9089", "mrqa_naturalquestions-validation-8444", "mrqa_newsqa-validation-2723", "mrqa_naturalquestions-validation-1969", "mrqa_triviaqa-validation-4893", "mrqa_newsqa-validation-2811", "mrqa_triviaqa-validation-4432", "mrqa_hotpotqa-validation-1584", "mrqa_newsqa-validation-2735"], "EFR": 1.0, "Overall": 0.7428794642857143}, {"timecode": 84, "before_eval_results": {"predictions": ["As of January 17, 2018, 201 episodes", "Atlanta", "1872", "1978", "Hodel", "Central Germany", "Kida", "the Outfield", "William Shakespeare's As You Like It", "in Christian eschatology", "Robin Williams", "artes liberales", "the sinoatrial node", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "Ellen is restored to life and is married to Bobby", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "December 11, 2014", "A vanishing point", "Janie Crawford", "UTC \u2212 09 : 00", "mining", "Scottish post-punk band Orange Juice", "blood plasma and lymph in the `` intravascular compartment ''", "39 %", "Missouri River", "iOS, watchOS, and tvOS", "Jason Marsden", "The Paris Sisters", "Linda Davis", "Fats Waller", "Aristotle", "Bill Belichick", "May 19, 2017", "season two", "south of Newfoundland", "the status line", "Samantha Jo `` Mandy '' Moore", "Dr. Lexie Grey", "Nodar Kumaritashvili", "TLC", "Welch, West Virginia", "Nicki Minaj", "Steve Russell", "Julia Roberts", "the church at Philippi", "control purposes", "to feed the poor, and would relieve some pressure of the land redistribution process", "Hotel barge", "The Lykan Hypersport", "five seasons", "Geothermal gradient", "soft contact lenses", "zak Starkey", "fire insurance", "Seoul, South Korea", "Saturday", "Al Capone", "14 years", "ambassadors", "Russia", "a bin", "the Equator", "Tiberius", "There's no chance"], "metric_results": {"EM": 0.59375, "QA-F1": 0.719935273060273}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.7692307692307692, 0.3333333333333333, 1.0, 1.0, 1.0, 0.7000000000000001, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-1767", "mrqa_triviaqa-validation-6303", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-16205", "mrqa_newsqa-validation-2213"], "SR": 0.59375, "CSR": 0.55625, "retrieved_ids": ["mrqa_squad-train-49521", "mrqa_squad-train-36708", "mrqa_squad-train-284", "mrqa_squad-train-54937", "mrqa_squad-train-13112", "mrqa_squad-train-9868", "mrqa_squad-train-53016", "mrqa_squad-train-50591", "mrqa_squad-train-80551", "mrqa_squad-train-72880", "mrqa_squad-train-27833", "mrqa_squad-train-75942", "mrqa_squad-train-77168", "mrqa_squad-train-14978", "mrqa_squad-train-41520", "mrqa_squad-train-8848", "mrqa_naturalquestions-validation-7624", "mrqa_newsqa-validation-585", "mrqa_hotpotqa-validation-2220", "mrqa_triviaqa-validation-2460", "mrqa_searchqa-validation-16168", "mrqa_newsqa-validation-3339", "mrqa_naturalquestions-validation-10495", "mrqa_newsqa-validation-566", "mrqa_triviaqa-validation-4173", "mrqa_naturalquestions-validation-3269", "mrqa_hotpotqa-validation-1090", "mrqa_naturalquestions-validation-3591", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-6642", "mrqa_naturalquestions-validation-2387", "mrqa_newsqa-validation-3909"], "EFR": 0.9230769230769231, "Overall": 0.7275841346153846}, {"timecode": 85, "before_eval_results": {"predictions": ["the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "the uvea", "25 June 1932", "Felicity Huffman", "a site for genetic transcription that is segregated from the location of translation in the cytoplasm, allowing levels of gene regulation that are not available to prokaryotes", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Roger Nichols and Paul Williams", "peninsular", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "a large, high - performance luxury coupe", "eleven separate regions of the Old and New World", "As of January 17, 2018, 201 episodes", "the Tennessee Titans", "Monk's", "B.R. Ambedkar", "the septum", "Cecil Lockhart", "Executive Residence of the White House Complex", "David Motl", "invertebrates", "far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "a hexamer in secretory vesicles", "New Croton Reservoir in Westchester and Putnam counties", "Detective Eddie Thawne", "Freddie Highmore", "the closing scene of the final episode of the first season", "1939", "74", "during prenatal development in the central part of each developing bone", "Zedekiah", "A footling breech", "Robert Gillespie Adamson IV", "Waylon Jennings", "A rear - view mirror", "brainstem", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Doug Diemoz", "$19.8 trillion", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "Frank Zappa", "1956", "Camping World Stadium in Orlando, Florida", "Louis XV", "the final episode of the series", "Jonathan Breck", "Jody Rosen of Rolling Stone", "British Kennel Clubs", "Wilt Chamberlain", "James Chadwick", "Olympia", "Wyatt `` Dusty '' Chandler ( George Strait )", "Swiss", "trademark", "Las Vegas", "Hindi", "Brookhaven", "January 2004", "space shuttle Discovery", "dismissed all charges", "\"Empire of the Sun,\"", "frittatas", "a crabitat", "a parking meter", "matlock"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7365655178155178}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5384615384615384, 0.0, 0.36363636363636365, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6485", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-7912", "mrqa_searchqa-validation-9160", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-10904"], "SR": 0.671875, "CSR": 0.557594476744186, "retrieved_ids": ["mrqa_squad-train-8342", "mrqa_squad-train-77496", "mrqa_squad-train-84037", "mrqa_squad-train-15837", "mrqa_squad-train-70219", "mrqa_squad-train-71516", "mrqa_squad-train-57390", "mrqa_squad-train-43172", "mrqa_squad-train-40648", "mrqa_squad-train-7406", "mrqa_squad-train-67713", "mrqa_squad-train-17702", "mrqa_squad-train-49725", "mrqa_squad-train-13726", "mrqa_squad-train-31", "mrqa_squad-train-75804", "mrqa_searchqa-validation-0", "mrqa_naturalquestions-validation-8171", "mrqa_searchqa-validation-14995", "mrqa_newsqa-validation-477", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-1338", "mrqa_newsqa-validation-2886", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-280", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-92", "mrqa_naturalquestions-validation-4288", "mrqa_searchqa-validation-6371", "mrqa_newsqa-validation-1981", "mrqa_hotpotqa-validation-138"], "EFR": 0.9047619047619048, "Overall": 0.7241900263012182}, {"timecode": 86, "before_eval_results": {"predictions": ["Los Angeles", "a dancing twins", "Mikhail Baryshnikov", "filibuster", "Christo", "Top Gun", "Cyrus Hall McCormick", "the Ziegfeld Girl", "ballpoint pen", "the tabernacle", "the butterfly life cycle", "tequila", "the Ottawa River", "Einstein", "a man's", "a roller ride", "George H.W. Bush's", "Elvis Presley", "a flushing toilet", "herbicides", "Nikolai Gogol", "the Hudson River", "Banquo", "Leo", "Texas", "a pardon", "an alligator", "General Mills", "the comb", "Ferdinand Magellan", "France", "a drop of Roses lime", "'Jersey Boys'", "the Tragedy of Macbeth", "Greyhounds", "the circadian rhythm", "John Molson", "a lottery", "Al Lenhardt", "Tanzania", "Utah", "Christopher Columbus", "the Caribbean Sea", "Prague", "polygons", "Frederic Remington", "\"St. Patrick's... -and that he had been married one year. His wife", "Answers in Genesis", "eleven", "64", "a pillar of salt", "Johannes Gutenberg", "203", "4 January 2011", "Neighbours", "curb-roof", "lion", "Norway", "Big Machine Records", "The Campbell Soup Company", "Stoke City.", "identity documents", "Los Angeles", "Venezuela"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6184027777777777}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-10854", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-6274", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-6174", "mrqa_searchqa-validation-11180", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-7345", "mrqa_searchqa-validation-2269", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-632", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-12877", "mrqa_searchqa-validation-13536", "mrqa_searchqa-validation-5092", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6649", "mrqa_newsqa-validation-1516"], "SR": 0.484375, "CSR": 0.5567528735632183, "retrieved_ids": ["mrqa_squad-train-47679", "mrqa_squad-train-86461", "mrqa_squad-train-22292", "mrqa_squad-train-2686", "mrqa_squad-train-52193", "mrqa_squad-train-23416", "mrqa_squad-train-2665", "mrqa_squad-train-19477", "mrqa_squad-train-50598", "mrqa_squad-train-67992", "mrqa_squad-train-73823", "mrqa_squad-train-34036", "mrqa_squad-train-35993", "mrqa_squad-train-12317", "mrqa_squad-train-30114", "mrqa_squad-train-23328", "mrqa_newsqa-validation-3105", "mrqa_naturalquestions-validation-5366", "mrqa_newsqa-validation-1159", "mrqa_naturalquestions-validation-2569", "mrqa_squad-validation-4621", "mrqa_triviaqa-validation-1710", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-8062", "mrqa_triviaqa-validation-5115", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-7617", "mrqa_searchqa-validation-4751", "mrqa_newsqa-validation-923", "mrqa_hotpotqa-validation-2897", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-2799"], "EFR": 1.0, "Overall": 0.7430693247126438}, {"timecode": 87, "before_eval_results": {"predictions": ["Shopgirl", "Easter Island", "True", "The Bridge on the River Kwai", "Edwin Starr", "Virgin Atlantic", "cheese", "Indira Gandhi", "pew", "Alfred Hitchcock", "circulatory", "the London Bridge", "Ho Chi Minh", "Mount Vesuvius", "the Himalaya", "Kodachrome", "A Doctor's Love Affair", "Paul Simon", "Sri Lanka", "A Prairie Home Companion", "Concord", "a frog", "Thames", "Taipei", "Melba", "the Caspian Sea", "Babe Ruth", "the Edo era", "Stromboli", "apples", "chocolates", "bay leaf", "Chaucer's Prioress", "Japan", "Ben & Jerry", "a Sweater girl", "Krakauer", "Saladin", "Sweden", "John Glenn", "the Andes mountain range", "the ink-producing glands", "Mitt Romney", "Goofy", "Peter Jackson", "an inch", "Neptune", "Hawaii", "a ray", "Simon Cowell", "the Northern Mockingbird", "origins of replication, in the genome", "1898", "57 days", "dame", "James Mason", "Tinie Tempah", "Richard Arthur", "Ghana's Asamoah Gyan", "Labour", "citizenship", "1,500", "Thabo Mbeki,", "Louis XV"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7786458333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3533", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-7940", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-13410", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-9282", "mrqa_searchqa-validation-13168", "mrqa_searchqa-validation-2075", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-8536", "mrqa_searchqa-validation-1736", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-6947", "mrqa_triviaqa-validation-1749", "mrqa_hotpotqa-validation-4185"], "SR": 0.6875, "CSR": 0.5582386363636364, "retrieved_ids": ["mrqa_squad-train-67354", "mrqa_squad-train-2337", "mrqa_squad-train-27913", "mrqa_squad-train-32644", "mrqa_squad-train-32219", "mrqa_squad-train-11162", "mrqa_squad-train-23549", "mrqa_squad-train-64999", "mrqa_squad-train-40323", "mrqa_squad-train-69189", "mrqa_squad-train-61199", "mrqa_squad-train-36739", "mrqa_squad-train-1825", "mrqa_squad-train-15583", "mrqa_squad-train-62907", "mrqa_squad-train-16652", "mrqa_hotpotqa-validation-4696", "mrqa_naturalquestions-validation-8903", "mrqa_triviaqa-validation-6112", "mrqa_newsqa-validation-3522", "mrqa_naturalquestions-validation-2426", "mrqa_triviaqa-validation-3069", "mrqa_squad-validation-9977", "mrqa_searchqa-validation-10127", "mrqa_triviaqa-validation-3087", "mrqa_naturalquestions-validation-2671", "mrqa_hotpotqa-validation-825", "mrqa_searchqa-validation-10904", "mrqa_hotpotqa-validation-327", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-4413", "mrqa_newsqa-validation-2338"], "EFR": 1.0, "Overall": 0.7433664772727273}, {"timecode": 88, "before_eval_results": {"predictions": ["E. A. Poe Society of Baltimore", "a fish", "Sherlock", "Grant Wood", "( Jeff) Probst", "\"Twelfth Night\"", "the Black Sea", "Eggs Benedict", "a parrot", "Agatha Christie", "a church", "Me: Stories of My Life", "the Mossad", "backstroke", "Swahili", "a defibrillator", "Katrina", "an airbags", "a proscenium arch", "tuna", "Pocahontas", "the sinuses", "a fish", "Jane Eyre", "Kandahr", "(Nolan) Ryan", "tofu", "George Washington", "clay", "the Jutland Peninsula", "(Alma) Thomas", "Fourteen Points", "Misery", "(Cora) Munro", "(Bo) Diddley", "the Osmonds", "Guitar", "Flack", "Belgium", "SHIELD", "Chicago", "an actuary", "latkes", "Montana", "a dulcimer", "a ventriloquist", "lead", "apocrypha", "a discus", "a fiesta", "(Giorgio) Moroder and Tom Whitlock", "a chimera ( a mixture of several animals )", "across a membrane", "Candace", "horse racing", "australia", "Mauna Kea", "Deftones", "The Design Inference", "Dutch", "Form Design Center.", "3-0", "bill that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "Ronald Lyle \" Ron\" Goldman"], "metric_results": {"EM": 0.578125, "QA-F1": 0.671474358974359}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.9743589743589743, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-7165", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-16778", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-13978", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-8423", "mrqa_searchqa-validation-10749", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-12604", "mrqa_searchqa-validation-2486", "mrqa_searchqa-validation-7014", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-13957", "mrqa_searchqa-validation-7883", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-6711", "mrqa_triviaqa-validation-5284", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-2402", "mrqa_hotpotqa-validation-3258", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-692", "mrqa_hotpotqa-validation-2410"], "SR": 0.578125, "CSR": 0.5584620786516854, "retrieved_ids": ["mrqa_squad-train-61348", "mrqa_squad-train-6488", "mrqa_squad-train-15609", "mrqa_squad-train-45880", "mrqa_squad-train-15787", "mrqa_squad-train-59093", "mrqa_squad-train-16103", "mrqa_squad-train-30773", "mrqa_squad-train-80137", "mrqa_squad-train-84766", "mrqa_squad-train-66434", "mrqa_squad-train-20726", "mrqa_squad-train-17778", "mrqa_squad-train-60223", "mrqa_squad-train-14605", "mrqa_squad-train-36347", "mrqa_newsqa-validation-2361", "mrqa_searchqa-validation-15874", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-3499", "mrqa_hotpotqa-validation-1038", "mrqa_newsqa-validation-3785", "mrqa_triviaqa-validation-7059", "mrqa_searchqa-validation-12962", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2844", "mrqa_hotpotqa-validation-4618", "mrqa_searchqa-validation-14825", "mrqa_newsqa-validation-3068", "mrqa_triviaqa-validation-2784", "mrqa_naturalquestions-validation-7447", "mrqa_newsqa-validation-3703"], "EFR": 1.0, "Overall": 0.7434111657303372}, {"timecode": 89, "before_eval_results": {"predictions": ["Four bodies", "Two men", "African National Congress Deputy President Kgalema Motlanthe,", "Two U.S. filmmakers were injured", "that the six imprisoned leaders of the religious minority were held for security reasons and not because of their faith.", "18th", "monarchy", "throwing three punches but said only one connected.", "Retailers who don't speak out against it", "near the equator,", "U.S. senators", "\"we have more work to do,\"", "one-shot victory", "that she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "Citizens", "death", "businessman", "his business dealings for possible securities violations", "plastic surgery", "Meira Kumar", "The exact cause of IBS remains unknown,", "\"very diverse\"", "The Mexican military", "The FBI's Baltimore field office", "UK", "The Delta Queen", "Adidas,", "\"stressed and tired force\"", "Zac Efron", "he wants a \"happy ending\" to the case.", "1 percent", "4.6 million", "Rima Fakih", "Airbus A330-200", "The Internet", "228", "London's O2 arena", "a more detailed necropsy.", "London Heathrow's Terminal 5.", "123 pounds of cocaine and 4.5 pounds of heroin,", "Bob Dole,", "Arlington National Cemetery's", "Muqtada al-Sadr", "Leaders of more than 30 Latin American and Caribbean nations", "At least 38", "two years,", "heavy turbulence", "alongside", "Florida", "tax", "The Human Rights Watch organization", "The genome", "2010", "the Jurchen Aisin Gioro clan in Manchuria", "Crimean Tatar", "w wJacobs", "serbia", "wine", "Tsavo East National Park", "\"Lucky\"", "bananas", "The Terrytoons", "Midas", "Home alone"], "metric_results": {"EM": 0.546875, "QA-F1": 0.629478571895059}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5454545454545454, 0.0, 0.0, 1.0, 1.0, 0.5, 0.41379310344827586, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.125, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1996", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-7311", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-1968", "mrqa_hotpotqa-validation-3474", "mrqa_searchqa-validation-15665"], "SR": 0.546875, "CSR": 0.5583333333333333, "retrieved_ids": ["mrqa_squad-train-5625", "mrqa_squad-train-12325", "mrqa_squad-train-82278", "mrqa_squad-train-75631", "mrqa_squad-train-45", "mrqa_squad-train-51994", "mrqa_squad-train-16694", "mrqa_squad-train-41191", "mrqa_squad-train-1534", "mrqa_squad-train-54246", "mrqa_squad-train-46865", "mrqa_squad-train-33669", "mrqa_squad-train-76158", "mrqa_squad-train-23342", "mrqa_squad-train-2995", "mrqa_squad-train-55741", "mrqa_searchqa-validation-7807", "mrqa_newsqa-validation-1372", "mrqa_triviaqa-validation-3690", "mrqa_hotpotqa-validation-3087", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-780", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-2482", "mrqa_naturalquestions-validation-527", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-726", "mrqa_naturalquestions-validation-4280", "mrqa_triviaqa-validation-2153", "mrqa_naturalquestions-validation-4365", "mrqa_searchqa-validation-6174"], "EFR": 1.0, "Overall": 0.7433854166666667}, {"timecode": 90, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-11678", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.880859375, "KG": 0.5359375, "before_eval_results": {"predictions": ["1974", "a small, hard, leather-cased ball with a rounded end wooden, plastic or metal bat", "tima", "\"Switzerland of England\"", "Ford Island", "Food and Agriculture Organization", "Beno\u00eet Jacquot", "Lincoln", "World War II", "Les Temps modernes", "Agra", "1986", "\"King of Cool\"", "841", "Missouri River", "1.23 million", "32", "Clarence Nash", "2015", "James Franco", "1970", "Sam Kinison", "1918", "Robert Gibson", "3730 km", "Washington State Cougars football team", "1939", "Forbes", "Univision", "minister and biographer", "1865", "Dealey Plaza", "237", "Franklin", "Orlando", "Oklahoma City", "Suspiria", "The School Boys", "Tarryl Lynn Clark", "2007", "Japan", "indoor", "Sir Konstantin Sergeevich Novoselov", "Balloon Street, Manchester", "\"Ted\"", "Vanarama National League", "St. Vincent", "Atlantic", "Samantha Spiro", "Fort Hood, Texas", "Hong Kong", "Clarence Darrow", "Ren\u00e9 Verdon", "Instagram", "2007", "Adam Smith", "geese", "Bill Klein,", "last summer.", "speed attempts", "Andrew Wyeth", "a calico", "biddy", "U.S. Holocaust Memorial Museum"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6636160714285715}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.26666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.4, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5341", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-2900", "mrqa_hotpotqa-validation-2970", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-7442", "mrqa_triviaqa-validation-6410"], "SR": 0.59375, "CSR": 0.5587225274725275, "retrieved_ids": ["mrqa_squad-train-82073", "mrqa_squad-train-27953", "mrqa_squad-train-56527", "mrqa_squad-train-21347", "mrqa_squad-train-80771", "mrqa_squad-train-54628", "mrqa_squad-train-19097", "mrqa_squad-train-27000", "mrqa_squad-train-36148", "mrqa_squad-train-32275", "mrqa_squad-train-17812", "mrqa_squad-train-32395", "mrqa_squad-train-77777", "mrqa_squad-train-39832", "mrqa_squad-train-36044", "mrqa_squad-train-26102", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-15479", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-8175", "mrqa_searchqa-validation-8081", "mrqa_searchqa-validation-11194", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-2462", "mrqa_naturalquestions-validation-3745", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2934", "mrqa_searchqa-validation-11749", "mrqa_hotpotqa-validation-3872", "mrqa_naturalquestions-validation-3241", "mrqa_searchqa-validation-3239"], "EFR": 0.8846153846153846, "Overall": 0.7314019574175823}, {"timecode": 91, "before_eval_results": {"predictions": ["Leo Tolstoy", "Montana", "Tigger", "the Huguenot", "Nintendo Entertainment System", "Lexington and Concord", "a ray", "mint", "Roxanne", "Salutatorian", "Roald Dahl", "ER", "Buffalo Bill", "a pager", "Hawaii", "Isaac Newton", "Radiohead", "Cain", "Lignite", "the Vietnam War", "Algebra", "Catherine of Aragon", "McCartney and Lennon", "Bumstead", "Drumline", "Donnie", "cytokinesis", "the Unabomber", "Tom Petty and the Heartbreakers", "Harry Potter", "The Sixth Sense", "American new wave band Talking Heads", "Gin", "Santa", "fashion", "Billy the Kid", "the Stone Age", "(Cecil) Rhodes", "James Garner", "the Danube", "Michelle Pfeiffer", "a double take", "Jacob Webster", "Phelps", "Pakistan", "the Chinook", "a spectro microscope", "sesame", "a quart", "hock", "Larry King", "UNICEF's global programing", "defense against rain rather than sun", "Vijaya Mulay", "pachacuti", "Istanbul", "10", "Dante", "George Raft", "Gr\u00e1inne O'Malley", "Zed", "Communist", "an empty water bottle", "The Fixx"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6625744047619048}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-9909", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-12917", "mrqa_searchqa-validation-5080", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-12260", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-15372", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-7656", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-2465", "mrqa_searchqa-validation-15001", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-11934", "mrqa_searchqa-validation-3638", "mrqa_searchqa-validation-5686", "mrqa_searchqa-validation-13368", "mrqa_searchqa-validation-11404", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-2238", "mrqa_triviaqa-validation-3479", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-8584"], "SR": 0.5625, "CSR": 0.5587635869565217, "retrieved_ids": ["mrqa_squad-train-49488", "mrqa_squad-train-38495", "mrqa_squad-train-37024", "mrqa_squad-train-63091", "mrqa_squad-train-7859", "mrqa_squad-train-36550", "mrqa_squad-train-56569", "mrqa_squad-train-3362", "mrqa_squad-train-36019", "mrqa_squad-train-59889", "mrqa_squad-train-78653", "mrqa_squad-train-27145", "mrqa_squad-train-81317", "mrqa_squad-train-11584", "mrqa_squad-train-47816", "mrqa_squad-train-41672", "mrqa_searchqa-validation-1276", "mrqa_triviaqa-validation-4490", "mrqa_naturalquestions-validation-9428", "mrqa_searchqa-validation-15998", "mrqa_naturalquestions-validation-9895", "mrqa_searchqa-validation-12619", "mrqa_naturalquestions-validation-9576", "mrqa_searchqa-validation-5723", "mrqa_newsqa-validation-2813", "mrqa_naturalquestions-validation-5007", "mrqa_searchqa-validation-10032", "mrqa_naturalquestions-validation-894", "mrqa_hotpotqa-validation-2269", "mrqa_naturalquestions-validation-2839", "mrqa_squad-validation-7888", "mrqa_hotpotqa-validation-2375"], "EFR": 0.9642857142857143, "Overall": 0.7473442352484472}, {"timecode": 92, "before_eval_results": {"predictions": ["John Foster Dulles", "aluminum", "Norman Rockwell", "The New York Times", "the PATRIOT Act", "hock", "John Madden", "Hemingway", "a rubaiyat", "Judge Roy Bean", "Malaysia", "the yam", "Mariner", "the Chunnel", "President Lincoln", "Smithfield", "Scorsese", "Poland", "The Mausoleum", "pep", "the Tabernacle", "the Indy 500", "the Galapagos Islands", "Boston", "Nautilus", "the Phoenicians", "parez", "ITALIAN", "Athens", "Calvin Klein Eternity", "Sherlock", "Pennsylvania", "Cotton Mather", "the Berlin Wall", "Suzanne Valadon", "Sexuality", "elephants", "an axe", "Erwin Rommel", "Navarre", "wheat", "Vermont", "Mending Wall", "Thomas Jefferson", "copper", "Wrigley", "rum", "towels", "Brian C. Wimes", "steel", "Zane Grey", "Eydie Gorm\u00e9", "A turlough, or turlach", "6,259 km ( 3,889 mi )", "Tutankhamun", "Funchal", "Ruth Rendell", "Anthony Ray Lynn", "The 2016 United States Senate election", "Martin Scorsese", "African National Congress Deputy President Kgalema Motlanthe,", "Angela Merkel", "September,", "Somali-based"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6522569444444444}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-13482", "mrqa_searchqa-validation-5947", "mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-16916", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-15747", "mrqa_searchqa-validation-4184", "mrqa_searchqa-validation-4976", "mrqa_searchqa-validation-4760", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-1937", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-10940", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-2767", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-335", "mrqa_hotpotqa-validation-5730", "mrqa_newsqa-validation-1382"], "SR": 0.5625, "CSR": 0.5588037634408602, "retrieved_ids": ["mrqa_squad-train-69219", "mrqa_squad-train-77344", "mrqa_squad-train-8385", "mrqa_squad-train-14126", "mrqa_squad-train-17822", "mrqa_squad-train-60008", "mrqa_squad-train-47542", "mrqa_squad-train-6401", "mrqa_squad-train-51122", "mrqa_squad-train-41438", "mrqa_squad-train-59438", "mrqa_squad-train-7451", "mrqa_squad-train-62829", "mrqa_squad-train-79615", "mrqa_squad-train-50951", "mrqa_squad-train-71797", "mrqa_searchqa-validation-14061", "mrqa_newsqa-validation-2241", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4208", "mrqa_newsqa-validation-3720", "mrqa_hotpotqa-validation-4311", "mrqa_triviaqa-validation-1811", "mrqa_newsqa-validation-314", "mrqa_searchqa-validation-7828", "mrqa_hotpotqa-validation-4407", "mrqa_squad-validation-5733", "mrqa_searchqa-validation-2036", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-6711", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-6642"], "EFR": 0.9642857142857143, "Overall": 0.747352270545315}, {"timecode": 93, "before_eval_results": {"predictions": ["Twister", "Romeo & Juliet", "doughboy", "Georgetown University", "Plum", "Cricket", "Duke", "chiropractic", "mummies", "Lot", "a hull", "aluminum", "Kevin Smith", "Jabez Stone", "Major", "the Monitor", "Pasteur", "cesarea", "Hudson", "forage", "Edgar Allan Poe", "Edison", "Manhattan", "Lambeau", "T.E. Lawrence", "Blake Lively", "ape", "Eliot Spitzer", "licorice stick", "the union", "a fox", "Poe", "a star", "impeachment", "Santo Domingo", "Dagger", "Columbus", "Nightingale", "'The Inside Story of the Wendy's'Where's the Beef?'", "Harvey Milk", "butterfly", "a computer", "a Velvet cake", "Goodyear", "corpulent", "Edinburgh", "Crumpets", "saddle bags", "Great Smoky Mountains National Park", "Sir Walter Scott", "Punjabi", "October 1986", "in the duodenum", "Anthony Hopkins", "six", "Dubai", "horse racing", "Abdul Razzak Yaqoob", "England", "Colonel Gaddafi", "nine-wicket", "Haiti,", "if Gadhafi suffered the wound in crossfire or at close-range", "Roberto Micheletti,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6504712301587301}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10577", "mrqa_searchqa-validation-11129", "mrqa_searchqa-validation-13621", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-16029", "mrqa_searchqa-validation-8858", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-7204", "mrqa_searchqa-validation-3704", "mrqa_searchqa-validation-3150", "mrqa_searchqa-validation-14177", "mrqa_searchqa-validation-5066", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-7436", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-12720", "mrqa_searchqa-validation-7897", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-221", "mrqa_hotpotqa-validation-3442", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2385"], "SR": 0.5625, "CSR": 0.558843085106383, "retrieved_ids": ["mrqa_squad-train-978", "mrqa_squad-train-32201", "mrqa_squad-train-33084", "mrqa_squad-train-43210", "mrqa_squad-train-86282", "mrqa_squad-train-49423", "mrqa_squad-train-42294", "mrqa_squad-train-3914", "mrqa_squad-train-76166", "mrqa_squad-train-20192", "mrqa_squad-train-78109", "mrqa_squad-train-50057", "mrqa_squad-train-52997", "mrqa_squad-train-51266", "mrqa_squad-train-7726", "mrqa_squad-train-53397", "mrqa_triviaqa-validation-2816", "mrqa_naturalquestions-validation-4442", "mrqa_triviaqa-validation-7613", "mrqa_naturalquestions-validation-215", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-2353", "mrqa_naturalquestions-validation-5036", "mrqa_triviaqa-validation-7083", "mrqa_naturalquestions-validation-2679", "mrqa_newsqa-validation-4069", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-6383", "mrqa_hotpotqa-validation-4539"], "EFR": 0.9642857142857143, "Overall": 0.7473601348784195}, {"timecode": 94, "before_eval_results": {"predictions": ["John Mayer", "Benjamin Franklin", "the Amstel", "Constantinople", "Georgie Porgie", "(Ch Chad) Siegel", "Adam", "Spain", "Charles Dickens", "Quebec", "Belshazzar", "the Cincinnati Reds", "Once", "(2)", "China", "Frederick Douglass", "Sitka", "the Amazons", "Debussy", "a prince", "Bojangles", "Aunt Jemima", "Frank Sinatra", "a flying saucer", "surfing", "KLM", "(Frederick Victor) Zeller", "a carriage", "a vest", "The Adventures of Sherlock Holmes", "Ned Kelly", "a prostitution scandal", "World of Warcraft", "Shakespeare", "the Inca", "Alaska", "Sam Kinison", "the Wii", "the high jump", "Champagne", "a new brush", "Danica Patrick", "pancreas", "Midway Atoll", "stars", "Henry Cisneros", "sacristy", "the Great Seal", "Rihanna", "\"24\"", "Tom Brady", "a star", "the final scene of the fourth season", "Billy Hill", "iron", "Henry Ford", "bartholomew", "1943", "Battle of Britain and the Battle of Malta", "Kaep", "did not", "Kaka,", "the vicious brutality which accompanied the murders of his father and brother.\"", "B-movie queen Lana Clarkson"], "metric_results": {"EM": 0.6875, "QA-F1": 0.743579306722689}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-10187", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-13912", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-15733", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-615", "mrqa_searchqa-validation-4359", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-11261", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-13716", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-5383", "mrqa_newsqa-validation-2196"], "SR": 0.6875, "CSR": 0.5601973684210526, "retrieved_ids": ["mrqa_squad-train-55777", "mrqa_squad-train-83639", "mrqa_squad-train-34450", "mrqa_squad-train-71392", "mrqa_squad-train-75143", "mrqa_squad-train-3456", "mrqa_squad-train-14475", "mrqa_squad-train-20119", "mrqa_squad-train-69540", "mrqa_squad-train-50260", "mrqa_squad-train-84851", "mrqa_squad-train-24818", "mrqa_squad-train-53708", "mrqa_squad-train-84766", "mrqa_squad-train-44826", "mrqa_squad-train-4395", "mrqa_naturalquestions-validation-3770", "mrqa_searchqa-validation-9067", "mrqa_triviaqa-validation-6615", "mrqa_searchqa-validation-9089", "mrqa_squad-validation-5054", "mrqa_newsqa-validation-2844", "mrqa_naturalquestions-validation-571", "mrqa_triviaqa-validation-1543", "mrqa_searchqa-validation-4607", "mrqa_hotpotqa-validation-516", "mrqa_naturalquestions-validation-4641", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-3258", "mrqa_searchqa-validation-1643", "mrqa_newsqa-validation-3592", "mrqa_searchqa-validation-11933"], "EFR": 0.95, "Overall": 0.7447738486842106}, {"timecode": 95, "before_eval_results": {"predictions": ["The Buckwheat Boyz was an American musical group founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O' Neal and Dougy Williams", "1998", "the south to West Bengal in the north through Andhra Pradesh and Odisha", "rapid, continuous warming", "Gatiman", "Dr. Rajendra Prasad", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "no more than 4.25 inches", "the Gentiles", "art of the Persian Safavid dynasty from 1501 to 1722, in present - day Iran and Caucasia", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "typically the player to the dealer's right", "his guilt in killing the bird", "February 6, 2005", "one of the most recognisable structures in the world", "in the episode `` Kobol's Last '', Commander Adama sends Boomer on a mission to destroy the basestar orbiting Kobol", "Philippe Petit", "Terrell Suggs", "anion", "four of the 50 states of the United States", "1952", "The User State Migration Tool", "2017", "restoring someone's faith in love and family relationships", "an illustration by Everest creative Maganlal Daiya back in the 1960s", "concentration of a compound exceeds its solubility", "solemniser", "1961", "36 months", "an integral membrane protein that builds up a proton gradient across a biological membrane", "11 p.m. to 3 a.m.", "Brenda", "Randy VanWarmer", "On the west", "Kevin Spacey", "Miller Lite", "the body - centered cubic ( BCC ) lattice", "1986", "their son Jack ( short for Jack - o - Lantern )", "Spektor", "9pm ET ( UTC - 5 )", "February 2017", "July 21, 1861", "her attractive Tatted neighbour ( Holden Nowell )", "Muhammad Yunus", "Jules Shear", "Juliet", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "Ethel Merman", "New Croton Reservoir in Westchester and Putnam counties", "Judith Cynthia Aline Keppel", "Kenny Everett", "Trinidad and Tobago", "Ghost", "Hawaii", "William Bradford", "Chuck Noll", "Italian Serie A", "three", "suicides", "John Henry", "compost", "Two and a Half Men", "2004 Paris Motor Show"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6642988054472188}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.16, 1.0, 0.2222222222222222, 0.28571428571428575, 1.0, 1.0, 0.9428571428571428, 0.8333333333333333, 0.0, 0.0, 1.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.2105263157894737, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 1.0, 0.2857142857142857, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-9683", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-8433", "mrqa_triviaqa-validation-4071", "mrqa_hotpotqa-validation-4553", "mrqa_newsqa-validation-2754"], "SR": 0.546875, "CSR": 0.56005859375, "retrieved_ids": ["mrqa_squad-train-28162", "mrqa_squad-train-8535", "mrqa_squad-train-59760", "mrqa_squad-train-28285", "mrqa_squad-train-71301", "mrqa_squad-train-65994", "mrqa_squad-train-48225", "mrqa_squad-train-79612", "mrqa_squad-train-83595", "mrqa_squad-train-940", "mrqa_squad-train-62729", "mrqa_squad-train-48248", "mrqa_squad-train-53444", "mrqa_squad-train-36463", "mrqa_squad-train-26599", "mrqa_squad-train-39082", "mrqa_hotpotqa-validation-3324", "mrqa_newsqa-validation-1214", "mrqa_triviaqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_hotpotqa-validation-3564", "mrqa_newsqa-validation-2509", "mrqa_searchqa-validation-8832", "mrqa_hotpotqa-validation-5052", "mrqa_newsqa-validation-1759", "mrqa_searchqa-validation-5719", "mrqa_triviaqa-validation-3042", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-2460", "mrqa_naturalquestions-validation-3882", "mrqa_searchqa-validation-12070"], "EFR": 0.7586206896551724, "Overall": 0.7064702316810345}, {"timecode": 96, "before_eval_results": {"predictions": ["trinidad", "art", "gay, bisexual, and transgender", "Pisces", "Law Society", "Russell Crowe", "15", "Iran", "Clint Eastwood", "1921", "france", "David Bowie", "Mental Floss", "france", "volcanoes", "porridge", "south africa", "New Orleans", "eye", "pooh and his friends", "Ringo Starr", "John Mortimer", "bushfires", "Boise", "Oswald Chesterfield Cobblepot", "Sweden", "four", "the AllStars", "rastafari", "Sydney", "800m", "Leo Tolstoy", "Sir Stirling Craufurd Moss", "belle", "Rick Wakeman", "Benghazi", "Brazil", "The Mary Tyler Moore Show", "Gordon Jackson", "scotland", "paulce", "west Sussex", "Laputa", "Colombia", "lurch", "mad", "Tanzania", "Darby and Joan", "wales", "Thailand", "hugh Laurie", "Los Angeles", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "the first to develop lethal injection as a method of execution", "1 January 1788", "guitar feedback", "Stephen Lee", "Arroyo and her husband", "artificial intelligence.", "North Korea", "Vietnam", "white granite", "the humerus", "pinch"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6979166666666667}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-1554", "mrqa_triviaqa-validation-3638", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-6139", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-6747", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-4932", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-1433", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-1941", "mrqa_newsqa-validation-887", "mrqa_searchqa-validation-14431"], "SR": 0.640625, "CSR": 0.560889175257732, "retrieved_ids": ["mrqa_squad-train-82659", "mrqa_squad-train-22933", "mrqa_squad-train-17034", "mrqa_squad-train-23085", "mrqa_squad-train-58759", "mrqa_squad-train-20164", "mrqa_squad-train-56241", "mrqa_squad-train-24061", "mrqa_squad-train-45216", "mrqa_squad-train-50783", "mrqa_squad-train-16512", "mrqa_squad-train-72858", "mrqa_squad-train-24239", "mrqa_squad-train-9887", "mrqa_squad-train-30253", "mrqa_squad-train-55229", "mrqa_triviaqa-validation-6262", "mrqa_naturalquestions-validation-2010", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4264", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-2945", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-5115", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-2807", "mrqa_triviaqa-validation-1816", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-2832", "mrqa_searchqa-validation-6881", "mrqa_naturalquestions-validation-9564"], "EFR": 0.9565217391304348, "Overall": 0.7462165578776334}, {"timecode": 97, "before_eval_results": {"predictions": ["40", "recall notices", "The result left United on 16 points, above local rivals Manchester City only on goal difference.", "Two United Arab Emirates based companies", "Britain.", "The monarchy's end after 239 years of rule", "Kerstin and two of her brothers,", "56,", "\"Freshman Year\" experience", "more than 200.", "Princess Diana", "1825", "maintain an \"aesthetic environment\" and ensure public safety,", "Caylee Anthony's", "The elections are slated for Saturday.", "Alexandre Caizergues, of France,", "Arroyo and her husband", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "Stratfor,", "India", "non-European Union player", "Haleigh Cummings,", "Glenn McConnell, Senate president pro tempore,", "Toffelmakaren.", "ended his playing career at his original club of Argentinos Juniors in 2007", "researchers", "between 1917 and 1924", "the fast-food chain is conquering one of the country's most valued cultural institutions --the Louvre.", "3,000", "10 percent", "$250,000", "April 22.", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Yusuf Saad Kamel", "Obama", "The total number of flights affected", "to try to make life a little easier for these families", "Jeffrey Jamaleldine", "more than two years,", "it was unjustifiable", "up to $50,000 for her,", "Citizens of the lower house of parliament,", "41,280 pounds", "Adam Lambert", "President Clinton.", "identity theft", "Iran's nuclear program.", "many riders say it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "133", "suppress the memories and to live as normal a life as possible", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "Richard Parker", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "electron pairs", "Anderson", "poland", "roman numbers", "Hawaii", "stolperstein", "Paper", "Las Vegas", "the hypothalamus", "the orangutan", "Enid Blyton"], "metric_results": {"EM": 0.5, "QA-F1": 0.6335602765290265}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true], "QA-F1": [0.2857142857142857, 0.0, 0.0, 1.0, 0.5, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 0.6666666666666666, 1.0, 0.9090909090909091, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.1, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.21428571428571427, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-1451", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-4055", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-7701", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-4975"], "SR": 0.5, "CSR": 0.5602678571428572, "retrieved_ids": ["mrqa_squad-train-58017", "mrqa_squad-train-73319", "mrqa_squad-train-58179", "mrqa_squad-train-82443", "mrqa_squad-train-69781", "mrqa_squad-train-35673", "mrqa_squad-train-7914", "mrqa_squad-train-57939", "mrqa_squad-train-61785", "mrqa_squad-train-48828", "mrqa_squad-train-4135", "mrqa_squad-train-58879", "mrqa_squad-train-26632", "mrqa_squad-train-76003", "mrqa_squad-train-74159", "mrqa_squad-train-77616", "mrqa_naturalquestions-validation-716", "mrqa_newsqa-validation-1303", "mrqa_triviaqa-validation-4264", "mrqa_newsqa-validation-1399", "mrqa_searchqa-validation-4054", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-4143", "mrqa_searchqa-validation-15128", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-5845", "mrqa_triviaqa-validation-4019", "mrqa_naturalquestions-validation-386", "mrqa_naturalquestions-validation-650", "mrqa_newsqa-validation-437", "mrqa_hotpotqa-validation-3343", "mrqa_naturalquestions-validation-3296"], "EFR": 0.90625, "Overall": 0.7360379464285713}, {"timecode": 98, "before_eval_results": {"predictions": ["does not", "Swedish Prime Minister Fredrik Reinfeldt", "Denver,", "Friday,", "Heshmat Tehran Attarzadeh", "Daniel Radcliffe", "Caster Semenya", "the hunt for Nazi Gold and possibly the legendary Amber Room", "Susan Atkins,", "Pakistan's High Commission in India", "\"Wicked,\"", "Rolling Stone", "Tsvangirai", "The oceans", "Asashoryu,", "US Airways Flight 1549", "autonomy.", "South Africa", "\"A good vegan cupcake has the power to transform everything for the better,\"", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "between 1917 and 1924", "Les Bleus", "Too many glass shards left by beer drinkers in the city center,", "collapsed ConAgra Foods plant", "40-year-old", "Krishna Rajaram,", "The island's dining scene", "in the U.S.", "clogs", "Kurt Cobain's", "Turkey,", "Moody and sinister,", "United States, NATO member states, Russia and India", "Ryder Russell", "spend billions to improve America's education, infrastructure, energy and health care systems.", "Palestinian Islamic Army,", "Haeftling,", "off the coast of Dubai", "change course", "Sri Lanka,", "eight-week", "drug cartels", "bench", "Anil Kapoor.", "\"The Closer.\"", "38 feet", "St. Louis, Missouri.", "64,", "\"totaled,\"", "fifth season", "order", "1977", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "Nasdaq", "Van Rijn", "the swan", "Little Big League", "Hulder", "Lucille Ball", "Ziploc", "South Africa", "ivory", "Kraftwerk"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7706272893772894}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-3636", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-522", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-2399"], "SR": 0.703125, "CSR": 0.5617108585858586, "retrieved_ids": ["mrqa_squad-train-18262", "mrqa_squad-train-13482", "mrqa_squad-train-77139", "mrqa_squad-train-33963", "mrqa_squad-train-14904", "mrqa_squad-train-50769", "mrqa_squad-train-74784", "mrqa_squad-train-24279", "mrqa_squad-train-8770", "mrqa_squad-train-11763", "mrqa_squad-train-28810", "mrqa_squad-train-79051", "mrqa_squad-train-79325", "mrqa_squad-train-63399", "mrqa_squad-train-71244", "mrqa_squad-train-79403", "mrqa_triviaqa-validation-3924", "mrqa_newsqa-validation-3372", "mrqa_searchqa-validation-8784", "mrqa_triviaqa-validation-1500", "mrqa_naturalquestions-validation-8851", "mrqa_newsqa-validation-2325", "mrqa_triviaqa-validation-1585", "mrqa_naturalquestions-validation-3548", "mrqa_squad-validation-6614", "mrqa_searchqa-validation-6174", "mrqa_naturalquestions-validation-8290", "mrqa_squad-validation-434", "mrqa_hotpotqa-validation-3311", "mrqa_naturalquestions-validation-7892", "mrqa_triviaqa-validation-3487", "mrqa_hotpotqa-validation-2038"], "EFR": 1.0, "Overall": 0.7550765467171716}, {"timecode": 99, "UKR": 0.80078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1232", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13865", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14527", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-622", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8108", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9502", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1713", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3633", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3883", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.892578125, "KG": 0.55, "before_eval_results": {"predictions": ["Number Ones", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "$40 billion during the operations phase.", "\"I am sick of life -- what can I", "10 municipal police officers", "Lifeway's 100-plus stores nationwide", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "2010", "two years,", "a group of college students of Pakistani background", "gasoline", "James Whitehouse,", "North Korea,", "the 1962 \"Tete de Cheval\" (\"Horse's Head\") and the 1944 \"Verre et Pichet\" (\"Glass and Pitcher\") by Picasso.", "Omar bin Laden,", "its nude beaches.", "1-1 draw", "Jason Chaffetz", "Martin \"Al\" Culhane,", "President Obama", "A member of the group dubbed the \"Jena 6\"", "Animal Planet", "Minerals Management Service Director Elizabeth Birnbaum", "Mandi Hamlin", "41,280", "Arlington National Cemetery's Section 60,", "Bill Haas", "Caster Semenya", "his health and about a comeback.", "Elisabeth", "Djibouti,", "123 pounds of cocaine and 4.5 pounds of heroin", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "more than 30", "second-degree aggravated battery.", "two", "Alina Cho", "the forward's lawyer", "Turkish President Abdullah Gul.", "the children of street cleaners and firefighters.", "secure more funds", "five female pastors", "14", "South Carolina Republican Party Chairwoman Karen Floyd", "African National Congress Deputy President", "1913,", "second", "Chinese President Hu Jintao", "Michael Arrington,", "asylum in Britain.", "Indian army", "New York City", "2009", "two of whom were his sons", "Phil Mickelson", "Quran", "Shanghai", "The University of the District of Columbia", "an advertisement figure", "the zona glomerulosa of the adrenal cortex", "freezing", "Power", "Maine", "November 17, 2017"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7375665395650861}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.28571428571428575, 0.4, 1.0, 1.0, 0.9302325581395349, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-1139", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3895", "mrqa_naturalquestions-validation-644", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-397"], "SR": 0.65625, "CSR": 0.5626562500000001, "retrieved_ids": ["mrqa_squad-train-85264", "mrqa_squad-train-50330", "mrqa_squad-train-65691", "mrqa_squad-train-73348", "mrqa_squad-train-25376", "mrqa_squad-train-47320", "mrqa_squad-train-72293", "mrqa_squad-train-61813", "mrqa_squad-train-48685", "mrqa_squad-train-54092", "mrqa_squad-train-21548", "mrqa_squad-train-50937", "mrqa_squad-train-43910", "mrqa_squad-train-64092", "mrqa_squad-train-37054", "mrqa_squad-train-46148", "mrqa_hotpotqa-validation-5341", "mrqa_triviaqa-validation-635", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-2395", "mrqa_searchqa-validation-2761", "mrqa_hotpotqa-validation-347", "mrqa_triviaqa-validation-6336", "mrqa_searchqa-validation-11643", "mrqa_naturalquestions-validation-5435", "mrqa_hotpotqa-validation-662", "mrqa_searchqa-validation-7828", "mrqa_hotpotqa-validation-5127", "mrqa_triviaqa-validation-7613", "mrqa_searchqa-validation-3139", "mrqa_newsqa-validation-755"], "EFR": 0.9090909090909091, "Overall": 0.7430213068181818}]}