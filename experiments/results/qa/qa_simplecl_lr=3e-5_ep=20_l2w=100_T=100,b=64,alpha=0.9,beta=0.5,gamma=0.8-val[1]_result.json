{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=20_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]', diff_loss_weight=100.0, gradient_accumulation_steps=1, kg_eval_freq=50, kg_eval_mode='metric', kr_eval_freq=50, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=100, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=20_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 8260, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["the Cobham\u2013Edmonds thesis", "15 February 1546", "special efforts", "17", "southwestern France", "CBS Sports", "different viewpoints and political parties", "Thomas Commerford Martin", "24 August \u2013 3 October 1572", "long, slender tentacles", "45 minutes", "Town Moor", "BBC HD", "Ealy", "August 15, 1971", "a squared integer", "declared Japan a \"nonfriendly\" country", "a cubic interpolation formula", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "1852", "an intuitive understanding", "the Small Catechism", "learning of the execution of Johann Esch and Heinrich Voes", "Super Bowl XLVII", "Ozone depletion and global warming", "widespread education", "chloroplasts", "Warraghiggey", "The Scotland Act 1998", "The Bachelor", "delivery of these messages by store and forward switching", "9000 BP", "criminal investigations", "2002", "sculptures, friezes and tombs", "Sonderungsverbot", "The Simpsons", "826", "English", "energize electrons", "Catholicism", "Robert R. Gilruth", "He prayed, consulted friends, and gave his response the next day", "young men who had not fought", "Manakin Town", "tidal delta", "A Charlie Brown Christmas", "formal", "Establishing \"natural borders\"", "(sworn brother or blood brother)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "structural collapse, cost overruns, and/or litigation", "severely reduced rainfall and increased temperatures", "sponges", "Cam Newton", "science fiction", "Sonia Shankman Orthogenic School", "an aided or an unaided school", "steam turbine plant", "metamorphic processes", "faith", "article 49", "the meeting of the Church's General Assembly", "missing self"], "metric_results": {"EM": 0.765625, "QA-F1": 0.781423611111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-2974", "mrqa_squad-validation-1763", "mrqa_squad-validation-4621", "mrqa_squad-validation-2394", "mrqa_squad-validation-8719", "mrqa_squad-validation-8896", "mrqa_squad-validation-5773", "mrqa_squad-validation-5812", "mrqa_squad-validation-2113", "mrqa_squad-validation-5676", "mrqa_squad-validation-5226", "mrqa_squad-validation-337", "mrqa_squad-validation-1662", "mrqa_squad-validation-6947"], "SR": 0.765625, "CSR": 0.765625, "EFR": 0.9333333333333333, "Overall": 0.8494791666666667}, {"timecode": 1, "before_eval_results": {"predictions": ["The Adventures of Ozzie and Harriet", "The Open Championship golf and The Wimbledon tennis tournaments", "32.9%", "365.2425 days of the year", "health care", "1970s", "Sunni Arabs from Iraq and Syria", "P,NP-complete, orNP-intermediate", "Daniel Burke", "the highest terrace", "major national and international patient information projects and health system interoperability goals", "three", "net force", "12 January", "1976\u201377", "E. W. Scripps Company", "zoning and building code requirements", "river Deabolis", "1968", "King George III", "Baden-W\u00fcrttemberg", "lines or a punishment essay", "Book of Discipline", "complicated definitions", "coordinating lead author", "TFEU article 294", "G. H. Hardy", "30-second", "Royal Ujazd\u00f3w Castle", "Church and the Methodist-Christian theological tradition", "main hall", "the Teaching Council", "One could wish that Luther had died before ever [On the Jews and Their Lies] was written", "Russell T Davies", "Cape Town", "Gospi\u0107, Austrian Empire", "Classic FM's Hall of Fame", "optimisation", "2014", "late 1970s", "30% less", "1983", "Happy Days", "1,230 kilometres", "23 November 1963", "Apollo 20", "six divisions", "scoil phr\u00edobh\u00e1ideach", "business", "teachers in publicly funded schools", "Stanford University Professor of Comparative Literature Richard Rorty", "1991", "organisms", "41", "carbon", "the fertile highlands", "harder", "50% to 60%", "Norman Greenbaum", "appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts", "The Prisoners ( Temporary Discharge for Ill Health ) Act", "Carol Ann Susi", "Daenerys Targaryen", "Raabta"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8206106635237826}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6323", "mrqa_squad-validation-9752", "mrqa_squad-validation-1791", "mrqa_squad-validation-6388", "mrqa_squad-validation-6059", "mrqa_squad-validation-8616", "mrqa_squad-validation-2611", "mrqa_squad-validation-6282", "mrqa_squad-validation-3352", "mrqa_squad-validation-1906", "mrqa_squad-validation-8035", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7792", "mrqa_hotpotqa-validation-1006"], "SR": 0.765625, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 2, "before_eval_results": {"predictions": ["235", "P", "\"Smith and Jones\"", "1767", "53,000", "Fu\u00dfach", "leptin, pituitary growth hormone, and prolactin", "reverse direction", "7 West 66th Street", "patent archives", "Any member", "4-week period", "six", "His wife Katharina", "Colorado Desert", "John Pell, Lord of Pelham Manor", "United States", "2014", "Alberto Calder\u00f3n", "Roger NFL", "1950s", "1980s", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "second use of the law", "free", "1973", "1971", "Mansfeld", "Warsaw Stock Exchange", "390 billion", "a suite of network protocols", "eighteenth century", "journal Nature", "2009", "Franz Pieper", "geochemical evolution of rock units", "three times", "rhetoric", "Genoese traders", "the flail of God", "Saudi Arabia and Iran", "149,025", "13 May 1899", "Lunar Module Pilot", "citizenship", "Merritt Island", "accountants", "return home", "June 4, 2014", "kinetic friction force", "\u2153 to Tesla", "signal amplification", "Lituya Bay in Alaska", "120 m ( 390 ft )", "the eighth season will have only six episodes", "100 members", "photoelectric", "Welch, West Virginia", "26 January was chosen as the Republic day because it was on this day in 1930", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Hal David and Burt Bacharach", "five points", "The Monitor and Merrimac", "the Atlantic Ocean, the Gulf of Mexico and the Straits of Florida"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7859293050699301}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.888888888888889, 0.4444444444444445, 1.0, 0.4, 1.0, 0.4090909090909091, 1.0, 0.0, 0.5, 0.5, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1759", "mrqa_squad-validation-4731", "mrqa_squad-validation-5972", "mrqa_squad-validation-2689", "mrqa_squad-validation-80", "mrqa_squad-validation-9173", "mrqa_squad-validation-4415", "mrqa_squad-validation-4673", "mrqa_squad-validation-1454", "mrqa_squad-validation-1841", "mrqa_squad-validation-1220", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-6125", "mrqa_naturalquestions-validation-2016", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-3996"], "SR": 0.703125, "CSR": 0.7447916666666667, "EFR": 0.8421052631578947, "Overall": 0.7934484649122807}, {"timecode": 3, "before_eval_results": {"predictions": ["immunosuppressive", "William of Volpiano and John of Ravenna", "April 1523", "Excellent job opportunities", "rebellion is much more destructive", "principle of inclusions and components", "they were accepted and allowed to worship freely", "12 December 2007", "six", "redistributive taxation", "rubisco", "Abercrombie was recalled and replaced by Jeffery Amherst", "Egypt", "algae", "4,404.5 people per square mile", "the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "chromoplasts and amyloplasts", "spy network and Yam route systems", "Stairs", "genetically modified plants", "around 300,000", "three", "Von Miller", "Africa", "clinical services that pharmacists can provide for their patients", "Raghuram Rajan", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "Mark Ronson", "Calvin cycle", "their Annual Conference", "Philo of Byzantium", "the mayor (the President of Warsaw)", "cloud storage", "Doritos", "Warsaw University of Technology building", "the Great Yuan", "Lenin", "Solim\u00f5es Basin", "Charles Darwin", "23 November", "oppidum Ubiorum", "Denver's Executive Vice President of Football Operations and General Manager", "Downtown Riverside", "Capital Cities Communications", "lamprey and hagfish", "physicians and other healthcare professionals", "Golden Gate Bridge", "Michael Schumacher holds the record for the most Grand Prix victories, having won 91 times", "10.5 %", "The Man", "President Gerald Ford", "Bud '' Bergstein", "Janie Crawford", "it extends from the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Jerry Ekandjo", "961", "Forney Hull ( James Frain ), the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "December 1971", "Land of the Valar", "the middle of the 15th century", "6 March 1983", "Viola Larsen", "horror fiction", "26,000"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7595238095238095}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.8, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.4, 0.0, 0.8571428571428571, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4108", "mrqa_squad-validation-8830", "mrqa_squad-validation-4759", "mrqa_squad-validation-8763", "mrqa_squad-validation-6154", "mrqa_squad-validation-298", "mrqa_squad-validation-6614", "mrqa_squad-validation-670", "mrqa_squad-validation-962", "mrqa_squad-validation-9298", "mrqa_squad-validation-384", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4433", "mrqa_hotpotqa-validation-454"], "SR": 0.65625, "CSR": 0.72265625, "EFR": 1.0, "Overall": 0.861328125}, {"timecode": 4, "before_eval_results": {"predictions": ["infrequent rain", "the king of France", "approximately 80 avulsions", "15", "Presque Isle", "wireless", "Coldplay", "the Yuan dynasty", "same-gender marriages with resolutions", "red algae red", "after their second year", "1960s", "narcotic drugs were controlled in all member states, and so this differed from other cases where prostitution or other quasi-legal activity was subject to restriction", "Napoleon", "Immunology", "geophysical surveys", "topographic gradients", "130 million cubic foot", "50 fund", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "ctenophores and cnidarians", "motivated students", "Michael Mullett", "15", "James Gamble & Reuben Townroe", "dissension and unrest", "Blaine Amendments", "\"Turks\" (Muslims) and Catholics", "six", "Big Ten Conference", "Thames River", "NDS, a Cisco Systems company", "shipping toxic waste", "anarchists", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "immunoglobulins and T cell receptors", "previously separated specialties", "a thylakoid", "University College London", "to protect their tribal lands from commercial interests", "religious beliefs", "evading it", "the kettle and the Cricket", "Gandhi", "Vlad the Impaler", "\"Take us the foxes, the little... Alexandra - first cousins - as a means of getting Horace's money", "VHS", "Vincent van Gogh, in which Nimoy played Van Gogh's brother Theo.", "Earth's orbital period is 365 & this fraction of a day", "Tiger Woods won the 1994 U.S. Amateur in dramatic, history-making fashion.", "It was Arizona's territorial capital from 1867 to 1877,", "Marshal Dillon", "They come from our heartJi, from our...   14 - American Radio History  Mar 14, 1997", "The Best Hotels on Bali", "The new nightspot Teddy's made this presidential Hollywood hotel a happening", "Light Amplification by Stimulated Emission by radiation", "Jean Dapra", "Saturn", "Hundreds of species of peat mosses are found in bogs throughout Canada", "Why", "Daya", "fear of meat", "American", "Mexican military"], "metric_results": {"EM": 0.546875, "QA-F1": 0.653638230981981}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14814814814814814, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2666666666666667, 0.0, 0.3076923076923077, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4, 0.1818181818181818, 0.25, 0.0, 0.0, 0.15384615384615385, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9357", "mrqa_squad-validation-10204", "mrqa_squad-validation-8840", "mrqa_squad-validation-4461", "mrqa_squad-validation-10186", "mrqa_squad-validation-1960", "mrqa_squad-validation-8131", "mrqa_squad-validation-2804", "mrqa_squad-validation-5214", "mrqa_squad-validation-6721", "mrqa_searchqa-validation-12428", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-10360", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12931", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-6541", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-16377", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-11224", "mrqa_naturalquestions-validation-124", "mrqa_triviaqa-validation-6073"], "SR": 0.546875, "CSR": 0.6875, "EFR": 0.7586206896551724, "Overall": 0.7230603448275862}, {"timecode": 5, "before_eval_results": {"predictions": ["bacteriophage T4", "6.7", "second-largest", "time and space", "the Meuse", "a Western Union superintendent", "Super Bowl XLIV", "1891", "New Orleans", "hunting", "the member state cannot enforce conflicting laws", "Graham Twigg", "a mouth that can usually be closed by muscles; a pharynx (\"throat\"); a wider area in the center that acts as a stomach; and a system of internal canals", "inversely", "Europe", "he was illiterate in Czech", "colonies", "$37.6 billion", "Kalenjin", "1269", "the 17th century", "Time Warner Cable", "toward the Atlantic", "economic", "CrossCountry", "ITV", "SAP Center in San Jose", "Variable lymphocytes receptors (VLRs)", "Edict of Fontainebleau", "Levi's Stadium", "ten million", "the Lippe", "Video On Demand content", "time and storage", "semester", "the courts of member states and the Court of Justice of the European Union", "Thomas Edison", "1971", "quantum mechanics", "Lawrence, Nassau County, New York", "League of the Three Emperors", "science", "143,007", "Clinton", "Waltham Abbey", "Secretariat", "coaxial", "Mary Harron", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Thomas Christopher Ince", "American Chopper", "drawing the name out of a hat", "German", "Fort Valley, Georgia", "American", "Easy", "Belvoir", "Congo River", "Abigail", "Murwillumbah, New South Wales, Australia", "Br'er Rabbit", "corruption", "24 hours", "Dover Beach"], "metric_results": {"EM": 0.75, "QA-F1": 0.8212253891941392}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1775", "mrqa_squad-validation-4919", "mrqa_squad-validation-1187", "mrqa_squad-validation-8544", "mrqa_squad-validation-457", "mrqa_squad-validation-6676", "mrqa_squad-validation-1672", "mrqa_squad-validation-7214", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2315", "mrqa_triviaqa-validation-1616", "mrqa_searchqa-validation-14229"], "SR": 0.75, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["1540s", "the Court of Justice of the European Union", "its circle logo", "three", "negative", "fear of their lives", "80%", "1521", "Gibraltar and the \u00c5land islands", "distorting the grana and thylakoids", "exceeds any given number", "Hulagu Khan", "poet", "quality rental units", "Grover Cleveland", "overthrow a government", "entertainment", "vote clerk", "high growth rates", "a vicious and destructive civil war", "Sony", "Stagecoach", "Silk Road", "San Diego", "a German Nazi colonial administration", "four public charter schools on the South Side of Chicago", "the means to invest in new sources of creating wealth", "Spanish", "Structural geologists", "president and CEO", "indulgences for the living", "BSkyB", "terrorist organisation", "Cam Newton", "The U2 360\u00b0 Tour", "The 5 foot 9 inch tall twins", "James Victor Chesnutt", "Ben Johnston", "a large green dinosaur", "Taylor Swift", "Eric Edward Whitacre", "Joint Chiefs of Staff", "Linux Format", "Jasenovac concentration camp", "Rabat", "between 11 or 13 and 18", "Heather Langenkamp (born July 17, 1964)", "Henry Gwyn Jeffreys Moseley", "paracyclist", "Vilnius Airport (IATA: VNO, ICAO: EYVI) (Lithuanian: \"Vilniaus oro uostas\"", "It is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index", "Charmed", "Lily Hampton", "English former international footballer", "Philadelphia Eagles", "Rickie Lee Skaggs", "48,982", "Ashanti", "79", "Algeria", "a novel", "the Eastern part", "Polar Bear", "Atlantic City"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7711046918767508}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.2, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3939", "mrqa_squad-validation-5774", "mrqa_squad-validation-6788", "mrqa_squad-validation-6029", "mrqa_squad-validation-9665", "mrqa_squad-validation-7983", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1291", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5300", "mrqa_newsqa-validation-3377", "mrqa_searchqa-validation-5279", "mrqa_searchqa-validation-1971"], "SR": 0.65625, "CSR": 0.6919642857142857, "EFR": 0.8636363636363636, "Overall": 0.7778003246753247}, {"timecode": 7, "before_eval_results": {"predictions": ["IgG", "Amazoneregenwoud", "co-NP", "BBC Radio Newcastle", "England, Wales, Scotland, Denmark, Sweden, Switzerland, the Dutch Republic", "the working fluid", "a suite of network protocols created by Digital Equipment Corporation", "American Baptist Education Society", "Dutch", "the solution", "those who already hold wealth", "center of mass", "attention-seeking and disruptive students", "more than $45,000", "Defensive ends", "MLB", "the papacy", "through homologous recombination", "canalized section", "in protest against the occupation of Prussia by Napoleon", "improved markedly", "northern (German) shore of the lake", "computer programs", "General Conference", "1996", "dreams", "The Judiciary", "single-tape", "Bart Starr", "allotrope", "Karluk Kara-Khanid", "Perth, Western Australia", "Ian Rush", "Gerry Adams", "New Orleans Saints", "2016", "four operas", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Alfred Edward Housman", "capital of the Socialist Republic of Vietnam", "Sevens", "fennec fox", "Bart Conner", "fantasy", "Martin \"Marty\" McCorm (born 20 July 1983)", "Black Mountain College", "Atat\u00fcrk Museum Mansion", "Bothtec", "Cody Miller", "140 to 219 passengers", "John Locke", "Garth Jennings, co-directed by Christophe Lourdelet", "Pablo Escobar", "recent African descent", "Mexico City", "Sleeping Beauty", "PeopleMover", "8 December 1985", "Doddi in Iceland, Purzelknirps in Germany and Hilitos in Spain", "Ali Bongo", "Wheat Chex", "Ray Harroun", "Juice Newton", "David Tennant"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7663194444444444}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3019", "mrqa_squad-validation-9287", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-3413", "mrqa_hotpotqa-validation-919", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-3885", "mrqa_triviaqa-validation-1573", "mrqa_newsqa-validation-3925", "mrqa_searchqa-validation-15869"], "SR": 0.71875, "CSR": 0.6953125, "EFR": 0.8888888888888888, "Overall": 0.7921006944444444}, {"timecode": 8, "before_eval_results": {"predictions": ["Russian", "cellular respiration", "railroad", "Revolutionary", "higher efficiency", "Lunar Excursion Module", "Zwickau prophets", "six years", "700", "fire", "arms", "two", "minor", "Fringe or splinter movements", "17", "lower temperatures", "architect or engineer", "1917", "Columbus Avenue and West 66th Street", "TeacherspayTeachers.com", "stratigraphic", "commensal flora", "a + bi", "General Conference in Dallas, Texas", "Central Asian Muslims", "from home viewers who made tape recordings of the show", "1330 Avenue of the Americas", "Alberta and British Columbia", "\"Pimp My Ride\"", "Don Johnson", "Section.80", "25 million", "8,515", "13 October 1958", "tailless", "Environmental Protection Agency", "between 1932 and 1934", "an English former international footballer", "Los Angeles", "England", "Armin Meiwes", "Jean- Marc Vall\u00e9e", "Miss Universe 2010", "Adam Amin", "boxer", "Boston University", "Fulham", "A55", "Ranulf de Gernon, 4th Earl of Chester", "\u00c6thelstan", "Special economic zone", "44", "Division I", "Harriet Tubman", "Manchester United", "Dragon TV", "Greek-American", "center", "Shirley Horn", "Israel", "Bigfoot", "Papua New Guinea", "Edgar Degas", "Manchester"], "metric_results": {"EM": 0.625, "QA-F1": 0.7145292207792209}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.2, 0.0, 0.28571428571428575, 0.0, 0.8, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3391", "mrqa_squad-validation-9859", "mrqa_squad-validation-7643", "mrqa_squad-validation-5972", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-3553", "mrqa_triviaqa-validation-3170", "mrqa_newsqa-validation-1268", "mrqa_triviaqa-validation-1423"], "SR": 0.625, "CSR": 0.6875, "EFR": 0.7916666666666666, "Overall": 0.7395833333333333}, {"timecode": 9, "before_eval_results": {"predictions": ["$32 billion", "centrifugal governor", "Orange County", "The chloroplast peripheral reticulum", "1962", "European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "Rugby", "Germany", "politically and socially unstable", "Theatre Museum", "90\u00b0", "iTunes", "unpaired electrons", "French", "Museum of the Moving Image in London", "sent missionaries", "pyrenoid and thylakoids", "Woodward Park", "non-violent", "25 May 1521", "essentially holy people", "diplomacy or military force", "increase in the land available for cultivation", "the value of the spin", "pivotal event", "transgender teenage girl", "John Alexander", "David Michael Bautista Jr.", "Black Friday", "actor, singer and a DJ", "Prince Amedeo", "Lambic", "Mazatl\u00e1n", "Assistant Director Neil J. Welch", "March 30, 2025", "England", "Kentucky, Virginia, and Tennessee", "Autopia, the Disneyland Monorail, and the Motor Boat Cruise", "Yasir Hussain", "USC Marshall School of Business", "Stephen James Ireland", "Marko Tapani \" Marco\" Hietala", "Estadio de L\u00f3pez Cort\u00e1zar", "Kohlberg K Travis Roberts", "Fort Albany", "I'm Shipping Up to Boston", "2500 ft", "Central Park", "Robert John Day", "Afroasiatic", "James Tinling", "Italy", "79th Masters Tournament", "Kristoffer Rygg", "University of Kentucky College of Pharmacy", "William Shakespeare", "Bob Dylan", "Erika Mitchell Leonard", "Santiago", "a North African dish of small steamed balls of semolina, usually served with a stew spooned on top", "more than 22 million", "morphine sulfate oral solution 20 mg/ml", "The Firm (1993 film)", "a freshwater, airbreathing catfish"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6596118771922259}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4147", "mrqa_squad-validation-2943", "mrqa_squad-validation-7674", "mrqa_squad-validation-3130", "mrqa_squad-validation-8651", "mrqa_squad-validation-4572", "mrqa_squad-validation-6797", "mrqa_squad-validation-9735", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-10208", "mrqa_triviaqa-validation-2522", "mrqa_newsqa-validation-1668", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-3622"], "SR": 0.578125, "CSR": 0.6765625, "EFR": 0.6666666666666666, "Overall": 0.6716145833333333}, {"timecode": 10, "before_eval_results": {"predictions": ["November 1979", "Timucuan", "suburban", "vertebrates", "Fears of being labelled a pedophile or hebephile", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "Panthers", "Sanders", "economic instability", "Gamal Abdul Nasser", "Immunodeficiencies", "counterflow", "John B. Goodenough", "not covered in any newspapers", "arrows, swords, and leather shields", "Cybermen", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "Standard Model", "Tolui", "the Rhine-Ruhr region", "course of study", "Prevenient grace", "Kansas Jayhawks", "Captain Cook's Landing Place", "Chris Pine", "Yoo Seung-ho", "World War II", "NCAA Division I", "The The Onion", "Mickey's PhilharMagic", "A Bug's Life", "1978", "May 2008", "Italy", "La Familia Michoacana", "Uzumaki", "Tom Jones", "Russell Humphreys", "Barbara Niven", "13\u20133", "Eliot Spitzer", "5,042", "European", "the first integrated circuit", "Tianhe Stadium", "1952", "the fourth Thursday", "Giuseppe Verdi", "Germanic", "New Jersey", "Bath, Maine", "Ector County", "Jim Davis", "Buck Owens", "World Health Organization", "Emmanuel Ofosu Yeboah", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Heather Stebbins", "ky", "Sir Giles Gilbert Scott", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills", "Comoros Islands", "Onomastic Sobriquets In The Food And Beverage Industry", "The Londoner"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7430986830514448}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 0.0, 1.0, 0.23255813953488372, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7269", "mrqa_squad-validation-797", "mrqa_squad-validation-6927", "mrqa_squad-validation-1166", "mrqa_squad-validation-6166", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-10351"], "SR": 0.65625, "CSR": 0.6747159090909092, "EFR": 0.8636363636363636, "Overall": 0.7691761363636365}, {"timecode": 11, "before_eval_results": {"predictions": ["UHF", "deflate", "Battle of Olustee", "Spanish", "100\u2013150", "Philo of Byzantium", "cooler", "marine waters worldwide", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "his mother's genetics and influence", "shock", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "new element", "the building is ready to occupy", "boom-and-bust cycles", "Edinburgh", "Richard Allen and Absalom Jones", "earn as much as a healthy young man", "Jamukha", "1969", "There is a step in the right direction. It educates the consumer on how much they are paying for having a low-MPG car and encourages them to get into a more efficient vehicle", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "body bags", "near Warsaw, Kentucky", "Arthur E. Morgan III", "April 2010", "Larry King", "does not involve MDC head Morgan Tsvangirai", "there's no evidence as to the cause of death,... you could have an accidental death and a mother that panics", "200", "The number of students getting into drugs is bigger and also getting younger and younger", "opposition party members", "Missouri", "\"Racism and racist conversations have no place today in America.\"", "executive director of the Americas Division of Human Rights Watch", "Dominican Republic", "90", "KARK", "a space for aspiring entrepreneurs to brainstorm with like-minded people", "in her home", "Employee Free Choice Act", "Bush administration", "more than 200", "This is not a project for commercial gain. It is done with the parents' full consent", "best-of-three series", "Kaka", "Japanese ex-wife", "Dan Parris, 25, and Rob Lehr, 26", "apartment near Fort Bragg in North Carolina", "two", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site", "Jacob", "Molotov cocktails, rocks and glass", "250,000", "Winehouse", "the Ark of the Covenant ( the Aron Habrit in Hebrew )", "Jean F Kernel ( 1497 -- 1558 )", "Thomas Hardy", "Richmond", "1994", "The Conjuring", "The Gallipoli Campaign", "Georgian Bay", "Nowhere Boy"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6329731995296211}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.4, 1.0, 0.0, 0.0, 0.21052631578947367, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.38095238095238093, 0.0, 1.0, 0.0, 0.4444444444444445, 0.8333333333333333, 1.0, 0.2608695652173913, 1.0, 1.0, 0.0, 0.0, 0.6, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1257", "mrqa_squad-validation-2493", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3068", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-5434", "mrqa_searchqa-validation-2548", "mrqa_searchqa-validation-8335"], "SR": 0.546875, "CSR": 0.6640625, "EFR": 0.7586206896551724, "Overall": 0.7113415948275862}, {"timecode": 12, "before_eval_results": {"predictions": ["threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "wealth", "every good work designed to attract God's favor", "Napoleon", "new technology and machinery", "James O. McKinsey", "private actors", "Bell Northern Research", "a body of treaties and legislation", "1227", "lower lake", "three", "Elders", "587,000", "Private Bill Committees", "Bruno Mars", "the Catechism", "Stagg Field", "Ian Botham", "Pyotr Tchaikovsky", "Kawasaki", "Al Shean", "Salvador Allende", "Marie Antoinette", "Redmond, United States", "Erik Thorvaldson", "Marsyas", "Pal Joey", "Mary Jane Grant", "\"The Homeland Security Advisory System\"", "Brunei", "supreme religious leader of all the subordinate priests", "Antonio", "European Economic Community (EEC)", "Christine Keeler", "Yehoshua", "Jack Nicholson Easy Rider", "four", "Netherlands", "\"Sugar Baby Love\"", "Coretta Scott King", "Sean", "Bill and Taffy Danoff", "zygote", "Travis", "Blue Peter", "Robert Kennedy", "Q", "umbrella", "an author and philosopher", "barber", "Rod Laver", "Murrah Federal Office Building", "Evita", "an old, unsavoury, and oily black clay pipe", "fortified complex", "bohrium", "Eleanor of Aquitaine", "Mickey Gilley", "\"It is very easy for comments to be taken out of context and create unnecessary drama", "a delegation of American Muslim and Christian leaders", "Royal Wives", "the Greek Village", "Kim Clijsters"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5663039434523809}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0625, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2262", "mrqa_squad-validation-5431", "mrqa_squad-validation-9418", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-6974", "mrqa_triviaqa-validation-712", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-3819", "mrqa_newsqa-validation-3987", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-5929", "mrqa_newsqa-validation-1150"], "SR": 0.515625, "CSR": 0.6526442307692308, "EFR": 0.7741935483870968, "Overall": 0.7134188895781638}, {"timecode": 13, "before_eval_results": {"predictions": ["Polignac's conjecture", "Bo'orchu", "Pittsburgh Steelers", "Sky Digital", "Allston Science Complex", "divergent boundaries", "9th century", "many", "1775\u20131795", "Dorothy and Michael Hintze", "William Ellery Channing and Ralph Waldo Emerson", "Fu\u00dfach", "Wesleyan Holiness Consortium", "Maxwell", "in whole by charging their students tuition fees", "Dublin, Cork, Youghal and Waterford", "Tangled", "dernell", "moles", "diogenes Laertius", "concrete", "Anne Boleyn", "garon d. kuren", "Steve McQueen", "Portugal", "jazz tenor saxophonist", "one", "komando Pasukan Khusus", "Carlisle", "liquid", "Chillicothe and Zanesville", "Lucas McCain", "Antarctica", "mercury gilding", "aniridia", "Charles A. Carpenter", "River Forth", "woe", "NOW Magazine", "j Jesse James", "Italy", "Canada", "typhoid fever", "Pavarotti", "action figure", "Walt Kowalski-Gran Torino", "2010", "volume of a given mass of a gas", "Venezuela", "altinge Burlesque Theater in New York City in 1935", "temperature inversion", "40", "phrenology", "San Francisco", "Fall 1998", "Regulus", "Christopher James \" Chris\" Weidman", "Athletics Stadium", "one", "Virgin America", "an illegitimate daughter", "albergull florist", "Iran's parliament speaker", "In Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0 through Gojko Kacar's second half strike.It meant Dutch side Heerenveen were eliminated despite a 5-0 home victory"], "metric_results": {"EM": 0.5, "QA-F1": 0.5322916666666666}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6078", "mrqa_squad-validation-6983", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4391", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1733", "mrqa_naturalquestions-validation-5675", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-2463", "mrqa_searchqa-validation-2972", "mrqa_searchqa-validation-15784", "mrqa_newsqa-validation-2281"], "SR": 0.5, "CSR": 0.6417410714285714, "EFR": 0.6875, "Overall": 0.6646205357142857}, {"timecode": 14, "before_eval_results": {"predictions": ["in an adult plant's apical meristems", "Tugh Temur", "Persia", "Parliament Square, High Street and George IV Bridge in Edinburgh", "Revolutionary civil disobedience", "Beijing", "three years", "27 July 2008", "chemically", "Aristotle", "St. George's Church", "Missy", "Strathclyde Regional Council debating chamber in Glasgow", "public official", "the most cost efficient bidder", "acorn", "Continent that spans 11 time zones", "thighbone", "Olympia", "Chornobyl", "Whatchamacallits", "Teri Hatcher", "a hopeful miner", "amber", "Yale Basmati", "The executioner's Song", "180 degree", "almaty", "Anamosa", "drei Akten", "Ephesus", "Camelot--Camelot", "film", "rope", "blinding light", "Cologne", "Leadership Academy for Girls", "an ingot", "Kosovo", "James Jeffords", "Prague", "tennis", "silk", "buffalo", "raoul", "rooileus virginianus", "japan", "\"A Beer Can Named Desire\"", "Thant", "boys", "windjammer", "Monsieur Verdoux", "George S. Klein", "Augusta", "counter clockwise", "2013", "Nick Hornby", "parachutes", "December 24, 1973", "David Weissman", "bikinis", "the Dalai Lama", "memories of his mother", "Israel"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4833333333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2105", "mrqa_squad-validation-7818", "mrqa_squad-validation-9402", "mrqa_squad-validation-6801", "mrqa_searchqa-validation-2291", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-4439", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12545", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-6518", "mrqa_searchqa-validation-2445", "mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-10412", "mrqa_naturalquestions-validation-325", "mrqa_triviaqa-validation-6129", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3084"], "SR": 0.40625, "CSR": 0.6260416666666666, "EFR": 0.7894736842105263, "Overall": 0.7077576754385965}, {"timecode": 15, "before_eval_results": {"predictions": ["younger", "gambling", "28,000", "Muhammad ibn Zakar\u012bya R\u0101zi (Rhazes) (865\u2013915)", "river Deabolis", "April 20", "Rhenus", "1996", "wine", "German-Swiss", "Melbourne", "enter the priesthood", "Seattle Seahawks", "IBM", "Arthur Wynne", "Eric Maguire", "Strongsville, Ohio", "(Flemish)", "Mastercard", "Roger B. Smith", "Nashville", "the olfactory nerve", "Ivan the Terrible", "Nancy Astor", "lentigo senilis", "rob a casino", "Toronto Maple", "Zsa Zsa Gabor", "acting", "Utah", "a liquor made from molasses or sugar cane", "(Rabbit) Angstrom", "Johann Strauss II", "joey", "pro bono", "the Universit degli Studi di Siena", "a candy store", "a brown", "Anthony Fokker", "Nacho Libre", "copper", "devils or demons", "hemlock", "Jeffrey Wigand", "Wordsworth", "a sesame seed bun", "a meager allowance", "1942", "blimps;blimp", "Robert Kirchhoff", "a geisha", "a mermaid", "altruism", "Frederic Remington", "Juan Francisco Ochoa", "ThonMaker", "a tin star", "dark", "The Legend of Sleepy Hollow", "Doc Hollywood", "Afghanistan", "two", "Belgium", "Rio de Janeiro"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6390625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6463", "mrqa_squad-validation-9248", "mrqa_squad-validation-9270", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-2440", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-10427", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-7531", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15167", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-12729", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-309", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-3675", "mrqa_newsqa-validation-2036"], "SR": 0.515625, "CSR": 0.619140625, "EFR": 0.7741935483870968, "Overall": 0.6966670866935484}, {"timecode": 16, "before_eval_results": {"predictions": ["Keraite", "respiration", "1997", "late 1920s", "\u00a31.3bn", "27 July 2008", "unequal", "October 1973", "dragonnades", "Isiah Bowman", "assembly center", "Ominde Commission", "the Weser River", "in an open casket", "(Ho) Minh", "(circum) circumference", "an Inuit dwelling", "Detroit Rock City", "Toronto Blue Jays", "President Lincoln", "Ray Bradbury", "crimes committed out of hatred for someone's race", "King Julien XIII", "Nicolas Sarkozy", "Rubicon", "(Conello)", "17", "Louisa May Alcott", "Play-Doh", "Aphrodite", "Jesus Christ", "The Prince and the Pauper", "Crystal Pepsi", "Hillary Clinton", "(Errol Lincoln)", "(Bellerophontes)", "Balaam", "business school of business", "the caine mutiny", "(The Allman Brothers Band)", "(The Clorox Company)", "(John) Coltrane", "peace sign", "oxygen", "Sphinx", "Jan Hus", "USA Network's (The Sing-Off)", "blue", "Onegin", "Macy's", "spinning jenny", "(Santa)", "(Denzel Washington)", "negligence", "judges", "attached to another chromosome", "Goosnargh", "Australia", "The Jefferson Memorial", "aged between 11 or 13 and 18", "Michoacan Family", "prisoners", "his salary", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5933264652014651}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.8, 0.5, 1.0, 0.2, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.23076923076923078]}}, "before_error_ids": ["mrqa_squad-validation-1796", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-12183", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-16726", "mrqa_searchqa-validation-6610", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-1355", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-11707", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-9150", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-15453", "mrqa_searchqa-validation-8757", "mrqa_searchqa-validation-15626", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-858", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-6265", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-794", "mrqa_triviaqa-validation-4973", "mrqa_hotpotqa-validation-3410", "mrqa_newsqa-validation-1759"], "SR": 0.453125, "CSR": 0.609375, "EFR": 0.8, "Overall": 0.7046875}, {"timecode": 17, "before_eval_results": {"predictions": ["September 5, 1985", "mannerist architecture", "stratigraphers", "trade unions", "23.9%", "earn as much as a healthy young man", "Centrum", "Robert Lane and Benjamin Vail", "ring", "Party of National Unity", "22", "Dauphin", "Phillip Marlowe", "piracy", "Roger Clemens", "(Myuran Sukumuran", "Puerto Rico", "The Mausoleum", "The World Through More Than One lens", "Syria", "Deutsche Lufthansa", "The Old Man", "French", "Joe Louis", "Hydra", "the three Musketeers", "the Bayeux Tapestry", "a porch", "China", "Sunni", "notes placed at the bottom of a page", "Stephen Hawking", "Cicero", "Memphis", "soda", "Blanche DuBois", "quilt", "FRAM", "House of Representatives", "beer company", "Michael Moore", "Oman", "Chevy", "Ingenue", "Pennsylvania", "Don Juan", "Ian Fleming", "Headless Horseman", "London", "Yellowstone", "Summer", "Fiddler", "Ethiopian", "six", "1992", "a salt", "Bromley", "the Ruul", "Cartoon Network", "a small child", "know what's important in life", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "nuclear", "The drama of the action in-and-around the golf course"], "metric_results": {"EM": 0.625, "QA-F1": 0.6614583333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_squad-validation-1659", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-11215", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-1920", "mrqa_searchqa-validation-12092", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-12176", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-12814", "mrqa_naturalquestions-validation-3267", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-316", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-4110"], "SR": 0.625, "CSR": 0.6102430555555556, "EFR": 0.7916666666666666, "Overall": 0.7009548611111112}, {"timecode": 18, "before_eval_results": {"predictions": ["Super Bowl XXXIII", "1993", "June 1979", "friend", "tentacles", "Robert R. Gilruth", "complexity", "same-gender marriages", "2006", "mid-18th century", "hollandaise", "A Raisin in the Sun", "Sistine Chapel", "bielaruski", "a spread formation", "parsnips", "Big Bang", "Sex Pistols", "endodontist", "Saturn", "Cliffs of Dover", "Genoa", "Fanchette", "Jersey Boys", "the door of the Castle Church in Wittenberg", "Indiana", "Sleepless", "polyantha", "pho- a popular Vietnamese dish", "21", "John Janetzko", "Copeina arnoldi", "Paul McCartney", "omega-3", "Pivot", "Bachman Turner Overdrive", "FEARnet", "caddy Shack", "Tokyo", "Panama", "Eppson Center for Seniors", "Narnia", "Finnegans Wake", "Wordsworth", "Norway", "The Bear", "nuclear bombs", "Betrayal of Christ", "elephants", "Mazur", "Denmark", "covert", "The Silver State", "May 2010", "in the United States", "Sugarloaf Mountain", "Thailand", "gender queer", "Minister for Social Protection", "Berga", "the estate", "McFerrin, Robin Williams, and Bill Irwin", "ase", "the upper peninsula of Michigan, south to northern Louisiana, west to Colorado, and east to Massachusetts In northern Europe, a population was found in Moscow, Russia in 2003"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5270469961240309}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.6976744186046512]}}, "before_error_ids": ["mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-13718", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-4853", "mrqa_searchqa-validation-7964", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-3043", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-10266", "mrqa_searchqa-validation-9572", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2612", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-2421", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-2870"], "SR": 0.421875, "CSR": 0.600328947368421, "EFR": 0.918918918918919, "Overall": 0.75962393314367}, {"timecode": 19, "before_eval_results": {"predictions": ["to avoid trivialization", "plants that are biologically contained", "Earth", "53,000", "one", "poet", "two points", "20,000", "the kip", "skeletal muscle and the brain", "2014", "peptide bonds", "Montreal", "the results show moved to Sunday evenings", "egg", "volcanic activity", "Montgomery", "Rock Island, Illinois", "April 9, 2012", "Squamish, British Columbia, Canada", "Proposition 103", "arousal recognition", "Charlene Holt", "Buffalo Bill", "1991", "electron shells", "The Cornett family", "acid rain", "October 22, 2017", "Points that lie strictly to the left of the curve are said to be inefficient", "he cheated on Miley", "2001", "flawed democracy", "735 feet", "1871", "Rick Rude", "Ohio", "a form of business network", "a cylinder of glass or plastic", "Abraham Gottlob Werner", "Wakanda and the Savage Land", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Necator americanus and Ancy lostoma duodenale", "February 28", "Furious 7", "the American Civil War", "an electrical potential or ion concentration difference across the membrane", "Cecil Lockhart", "Mara Jade", "British and French Canadian fur traders", "semi-autonomous organisational units", "Lou Rawls", "Hermia", "Jupiter", "Latin", "15", "John Robert Cocker", "Israel", "a simple puzzle video game", "a palace", "the olfactory nerve", "Mallee", "a horse", "oxygen"], "metric_results": {"EM": 0.53125, "QA-F1": 0.605859375}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.625, 0.0, 0.2, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8896", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-5804", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5964", "mrqa_hotpotqa-validation-4926", "mrqa_newsqa-validation-3747", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-2227"], "SR": 0.53125, "CSR": 0.596875, "EFR": 0.8666666666666667, "Overall": 0.7317708333333334}, {"timecode": 20, "before_eval_results": {"predictions": ["petroleum", "Cloth of St Gereon", "Thomas Sowell", "more than 70", "death of a heretic", "His Biblical ideal of congregations' choosing their own ministers", "1886", "\"Blue Harvest\" and \"420\"", "Jacob Zuma", "gang rape", "illegal crossings", "10", "Wednesday", "201-262-2800", "different women coping with breast cancer", "well over 1,000 pounds", "political dead-end", "Mutassim", "Oklahoma", "Polo", "Joe Jackson", "in Amstetten", "computer problems", "Silvan Shalom", "Climatecare", "Steve Wozniak", "12-hour-plus shifts", "prisoners", "childbirth", "consumer confidence", "5:20 p.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian", "India", "1964", "Davidson", "Pakistan's combustible Swat Valley", "Friday", "1979", "the United States", "GospelToday", "chief executive officer", "\"There's no chance of it being open on time. Work has basically stopped,\" Bloomberg", "file papers shortly with an appeals court seeking an emergency stay", "non-European Union player in Frank Rijkaard's squad.Mexican forward Giovani dos Santos is set to take up the vacant slot alongside Cameroon international Samuel Eto'o and Ivory Coast midfielder Yaya Toure", "Michael Schumacher", "Hurricane Gustav", "gun", "Henrik Stenson", "orphans", "40", "Derek Mears", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "two years", "1966", "winter", "Whitsunday", "Dee and the Don", "Dumb and Dumber", "2004 Nokia Sugar Bowl", "Minton", "converging lens", "autu", "season five", "Revenge of the Sith"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5865172674547674}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.18181818181818182, 0.5, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.72, 0.16666666666666663, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-2466", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-911", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-1549", "mrqa_naturalquestions-validation-7266", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-3226", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9508", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3422"], "SR": 0.453125, "CSR": 0.5900297619047619, "EFR": 0.6, "Overall": 0.595014880952381}, {"timecode": 21, "before_eval_results": {"predictions": ["Cologne, Germany", "occupational stress among teachers", "El Centro metropolitan area and San Diego-Carlsbad-San Marcos metropolitan area", "chief electrician", "Newton", "static friction", "the assassination of US President John F. Kennedy", "responsibility for the abductions", "Winter Park at Union Station in Denver, Colorado.", "Casalesi Camorra clan", "Kenneth Cole", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", ", a North Korean Foreign Ministry spokesman described Rice as \"no more than an official of the most tyrannical dictatorial state in the world.", "\"Maude\"", "Climatecare, one of Europe's most experienced providers of carbon offset,", "Wednesday", "Cash for Clunkers", "Bobby Jindal", "9:20 p.m. ET Wednesday.", "Kim Clijsters", "Mashhad", "Amanda Knox's aunt", "jazz", "$17,000", "Barney Stinson", "Luiz Inacio Lula da Silva", "his father's parenting skills.", "two", "Bill", "J.G. Ballard", "a nurse who tried to treat Jackson's insomnia with natural remedies", "Sarah", "\"I saw guys who were 34, 35, 36 years old -- still young guys -- about to get out of the game, and I wondered what will they do now?\"", "1981", "17 Again", "Nigeria", "$83,27014", "Republicans", "EU naval force", "Chris Robinson", "son of Gabon's former president", "steam-driven, paddlewheeled overnight passenger boat.", "Hyundai Steel", "a bone-growth disorder that causes dwarfism", "London Heathrow's Terminal 5.", "canceled the swimming privileges", "February 12", "more than 30", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "India", "Steve Williams", "military action because we're getting frustrated", "The White House Executive chef", "Russell Huxtable", "Willy Russell", "Budapest", "\"The Expendables 2\" (2008)", "Northumbrian", "\"get thee to a nunnery\"", "wife Elena", "Argentinian", "Mercedes-Benz Superdome in New Orleans, Louisiana.", "Otto Eduard Leopold, Prince of Bismarck, Duke of Lauenburg"], "metric_results": {"EM": 0.5, "QA-F1": 0.6027802620173944}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.4, 1.0, 1.0, 0.11764705882352941, 0.5454545454545454, 0.0, 0.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.05714285714285714, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2717", "mrqa_squad-validation-7746", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-729", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-110", "mrqa_hotpotqa-validation-4514", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-8602", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-1056"], "SR": 0.5, "CSR": 0.5859375, "EFR": 0.8125, "Overall": 0.69921875}, {"timecode": 22, "before_eval_results": {"predictions": ["X-rays", "WMO Executive Council and UNEP Governing Council", "everyday Germans", "New York and Virginia, especially.", "two", "glowed even when turned off", "Pastor Paula White", "resources that could sustain future exploration of the moon and beyond.", "resources that could be found there", "April 6, 1994", "Prague", "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry rewards.", "a federal judge in Mississippi", "the department has been severely affected by the earthquake, with thousands of officers injured, killed or unaccounted for.", "22 million", "severe flooding", "a music video on his land.", "at the Lindsey oil refinery in eastern England.", "\"Slumdog Millionaire\" (No. 4)", "\"The Real Housewives of Atlanta\"", "18", "88", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide", "a president who understands the world today, the future we seek and the change we need.", "military trials for some Guant Bay detainees.", "Rany Freeman, an Egyptian living in Canada,", "Larry King", "Steven Chu", "racially motivated.", "Michael Partain", "women.", "the longest domestic relay in Olympic history,", "Zimbabwe's dire economic situation.", "No. 1 slot", "nine", "ash and rubble", "Friday", "some truly mind-blowing structures", "a Muslim with Lebanese heritage,", "the two-hour finale.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Ben Roethlisberger", "one", "Lee Myung-Bak", "Damon Bankston", "researchers", "involvement during World War II in killings at a Nazi German death camp in Poland.", "opium", "warning", "84-year-old", "Robert Park", "Rima Fakih", "the Isthmus of Corinth", "Nalini Negi", "2017 - 12 - 10", "Runcorn", "collarbone", "horiz\u014dn kyklos", "UFC 50: The War of '04", "June 11, 1973", "San Diego County Fair", "Toy Story", "Viva Zapata", "# Quiz # Question"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5575515140267816}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.0, 0.8235294117647058, 0.0, 0.5, 0.19999999999999998, 0.967741935483871, 1.0, 0.6086956521739131, 1.0, 1.0, 0.5714285714285715, 0.4, 0.4444444444444445, 1.0, 0.0, 1.0, 0.923076923076923, 0.9565217391304348, 0.10256410256410256, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1407", "mrqa_squad-validation-3127", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-2760", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1418", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-3875", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-482", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-4464"], "SR": 0.390625, "CSR": 0.5774456521739131, "EFR": 0.717948717948718, "Overall": 0.6476971850613156}, {"timecode": 23, "before_eval_results": {"predictions": ["phycoerytherin", "lost in the 5th Avenue laboratory fire of March 1895.", "economic inequality", "Davros", "Church and the Methodist-Christian theological tradition", "Behind the Sofa", "Tulsa, Oklahoma", "56", "Yemen", "2005", "Karen Floyd", "Four Americans", "those missing", "Haiti", "Susan Boyle", "Saturday", "Spain", "Jared Polis", "Janet and La Toya", "Dangjin", "30", "Michael Krane,", "lightning strikes", "Evans", "Italian government", "flooding", "threatening messages", "Noriko Savoie", "Citizens", "fake his own death", "\"in the interest of justice.\"", "martial arts", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "then-Sen. Obama", "Congress", "curfew", "Anne Frank, whose account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "once on New Year's Day and once in June, to mark the queen's \"official\" birthday.", "the government in Islamabad \"has so far not received any information or evidence relating to the Mumbai incident from the government of India.\"", "Zuma", "haute, bandeau-style little numbers", "five", "Iraqi Prime Minister Nouri al-Maliki", "September 11, 2001", "about 50", "a group of teenagers", "body bags on the roadway near the bus,", "al Fayed", "Desmond Tutu", "$17,000", "Toy Story", "$81,88010", "provide school districts with federal funds", "a transformiation, change of mind, repentance, and atonement", "Jason Lee", "phase of sleep", "substantive", "Kent", "beer and soft drinks", "five aerial victories.", "Cherokee River", "\"Manor Farm\"", "James A. Lovell", "Florida"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6476298418321444}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.5, 0.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.15789473684210525, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1721", "mrqa_hotpotqa-validation-162", "mrqa_searchqa-validation-8458", "mrqa_searchqa-validation-10787"], "SR": 0.578125, "CSR": 0.5774739583333333, "EFR": 0.9259259259259259, "Overall": 0.7516999421296295}, {"timecode": 24, "before_eval_results": {"predictions": ["black-and-yellow", "Frederick II the Great", "The Muslims in the semu class", "manually suppress the fire", "compound", "Nigeria", "Lindsey Vonn", "one of the shocks of the year", "him to step down as majority leader.", "The EU naval force", "gang rape", "ClimateCare, one of Europe's most experienced providers of carbon offset,", "The Louvre", "club", "like the video-game challenge of continuously trying to best your own fuel economy achievements,", "1979", "Iranian diplomat", "jazz", "an antihistamine and an epinephrine auto-injector", "Bangladesh's southern Bhola district.", "technology experts Michael Arrington, founder and former editor of Tech Crunch, and Vivek Wadhwa,", "one out of every 17 children under 3 years old in America", "President Sheikh Sharif Sheikh Ahmed", "a government-run health facility that provides her with free drug treatment.", "\"Britain's Got Talent\"", "military personnel", "behind the counter.", "11 healthy eggs", "one Iraqi soldier,", "Michael Partain,", "her fianc\u00e9", "racial intolerance.", "all animal products.", "\"El Viceroy\" and \"El General,\"", "Symbionese Liberation Army", "8.8 million", "work together to stabilize Somalia and cooperate in security and military operations.", "the ad would compromise the public broadcaster's appearance of unbiasedity.", "it -- you know -- black is beautiful", "$104,168,000", "Picasso's muse and mistress, Marie-Therese Walter.", "to provide alternative work for poor Afghan farmers to encourage them to give up opium production", "supplemental spending bill provides nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "off the coast of Dubai", "fallen comrades lost in the heat of battle.", "near Takoma Park, Maryland.", "27 Awa", "Mark Obama Ndesandjo", "\"Dance\" rated highly for Oxygen, with more than 1 million viewers tuning in.", "famous faces from the world visiting France, Russia, India, South Korea, China, South Africa, Brazil, Beirut and Poland.", "\"The Screening Room\"", "fatally shooting a limo driver on February 14, 2002.", "nucleus", "Treaty of Paris between France and the Sixth Coalition, and the Treaty of Kiel", "Sebastian Lund ( Rob Kerkovich )", "President Jimmy Carter", "Tom Watson", "Sandi Toksvig", "Hispania Racing F1 Team", "Viscount Cranborne", "Walt Disney World Resort in Lake Buena Vista, Florida", "Iceland", "wedlock", "platinum"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5458495312735243}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.5, 0.08695652173913043, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.23529411764705882, 0.33333333333333337, 0.0, 1.0, 0.0, 0.8, 0.5, 0.5, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.6666666666666666, 0.0, 1.0, 0.07407407407407407, 0.34285714285714286, 1.0, 0.0, 0.0, 0.4, 0.5, 0.14285714285714288, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-899", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-1744", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-4927", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-8678"], "SR": 0.40625, "CSR": 0.5706249999999999, "EFR": 0.7894736842105263, "Overall": 0.6800493421052631}, {"timecode": 25, "before_eval_results": {"predictions": ["unity of God", "Treaty of Logstown", "Jordan Norwood", "RNA silencing", "concurring, smaller assessments of special problems instead of the large scale approach", "Anthony Hopkins", "New Zealand", "Tamar", "rhododendron", "35", "specialist", "insect", "phylum", "Wayne Allwine", "St Pauls", "holography", "Pelias", "Sarah Collins", "Northumbria", "Harvard", "cricketer", "Seymour Hersh,", "long pole", "copper and zinc", "Tigris", "Cordelia", "pamphlets, posters, ballads", "dermatitis", "33", "spicy varietal", "Joseph Smith,", "Chicago", "palladium", "moon", "a number that can multiply with each other to form 169.", "petticoat", "The Virgin Spring", "Canada", "Winston Churchill", "Stockholm", "Peter Parker", "Goldie Myerson,", "Lesa Ukman", "bullfight", "Sparks", "Ginger Rogers", "Plymouth Rock", "Comedy Playhouse", "about 25 grams of citric acid per liter, or 1,000 grams, of juice", "Charles Darwin", "John", "Mr. Boddy", "Marie Van Brittan Brown", "Southern California Timing Association ( SCTA )", "1995", "Bourbon", "Taylor Swift.", "Clarence Coffee Jr., Kiesza, Charli XCX, Jacob Plant, and Jennifer Lopez.", "\"The three gunshot wounds to the head included two nonfatal rounds with entry points below the chin, and one fatal shot that entered Peterson through the right side of the head,\"", "a class to help women \" learn how to dance and feel sexy,\"", "Amy Bishop", "calathus", "the Louvre", "Seaver"], "metric_results": {"EM": 0.421875, "QA-F1": 0.505907154528478}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.9523809523809523, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 0.1764705882352941, 0.7777777777777778, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_squad-validation-8618", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-147", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-7210", "mrqa_triviaqa-validation-3096", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-3082", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-1059", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-7521", "mrqa_naturalquestions-validation-1399", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-2376"], "SR": 0.421875, "CSR": 0.5649038461538461, "EFR": 0.7027027027027027, "Overall": 0.6338032744282744}, {"timecode": 26, "before_eval_results": {"predictions": ["\"The Space Museum\"", "eight", "affordable housing", "Mao Zedong", "Verona", "Pontiac Silverdome", "elephants", "to cook in front of a large party of guests.", "Frank McCourt", "Harry Burton", "Judy Cassab", "margo Leadbetter", "Schengen Area", "A", "city of Sheffield, England", "Famous Players", "the Monkees", "Gerald Durrell", "jzebel", "Cork", "Pelias", "Arabian", "Halifax", "Noises Off", "jimmy osmond", "Frank Wilson", "Carlos the Jackal", "Edwina Currie", "sonja Henie", "Robert Maxwell", "1768", "For Gallantry", "Tuesday", "Caucasus", "Cahaba", "The Good Life", "Tahrir Square", "uranium", "Count de la F\u00e8re", "27", "Jack Ruby", "tintoretto", "Eric Coates", "Saudi Arabia", "Lester", "Thailand", "Sydney", "dove", "Tunisia", "Prince Philip", "Wellington", "Tokyo", "Edgar Lungu", "49 cents", "heart rate that exceeds the normal resting rate", "672", "\"Linda McCartney's Life in Photography\", \"Some Like It Hot\", \"Kubrick's Napoleon: The Greatest Movie Never Made\", \"Saturday Night Live: The Book\",", "Stonecoast MFA Program in Creative Writing", "keystroke", "Juan Martin Del Potro.", "27", "Edgar Allan Poe", "Richard Cory", "Buddhism"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5596354166666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4166666666666667, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2797", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-6100", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-7193", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-3354", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-733", "mrqa_newsqa-validation-279", "mrqa_searchqa-validation-12829"], "SR": 0.515625, "CSR": 0.5630787037037037, "EFR": 0.8709677419354839, "Overall": 0.7170232228195939}, {"timecode": 27, "before_eval_results": {"predictions": ["two", "80", "70", "forced Tesla out leaving him penniless.", "benazir Bhutto", "Iran's nuclear program.", "at least 27 Awa", "Larry King", "Daniel Cain", "acid", "Wally", "2008", "after Wood went missing off Catalina Island, near the California coast,", "Behar", "Afghanistan", "Everglades", "made 109 as Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "1950s", "64", "Iran's parliament speaker", "27-year-old", "young self-styled anarchists", "$163 million (180 million Swiss francs)", "baggage from the 80s and has grown beyond a resort town into something more substantial.", "around 3.5 percent of global greenhouse emissions.", "ensenada,", "Orbiting Carbon Observatory", "Switzerland", "Robert Redford", "Janet and La Toya", "more than 22 million people in sub-Saharan Africa", "hours", "returning combat veterans", "improve health and beauty.", "U.S. Chamber of Commerce", "burned badly on the backs of his knees and every time he moves his knee, it pulls, and if it's healing, it starts to bleed.\"", "al-Shabaab", "posting a $1,725 bail", "sustain future exploration of the moon and beyond.", "business dealings for possible securities violations", "Opry Mills,", "Number Ones", "normal maritime traffic", "he was diagnosed with skin cancer.", "al Qaeda", "Evan Wolfson", "\"gotten the balance right\"", "oceans", "brutalized", "Dr. Conrad Murray", "off the coast of Dubai", "Bill Haas", "Oona Castilla Chaplin", "1932", "between 1923 and 1925", "gilda", "j john dryden", "table tennis", "Tamil", "DreamWorks Animation", "Indianola", "Empire", "Benjamin Disraeli", "a rising sun"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5893771201356887}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.4, 0.8, 0.28571428571428575, 0.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.125, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.19354838709677416, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.8, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1640", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-563", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2976", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-4193", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-14496", "mrqa_searchqa-validation-15354"], "SR": 0.453125, "CSR": 0.5591517857142857, "EFR": 0.6857142857142857, "Overall": 0.6224330357142858}, {"timecode": 28, "before_eval_results": {"predictions": ["Bermuda 419 turf", "25-foot (7.6 m)", "symbols", "Hyundai Steel", "Monday night", "Bailey, Colorado", "the children on a plane to France, where the charity said host families were waiting to take the children in.", "40", "Illuminati was this secret society which was brutalized by the Catholic Church in the 1600s.", "in a public housing project, not too far from the stadium of her favorite team", "toxic smoke from burn pits", "Lucky Dube,", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "space shuttle Discovery", "Gavin de Becker", "a nuclear weapon", "the IV cafe.", "Arizona", "between South America and Africa.", "Tetris", "outside influences", "aid to Gaza,", "flipped and landed on its right side,", "suppress the memories and to live as normal a life as possible;", "Tuesday", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the area was sealed off, so they did not know casualty figures.", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England this weekend.", "Cash for Clunkers", "project work", "one of the most influential, powerful and admired public figures of our time, by the most widely read biographer of our era.", "80 percent of the woman's face", "London's 20,000-capacity O2 Arena.", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Ozzy Osbourne", "$50", "Australian officials", "the Walk of Fame.", "Bill Klein,", "gun", "38", "Argentina", "mayor of Seoul from 2002 to 2004,", "Somalia's piracy problem was fueled by environmental and political events.", "\"State of Play\"", "Kabul", "22", "Steven Gerrard", "12.3 million people worldwide", "the area was sealed off, so they did not know casualty figures.", "Rima Fakih", "Old Trafford", "help bring creative projects to life", "season two", "Mary Elizabeth Patterson", "mozart's finales", "Fifth", "Nepal", "Merck Sharp & Dohme", "Fort Albany", "Knoxville, Tennessee", "Nehru", "Transpiration", "hypomanic"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5666516286001579}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.47058823529411764, 0.0, 0.6666666666666666, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 1.0, 0.0, 0.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5100", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-3575", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1265", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-79", "mrqa_hotpotqa-validation-4763", "mrqa_searchqa-validation-5587", "mrqa_searchqa-validation-4465"], "SR": 0.46875, "CSR": 0.5560344827586207, "EFR": 0.6764705882352942, "Overall": 0.6162525354969575}, {"timecode": 29, "before_eval_results": {"predictions": ["Mike Carey", "100% oxygen", "Betty Meggers", "ancient cult activity", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "sex organs", "Russian army", "diffuse nebulae", "August 6 and 9, 1945", "Doug Diemoz", "Colony of Virginia", "Monk's Caf\u00e9", "central plains", "al - Mamlakah al - \u02bbArab\u012byah", "Southport, North Carolina", "Iran", "maintenance utility", "July 4, 1776", "pick yourself up and dust yourself off and keep going", "John Garfield as Al Schmid   Eleanor Parker as Ruth Hartley", "enabled European empire expansion into the Americas and trade routes to become established across the Atlantic and Pacific oceans", "October 12, 1979", "Lorazepam", "the 2013 non-fiction book of the same name by David Finkel", "salami", "Brenda ''", "ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Husrev Pasha", "Stephanie Judith Tanner", "the palmar aspect of these fingers, including both front and back of the tips, perhaps as far back as the fingernail beds", "McFerrin, Robin Williams, and Bill Irwin", "Watson and Crick", "Gorakhpur", "Patris et Filii et Spiritus Sancti", "the first four caliphs ( successors )", "Lake Powell", "ornament", "September 6, 2019", "population", "substitute good", "Veronica", "over 74", "1987", "cunnilingus", "October 2000", "New York City", "Mamata Banerjee", "the United States economy first went into an economic recession.", "closing of the atrioventricular valves and semilunar valves", "Hermann Ebbinghaus", "Marvin Gaye", "used their knowledge of Native American languages as a basis to transmit coded messages", "Donny Osmond", "army", "43rd", "gmbH", "7.63\u00d725mm Mauser", "seven", "Muslim", "the two remaining crew members from the helicopter,", "Saturday's Hungarian Grand Prix.", "Rickey Henderson", "Baikal", "adventure park"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5114315043194353}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5555555555555556, 0.0, 1.0, 0.3076923076923077, 0.9090909090909091, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.7142857142857143, 0.33333333333333337, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.5, 0.5, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.9333333333333333, 1.0, 0.0, 0.20689655172413793, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-10459", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-5010", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-9259", "mrqa_searchqa-validation-5753"], "SR": 0.34375, "CSR": 0.5489583333333333, "EFR": 0.5714285714285714, "Overall": 0.5601934523809524}, {"timecode": 30, "before_eval_results": {"predictions": ["address information", "Pleurobrachia", "1953", "AT&T", "pioneers.", "Hutter", "shoes", "ten", "Isaac Newton,", "acetylene", "in an illegal substance", "fiber", "gray deer", "Whats", "Marcus Garvey", "sand", "Nanjing", "Custer", "ry Python", "roi-Soleil", "\"What a joy to breathe the balmy air of Grosvenor Square\"", "Fox Network", "the Belgae", "Joe Lieberman", "the Boston Marathon", "fibreboard", "tin", "cvp70142", "Frida Kahlo", "sixth", "\"Y\" 2 \"K\": An Eskimo", "Fat Man", "Hair", "William Randolph Hearst", "basalt", "ale", "primate", "telephone", "\"When You Look Me in The Eyes\"", "Luther", "The New Colossus", "yelped in pain when the bee stung.", "Richard Wagner", "Princess Beatrice of York", "getting married tomorrow", "middleweight champion", "bronchodilator", "Forty", "a mixture of neon (99.5%) and argon gas.", "Sixberry", "a Chenard", "Earl Long", "Neil Patrick Harris", "Wyatt and Dylan Walters", "1999", "vitamin D", "three times", "Alberto juantorena", "R&B", "Awake", "Doctor of Philosophy", "Pakistan's", "in Seoul.", "an African-American woman"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4395833333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false], "QA-F1": [0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_searchqa-validation-10169", "mrqa_searchqa-validation-13591", "mrqa_searchqa-validation-135", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-13464", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-1693", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-3715", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15750", "mrqa_searchqa-validation-15306", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-4165", "mrqa_searchqa-validation-14012", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-3528", "mrqa_naturalquestions-validation-5485", "mrqa_triviaqa-validation-7493", "mrqa_triviaqa-validation-282", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5297", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-723"], "SR": 0.40625, "CSR": 0.5443548387096775, "EFR": 0.8421052631578947, "Overall": 0.6932300509337861}, {"timecode": 31, "before_eval_results": {"predictions": ["non-Mongol physicians", "Prospect Park", "the macula", "volume", "a crossword clue", "Breakfast at Tiffany's", "Diners' Club Card", "Christian Dior", "The Pittsburgh Cycle", "Juliet", "Notre Dame", "Tablecloth", "Tate", "Lt. Bligh", "Cecil Rhodes", "Edinburgh", "Swaziland", "Kevin Spacey", "Union Square", "Pennsylvania Railroad", "Mike Huckabee", "Queen", "a beta-blocker", "Chance", "kozo", "Tenzing Norgay", "Samuel Beckett", "Rachel Carsons", "Vietnam", "sports", "David Geffen", "Franklin D. Roosevelt", "Kate Middleton", "America Ferrera", "R", "Zechariah", "New Jersey", "Lake Ontario", "Matthew Perry", "Baltimore", "\"The Alamo\"", "fortune", "Charlie and the Chocolate Factory", "artillery", "aluminum", "General McClellan", "Ned Kelly", "a piles of papers", "gravitational field", "Osiris", "quiveir", "Heroes", "on the two tablets", "the organ transplant of a kidney into a patient with end - stage renal disease", "seven units", "Geheimrat Dr. Max", "Duke Ellington", "Stevie Wonder", "Ludwig van Beethoven", "March 13, 2013", "February 20, 1978", "two years", "Arsene Wenger", "in an interview Tuesday on CNN's \"Larry King Live.\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5296875}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2]}}, "before_error_ids": ["mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-16496", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-8269", "mrqa_searchqa-validation-11182", "mrqa_searchqa-validation-12766", "mrqa_searchqa-validation-3537", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-5737", "mrqa_searchqa-validation-2228", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-10344", "mrqa_searchqa-validation-9799", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-4939", "mrqa_searchqa-validation-10042", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-10526", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-114", "mrqa_newsqa-validation-2123"], "SR": 0.453125, "CSR": 0.54150390625, "EFR": 0.6571428571428571, "Overall": 0.5993233816964285}, {"timecode": 32, "before_eval_results": {"predictions": ["the tin had increased in weight and that increase was the same as the weight of the air that rushed back in.", "Fresno Street and Thorne Ave", "Black Death", "Kenneth", "John Stuart Mill", "Joshua Abraham Norton", "CIA", "piano", "Rickey Henderson", "Jawaharlal Bhutto", "D. carota ssp. maritimus", "John Grunsfeld", "Llados", "1976", "Galileo Descartes", "a quark", "Dust", "Rudy Giuliani,", "the Espionage Act", "a scallop", "Sif", "New Jersey", "The Omega Man", "a walk-in pantry", "a barrel", "the Olympic Olympics", "Hugo Chvez", "Shamir", "Hinduism", "tin", "Maid Tells of Seeing Jackson", "The Rime of the Ancient Mariner", "pine tar", "the Lincoln Tunnel", "Michael Collins", "Lindsey Davenport", "Los Angeles", "Favonius", "Richard III", "Labour", "pen", "Croatia", "Douglas Adams", "Strindberg", "Hawaii", "Stephen Crane", "Prussia", "Sophocles", "Mark Cuban", "Thought Police", "a bust", "Central Park", "Alice", "Part 2", "Coconut Cove", "a terPodion", "trumpet", "Amanda Hess", "2.1 million", "Edward James Olmos", "ZZ Top", "Omar Bongo,", "South Africa", "Ignazio La Russa"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6006944444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4870", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-10316", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-12683", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6555", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-13862", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-10213", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-1405", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1310", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-7602", "mrqa_hotpotqa-validation-4767"], "SR": 0.546875, "CSR": 0.5416666666666667, "EFR": 0.7586206896551724, "Overall": 0.6501436781609196}, {"timecode": 33, "before_eval_results": {"predictions": ["the BBC", "pathogens, an allograft", "a large concrete block is next to his shoulder, with shattered pieces of it around him. Blood trickles down the road.", "hours", "28", "back at work", "Oxbow,", "201-262-2800", "opium", "\"I think she's wacko.\"", "the annual White House Correspondents' Association dinner Saturday,", "Hussein's Revolutionary Command Council", "drugs", "the Dalai Lama", "Myanmar", "The station", "Hundreds of women protest child trafficking and shout anti-French slogans", "forgery and flying without a valid license,", "Little Rock military recruiting center", "Cash for Clunkers", "environmental", "North Korea intends to launch a long-range missile in the near future,", "terrorism", "hardship for terminally ill patients and their caregivers", "different women coping with breast cancer in five vignettes.", "a missile", "Police", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "Roger Federer", "Miami Beach, Florida", "over 1000 square meters in forward deck space, allowing for such features as a full garden and pool, a tennis court, or several heli-pads.", "CNN", "no chance", "a children's hospital in St. Louis, Missouri.", "he was one of 10 gunmen who attacked several targets in Mumbai", "two years ago", "two", "a very beautifully painted ruff of Italian lacework", "Symbionese Liberation Army", "he acted in self defense in punching businessman Marcus McGhee.", "two tickets to Italy", "Colombia", "in-cabin lighting system", "resources", "1981", "Los Angeles", "16", "Pope Benedict XVI", "Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "Appathurai", "$40 and a bread.", "Kgalema Motlanthe,", "the Ming dynasty", "George II", "2014 -- 15", "November 5, 2013", "Javier Bardem", "Scotland", "Erika Girardi", "Terry the Tomboy", "Araminta Ross", "Mrs. Potts", "Spokescandy", "The Star-Spangled Banner"], "metric_results": {"EM": 0.640625, "QA-F1": 0.714764968487395}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7058823529411765, 0.5, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.7499999999999999, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1379", "mrqa_triviaqa-validation-6451", "mrqa_hotpotqa-validation-145", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-3588"], "SR": 0.640625, "CSR": 0.5445772058823529, "EFR": 0.9130434782608695, "Overall": 0.7288103420716112}, {"timecode": 34, "before_eval_results": {"predictions": ["3", "the Koori", "anti- strike", "Washington State's decommissioned Hanford nuclear site,", "Yemen", "creditors", "nearly $2 billion in stimulus funds", "a businessman, team owner, radio-show host and author.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Spaniard Carlos Moya", "Bahrain", "children of street cleaners and firefighters.", "Joan Rivers", "$3 billion, with further foreign direct investment exceeding $40 billion during the operations phase.", "hardship for terminally ill patients and their caregivers", "Honduras", "Brazil", "environmental", "strife in Somalia,", "Roy", "the WBO welterweight title from Miguel Cotto on a 12th round technical knockout in Las Vegas.", "relatives of the five suspects,", "Meredith Kercher.", "lawyers trying to save their client from the death penalty", "Aniston, Demi Moore and Alicia Keys", "work together to stabilize Somalia and cooperate in security and military operations.", "Friday,", "cancerous tumors.", "20", "Matthew Fisher", "$1.5 million", "Tim Clark, Matt Kuchar and Bubba Watson", "40", "a model of sustainability.", "glamour and hedonism", "J. Crew", "Department of Homeland Security Secretary Janet Napolitano", "543 elected members, of which 58 are women.", "The woman who received the first-ever near-total face transplant in the United States", "Robert Gates", "Israel", "rural Tennessee.", "in critical condition", "Seoul,", "Nicole", "cheering", "next week.", "Adam Lambert", "regulators in the agency's Colorado office", "early detection and helping other women cope with the disease.", "James Whitehouse,", "hopes the journalists and the flight crew will be freed,", "gentry Buddhism", "Lionel Hardcastle", "Stephen Lang", "Dick Van Dyke", "noreg", "Beer", "Revengers Tragedy", "1972", "Hilda Neihardt", "Clare Harper", "fish", "Peter"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5176131543089264}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.4444444444444445, 0.2222222222222222, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-2761", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-3472", "mrqa_hotpotqa-validation-4378", "mrqa_searchqa-validation-16463", "mrqa_searchqa-validation-3719", "mrqa_searchqa-validation-7879"], "SR": 0.421875, "CSR": 0.5410714285714286, "EFR": 0.7297297297297297, "Overall": 0.6354005791505792}, {"timecode": 35, "before_eval_results": {"predictions": ["walked to the Surveyor, photographed it, and removed some parts", "Border Reiver", "July 4, 1826", "rum", "Nantucket", "an Islamic leadership position", "sap", "Malibu", "Sisyphus", "sound absorption", "Australia", "Ayla", "Rudolf Hess", "Cubism", "Gettysburg", "Paul Simon", "crabs", "Caliban", "crayon", "the Aegean Sea", "Battle of the Little Bighorn", "Shakers", "a bellwether", "The Disciple", "chips", "Boxer", "The Spiderwick Chronicles", "Mabel Harding", "Las Vegas", "the Bible", "the Rose Bowl", "Norman Rockwell", "the bouffant and the beehive", "a type that's best for you to buy if you want to eat tuna relatively often (for example, about one meal per week)", "Napa Valley", "Eurail France", "Washington, D.C.", "Atlanta", "klezmer", "Japan", "The Bodyguard", "12 men", "Nancy Pelosi", "blogging", "Jupiter", "Sadat", "a sundae", "Grace Evans", "50 million cells per litre", "Volitan Lionfish", "Charlie Sheen", "(the three sons all have the middle", "Bonnie Aarons", "Wednesday, 5 September 1666", "a pop ballad", "Seth", "Lou Gehrig", "meaning and origin", "1949", "Aamir Khan", "My Gorgeous Life", "Argentina has always claimed sovereignty over the islands and invaded them in 1982, prompting a war in which more than 600 Argentinean and 255 British military personnel died.", "High Court Judge Justice Davis", "Cipro, Levaquin, Avelox, Noroxin and Floxin."], "metric_results": {"EM": 0.53125, "QA-F1": 0.641261574074074}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.07407407407407407, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4001", "mrqa_searchqa-validation-193", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-4034", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-1389", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-12541", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-5061", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-821", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-2511", "mrqa_searchqa-validation-9342", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-12049", "mrqa_searchqa-validation-12788", "mrqa_triviaqa-validation-7591", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3884"], "SR": 0.53125, "CSR": 0.5407986111111112, "EFR": 0.8333333333333334, "Overall": 0.6870659722222223}, {"timecode": 36, "before_eval_results": {"predictions": ["lower-pressure boiler feed water", "Luzon", "Virginia", "nothing gained", "numismatic", "Supernanny", "Atlantic", "Cincinnati", "mosque", "Henry Hudson", "a gun blast tubes", "dry ice", "Theodore Roosevelt", "Entourage", "eel", "Philadelphia", "The Museum of Modern Art", "the unicorns", "John C. Frmont", "Russia", "BarBRA STREISAND", "Hermann Hesse", "the Taj Mittal", "English Monarchs", "Carmen", "Margaret Mitchell", "Quasimodo", "Money for Nothing", "Pandarus", "(bumrushed)", "The Longest Yard", "Sphinx", "Satchmo Armstrong", "Saudi Arabia", "American new wave", "Arby's", "coffee", "chivalry", "Burns", "Hulk", "Atlanta", "Memphis Belle", "Burkina Faso", "Central Pacific", "Prosecutor", "Icelandic", "(Buffalo)", "Tony Reali", "Edith Piaf", "Ivan I", "prologue", "birch", "Anthony Mayfield", "Jack Gleeson", "Phil Hurtt", "animals", "Massachusetts", "City of Starachowice", "Fredric March", "2009", "Democratic", "meteorologist", "$104,327,006", "17 Again"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7109375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15899", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-13205", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-11632", "mrqa_searchqa-validation-8556", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-15286", "mrqa_searchqa-validation-2262", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-8958", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-9131", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-5571", "mrqa_naturalquestions-validation-2026", "mrqa_triviaqa-validation-3956", "mrqa_newsqa-validation-3951"], "SR": 0.65625, "CSR": 0.5439189189189189, "EFR": 0.9090909090909091, "Overall": 0.726504914004914}, {"timecode": 37, "before_eval_results": {"predictions": ["Liechtenstein", "impressionist", "John Y. Brown Jr.", "oats", "Romney", "Ivan the Terrible", "Sally Field", "May 21, 1927", "Egypt", "pi", "tin", "Lake Maurapas", "Cousin Eddie", "W", "Marriott International", "Monaco", "Canada", "The Secret", "the Australian gold rush", "Collagen", "China", "a compound", "the cranes", "Juggling", "Alzheimer", "the Gulf of Mexico", "J. W. Scott,", "Euclid's Elements", "Eva Peron", "Cain", "Edward Asner", "X-Men: The Last Stand", "the Louvre", "King salmon", "Prison Break", "Mars", "Maine", "a sheep's milk cheese", "Meg March", "Rainer Maria Rilke", "deuce", "Hans", "Peter Bogdanovich", "Billy Joel", "Pilate", "BOAT PROPULSION", "the Quaternary Period", "nolo contendere", "Jr. Walker", "Czech Republic", "a seewhat", "the NIRA", "John Ernest Crawford", "beta decay", "France", "Priam", "Mariette", "Charles Quinton Murphy", "\"The Disaster Artist\"", "Australian", "the sins of the members of the church,", "22 million", "17 Again", "Nelson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6286458333333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-15864", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-10268", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-7541", "mrqa_searchqa-validation-10441", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-15664", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-1987", "mrqa_searchqa-validation-3594", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-477", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-12168", "mrqa_searchqa-validation-8068", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-5774"], "SR": 0.515625, "CSR": 0.5431743421052632, "EFR": 0.8709677419354839, "Overall": 0.7070710420203735}, {"timecode": 38, "before_eval_results": {"predictions": ["in whole by charging their students tuition fees", "Holden Caulfield", "Bill Hickok", "Leptospirosis", "recession", "Glashtyn", "Jay Silverheels", "Singapore", "the M1 Abrams", "marimba", "a bourn", "forgetting Sarah Marshall", "Witness", "Jack the Ripper", "3800", "Alan Shore", "taxonomy", "Spain", "the brain", "William McMaster Murdoch", "William Shakespeare", "comedy", "Mary Poppins", "Casowasco", "Fresh Prince of Bel-Air", "Nod", "Nitrides of boron & silicon are used to make crucibles", "\"Don't throw the baby out with the bathwater\"", "a married ceremony", "Livin' On A Prayer", "Sherlock Holmes", "licorice", "Marie Antoinette", "Ford", "Marie Curie", "Roger Brooke Taney", "g", "German", "Katamari Damacy", "Bill Murray", "Margaret Thatcher", "The Queen of Spades", "tungsten", "largest national forests", "Olympia", "Waylon Jennings", "The Bridge on the River Kwai", "Brazil", "B.C.", "Sydney Pollack", "scrapple", "Oona Castilla Chaplin", "October 6, 2017", "John Cooper Clarke", "helps managers understand employees' needs in order to further employees' motivation", "one", "Norfolk Island", "Wright brothers", "sexual activity", "Canada Goose", "Sandro Bondi refused to attend", "voluntary homicide", "\"deep sorrow\"", "Pygmalion"], "metric_results": {"EM": 0.4375, "QA-F1": 0.509375}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true], "QA-F1": [0.4, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1, 0.5, 0.16666666666666669, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6983", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-14698", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-16680", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-379", "mrqa_searchqa-validation-4413", "mrqa_searchqa-validation-6803", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8689", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-9146", "mrqa_searchqa-validation-4043", "mrqa_searchqa-validation-1961", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-2282", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-2005", "mrqa_newsqa-validation-630", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-600"], "SR": 0.4375, "CSR": 0.5404647435897436, "EFR": 0.8611111111111112, "Overall": 0.7007879273504274}, {"timecode": 39, "before_eval_results": {"predictions": ["Brazil", "Boogie Woogie Bugle Boy", "Europe", "Jack Nicholson", "a tutu", "Sweeney Todd", "The Bridge on the River Kwai", "the Byzantine Empire", "Thisbe", "Jefferson", "Ford Madox Ford", "The Orinoco", "a ready-to-use cotton swab", "Alaska", "Dixie's Land", "RAND Corporation", "Warren Harding", "engrave", "Shue", "Francis Crick", "Jay and Silent Bob", "Heath", "Abkhazia", "Twelfth Night", "Hawaii", "a key", "Tito", "conformation dog shows", "Ratatouille", "circadian rhythms", "Calvin Coolidge", "Mark Cuban", "Rudy Giuliani", "eyes", "Tony Dungy", "the Danube", "Andrew Johnson", "26", "life", "a plant", "chess", "GIGO", "Johannes Brahms", "Charleston Southern", "Italian", "The Grapes of Wrath", "a bicentennial", "Byzantium", "Mayo", "Led Zeppelin", "a Tesla coil", "Denmark", "Anna Murphy", "March 15, 1945", "Charles Darwin", "Old Trafford", "Miles Morales", "Honey Irani", "theme park song \"It's a Small World (After All)", "Kalahari Desert", "Alan Graham", "Bob Dole", "Bollywood superstar Amitabh Bachchan", "managing his time"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5621279761904762}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-3741", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-6190", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-7830", "mrqa_searchqa-validation-14076", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-2211", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-5025", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-15687", "mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-7544", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-6266", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-4134", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-4073"], "SR": 0.484375, "CSR": 0.5390625, "EFR": 0.7575757575757576, "Overall": 0.6483191287878788}, {"timecode": 40, "before_eval_results": {"predictions": ["63", "Baden-W\u00fcrttemberg", "James Weldon Johnson", "horror", "Oakdale", "Missouri", "FAI Junior Cup", "Flaw", "alt-right", "The Drudge Report", "15,000 people", "Yellow fever", "Pitch Perfect 2", "1934", "13\u20133", "\"The Andy Williams Christmas Album\"", "Tsavo East National Park", "New York Islanders", "Algirdas", "nearly 80 years", "Jean Acker", "the Championship", "The Gettysburg Address", "an American singer, actress, producer, and model.", "Premier League club Manchester United and the England national team.", "The Rite of Spring", "David Allen", "26,000", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn.", "Mayor Ed Lee", "1958", "1993", "burlesque", "Afro-Russian", "Loretta Lynn", "England", "a B-17 Flying Fortress", "1 December 1948", "11", "the XXIV Summer Universiade", "bronze", "1994", "Kansas City", "1999", "Pinellas County", "beer", "London", "Ployer Peter Hill", "Mindy Kaling", "1988", "Leonard Cohen", "Erika Mitchell Leonard", "Mase Dinehart", "Tevye", "Sir Tom Finney", "CameroonCameroon", "obtaining and proper handling of human blood.", "toxic smoke from burn pits", "two", "Iggy Pop invented punk rock.", "a lawyer", "a man", "Leonardo DiCaprio", "a narcissistic ex-lover who did the protagonist wrong"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6542994281045751}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.25, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.4, 0.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-2151", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-3552", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1030", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-16547", "mrqa_naturalquestions-validation-6326"], "SR": 0.5625, "CSR": 0.5396341463414633, "EFR": 0.7857142857142857, "Overall": 0.6626742160278745}, {"timecode": 41, "before_eval_results": {"predictions": ["a interception", "10", "was \"in bad condition at the scene,\"", "Les Bleus", "2005", "more than 4,000", "Arlen Specter", "an angry mob.", "normal maritime traffic", "Sri Lanka", "death", "an average of 25 percent", "he was led away in handcuffs after being sentenced in a New Jersey court for fatally shooting a limo driver", "Al Nisr Al Saudi", "as many as 50,000", "piano", "$250,000", "a \"prostitute\"", "a Columbian mammoth fossil", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "Brazil", "psychiatric nurse-practitioner failed to diagnose the troubled infantryman", "Russia", "Twitter", "through a facility in Salt Lake City, Utah", "Manmohan Singh", "Haiti", "Tuesday afternoon", "militants", "23 years", "a head injury.", "Bahrain", "an open window that fits neatly around him", "Leo Frank", "Paul McCartney", "Washington", "Zimbabwean official.", "don't have to visit laundromats", "three", "Diversity", "on-loan David Beckham claimed his first goal in Italian football.", "\"He is more American than German.\"", "Twilight", "forgery and flying without a valid license", "11", "A third beluga whale belonging to the world's largest aquarium has died", "Fayetteville, North Carolina,", "Indonesia military transport plane crashed into a residential area in East Java early Wednesday, killing 98 people, military officials said.", "al Qaeda", "Hillary Clinton", "a Christian farmer", "angular rotation", "heart", "54 Mbit / s", "in the County of Gloucestershire", "the Consolidated B-24 Liberator", "an Incredible, Edible... breakfast cereals.", "Oakdale", "Melbourne", "David S. Goyer", "stocks", "Monty Python and the Holy Grail", "Sweden", "Department of Transportation"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6068239180141753}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.11764705882352942, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 0.4, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.1111111111111111, 1.0, 0.0, 0.5, 0.0, 0.7272727272727273, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5714285714285715, 0.0, 0.0, 0.625, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8571428571428571, 0.11428571428571428, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.28571428571428575, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-3095", "mrqa_naturalquestions-validation-5552", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-376", "mrqa_hotpotqa-validation-1275"], "SR": 0.453125, "CSR": 0.5375744047619048, "EFR": 0.6857142857142857, "Overall": 0.6116443452380953}, {"timecode": 42, "before_eval_results": {"predictions": ["Accountants", "Las Vegas", "Zimbabwe", "Italian Serie A title", "Darrel Mohler", "dancing against a stripper's pole.", "Michoacan Family", "WTA Tour titles at Strasbourg and Bali prior to Madrid", "Morgan Tsvangirai.", "42", "taking on the swords of the Taliban. She crafts poems telling of the pain and suffering of children just like her; girls banned from school, their books burned,", "If huge hunks of ice -- such as parts of Greenland and the western shelf of Antarctica -- melt, then the rise is expected to be more dramatic.\"", "80 percent", "1979", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Elena Kagan", "CNN's \"Larry King Live,\" ABC's \"Good Morning America\" and \"The View.\"", "an auxiliary lock", "1-1", "AbdulMutallab", "Myanmar", "Rambosk", "Marcus Schrenker, a suburban Indianapolis financial manager who authorities say tried to fake his own death by crashing his private plane into a Florida swamp.", "Philippine National Police.", "poems", "Channel 4 said the program was made with the parents' full consent.", "(the Democratic VP candidate delivers a big speech next Wednesday)", "The Red Cross, UNHCR and UNICEF", "Moscow", "debris", "The jury at Liverpool Crown Court took a little over an hour to clear Gerrard of charges relating to a fracas in a nightclub bar in the north-western of England city on December 29 of last year.", "capital murder and three counts of attempted murder", "Basel", "17", "Daytime Emmy Lifetime Achievement Award", "state senators", "31 meters (102 feet) long and 15 meters (49 feet) wide", "nude beaches.", "how preachy and awkward cancer movies can get.", "her father's", "shark River Park in Monmouth County", "three out of four", "Islamabad", "partying", "Capitol Hill,", "\"theoretically\" Iran could develop a nuclear weapon", "1940's", "March 22", "three different videos that we like and want to know which ones you think are the best.", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Antichrist", "a major fall in stock prices", "Thomas Jefferson", "Alexander Salkind", "Orion", "brown", "Selfie", "2002", "England", "Manhattan Project", "the Rat", "rain", "Crawford", "Pyrenees"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6550543504174018}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.5, 0.6486486486486487, 0.0, 0.5714285714285715, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.16, 0.0, 1.0, 0.8235294117647058, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-495", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5989", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5834"], "SR": 0.546875, "CSR": 0.5377906976744187, "EFR": 0.8275862068965517, "Overall": 0.6826884522854852}, {"timecode": 43, "before_eval_results": {"predictions": ["north,", "legitimacy of that race.", "At least 88", "North Korea intends to launch a long-range missile in the near future,", "Kurt Cobain", "Former detainees of Immigration and Customs Enforcement", "33-year-old", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "hardship for terminally ill patients and their caregivers,", "Jaime Andrade", "Zac Efron", "finance", "nearly $2 billion", "The National Infrastructure Program, as he called it,", "1941", "\"The station", "Krishna Rajaram,", "a man's lifeless, naked body", "Robert Mugabe", "the wife of Gov. Mark Sanford,", "Camp Lejeune, North Carolina", "Saturday.", "$1.5 million", "a violent government crackdown seeped out.", "Iran could be secretly working on a nuclear weapon", "the fact that the teens were charged as adults.", "death squad killings carried out during his rule in the 1990s.", "Elena Kagan", "Hyundai", "100 percent", "Saturday", "Afghanistan,", "prisoners at the South Dakota State Penitentiary", "seven", "200", "Pakistan", "Seminole Tribe", "Rima Fakih", "South Africa", "Obama", "helicopters and unmanned aerial vehicles", "Secretary of State Hillary Clinton,", "maintain an \"aesthetic environment\" and ensure public safety,", "165-room", "second", "Jund Ansar Allah", "1,500", "a receptionist with a gunshot wound in her stomach", "$50 less", "$60 billion on America's infrastructure.", "ALS6,", "Malayalam", "Mad - Eye Moody and Hedwig", "1960 Summer Olympics in Rome", "Aston Villa", "peasants, small and medium-size farmers, landless people, women farmers, indigenous people, migrants and agricultural workers", "pool", "1822", "The Dressmaker", "Trilochanapala", "crote", "a buffalo", "\"The Wizard of Oz\"", "the frontal lobe"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6501830748746866}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 0.2105263157894737, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.08333333333333333, 1.0, 0.9523809523809523, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.7499999999999999, 0.33333333333333337, 0.5, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1975", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-8741", "mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-2278", "mrqa_searchqa-validation-11223", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-2281"], "SR": 0.546875, "CSR": 0.5379971590909092, "EFR": 0.7931034482758621, "Overall": 0.6655503036833856}, {"timecode": 44, "before_eval_results": {"predictions": ["Bermuda 419 turf.", "Los Angeles", "Chris Eubank Jr.", "Duval County", "Benj Pasek and Justin Paul,", "Andes", "1952", "Angola", "19th", "January 28, 2016", "Araminta Ross", "Roger Staubach", "1944", "Highlands Course", "Franconia, New Hampshire", "Operation Watchtower", "Dan Crow", "War & Peace", "Amberley Village", "What Are Little Boys Made Of", "Berea College", "Omaha Nighthawks", "Luca Guadagnino", "Liesl", "Germany and other parts of Central Europe,", "New York Islanders", "Todd Phillips", "26,788", "the Troubles", "1967", "Marktown", "law", "Radcliffe College", "Charles Guiteau", "Ford", "If the citizen's heart was heavier than a feather they would face torment in a lake of fire.", "India", "Lutheranism", "armed", "25 million", "The Snowman", "Ella Fitzgerald", "Chris Claremont", "Rain Man", "Interscope Records", "British East India Company", "3,672", "\"the most influential private citizen in the America of his day\"", "I'm Shipping Up to Boston", "American", "Believe", "the dynasty", "the largest country in Oceania and the world's sixth - largest country by total area", "the first to develop lethal injection as a method of execution", "Nicola Adams", "expedition sent by Jan van Riebeeck to the area, described meeting people of the so-called Chainouqua-tribe near Baardskeerdersbos,", "Russia", "dependable Camry", "Steven Green", "in a hotel,", "Chaucer", "rattlesnakes", "Riddles", "healthy"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6321690142825012}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.2105263157894737, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-1058", "mrqa_hotpotqa-validation-1815", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2515", "mrqa_searchqa-validation-12418", "mrqa_searchqa-validation-13986", "mrqa_searchqa-validation-4414"], "SR": 0.578125, "CSR": 0.5388888888888889, "EFR": 0.7407407407407407, "Overall": 0.6398148148148148}, {"timecode": 45, "before_eval_results": {"predictions": ["Kelvin Benjamin", "murder in the beating death of a company boss who fired them.", "the United States, NATO member states, Russia and India", "30", "crocodile eggs", "Colorado prosecutor", "Jason Chaffetz", "the second she got back from Mexico, she pulled me into a hug and gave me a big kiss -- told me to get down there myself.", "in Texas", "in July for A Country Christmas, and the festivities run from mid-November until the holidays end.", "trail the illegal traffic.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Herman Cain", "\"17 Again,\"", "the country only intends short- and medium-range missile tests, according to one U.S. intelligence official.", "Wigan Athletic", "Romney", "two years ago", "businessman", "Picasso's muse and mistress, Marie-Therese Walter.", "a diet for a few months and lose weight and then go back to exactly how you were living before", "Heshmatollah Attarzadeh", "the ireport form", "the government", "Nine out of 10 children", "police", "Sen. Joe Lieberman, I-Connecticut,", "a crocodile", "a bronze medal in the women's figure skating final,", "killed at least 63 people and wounded more than 200.", "Congress", "Susan Boyle", "a law signed Tuesday by President Obama.", "Phillip A. Myers.", "Obama's", "King Gyanendra,", "homicide", "Casey Anthony, 22,", "officers at a Texas  airport", "10 municipal police officers", "UNICEF", "surrogate", "228", "Kerstin and two of her brothers, ages 18 and 5,", "since 2004.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Joan Rivers", "supermodel and philanthropist", "Jacob Zuma", "in the Oaxacan countryside of southern Mexico", "Arsene Wenger", "slavery", "Kat ( Jesse Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Latin liberalia studia", "a British children's writer", "Johnny Mathis", "Beverly Hills Cop,", "Champion Jockey", "Luca Guadagnino", "Ms. Jackson", "the caged bird", "timing shapes and supports brain function", "a jigger", "a Bristol Box Kite"], "metric_results": {"EM": 0.484375, "QA-F1": 0.588158195970696}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.47619047619047616, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.6666666666666666, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-994", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1360", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5640", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-2431", "mrqa_triviaqa-validation-7461"], "SR": 0.484375, "CSR": 0.5377038043478262, "EFR": 0.7272727272727273, "Overall": 0.6324882658102767}, {"timecode": 46, "before_eval_results": {"predictions": ["acular", "bipartisan", "Nirvana", "via phone calls or by text messaging,", "without bail and will be arraigned June 25,", "12.3 million", "Mexico", "Argentine", "Vivek Wadhwa,", "Brett Cummins,", "eight Indian army troopers, including one officer, and 17 militants,", "Saturday", "Nicole", "legitimacy of that race.", "Adidas", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "Africa", "American", "bartering -- trading goods and services without exchanging money", "Wednesday.", "improve health and beauty.", "Chinese", "Newcastle", "Nothing But Love", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry", "June 6, 1944,", "almost all [Middle East and North Africa] countries,", "twice", "October 19", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Seoul,", "promotes fuel economy and safety while boosted the economy.", "ALS6,", "eight", "Siri", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "246", "VBS.TV", "children of street cleaners and firefighters.", "North Korea intends to launch a long-range missile in the near future,", "the area was sealed off, so they did not know casualty figures.", "attempting illegal crossings", "The American Civil Liberties Union", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "38", "Her husband and attorney, James Whitehouse,", "test scores and graduation rates are improving faster in other nations than in the United States and this threatens our quality of life in a competitive world.", "two", "the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "cancer,", "two", "Arnold Schoenberg", "Brooklyn, New York", "Jean Fernel ( 1497 -- 1558 )", "Discworld", "Japan", "fox hunting", "New York", "travel diary", "16,116", "\"Cry-Baby\"", "sugar", "a bumblebee", "Rowan Blanchard"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7099153884310134}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5333333333333333, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7692307692307693, 0.0, 1.0, 0.625, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.04761904761904762, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615388, 1.0, 1.0, 0.07142857142857142, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-89", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1764", "mrqa_naturalquestions-validation-5769", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-11573"], "SR": 0.65625, "CSR": 0.5402260638297872, "EFR": 0.9090909090909091, "Overall": 0.7246584864603481}, {"timecode": 47, "before_eval_results": {"predictions": ["Corendon Dutch Airlines", "A Rush of Blood to the Head", "5", "Chicago", "\"The Ones Who Walk Away from Omelas\"", "child actor", "Democratic", "drawing the name out of a hat", "Ryan Eldredge", "I-League", "two or three", "Badfinger", "Lady Frederick Windsor", "point-coloration", "1853", "1983", "Citizens for a Sound Economy", "2027 Fairmount Avenue", "1946 and 1947", "5,112", "1992", "many artists' lofts and art galleries, but is now better known for its variety of shops ranging from stylish upscale boutiques to national and international chain store outlets.", "14,673", "6'5\" and 190 pounds", "Mickey Gilley's", "Swiss federal popular initiative \"against mass immigration", "German shepherd", "Mexican", "December 24, 1973", "1933", "the backside", "Kristoffer Rygg", "1730", "London Luton Airport.", "the Salzburg Festival", "Mississippi", "Afghanistan", "1959", "Imelda Marcos", "Randall Boggs", "Part II", "Bunker Hill", "lion", "Royal", "World War II", "Knoxville, Tennessee", "\"Three's Company\"", "P.O.S,", "Labour", "Linda McCartney's Life in Photography", "English", "September 14, 2008", "79", "Frank Theodore `` Ted '' Levine", "Romania", "Farlake", "Mt Kenya", "Aung San Suu Kyi", "Afghan National Security Forces at the site.", "Her husband and attorney, James Whitehouse,", "Cairo", "Secretariat", "ante rooms", "Lehman Bros International (Europe)"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6873511904761904}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.13333333333333333, 0.0, 0.4, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-5531", "mrqa_naturalquestions-validation-4043", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5309", "mrqa_newsqa-validation-1795", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-10434", "mrqa_triviaqa-validation-2701"], "SR": 0.578125, "CSR": 0.541015625, "EFR": 0.7777777777777778, "Overall": 0.6593967013888888}, {"timecode": 48, "before_eval_results": {"predictions": ["ragweed", "St Petersburg", "date", "scrimmage", "Erinyes", "the hexameters", "Fawn Hall", "waive", "ding-dong", "Barnum", "Johnny Weissmuller", "cherries", "torque", "gold", "Marlon Brando", "Middle Dutch", "Renoir, Degas", "University of Kentucky", "ruddy", "Brussels", "Macbeth", "General Lee", "piracy", "suicide", "Martin Luther", "Clue", "Poe", "Norway", "Stephen A. Douglas", "15", "Mike Connors", "Juno Jim", "Jim Inhofe", "sancire", "Corpus Christi", "South Africa", "an ostrich", "Declaration of Independence", "night shift", "mug", "Desperate Housewives", "Galileo Galilei", "Canada", "the Princess Diaries", "Split", "the Grail", "West Virginia", "Thomas Jefferson", "movie house", "Six Flags Entertainment Corporation", "critic", "Ulbricht", "1904", "young girl", "Bobby Tambling", "ambilevous", "chariot", "Humberside Airport", "more than 265 million", "100 million", "freezing gasoline prices for the rest of the year and lowering natural gas prices by 10 percent.", "head injury.", "Pope Benedict XVI", "Charles II"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5216711956521739}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-599", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-6362", "mrqa_searchqa-validation-10470", "mrqa_searchqa-validation-507", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-3406", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-10801", "mrqa_searchqa-validation-9942", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-14589", "mrqa_naturalquestions-validation-3714", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2811", "mrqa_hotpotqa-validation-2171", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1663"], "SR": 0.4375, "CSR": 0.5389030612244898, "EFR": 0.7777777777777778, "Overall": 0.6583404195011338}, {"timecode": 49, "before_eval_results": {"predictions": ["the NSA", "Heisman Trophy", "Brandi Chastain", "the Colorado River", "Pamela Anderson", "carnaval", "Treasure Island", "Pocahontas", "\"Whose Line Is It Basically?\"", "(Whizzer) White", "an octave lower than the lowest pitched four strings", "an inhalant or deodorant", "The Plot", "Matthew Broderick", "Joseph Campbell", "Margaret Mitchell", "Charles Busch", "the Percheron", "Ernest Lawrence", "rodeo", "a fresco", "Nevil Shute", "a poet", "Jesse Jackson", "Tudor", "Department of Homeland Security", "the Black Sea", "a leotard", "Bulworth", "a shovelfuls", "the mouthpiece", "Key West", "the Fellowship of the Ring", "Vinyl", "repellents", "Manhattan", "Feb 2, 2016", "Leontyne Price", "compost", "BUNNY", "Christopher Columbus", "Phil Mickelson", "Carrie Bradshaw", "the Pierian spring", "Hungary", "a burnoose", "Philadelphia", "peanut butter", "Invisible Man", "leather", "Lex Luthor", "food and clothing", "Schwarzenegger ( Schwarzenegger ) and his companion, the thief Malak ( Walter )", "Master Christopher Jones", "Cuneiform", "\"Meadowbank II The Sequel - Scotland Mad", "St Moritz", "October", "Drifting", "Ellesmere Port, United Kingdom", "The incident Sunday evening", "three out of four", "poems", "\"Nebo Zovyot\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5538194444444444}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9727", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-8249", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-10212", "mrqa_searchqa-validation-4813", "mrqa_searchqa-validation-7250", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-13182", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-8175", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-2904", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1028", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1301"], "SR": 0.5, "CSR": 0.538125, "EFR": 0.78125, "Overall": 0.6596875}, {"timecode": 50, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1046", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2118", "mrqa_hotpotqa-validation-2280", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3527", "mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4638", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-3641", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-363", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4090", "mrqa_newsqa-validation-4135", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-511", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-11686", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-11948", "mrqa_searchqa-validation-1197", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12405", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-13875", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-14624", "mrqa_searchqa-validation-14703", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-15062", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-15186", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-15354", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-15556", "mrqa_searchqa-validation-16418", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-16666", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-2376", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3496", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-3952", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6959", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7879", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-8505", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-8597", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-9107", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-945", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-9903", "mrqa_squad-validation-1002", "mrqa_squad-validation-10020", "mrqa_squad-validation-10100", "mrqa_squad-validation-10186", "mrqa_squad-validation-10254", "mrqa_squad-validation-10306", "mrqa_squad-validation-1146", "mrqa_squad-validation-1204", "mrqa_squad-validation-1506", "mrqa_squad-validation-1758", "mrqa_squad-validation-1906", "mrqa_squad-validation-1943", "mrqa_squad-validation-1960", "mrqa_squad-validation-2059", "mrqa_squad-validation-2225", "mrqa_squad-validation-2351", "mrqa_squad-validation-2466", "mrqa_squad-validation-2487", "mrqa_squad-validation-2530", "mrqa_squad-validation-2880", "mrqa_squad-validation-298", "mrqa_squad-validation-3265", "mrqa_squad-validation-3279", "mrqa_squad-validation-3703", "mrqa_squad-validation-3840", "mrqa_squad-validation-4047", "mrqa_squad-validation-4290", "mrqa_squad-validation-4315", "mrqa_squad-validation-4330", "mrqa_squad-validation-4353", "mrqa_squad-validation-4415", "mrqa_squad-validation-4455", "mrqa_squad-validation-4468", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-4673", "mrqa_squad-validation-4759", "mrqa_squad-validation-4812", "mrqa_squad-validation-4876", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5170", "mrqa_squad-validation-549", "mrqa_squad-validation-5568", "mrqa_squad-validation-5581", "mrqa_squad-validation-5643", "mrqa_squad-validation-5812", "mrqa_squad-validation-5917", "mrqa_squad-validation-6106", "mrqa_squad-validation-6176", "mrqa_squad-validation-6218", "mrqa_squad-validation-6282", "mrqa_squad-validation-6547", "mrqa_squad-validation-6645", "mrqa_squad-validation-6694", "mrqa_squad-validation-670", "mrqa_squad-validation-6741", "mrqa_squad-validation-6797", "mrqa_squad-validation-6801", "mrqa_squad-validation-6842", "mrqa_squad-validation-6927", "mrqa_squad-validation-6941", "mrqa_squad-validation-7035", "mrqa_squad-validation-7069", "mrqa_squad-validation-7159", "mrqa_squad-validation-7674", "mrqa_squad-validation-7674", "mrqa_squad-validation-7757", "mrqa_squad-validation-7790", "mrqa_squad-validation-7818", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-8047", "mrqa_squad-validation-8503", "mrqa_squad-validation-8651", "mrqa_squad-validation-8733", "mrqa_squad-validation-8745", "mrqa_squad-validation-8833", "mrqa_squad-validation-8836", "mrqa_squad-validation-8896", "mrqa_squad-validation-9080", "mrqa_squad-validation-910", "mrqa_squad-validation-9170", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9311", "mrqa_squad-validation-9398", "mrqa_squad-validation-940", "mrqa_squad-validation-9411", "mrqa_squad-validation-9543", "mrqa_squad-validation-9726", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1474", "mrqa_triviaqa-validation-1546", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1729", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-1959", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5445", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-993"], "OKR": 0.72265625, "KG": 0.5, "before_eval_results": {"predictions": ["Fatih Ozmen", "the 850 saloon", "Skyscraper", "Cadillac Stingrays", "Norway", "Tom Jones", "VIMN Russia", "Homer Hickam, Jr.", "2015", "Hawaii County", "Robert Downey, Jr.", "Continental AG", "band director", "Germanic", "Anaheim", "Reinhard Heydrich", "Big Ben", "Standard Oil", "\"The Longest Yard\"", "Chiwetel Umeadi Ejiofor", "president of Guggenheim Partners", "19th", "Hillary Scott", "WikiLeaks", "President of the United States", "Tottenham Hotspur", "10 November 2017", "Vixen", "Forbidden Quest", "Rymill Park", "balloon Street, Manchester", "May 1, 2011", "Santa Fe", "political commentator", "Adelaide Lightning", "Operation Neptune", "Lancia-Abarth", "Trouble", "ten", "Diamond White", "Ferrara", "created the American Land-Grant universities and colleges", "Indooroopilly Shoppingtown", "2006", "Matt Flynn", "Indian", "hamburgers", "Liverpool", "little hairs", "Luigi Segre", "The United States House of Representatives", "February 16, 2018", "the studies and developments department of the French firm R2E Micral", "Nacio Herb Brown ( music ) and Arthur Freed ( lyrics )", "Michael Hart", "Proterozoic", "Iona", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England this weekend.", "had a relationship with another person.", "the infant who became the center of an international end-of-life debate,", "Paul Newman", "Puccini", "Planes, Trains & Automobile", "milk and honey"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5116319444444445}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.2, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.13333333333333333, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4170", "mrqa_hotpotqa-validation-5283", "mrqa_hotpotqa-validation-1509", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-221", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4995", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-2260", "mrqa_hotpotqa-validation-2137", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-5600", "mrqa_triviaqa-validation-7101", "mrqa_triviaqa-validation-4774", "mrqa_triviaqa-validation-535", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-355", "mrqa_searchqa-validation-3825", "mrqa_searchqa-validation-13015"], "SR": 0.40625, "CSR": 0.5355392156862745, "EFR": 0.8421052631578947, "Overall": 0.6751382707688338}, {"timecode": 51, "before_eval_results": {"predictions": ["1979", "an Indian", "Chiltern Shakespeare Company", "1961", "Stacey Kent", "1970s", "Arthur Freed", "Elizabeth Kekaikuihala Keka\u02bbaniauokalani Kalaninuiohilaukapu La\u02bbanui Pratt", "Gothic Revival", "Buffalo", "Sam Waterston", "Johan Leysen", "January 4, 1976", "237", "11,163", "an album", "its air-cushioned sole", "the White Knights of the Ku Klux Klan", "WikiLeaks", "Nine-card Brag", "Montana State University", "Tool", "Wikimedia Foundation", "Flashback: The Quest for Identity", "ARY Films", "1987", "dementia", "two Grammy awards", "Port of Boston", "Denmark", "Las Vegas", "1961", "Rochdale, North West England", "Israeli Declaration of Independence", "1971", "Blue Origin", "Target Corporation", "small forward", "2012", "United States", "Sargent Shriver", "35", "Mark Neary Donohue Jr.", "Peach", "Switzerland", "Richard Price", "Archie Andrews", "George Mikan", "June 11, 1986", "2018\u201319 UEFA Europa League", "Magdalen College", "Lake Powell", "Malvolio", "the Royal Air Force ( RAF )", "Separate Tables", "Holyhead", "a sentence", "near the Somali coast", "Daytime Emmy Lifetime Achievement Award", "October 29", "The Baldwin Project", "hunter sauce", "\"The Quest of Erebor\"", "carbon"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7123461174242424}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.5, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2803", "mrqa_naturalquestions-validation-950", "mrqa_triviaqa-validation-2289", "mrqa_triviaqa-validation-3042", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-6747", "mrqa_searchqa-validation-1741"], "SR": 0.65625, "CSR": 0.5378605769230769, "EFR": 0.8181818181818182, "Overall": 0.670817854020979}, {"timecode": 52, "before_eval_results": {"predictions": ["My ntonia", "Henry VIII", "lead", "the Rose Bowl", "VC-25", "amber", "Denmark", "terriers", "Katrina & the Waves", "Jerusalem", "freestyle", "wrestler", "sexual literature", "the Stargate", "Lou Reed", "Stonewall Jackson", "Fennoscandia", "catsuit", "canvas", "potted plants", "The X-Files", "Frankie Muniz", "the undersea world", "Huron", "Coupvray, France", "kinetic", "Santera", "Starsky", "a torch", "quicksand", "The Return of the Native", "AOL", "Pop-Tarts", "Red Lake", "Gatun Lake", "cornucopia", "All That Jazz", "Ankara", "water changing from a vapor to a liquid", "be", "Uberti Winchester", "Chinese", "The Larry Sanders Show", "The Virgin Spring", "Como agua para chocolate", "NigerCongo", "chelo's", "John Tyler", "Daniel Craig", "humility", "programming", "Mount Mannen in Norway and at the Isle of Sheppey in England", "A footling breech", "concentration of a compound exceeds its solubility", "Doctor Zhivago", "Bishopston", "governor of England", "Pan Am Railways", "Berthold Heinrich K\u00e4mpfert", "1961", "Atlantic Ocean", "glamorous, sexy and international", "fake his own death", "Stockton & Darlington Railway"], "metric_results": {"EM": 0.5, "QA-F1": 0.5635230654761905}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false], "QA-F1": [0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.625, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-7806", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8158", "mrqa_searchqa-validation-5567", "mrqa_searchqa-validation-7184", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-2922", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-15252", "mrqa_searchqa-validation-6879", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-9100", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-4716", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-16650", "mrqa_searchqa-validation-15002", "mrqa_searchqa-validation-4954", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-3189", "mrqa_naturalquestions-validation-3230", "mrqa_triviaqa-validation-4700", "mrqa_triviaqa-validation-5698", "mrqa_hotpotqa-validation-4336", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-2686", "mrqa_triviaqa-validation-5426"], "SR": 0.5, "CSR": 0.5371462264150944, "EFR": 0.78125, "Overall": 0.6632886202830189}, {"timecode": 53, "before_eval_results": {"predictions": ["Michael Rosen", "Neil Young", "After Shawn's kidnapping", "to manage the characteristics of the beer's head", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "birch", "on the microscope's stage by slide clips, slide clamps or a cross-table", "Gary Player", "Nicolas Anelka", "season two", "in the sequence of pieces of DNA called genes", "to `` help bring creative projects to life ''", "hot and humid, with an average high around 41 \u00b0 C ( 106 \u00b0 F )", "David Motl", "Christopher Columbus", "Madison, Wisconsin", "September 1972", "2017", "Gustav Bauer", "detritus", "the magnetic stripe `` anomalies '' on the ocean floor", "126", "Brooke Wexler", "John Barry", "1961", "111", "Brazil, Turkey and Uzbekistan", "an even - toed ungulate in the genus Camelus", "13", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "complex sentence", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "the Coriolis force", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "James Rodr\u00edguez", "Donald Sutherland", "James Madison", "the NFL", "Daya Jethalal Gada", "74", "warning signs", "gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "noble gas", "Department of Health and Human Services, Office of Inspector General,", "four distinct levels", "Janie Crawford", "Justin Timberlake", "1966", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "September 2017", "Hercule Poirot", "Paul Gauguin", "USA Today serving as its megaphone.", "creeks, fringing the southwest mouth of Lagos Lagoon, while protected from the Atlantic Ocean by barrier islands and long sand spits such as Bar Beach,", "Martin Scorsese", "Ian Fleming", "well over 1,000 pounds", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "off the coast", "the Northwest Territories", "Chekhov", "a robe", "the death of a pregnant soldier"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6354260698010697}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.7692307692307692, 0.8, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3076923076923077, 0.7567567567567568, 1.0, 0.7499999999999999, 0.5, 1.0, 1.0, 0.5, 0.3076923076923077, 0.4444444444444445, 1.0, 1.0, 0.2857142857142857, 0.08333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-9812", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-388", "mrqa_naturalquestions-validation-8483", "mrqa_triviaqa-validation-4748", "mrqa_hotpotqa-validation-3974", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-5562", "mrqa_searchqa-validation-8459"], "SR": 0.46875, "CSR": 0.5358796296296297, "EFR": 0.7058823529411765, "Overall": 0.6479617715141612}, {"timecode": 54, "before_eval_results": {"predictions": ["Oona Castilla Chaplin", "Mike Czerwien", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "Tex - Mex cuisine is characterized by its heavy use of shredded cheese, meat ( particularly beef and pork ), beans, peppers and spices, in addition to flour tortillas", "Paul McCartney", "Kanawha Rivers", "1803", "the heads of federal executive departments who form the Cabinet of the United States", "3000 BC", "password recovery tool for Microsoft Windows", "Charlotte Thornton", "Western Australia", "Buffalo Bill", "May 3, 2005", "restored to life", "California, Utah and Arizona", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "1773", "John J. Flanagan", "1988", "appointed by means of a commission ( letters patent ) to keep the peace", "at slightly different times when viewed from different points on Earth", "Jeff East", "Charlene Holt", "December 1, 1969", "in over seven years", "Sets heart in mediastinum and limits its motion", "Alex Skuby", "Matt Monro", "12.65 m ( 41.5 ft )", "The management team", "1999", "supervillains", "the courts", "Malvolio", "Coldplay", "Arkansas", "the birth centenary of Pandit Jawaharlal Nehru", "island owner `` U.N. Owen '' ( i.e., `` Unknown '' )", "Atlanta", "22", "Helena", "Joseph Sherrard Kearns", "Cyndi Grecco", "Michael Phelps", "Taron Egerton", "Joe Pizzulo and Leeza Miller", "Secretary of Homeland Security", "cylinder of glass or plastic that runs along the fiber's length", "anembryonic gestation", "741 weeks", "(1923 - 1953)", "Lugano, Switzerland", "Barack Obama's", "Tampa", "Battle of Prome", "itty Hawk", "John Lennon and George Harrison", "the ship of violating Chinese and international laws during its patrols,", "beautiful", "Tater Tots", "Yemen", "QED", "the Dalton Gang"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5678096750063661}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.56, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.058823529411764705, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.0, 0.060606060606060615, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.25, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428572, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-3311", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-7692", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-3099", "mrqa_hotpotqa-validation-2751", "mrqa_newsqa-validation-3310", "mrqa_newsqa-validation-2827", "mrqa_searchqa-validation-16172", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-8575"], "SR": 0.46875, "CSR": 0.5346590909090909, "EFR": 0.7058823529411765, "Overall": 0.6477176637700535}, {"timecode": 55, "before_eval_results": {"predictions": ["on one day, while listening to what seems to be a crossed telephone connection, she hears two men planning a woman's murder", "Matt Monro", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Aristotle", "reproductive system", "Peter Andrew Beardsley MBE", "USS Chesapeake", "In 1967, Celtic became the first British team to win the competition", "U.S. states of Oregon and Washington", "Northeast Monsoon", "2013", "Nitty Gritty Dirt Band", "land, fresh water, air, rare earth metals and heavy metals including ores such as gold, iron, copper, silver", "annual income of US $11,770", "Neil Young", "the closing of the atrioventricular valves and semilunar valves, respectively", "minimum viable product that addresses and solves a problem or need that exists", "London", "Incumbent Democratic mayor Marty J. Walsh", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Ernest Rutherford", "The sinus rhythm is any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "SI joint", "HTTP / 1.1", "Brooklyn, New York", "1 mile ( 1.6 km ) in width in several places", "pop ballad", "1985", "during meiosis", "2007", "Arnold Schoenberg", "notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "Orographic lift", "in the books of Exodus and Deuteronomy", "Scarlett Johansson", "InterContinental Hotels Group", "Benzodiazepines", "Steve Valentine", "John J. Flanagan", "the outside ( skin ) and the inside cavities and lumina of bodies", "2007", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "10,605", "Adrian Hough", "Sebastian Vettel", "San Antonio", "Sun Harvester", "Eukarya", "depending on the gender of the reigning monarch", "pathology", "Wayne Allwine", "Celebrity Big Brother", "John Devoy", "James Garner", "Boston, Massachusetts", "Robert Jenrick", "Bobby Hurley", "urged more help for military members, especially for those returning from war.", "five", "Deputy ChiefBrenda Johnson", "zuentibaldi", "Madonna", "the Eiffel Tower", "Teddy Riley"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6143443033089617}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.09523809523809522, 0.5, 0.0, 1.0, 0.6153846153846153, 1.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.7878787878787877, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.0, 0.1290322580645161, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-4388", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9047", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-46", "mrqa_triviaqa-validation-6540", "mrqa_hotpotqa-validation-4760", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-104", "mrqa_searchqa-validation-1762", "mrqa_searchqa-validation-14136"], "SR": 0.484375, "CSR": 0.5337611607142857, "EFR": 0.6666666666666666, "Overall": 0.6396949404761905}, {"timecode": 56, "before_eval_results": {"predictions": ["Gerald Ford", "10 May 1940", "Sumitra", "16 episodes", "1877", "1999", "Old Trafford", "Tami Lynn", "symbol", "United States", "Max", "April 13, 2018", "Jenna Boyd", "Spencer Treat Clark", "Sedimentary rock", "Theodore Roosevelt", "Nepal", "Jurriaen Aernoutsz", "4 September 1936", "hydrogen and oxygen", "1940", "Authority", "April 1st", "Muno", "The claims process starts at noon Eastern Time and ends 24 hours later", "Francisco Pizarro", "habitat", "Ben Faulks", "Lady Gaga", "mental disorder characterized by at least two weeks of low mood that is present across most situations", "1989", "Liam Cunningham", "Lorenzo Lamas", "Walter Pauk", "1980", "the septum", "Buddhism", "the forex market", "`` Singing the Blues '' by Guy Mitchell in 1957", "Sir Ernest Rutherford", "Nigel Lythgoe, Mia Michaels, and Adam Shankman", "December 2, 2013", "gastrocnemius muscle", "Art Carney", "introduced and elaborated as early as in 1651 by Thomas Hobbes in his Leviathan", "March 26, 1973", "1986", "on location", "President Lyndon Johnson", "prenatal development of the human heart", "a Christmas Tree", "1840", "2007", "Branson, Missouri", "first baseman", "Tumi Holdings, Inc.", "River Shiel", "Ozzy Osbourne", "Polo", "is \"very interested\" in buying the studios,", "ego", "Nova Scotia", "Sir Isaac Newton", "Love Letter"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7595133667502089}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 1.0, 0.4000000000000001, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-305", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-7486", "mrqa_triviaqa-validation-7674", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2096", "mrqa_searchqa-validation-10569"], "SR": 0.671875, "CSR": 0.5361842105263157, "EFR": 0.8571428571428571, "Overall": 0.6782747885338345}, {"timecode": 57, "before_eval_results": {"predictions": ["France", "the namesake town of Manchester - by - the - Sea, Massachusetts", "the one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "a numeric scale used to specify the acidity or basicity of an aqueous solution", "Casino promotions such as complimentary matchplay vouchers or 2 : 1 blackjack payouts allow the player to acquire an advantage without deviating from basic strategy", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan in response to that country's surprise attack on Pearl Harbor the prior day", "the coffee shop Monk's", "Fred E. Ahlert", "the original game release", "Ozzie Smith", "Mark Jackson", "2017", "two - year terms", "January 2018", "in all land - living organisms, both alive and dead, as well as carbon stored in soils", "September 30", "avian origin", "Gerald Ford", "September 8, 2017", "1998", "political ideology", "Spektor", "an object", "Cell nuclei", "1955", "October 12, 2017", "Ren\u00e9 Verdon", "changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "the contestant", "\u05de\u05b7\u05dc\u05b0\u05db\u05b8\u05bc\u05dd\u200e Malkam `` great king ''", "P.V. Sindhu", "Carpenter", "Asuka", "126", "Herman Rarebell", "Brazil, Turkey and Uzbekistan", "UNESCO / ILO Recommendation concerning the Status of Teachers", "upon a military service member's retirement", "on an inward spiral where it would eventually cross the event horizon", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Rich Mullins", "prenatal", "skeletal muscle and the brain", "Don Cook", "Ireland", "Felicity Huffman", "1908", "Sir Henry Cole", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "Eukarya", "commemorating fealty and filial piety", "Luigi Pirandello", "Russ Conway", "the liver", "Heineken", "Irish Chekhov", "Gust Avrakotos", "Mark Sanford", "Lance Cpl. Maria Lauterbach", "step up", "1920", "Joe Louis", "Richard Cory", "ancient Mayan settlement"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5528783501269395}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.7272727272727273, 0.375, 0.14285714285714288, 0.07692307692307693, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.21052631578947367, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.06451612903225806, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.4444444444444445, 0.3478260869565218, 0.0, 0.9600000000000001, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 0.3, 0.8181818181818181, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4497", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-8275", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-2299", "mrqa_hotpotqa-validation-572", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-5291", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-2524", "mrqa_searchqa-validation-6103", "mrqa_searchqa-validation-5902"], "SR": 0.421875, "CSR": 0.5342133620689655, "EFR": 0.7567567567567568, "Overall": 0.6578033987651445}, {"timecode": 58, "before_eval_results": {"predictions": ["William Wyler", "Mercedes -Benz G - Class", "1986", "Idaho", "July 18, 2013", "the books of Exodus and Deuteronomy", "a key role in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "1979", "separately in England and Wales", "iron", "the Reverse - Flash", "Los Angeles, California", "the thirteen American colonies regarded themselves as a new nation, the United States of America, and were no longer part of the British Empire", "Jaffa cake", "`` Davison's Chamber '', `` Wellington's chamber '', weighing in total about 400 tons, are five compartments known as Relieving Chambers", "Zedekiah", "Eddie Murphy", "17 - year - old", "1923", "the brain and spinal cord", "Seattle, Washington", "first appeared in serial format in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "on the slopes of Mt. Hood in Oregon", "Emma Watson", "LED illuminated display", "turkey", "1917", "2004", "Anna Faris", "smen", "Mount Sinai", "Macon Blair", "non-coding sequences", "the state's DMV", "France's Legislative Assembly", "four", "divergent tectonic plate", "Steve Russell", "peace between two entities ( especially between man and God or between two countries ), or to the well - being, welfare or safety of an individual or a group of individuals", "New York University", "into the intermembrane space", "Northeast Monsoon or Retreating Monsoon", "13 February", "291 episodes", "early 1960s", "Yahya Khan", "Thespis", "France", "Wednesday, 5 September 1666", "2003", "Zoe Zebra", "endometriosis", "1960", "Justin Trudeau", "2006", "Walldorf", "superhero roles", "crude oil", "peppermint oil, soluble fiber, and antispasmodic drugs", "the FBI", "ferry", "Leland Stanford", "Mexico", "Asian"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6250267034538851}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 0.9811320754716981, 0.5, 0.9090909090909091, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.13333333333333333, 0.8750000000000001, 0.0, 0.2857142857142857, 0.5, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.8571428571428571, 1.0, 0.5945945945945945, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-9332", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-10561", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-98", "mrqa_triviaqa-validation-1216"], "SR": 0.4375, "CSR": 0.5325741525423728, "EFR": 0.7222222222222222, "Overall": 0.6505686499529191}, {"timecode": 59, "before_eval_results": {"predictions": ["regulatory site", "Jason Marsden", "Ireland", "Vicente Fox", "Daryl Sabara", "February 6, 2005", "Justin Timberlake", "Biotic -- Biotic resources are obtained from the biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "IIII", "useless, time - wasting activity", "quarterback", "July 2012", "Audrey II ''", "Tim Russert", "Jodie Foster", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "by January 2018", "George Strait", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "Herman Hollerith", "94 by 50 feet", "transmission", "Gibraltar", "chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "October 1, 2014", "The Miracles", "hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "Marie Fredriksson", "Long Island", "1988", "British Empire", "Rococo - era France", "Michael Crawford", "Devastator", "the final episode of the series", "2010", "Ram Nath Kovind", "Abid Ali Neemuchwala", "August 9, 1945", "1950s", "Maximilien Robespierre", "XXXX", "by the early - to - mid fourth century", "Made a decision to turn our will and our lives over to the care of God as we understood Him", "Leon Battista Alberti", "January 2, 1971", "J. Presper Eckert and John William Mauchly's ENIAC", "diastema ( plural diastemata )", "July 21, 1861", "the Mayflower", "Efren Manalang Reyes", "Joker Wild,", "Chicago", "duchy of Burgundy (corresponding to the modern region)", "Toby Kennish", "15,024", "model", "the results by a chaplain about 1:45 p.m., per jail policy.", "15-year-old", "Sunday", "Vietnam", "jazz", "Richard", "Son of Sam"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6503208132593213}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 0.0, 0.8, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.9714285714285714, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.9090909090909091, 0.2580645161290323, 0.5454545454545454, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-234", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-7165", "mrqa_triviaqa-validation-34", "mrqa_triviaqa-validation-4764", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3979", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3928", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-8465"], "SR": 0.546875, "CSR": 0.5328125, "EFR": 0.7931034482758621, "Overall": 0.6647925646551724}, {"timecode": 60, "before_eval_results": {"predictions": ["a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "1998", "used their knowledge of Native American languages as a basis to transmit coded messages", "the Gilbert building", "Tom Brady", "James Rodr\u00edguez", "a computer maintenance utility included in Microsoft Windows designed to free up disk space on a computer's hard drive", "1837", "writ of certiorari", "southern USA", "Ferm\u00edn Francisco de Lasu\u00e9n", "Fats Waller", "honey", "73", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "49 cents", "Jason Lee", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Dennis C. Stewart", "Mahatma Gandhi", "people of France to the people of the United States", "eighth and final season", "Erica Rivera", "John Young", "Russia", "2020 National Football League ( NFL ) season", "Charles Perrault", "August 2, 1990", "James `` Jamie '' Dornan", "left coronary artery", "Sam Waterston", "Brazil, Bolivia, Paraguay and Argentina", "Nicklaus", "1957", "Pink Floyd", "ummat al - Islamiyah", "Brazil", "Parashara", "Domhnall Gleeson", "Brazil and Paraguay", "the tradable economy", "St. John's, Newfoundland and Labrador", "Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s, meaning `` herdsman '' or `` plowman ''", "plant anatomy", "the bloodstream or surrounding tissue following surgery, disease, or trauma", "1923", "1871 A.D.", "`` Mirror Image ''", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "1966", "on the alluvial plain", "\"The closest approach to the original sound\"", "Peter Sellers", "Colonel Thomas Andrew \u201cTom\u201d Parker", "Atlantic", "mistress of the Robes", "Australian Electoral Division", "Conway", "Kurt Cobain", "\"Empire of the Sun,\"", "Stephen Dedalus", "The Killing Fields", "the Endeavour", "News of the World tabloid."], "metric_results": {"EM": 0.5, "QA-F1": 0.620070459054834}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false], "QA-F1": [0.625, 0.0, 0.06666666666666667, 1.0, 0.0, 1.0, 0.9142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08000000000000002, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7272727272727272, 0.6363636363636364, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.05714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9371", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-4973", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-9085", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-9058", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-2476", "mrqa_hotpotqa-validation-2900", "mrqa_hotpotqa-validation-3716", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-1963", "mrqa_searchqa-validation-1451", "mrqa_newsqa-validation-1282"], "SR": 0.5, "CSR": 0.5322745901639344, "EFR": 0.75, "Overall": 0.6560642930327869}, {"timecode": 61, "before_eval_results": {"predictions": ["March 21, 2016", "to form a higher alkane", "shared between three players both times", "Jason Marsden", "New Mexico", "In 1889", "Poems : Series 1", "the Norman Conquest of England", "March 2, 2016", "July 20, 2017", "seven", "September 1972", "James Rodr\u00edguez", "the world's sixth - largest country by total area", "The Vamps", "Mickey Rourke", "John Donne", "the 1980s", "David Gahan", "Emma Watson", "legislation ( i.e., `` statutes '' or `` statutory law '' )", "2018", "the first bull running is on 7 July, followed by one on each of the following mornings of the festival, beginning every day at 8 am", "the 2009 model year", "no more than 4.25 inches ( 108 mm )", "Judi Dench", "Monthly Comic Alive", "159", "Chris Rea", "between $10,000 and $30,000", "Kelly Reno", "Ozzie Smith", "8 December 1985", "the 18th century", "Thomas Jefferson's", "Ian Holm", "Space is the Place   Jumping on the Moon   No Night Today   Planet Name Game   Top of the Sky    Tough Enough", "Brad Dourif", "counter clockwise direction", "Joanne Wheatley", "President pro tempore of the Senate", "the rise of literacy, technological advances in printing, and improved economics of distribution", "Donna Mills", "1994 season", "Matt Flinders", "parthenogenic", "the major contributor and the associated free software philosophy", "the efferent nerves that directly innervate muscles", "1773", "The Union's forces", "American country music duo Brooks & Dunn", "kippis", "(modern Antarctic Pole)", "The Pilgrim's Progress", "Bourbon County", "Argentina", "Hungary and Croatia", "Sen. Barack Obama", "Sri Lanka's Tamil rebels", "One of Osama bin Laden's sons", "(Jack) London", "(Arthur C. Clarke)", "the Koran", "whooping cough"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6483943833943833}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 0.24000000000000002, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.07692307692307691, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.32, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.3, 1.0, 0.8, 1.0, 0.28571428571428575, 0.5, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-8452", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-3340", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-1069", "mrqa_hotpotqa-validation-5483", "mrqa_newsqa-validation-648", "mrqa_triviaqa-validation-4519"], "SR": 0.53125, "CSR": 0.532258064516129, "EFR": 0.7666666666666667, "Overall": 0.6593943212365592}, {"timecode": 62, "before_eval_results": {"predictions": ["December 13, 1917 -- September 20, 2009", "beneath the liver", "Rudy Clark", "Abbot Suger", "Yuzuru Hanyu", "Tim Russert", "Byzantine Greek culture and Eastern Christianity became founding influences in the Arab / Muslim world and among the Eastern and Southern Slavic peoples", "toys or doorbell installations", "microfilament", "in the pancreas", "the center of the Northern Hemisphere", "CeCe Drake", "Eduardo", "M\u00e1ximo Gomez and Antonio Maceo", "1971", "L\u00e9o Arnaud ( / \u02c8le\u026a. o\u028a \u0251\u02d0r \u02c8no\u028a /", "Emmanuelle Chriqui", "Carlos Alan Autry Jr.", "16 March 2018", "the Dolby Theatre in Hollywood, Los Angeles, California", "Merry Clayton", "statue `` is officially known as the Statue of Freedom ''", "the courts", "1936", "Eric Clapton", "Djokovic", "James Hutton", "1922", "2017", "scythe", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "Leonard Bernstein", "Toronto, Ontario, Canada", "listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "September 2017", "October 2012", "Kaley Christine Cuoco", "2013", "Dido", "Isekai wa Sum\u0101tofon to Tomo ni", "the fourth season", "Phillip Paley", "1981", "Jakkur, Bangalore, India", "New Orleans going north through Chicago and to New York", "regions of Mesoamerica", "10.5 %", "U.S. Interior Highlands region", "White House Executive chef", "the International Border ( IB )", "Bart Millard", "a woman named Sarah Whitehead", "Thabo Mbeki", "Midnight Cowboy", "Austrian", "heavy metal", "Selden", "Muslims as Americans.", "the day before.", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "Queen Elizabeth", "the Entente Council", "Chief Oshkosh", "65 mi long"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6620704816017315}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.28571428571428575, 1.0, 0.5, 0.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4954", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-7211", "mrqa_triviaqa-validation-7273", "mrqa_hotpotqa-validation-529", "mrqa_hotpotqa-validation-5848", "mrqa_newsqa-validation-1307", "mrqa_newsqa-validation-1292", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-9194", "mrqa_searchqa-validation-9115", "mrqa_hotpotqa-validation-1201"], "SR": 0.578125, "CSR": 0.5329861111111112, "EFR": 0.8888888888888888, "Overall": 0.683984375}, {"timecode": 63, "before_eval_results": {"predictions": ["Robyn", "1998", "the closing of the atrioventricular valves and semilunar valves", "the Coppolas", "sacroiliac joint", "Identification of alternative plans / policies", "Mexico", "development of electronic computers", "employer", "Balaam ( Numbers 22 : 28 )", "Bhupendranath Dutt", "Charlotte of Mecklenburg - Strelitz", "March 23, 2018", "at luncheon", "Andrew McCarthy as Blane McDonough", "Jakkur, Bangalore, India", "Five years later", "first year at Harry Potter School of Witchcraft and Wizardry as he discovers that he is a famous Wizard and begins his education", "the European economy had collapsed", "brothers Henry, Jojo and Ringo Garza", "Ben Findon", "Incudomalleolar joint", "Terry Reid", "long - standing policy of neutrality was tested on many occasions during the 1930s", "Kennedy Space Center ( KSC ) in Florida", "supported modern programming practices and enabled business applications to be developed with Flash", "Forbes Burnham", "on Saturday", "Isekai wa Sum\u0101tofon to Tomo ni", "fortified complex", "the court from its members", "Alicia Vikander", "over 300,000", "April 21, 2015", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "115", "eight", "Lori Rom", "a Czech word, robota", "Arlen `` The Chief '' Bitterbuck", "Cameron Fraser", "Austin and Pflugerville", "1933", "blasphemy", "four", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "Utah, Arizona, Wyoming, and Oroville, California", "Dan Enright", "Hugo Weaving", "from whom Loyola derives its name", "American singer and songwriter Lana Del Rey", "The Matterhorn", "Trinidadian and Tobagonian British", "Jiles Perry (JP) Richardson Jr.", "The Pentagon", "Pisgah National Forest", "Johnnie Ray", "Morgan Tsvangirai", "Capitol Hill", "provided Syria and Iraq 500 cubic meters of water a second,", "impressionist", "Pussycat Dolls Present", "tuberculosis", "May 4"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5115506429844665}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.9333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5882352941176471, 0.3333333333333333, 0.0, 1.0, 0.33333333333333337, 0.0, 0.5882352941176471, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.5714285714285715, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.9523809523809523, 0.5, 0.0, 1.0, 0.2, 0.6, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-8460", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-225", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-3801", "mrqa_naturalquestions-validation-7669", "mrqa_triviaqa-validation-6825", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-4240", "mrqa_newsqa-validation-3392", "mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-8333"], "SR": 0.359375, "CSR": 0.5302734375, "EFR": 0.6585365853658537, "Overall": 0.6373713795731708}, {"timecode": 64, "before_eval_results": {"predictions": ["Agra", "2018\u201319 UEFA Europa League group stage", "FIFA Women's World Cup", "Dan Brandon Bilzerian", "Len Wiseman", "Viglen Ltd", "1896", "Randall Boggs", "Detroit, Michigan", "Roots: The Saga of an American Family", "St. Louis Cardinals", "Foxborough", "local South Australian and Australian produced content", "Hindi", "Ronald Wilson Reagan", "Los Angeles", "Ben Johnston", "Nia Temple Sanchez", "Vanessa Hudgens", "top division", "Amber Laura Heard", "Peter Seamus O'Toole", "March 8, 1942", "\"Electron Blue\"", "January 30, 1930", "Doctor of Philosophy", "Government of Ireland", "James Weldon Johnson (June 17, 1871June 26, 1938)", "Wilmington, North Carolina, United States", "1979", "Taylor Swift", "first and second segment", "Kew Gardens", "7 January 1936", "Towards the Sun", "The Pogues", "Westminster system", "Ionolyce", "For Love alone", "October 4, 1970", "King of the Polish-Lithuanian Commonwealth", "Sam Waterston", "Transporter 3", "2000", "Gauteng province", "Vietnam War", "Bill Walton", "Darling River", "Brian Keith Bosworth", "140 million", "English", "Teri Garr", "Employers", "1962", "Wyoming", "Wee Jimmy Krankie", "money", "ordered the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guant Bay, Cuba.", "The clothing must be black, red or white,", "trying to prevent attempted defections as the country goes through a tumultuous transition, the report said.", "Billy Corgan", "Ellicott City", "sprints", "Southport, North Carolina"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5991714015151515}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.8, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.8, 0.5, 0.0, 1.0, 0.5, 0.0, 0.5454545454545454, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.48484848484848486, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-257", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-5143", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2569", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2442", "mrqa_hotpotqa-validation-4810", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-1171", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-9071", "mrqa_searchqa-validation-16474"], "SR": 0.46875, "CSR": 0.5293269230769231, "EFR": 0.7058823529411765, "Overall": 0.6466512302036199}, {"timecode": 65, "before_eval_results": {"predictions": ["T. D. Lee", "Arrested Development", "Albert", "September 30, 2017", "322,520", "New York Giants", "the Swiss tourism boom", "Eliot Cutler", "the 1946 Winecoff Hotel fire", "Odense Boldklub", "Stephen of Blois", "Scott Eastwood", "Gweilo", "Tufts College", "Prince Amedeo, 5th Duke of Aosta", "1936", "The Wu-Tang Clan", "For Love alone", "a midtempo hip hop ballad", "melodic hard rock", "G\u00e9rard Depardieu", "rural", "Las Vegas", "Appleby-in-Westmorland", "from 1345 to 1377", "Indiana University", "James Bond", "Syracuse", "Kings Point, New York", "Robert Paul \"Robbie\" Gould III", "It's Always Sunny in Philadelphia", "Baldwin", "Port Clinton", "November 20, 1942", "Wayne Conley", "on the Australian coast", "Faith", "turned out to be a terrible date", "the Cleveland Celtics", "Everybody Hates Chris", "acid", "eight", "the regime of Emperor Napoleon III", "a recreational drug", "Jim Harrison", "the third Viscount", "Arabella Churchill, sister of the 1st Duke of Marlborough", "Lester Ben \"Benny\" Binion", "two Grammy awards", "S7", "2017", "Qutab Ud - Din - Aibak", "14 November 2001", "Thomas Jefferson", "Luxembourg", "Golda Meyerson", "The Muffin Man", "President George Bush", "250,000", "Vernon Forrest,", "HIV", "blown", "humans", "Jamie Lee Curtis"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6795043498168498}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false], "QA-F1": [0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.7692307692307692, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.7142857142857143, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-3246", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-1542", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-2978", "mrqa_naturalquestions-validation-10202", "mrqa_newsqa-validation-2677", "mrqa_newsqa-validation-368", "mrqa_searchqa-validation-7440", "mrqa_searchqa-validation-3848", "mrqa_naturalquestions-validation-1925"], "SR": 0.5625, "CSR": 0.5298295454545454, "EFR": 0.8571428571428571, "Overall": 0.6770038555194805}, {"timecode": 66, "before_eval_results": {"predictions": ["Captain Mark Phillips", "Hillsborough", "Paraguay", "steam locomotives", "Absalom", "Terry Hall", "December", "Anthony Joshua", "George IV", "Zsa Zsa Gabor", "ambilevous", "Louis Daguerre", "Richard Feynman", "strata", "a living architect or architects whose built work demonstrates a combination of those qualities of talent, vision, and commitment,", "Pipaluk", "a cat", "Kimbe", "Orange", "Oscar II", "pyrotechnic", "badminton", "Morgan Choir", "roast goose", "Olympics", "Typhon", "Syria", "Wyoming", "Professor Brian Cox", "Stephen King", "Albert Finney", "Scotland", "24", "George Washington James Monroe Thomas Jefferson Abraham Lincoln", "Ellice Islands", "Meta", "the Oil Capital of Europe", "Isle of Man", "a double-reed instrument", "Spice Girls", "\"Mr Loophole\"", "Istanbul", "drinking song", "Texas", "Erik Aunapuu", "the Versailles Treaty", "Rajasthan", "African violet", "\u0430\u0440\u0430\u043b\u0438", "MetroLyrics", "Cardigan", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Djokovic", "1912", "fennec fox", "1950", "Prince Nikolai Sergeyevich Trubetzkoy", "Vernon Forrest", "Linda Hogan", "development of two courses on the Black Sea coast in Bulgaria.", "N.C. Wyeth", "parasites", "Yonkers", "substitute good"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5625}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2232", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-4157", "mrqa_triviaqa-validation-2064", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-1689", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-6457", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-4970", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-3610", "mrqa_naturalquestions-validation-3922", "mrqa_hotpotqa-validation-4090", "mrqa_hotpotqa-validation-5590", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-9098", "mrqa_searchqa-validation-7189"], "SR": 0.515625, "CSR": 0.5296175373134329, "EFR": 0.7419354838709677, "Overall": 0.6539199792368802}, {"timecode": 67, "before_eval_results": {"predictions": ["$50 less", "Thailand", "France", "nude beaches.", "flooding was so fast that the thing flipped over", "Werder Bremen", "Secretary of State", "Ali Larijani", "nearly three out of four", "Fernando Caceres", "an Italian and six Africans", "a homicide.", "best-of-three series.", "the 11th century Preah Vihear temple", "Uzbekistan", "voluntary manslaughter", "Jenny Sanford,", "Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "Miami Beach, Florida", "\"Dear John,\"", "cell phones.", "two contestants.", "Scarlett Keeling", "Teresa Hairston", "Graeme Smith", "former U.S. secretary of state", "tried to fake his own death by crashing his private plane into a Florida swamp.", "8", "Thursday and Friday", "helicopters and boats, as well as vessels from other agencies,", "terrorize", "tickets to Italy", "Oxbow, a town of about 238 people,", "FAA received no reports from pilots in the air of any sightings but the agency recieved \"n numerous\" calls from people on the ground from Dallas, Texas, south to Austin, Texas.", "21-year-old", "Jacob Zuma", "toffelmakaren", "Gary Brooker", "a civil disturbance call", "Pew Research Center", "people would call her names on the street,", "Kenyan and Somali governments", "30,000", "1983", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "North Korea", "Steve Jobs", "Garth Brooks", "40-year-old", "Twitter", "1983", "Carolyn Sue Jones", "Hold On", "in Christian eschatology", "Phil Mickelson", "Dumbo", "blues", "1969", "\"$10,000 Kelly,\"", "Estadio de L\u00f3pez Cort\u00e1zar", "julie taymor", "Perkins", "director", "batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship"], "metric_results": {"EM": 0.4375, "QA-F1": 0.527074864498645}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.8571428571428571, 0.2857142857142857, 0.24390243902439027, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.1, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-2271", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-190", "mrqa_naturalquestions-validation-833", "mrqa_triviaqa-validation-6636", "mrqa_searchqa-validation-5451", "mrqa_searchqa-validation-2492", "mrqa_naturalquestions-validation-9387"], "SR": 0.4375, "CSR": 0.5282628676470589, "EFR": 0.9166666666666666, "Overall": 0.6885952818627451}, {"timecode": 68, "before_eval_results": {"predictions": ["BADBUL", "2050,", "Molotov cocktails, rocks and glass.", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "German Chancellor Angela Merkel", "son of Gabon's former president", "to put a lid on the marking of Ashura this year.", "their homes in Bhola", "U.S. ship that was hijacked off Somalia's coast.", "Michigan", "AS Roma beat Lecce 3-2", "President Barack Obama,", "Southern Baptist Convention,", "in body bags on the roadway near the bus,", "Tuesday", "an American who entered the country illegally", "2000", "at least 300", "Thursday,", "volatile and dangerous", "Hanin Zoabi,", "the same drama that pulls in the crowds", "2008", "root out terrorists within its borders", "25 years", "a key find by paleontologists at Los Angeles' George C. Page Museum.", "Ciudad Juarez, across the border from El Paso, Texas.", "105-year", "Michael Schumacher", "Jobs", "Jenny Sanford,", "northwestern Montana", "genocide", "allegedly involved in forged credit cards and identity theft", "Bailey, Colorado,", "John Demjanjuk,", "Venus Williams", "two weeks", "Doogie Howser, M.D.", "British", "six", "requested the pardon.", "bank", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Barnes & Noble", "14", "Michael Arrington", "well over 1,000 pounds", "stand down", "his past and his future", "Mombasa, Kenya", "a loanword of the Visigothic word", "Taron Egerton", "Italy", "\"Book 1: Sowing\"", "purpurea", "Nellie Melba", "\"The King of Hollywood\"", "1947", "the backside", "Sweden", "spotted hyena", "Pablo Picasso", "improved the speed of encryption of communications at both ends in front line operations during World War II"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5720441017316017}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.8, 0.4, 0.5454545454545454, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.22727272727272727, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-241", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-2374", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-1997", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-2426", "mrqa_naturalquestions-validation-3019", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-4494", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-7337", "mrqa_searchqa-validation-10753"], "SR": 0.46875, "CSR": 0.5274003623188406, "EFR": 0.8235294117647058, "Overall": 0.6697953298167093}, {"timecode": 69, "before_eval_results": {"predictions": ["the cavities and surfaces of blood vessels and organs throughout the body", "Tim Russert", "on the microscope's stage", "P.V. Sindhu", "Nick Kroll", "April 1917", "Sir Donald Bradman", "two - stroke engines and chain drive", "revenge", "Kevin Sumlin", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "Detective Eddie Thawne", "Hathi Jr", "LED illuminated display", "Spektor", "The Star Spangled Banner", "Bill Russell", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "by October 1986", "http://www.example.com/index.HTML", "1,484 m / s in water ( 4.3 times as fast as in air )", "1997", "Carol Worthington", "September 6, 2019", "1973", "1902", "SURFACE areaA of ROOTS", "back", "Battle of Antietam", "National Park Service's Shenandoah National Park in the Blue Ridge Mountains of Virginia", "Clarence Anglin", "Andrew Garfield", "Under normal conditions", "the 1980s", "Pasek & Paul", "in a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "2013", "Billie `` The Blue Bear ''", "eusebeia", "only drivers who were 2016 Pole Award winners", "White House Executive chef", "place of trade, entertainment, and education", "25 years after the release of their first record", "the bank", "The Abbott and Costello Show", "Waylon Jennings", "libretto", "the Rolling Stones", "Sun Tzu", "Pre-evaluation, strategic planning, operative planning, implementation", "eucalyptus", "inflation", "Charlie Hall", "John M. Dowd", "December 17, 1974", "Northrop P-61 Black widow", "26", "The patient, who prefers to be anonymous,", "as soon as 2050,", "America's Library", "Paul Bunyan", "thyroid", "1965"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6499811338713992}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.8181818181818182, 0.3333333333333333, 1.0, 0.2857142857142857, 1.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.75, 0.19999999999999998, 1.0, 0.08, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9411764705882353, 0.17391304347826084, 1.0, 1.0, 0.0, 0.3783783783783784, 1.0, 1.0, 0.09523809523809525, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-8374", "mrqa_triviaqa-validation-6937", "mrqa_newsqa-validation-1092", "mrqa_searchqa-validation-10785"], "SR": 0.546875, "CSR": 0.5276785714285714, "EFR": 0.7586206896551724, "Overall": 0.6568692272167487}, {"timecode": 70, "before_eval_results": {"predictions": ["the President", "Walter Pauk", "Madison", "Brevet Colonel Robert E. Lee", "a specific task", "January 2, 1971", "minced meat", "St. Louis Cardinals", "mascot", "1792", "Longline", "Sebastian Vettel", "Niles", "China", "2017", "Upstate New York", "Carol Ann Susi", "a stem", "Nala", "ZZ Top", "P.V. Sindhu", "Anglican", "the closing of the atrioventricular valves and semilunar valves, respectively", "investment bank Friedman Billings Ramsey", "the NFL", "the world map", "1 January 1904", "password recovery tool for Microsoft Windows", "Companies commonly require from 35 to 40 hours per week to be defined as full - time and therefore eligible for benefits", "by week 4 of development", "contemporary Earth", "somatic cell nuclear transfer", "UN General Assembly", "anxiety", "two", "David Ben - Gurion", "as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "7th century", "St. Theodosius Russian Orthodox Cathedral", "Ray Charles", "a jazz funeral without a body", "2004", "May 31, 2012", "Edward Furlong", "Malware", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "Beorn", "South Dakota", "Donald Trump", "100,000", "1967", "Rajasthan", "Sodor", "eye", "44,300", "2008", "Anglo-Frisian", "Long Island", "\"trying to steal the election\" and \"intimidating the population and election officials as well.\"", "11", "bones", "Humpty Dumpty", "Thailand", "500-room"], "metric_results": {"EM": 0.578125, "QA-F1": 0.636792619825708}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.35294117647058826, 0.058823529411764705, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5185185185185185, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 0.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-2146", "mrqa_hotpotqa-validation-2098", "mrqa_newsqa-validation-3943", "mrqa_searchqa-validation-7551"], "SR": 0.578125, "CSR": 0.5283890845070423, "EFR": 0.8148148148148148, "Overall": 0.6682501548643713}, {"timecode": 71, "before_eval_results": {"predictions": ["Mae West", "Diana Vickers", "Tina Turner", "Woodrow Wilson", "Striding Edge", "photographer", "a clown, as he is called  in the cast of characters of \"As You Like  It,\"", "the Titanic", "Campania", "Hadrian", "Madagascar", "the Barbizon school", "Michel Denance", "Manet", "Gary Sparrow", "Challenger", "Edinburgh City F.C.", "Lacock Abbey", "Clive Cussler", "Canada", "'Hansel and Gretel' cottage", "brake", "Greenock", "ABBA", "Sonja Henie", "six", "Lord Snooty", "Greyfriars Bobby, Skye terrier", "Rudolf Hess", "Institute of Chartered Surveyors", "Stieg Larsson", "Facebook Music Stories", "1957", "giant menhir", "steel", "Rotherham United", "Joseph Priestley", "a greyhound, gazelle hound or tazi", "It is the largest annual international team competition in sport, with 135 nations entered in 2016.", "Periodic Table", "francs", "region of SW Asia between the lower and middle reaches of the Tigris and Euphrates rivers", "Martin Clunes", "Cuba", "ab initio", "Patience", "Ernest Evans", "Quentin Tarantino", "smartphones and similar devices to establish radio communication with each other by touching them together or bringing them into close proximity, usually no more than a few centimetres", "Salvador Dal\u00ed", "par three 12th hole", "San Francisco", "90s", "Brooke Wexler", "The Tigers compiled an 11\u20131 regular season record and then defeated the No. 5 Georgia Bulldogs in the SEC Championship Game,", "aging issues", "January", "Jet Republic", "businesses hiring veterans as well as job training for all service members leaving the military.", "40 lash after he was convicted of drinking alcohol in Sudan where he plays for first division side Al-Merreikh of Omdurman.", "6 counties in Ulster", "unexpected and prolonged", "a peashooter", "UFC Fight Pass"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6375748487800917}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.07142857142857142, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.08695652173913042, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-3237", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-5576", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5039", "mrqa_triviaqa-validation-5665", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-4791", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-7482", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-4282", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-5816", "mrqa_hotpotqa-validation-1094", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-1758", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-11196"], "SR": 0.59375, "CSR": 0.529296875, "EFR": 0.6923076923076923, "Overall": 0.6439302884615385}, {"timecode": 72, "before_eval_results": {"predictions": ["bobby Darin", "Thames", "Altamont Speedway in Livermore, California.", "The Jetsons", "26.22", "ankle joint", "overprotective clownfish", "Samson", "Connecticut", "Daedalus", "hana", "Pandemonium", "a goad", "Miles Morales", "up to 14", "radars", "Queen Elizabeth II", "tonto", "hippocampus", "Marshal Kane", "tennis", "Orwell", "Atlantic Ocean", "treaty of Waitangi", "chatsworth house", "Salvatore Ferragamo", "London", "glands on the dog's eyelid", "Husqvarna", "augusto", "aug. 24, 1572", "fatty hump on their shoulders, drooping ears and a large dewlap", "Augustus", "Venezuela", "Southwest Airlines", "SUNSET BOULEVARD", "Johnny Colla", "Derwill", "unhulled", "Laos", "Allardyce", "Petain", "Ryan O\u2019 Neal", "Miami", "Bill Haley & His comets", "spanish", "1768", "Joan Rivers", "Athens", "William Refrigerator Perry", "Ghana", "in use around 4500 BC in the Near East", "observing the magnetic stripe `` anomalies '' on the ocean floor", "2001", "Easy", "seven", "Karl Johan Schuster", "U.S. Holocaust Memorial Museum", "Robert Barnett,", "Diego Milito's goal gave them a 1-0 win at Siena on Sunday.", "Dick Grayson", "Dumbo the Flying elephant", "Casey at the Bat", "pythons"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49915501165501164}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.923076923076923, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.3076923076923077, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-5979", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-5614", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-1380", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-4492", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2423", "mrqa_triviaqa-validation-7682", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-5164", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-2048", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-7344", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-9987", "mrqa_hotpotqa-validation-3333", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2017", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2755", "mrqa_searchqa-validation-2901", "mrqa_searchqa-validation-7110", "mrqa_searchqa-validation-4706", "mrqa_searchqa-validation-4802"], "SR": 0.40625, "CSR": 0.527611301369863, "EFR": 0.7894736842105263, "Overall": 0.6630263721160778}, {"timecode": 73, "before_eval_results": {"predictions": ["Scottish national team", "Speedway World Championship", "Bears", "\"Time\"", "Babylon", "1501", "The Shins", "11,791", "Eliot Cutler", "Manchester", "Hellenism", "The Ansonia Hotel", "Washington", "Helen Mirren", "racehorse breeder and owner", "Nazi Party (NSDAP)", "Edward Albert Heimberger", "The Bye Bye Man", "Chevron Corporation", "ragby", "Indianapolis", "pastels and oil painting", "Premier League", "Sleepy Hollow", "Jane Mayer", "Obafemi Akinwunmi Martins", "Knowlton School", "143,007", "Philadelphia", "7", "American former model, actress and television host.", "1967", "mathematician, physicist, and spectroscopist", "King Duncan", "St Andrews Agreement", "Royal College of Music", "4145 ft above mean sea level", "Japan Airlines Flight 123", "near North Chicago, in Lake County, Illinois", "HBO miniseries \"Empire Falls\"", "2013", "The American relay of Michael Phelps", "a suburb of Adelaide in the City of Port Adelaide Enfield", "schoolteacher", "People v. Turner", "William Harold \"Bill\" Ponsford", "Aamina Sheikh, Hasan Ahmed, Neelam Muneer, Zaheen Tahira, Ismat Zaidi, Sami Khan", "one", "Mortal Kombat", "Mike Holmgren", "Gauteng", "Herman Hollerith", "6 -- 14 July", "weekly Torah portion", "paramitas", "1881", "writing", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "Pixar's \"Toy Story\"", "Christianity and Judaism", "blintz", "Texas Chainsaw Massacre", "Joe DiMaggio", "Caster Semenya"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6829708467001665}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0625, 0.6, 0.0, 1.0, 0.5714285714285715, 0.19999999999999998, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.8918918918918919, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-680", "mrqa_hotpotqa-validation-821", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-1002", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-3539", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-2621"], "SR": 0.5625, "CSR": 0.5280827702702703, "EFR": 0.7857142857142857, "Overall": 0.6623687861969112}, {"timecode": 74, "before_eval_results": {"predictions": ["Jesuits", "ribonucleic acid", "ketchup", "igloo", "a fly", "Timberland", "Former Texas Gov. Rick Perry", "bama pyi", "Latvia", "spleen", "auf wiedersehen", "rely", "Ramesses II", "wine coolers", "esophagus", "Dallas Cowboys", "the Bible", "twist", "Marie Tussaud", "Biscay", "Ziz", "March", "Ferdinand Magellan", "Kevin Spacey", "children of prostitutes", "oblate", "The Aviator", "Gioachino Rossini", "Veracruz", "tail", "Tennessee", "Hanging Gardens of Babylon", "The Last Starfighter", "Billy Crystal", "skin cancer", "Gerard", "kbec", "pontificio", "Mimi Bobeck", "New Zealand", "Moonlighting", "Corpus Christi", "Odysseus", "Ruth Bader Ginsburg", "Edward R. Murrow", "bay of Bengal", "in vitro fertilization", "Diogenes of Sinope", "pastries", "chocolate milk drink", "the Electric Company", "September 24, 2012", "Roger Dean Stadium", "March 31, 2013", "Helen Reddy", "Celsius", "Jeremy Irons", "January", "Jennifer Grey", "Donald Wayne Johnson", "giving birth to baby daughter Jada, who was watching her mum from the stands again on Saturday.", "4,000", "Princess Diana", "Melbourne"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6508605072463768}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5217391304347825, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-4940", "mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-10572", "mrqa_searchqa-validation-7401", "mrqa_searchqa-validation-6347", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-1613", "mrqa_searchqa-validation-6459", "mrqa_searchqa-validation-2269", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-3466", "mrqa_searchqa-validation-13743", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-5131", "mrqa_searchqa-validation-85", "mrqa_searchqa-validation-14509", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-16566", "mrqa_searchqa-validation-9143", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-976", "mrqa_naturalquestions-validation-5096", "mrqa_newsqa-validation-801"], "SR": 0.5625, "CSR": 0.5285416666666667, "EFR": 0.7857142857142857, "Overall": 0.6624605654761905}, {"timecode": 75, "before_eval_results": {"predictions": ["eleven", "Randy VanWarmer", "October 2012", "Sylvester Stallone", "1765 and 1783", "The Miracles", "1900", "provides a site for genetic transcription that is segregated from the location of translation in the cytoplasm, allowing levels of gene regulation that are not available to prokaryotes", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "2009", "1976", "The Satavahanas", "Jos\u00e9 Mart\u00ed", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "1975", "MFSK and Olivia", "28 July 1914 to 11 November 1918", "St. Pauli Girl Lager", "908 mbar", "North Atlantic Ocean", "February 7, 2018", "October 2000", "The Lutheran Church of Sweden", "commemorating fealty and filial piety", "on a sound stage in front of a live audience in Burbank, California", "Allsup", "American singer - songwriter - actress Debbie Gibson", "Lula", "31 January 1934", "at the mayor's home", "United States", "gastrocnemius", "Daniel A. Dailey", "Jesus'birth", "Yahya Khan", "Ramanaa", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids", "Kyla Coleman", "Bill Patriots", "December 19, 1971", "Tim Passmore", "Garbi\u00f1e Muguruza", "Spanish / Basque origin", "Lilian Bellamy", "4.37 light - years ( 1.34 pc )", "Shirley Mae Jones", "mobile telephony was limited to phones installed in cars and other vehicles", "Neil Young", "`` 5 lakhs", "Chuck Noland", "forested parts", "arithmetic", "Shooting Glove", "Red squirrels", "Michael Swango", "Maria von Trapp", "Skatoony", "Felipe Calderon", "low-calorie meals", "0-0 draw", "Hapsburg", "Mexico", "coyote", "Majid Movahedi,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7139296343537416}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.9795918367346939, 0.9523809523809523, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.4, 0.28571428571428575, 0.05714285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-6066", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-7133", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-1705", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-178", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-1646"], "SR": 0.546875, "CSR": 0.528782894736842, "EFR": 0.8620689655172413, "Overall": 0.6777797470508167}, {"timecode": 76, "before_eval_results": {"predictions": ["$657.4 million", "Total Drama Action", "Christopher Lloyd", "senators", "rape", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "sovereignty", "Authority", "Jughead Jones", "brothers Henry, Jojo and Ringo Garza", "ecological regions", "bars or in small packs, and in larger and smaller sizes", "Kiss", "from 18 September to 31 October", "Julie Adams", "During World War II", "Kirk Douglas", "January 2004", "The Vamps", "William T. Deutschendorf", "Tennessee Titan", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "southern Turkey", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Ashoka", "Spanish / Basque origin", "a contemporary drama in a rural setting", "1916", "Billie Jean King", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "2014 -- 15", "October 28, 2007", "Chasty Ballesteros", "Transvaginal ultrasonography", "It acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Matt Monro", "a Christmas Tree", "IV", "Virginia", "Saphira", "September 2017", "an Irish feminine name", "Ace", "Spike", "regulatory site", "When the others arrive", "Aegisthus", "InterContinental Hotels Group", "the Isthmus of Corinth", "Jason Flemyng", "the Isthmus of Corinth", "Norman Mailer", "a Bristol Box Kite", "bMC", "Part II", "17 October 2006", "Dr. Alberto Taquini", "$1.5 million", "San Diego County.", "Jason Chaffetz", "jazz", "echidna", "Forrest Gump", "31 meters (102 feet) long and 15 meters (49 feet) wide"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6532431813435102}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true], "QA-F1": [0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.8181818181818181, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7999999999999999, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 0.0, 0.72, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-921", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2067", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-6055", "mrqa_hotpotqa-validation-2921", "mrqa_newsqa-validation-1434", "mrqa_searchqa-validation-14736"], "SR": 0.546875, "CSR": 0.5290178571428572, "EFR": 0.7586206896551724, "Overall": 0.6571370843596058}, {"timecode": 77, "before_eval_results": {"predictions": ["Pegasus", "As You Like It", "Apollo 11", "Live and Let Die", "Giuliano Bugiardini", "metal", "pulsar", "Seth", "Honda", "John & Yoko and The Plastic Ono Band with the Harlem Community Choir - \" Happy Christmas (War Is Over)\"", "Ozzy Van Halen", "Merchant of Venice", "the 2010 FIFA World Cup", "Elizabeth I", "June", "Italy", "1960s", "Mel Brooks", "Marengo", "chlorophyll", "Paul Dukas", "Iceland", "Uranus", "rum", "apples", "brchin", "parcker and Warburg", "four Successive Olympics", "Separate Tables", "sound of the human voice", "Beatrix Potter", "magpie", "comets", "volleyball (indoors and beach)", "Kansas City", "Raul Castro", "Space Oddity", "Scotland", "UK Butterflies", "Illinois", "red", "Splash", "South Africa", "shabbat", "Good Will Hunting", "gollum", "otters", "John McCarthy", "John Mortimer", "Kellogg\u2019s Special K", "line code", "Asia", "Liam Cunningham", "Brobee", "Madrid", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "E22", "security breach", "at checkposts and military camps in the Mohmand agency,", "Mashhad, Iran.", "Saint Bernard", "France", "Barnard College", "one day we won't have any more oil anywhere."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6077380952380953}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.09523809523809525, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-693", "mrqa_triviaqa-validation-6105", "mrqa_triviaqa-validation-5582", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-7621", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-2314", "mrqa_hotpotqa-validation-844", "mrqa_hotpotqa-validation-3964", "mrqa_hotpotqa-validation-2404", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1775", "mrqa_searchqa-validation-4278", "mrqa_newsqa-validation-3978"], "SR": 0.5625, "CSR": 0.5294471153846154, "EFR": 0.75, "Overall": 0.6554987980769231}, {"timecode": 78, "before_eval_results": {"predictions": ["France", "Granada", "Verdi", "April", "Al Pacino", "Britain", "by increasing the number of arcs", "William Golding", "a group of nerve cell bodies located in the autonomic nervous system", "vitamin B3", "director of the Security Service", "ship", "Funchal", "hard", "BBC\u2019s pasta harvest", "Ireland", "passport", "Marcel Duchamp", "The Quatermass Experiment", "Mumbai", "a great invetor", "1875", "raven", "hound", "Sue", "Estimate", "algebra precalculus", "Narendra Modi", "Richard Wagner", "arpad \u2018Arki\u2019 Busson", "Argentina", "shorthand", "in Kitzb\u00fchel", "Tunisia", "Crystal Gayle", "steppes steppe", "Romania", "brindisi", "Muriel", "Emeril Lagasse", "Casino Square", "Darrin Stephens", "springtime for Hitler", "Holocaust memorial", "Sabraine Banado", "David Hockney", "Ireland", "rigbit", "Carrie", "Colombia", "rolling hillsides", "1 - 2 spinal nerve segments above the point of entry", "magnetic stripe `` anomalies '' on the ocean floor", "the ruling city of the Northern Kingdom of Israel, Samaria", "Tudor music and English folk-song", "Martin Joseph O'Malley", "1992", "sculptures", "when people gathered outside as the conference in the building ended.", "Kenyan forces who have entered Somalia,", "the Old Man and the Sea", "Edward I", "the Cranberries", "there were no radar outages and said it had not lost contact with any planes during the computer glitches."], "metric_results": {"EM": 0.515625, "QA-F1": 0.596771978021978}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.30769230769230765, 0.6666666666666666, 0.28571428571428575, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-4300", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-5551", "mrqa_triviaqa-validation-5714", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-1150", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-518", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-6490", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-2699", "mrqa_naturalquestions-validation-7511", "mrqa_hotpotqa-validation-4500", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-7161", "mrqa_newsqa-validation-904"], "SR": 0.515625, "CSR": 0.5292721518987342, "EFR": 0.7419354838709677, "Overall": 0.6538509021539405}, {"timecode": 79, "before_eval_results": {"predictions": ["Astor", "Addis Ababa", "peacock", "French", "HMS amethyst", "Libya", "tomato", "Kyoto", "costume", "Bull Moose Party", "european", "Jake La Motta", "resistance", "Hattie McDaniel", "South Africa", "bowel movements", "discretion", "swimmers", "The Apprentice", "Ernie Pyle", "Corinth Canal", "human rights lawyer", "Iceland", "ascot", "pearls", "Bruce Jenner", "gangsters", "bitches", "Duncan", "UKIP", "Argentina", "South Sudan", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Darby and Joan", "Toplis", "julian acan", "IT Crowd", "\u00ef\u00bf\u00bd", "local police officer Rip Nix", "Mario Lemieux, Mike Bossy, and Patrick Roy", "Richard Curtis", "Terms of Endearment", "China", "lagertha", "1790", "bay", "chamomile", "driving Miss Daisy", "orchid", "Hilary Swank", "abdeen", "the northernmost point on the Earth, lying diametrically opposite the South Pole", "the 18th century", "UTC \u2212 09 : 00", "just 18 minutes", "England", "Sri Lanka Freedom Party", "Raiders of the Lost Ark", "Afghanistan's Helmand province,", "Rodong Sinmun", "theology", "Cyd Charisse", "sanctions", "February"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6269791666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2433", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-5552", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-1338", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-2354", "mrqa_triviaqa-validation-889", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-2200", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1797", "mrqa_triviaqa-validation-3226", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-3505", "mrqa_hotpotqa-validation-4993", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-1857", "mrqa_searchqa-validation-2116"], "SR": 0.5625, "CSR": 0.5296875, "EFR": 0.8214285714285714, "Overall": 0.6698325892857142}, {"timecode": 80, "before_eval_results": {"predictions": ["Wisconsin", "Charles Habib Malik", "senators", "2", "dress shop", "Robert Gillespie Adamson IV", "Colon Street", "the outside world", "Los Angeles", "1969", "Tim Passmore", "2002 -- 2003", "First Epistle of John at 5 : 7 -- 8", "the Hudson Bay", "the chemical formula H CO ( equivalently OC (OH ) )", "Miami Heat", "end on September 30", "four", "manifestation of God's presence as perceived by humans according to the Abrahamic religions", "Emmanuelle Chriqui", "British Indian Association", "autopistas", "usually, the significance level is set to 0.05 ( 5 % )", "Australian reality television talent show which premiered on 18 February 2007 on the Seven Network", "a stack of thin films is called a multilayer", "Tulsa, Oklahoma", "Kristy Swanson", "Corey Taylor", "Bonanza Creek Ranch", "Tbilisi", "genome", "North Atlantic Ocean", "Native American nation", "United Nations", "as of October 1, 2015, when the green class A was retired", "2026", "318", "Director of National Intelligence", "Michael Crawford", "nomine Patris et Filii et Spiritus Sancti", "Kida", "The fourteenth season of the American television medical drama Grey's Anatomy was ordered on February 10, 2017, by American Broadcasting Company ( ABC ), and premiered on September 28, 2017", "Staci Keanan", "Brooklyn, New York", "1837", "Lagaan", "1996", "American rock band Los Lonely Boys", "Games", "the foreign exchange market (FX )", "the Coppolas", "Sunday Post", "Karl Pilkington", "pei Tang", "1860", "\"Back to December\"", "Ringo Starr", "\"Up\" mixes allegory with adventure and dumb imaginative exuberance.", "British author J.G. Ballard, whose boyhood experience in a World War II internment camp became the novel and film \"Empire of the Sun,\"", "off east  Africa", "modificre", "nmolly ringwald", "faerie", "the skull and crossbones"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6866110123114433}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8333333333333333, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.09523809523809523, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.20689655172413793, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.09090909090909093, 0.5714285714285715, 0.3333333333333333, 0.0, 0.5, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-8711", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-2967", "mrqa_triviaqa-validation-7286", "mrqa_hotpotqa-validation-5254", "mrqa_newsqa-validation-4104", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-11768", "mrqa_searchqa-validation-10600", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-691"], "SR": 0.5625, "CSR": 0.5300925925925926, "EFR": 0.9285714285714286, "Overall": 0.6913421792328043}, {"timecode": 81, "before_eval_results": {"predictions": ["Sun Tzu", "a container", "Cleopatra", "nuclear tests", "capitals", "pizza roll", "Sarah Jessica Parker", "Long Island Sound", "Hawaii", "fauvism", "Auguste Deter", "the People vs OJ Simpson trial", "spinning jenny", "gestation", "ravens", "J.R. Tolkien", "James Franco", "Blue Ridge Mountain range", "Georgia", "a mixture of iron oxide and aluminum oxide", "belly button", "Apple", "Thomas R. Gray", "a catfish", "A Chorus Line", "Frommer's", "Keira Knightley", "regret not speaking", "de Havilland", "Virginia", "College of William and Mary", "small", "fish", "Matthew Vassar", "Japan", "setlery", "The Police", "Air France", "Scarlatti", "slew them all and incinerated two of the children of his", "trudge", "Violent Femmes", "Albert Camus", "Volvo", "Rhode Island", "yodeling", "Indian Ocean", "a painkiller", "Jean-Paul Marat", "nanosecond", "The Gambit", "Mason Alan Dinehart", "chain elongation", "on location", "2010", "cymbal", "Madagascar", "Cassio", "Estadio Victoria", "Allerdale", "Mugabe's opponents", "70,000", "Israel", "Owsley Stanley"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6138392857142857}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-13181", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-1201", "mrqa_searchqa-validation-4338", "mrqa_searchqa-validation-1527", "mrqa_searchqa-validation-12500", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-9661", "mrqa_searchqa-validation-6884", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-9895", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-13846", "mrqa_searchqa-validation-5489", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-506", "mrqa_searchqa-validation-6250", "mrqa_searchqa-validation-1975", "mrqa_hotpotqa-validation-980", "mrqa_hotpotqa-validation-868"], "SR": 0.53125, "CSR": 0.5301067073170731, "EFR": 0.8666666666666667, "Overall": 0.678964049796748}, {"timecode": 82, "before_eval_results": {"predictions": ["Nautilus", "albinism", "Vatican City", "Pope John Paul II", "Yangtze", "Gnarls Barkley", "the Parthenon", "Namita Jain", "Marilyn Monroe", "Souvlaki", "Richard III", "the bald eagle", "the National Gallery of Art", "4,840 square yards", "the \"Nature Cruise of the Century\"", "Dirck Hals", "Chicago White Sox", "a lynx", "Monin", "Constantine", "the Aleutian Islands", "alchemy", "art nouveau", "Autobahn", "Anglo-Saxon", "California quail", "curtsy", "lacrosse", "Toronto", "grave accent", "King David", "Riboflavin", "plumes", "Indiana Jones", "Michigan", "Blue", "freelance", "Philadelphia", "Goodyear", "The hobbit", "Boston Red Sox", "William Claude Dukenfield", "Yale University", "Graceland", "the Caspian Sea", "point d'Angleterre", "Lee Marvin", "Prince Edward Island", "Westminster Abbey", "Superbad", "the Granite State", "1885", "$2.187 billion", "On the west", "France", "maqui berry", "Benedictine Order", "Pansexuality", "authorship of \"Titus Andronicus\"", "an American actor, singer and a DJ (under the name Ans\u00f8lo)", "Drew Kesse,", "the pregnancy.", "eight-week", "1999"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6227430555555555}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.22222222222222224, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-9065", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-15986", "mrqa_searchqa-validation-4928", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-12190", "mrqa_searchqa-validation-2505", "mrqa_searchqa-validation-13324", "mrqa_searchqa-validation-14340", "mrqa_searchqa-validation-6941", "mrqa_searchqa-validation-13625", "mrqa_searchqa-validation-11580", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-16140", "mrqa_searchqa-validation-9427", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5308", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-3637", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-3145", "mrqa_newsqa-validation-3331", "mrqa_hotpotqa-validation-943"], "SR": 0.5625, "CSR": 0.5304969879518072, "EFR": 0.8928571428571429, "Overall": 0.68428020116179}, {"timecode": 83, "before_eval_results": {"predictions": ["George Washington", "The Office", "Jesus", "penguins", "vrai", "Napoleon Bonaparte", "A.J. Foyt", "vulture", "Nantucket", "Ebony", "Trinity College", "Algeria", "Joseph Haydn", "Dick Cheney", "Jeopardy", "a number", "Saturday Night Fever", "(AFI)", "pizza al taglio", "turtle", "the Empire State Building", "white blood cells", "a picayune", "dogwood", "Qubec", "Larry McMurtry", "Kellogg's", "Helen of Troy", "sweats sweatshirt", "W=Fd", "Napoleon", "ivory", "Lapland", "Ben & Jerry", "Rigoletto", "Tim Tebow", "schizophrenia", "Catherine of Aragon", "Ba", "Pancho Gonzales", "the Aleutians", "the Mormon Trail", "Lady Jane Grey", "Tommy Tutone", "the crescent moon", "the Hijaz", "a leafhopper", "Aristotle", "Stuffed Poblano Chiles", "William Safire", "( Leonardo da Vinci)", "a journey from London to Canterbury to visit the shrine of Saint Thomas Becket at Canterbury Cathedral", "A.E. Southon", "Andrea Brooks", "the fallopian tube", "Some Like It Hot", "Barbara Eden", "Casablanca", "Theodore Roosevelt Mason", "Parlophone", "Wednesday.", "Daryeel Bulasho Guud", "1995", "four"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6457031249999999}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0, 0.8, 0.125, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-12923", "mrqa_searchqa-validation-16443", "mrqa_searchqa-validation-2764", "mrqa_searchqa-validation-7924", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-5131", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1070", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-8128", "mrqa_searchqa-validation-12014", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-699", "mrqa_searchqa-validation-12489", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-1989", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-7457", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-3182", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-537"], "SR": 0.546875, "CSR": 0.5306919642857143, "EFR": 0.8275862068965517, "Overall": 0.6712650092364532}, {"timecode": 84, "before_eval_results": {"predictions": ["Russia", "Henry VIII", "Judas Iscariot", "Windsor, Ontario", "Stephen Douglas", "COMMUNIST", "Paradise", "foxes", "Sexuality", "Salaries", "David", "John McEnroe", "a bicycle", "Johnson County", "Jericho", "push", "Alexander Solzhenitsyn", "farce", "Mexico", "Easter", "John Denver", "Hurricane Katrina", "Paris", "leeches", "Daughters of the American Revolution", "Manila", "St Mark", "Eragon", "\"Strawberry Fields Forever\"", "Louisiana", "Mexico", "a pirate", "engrave", "Daisy Miller", "the Legion of Honour", "X", "a ship", "Kamehameha", "a raccoons", "Virginia", "Wikiquote", "the north magnetic pole", "oysters", "an Italian-American", "Santa FRANCISCO", "Zimbabwe", "0.304 m", "Patty Duke", "Pronouns", "Hoffmann", "a calico cat", "Frankie Muniz", "season two", "A complex sentence", "40", "outer", "Nowhere Boy", "August 1973", "boar-crested helm", "Richa Sharma", "Carrefour", "financial gain", "concentration camps,", "agent Mark Steinberg"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6401041666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-10542", "mrqa_searchqa-validation-6222", "mrqa_searchqa-validation-6736", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-6178", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10841", "mrqa_searchqa-validation-8044", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-3347", "mrqa_searchqa-validation-11098", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-9898", "mrqa_naturalquestions-validation-9752", "mrqa_triviaqa-validation-2049", "mrqa_hotpotqa-validation-1226", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3759"], "SR": 0.578125, "CSR": 0.53125, "EFR": 0.8888888888888888, "Overall": 0.6836371527777778}, {"timecode": 85, "before_eval_results": {"predictions": ["Boston", "Winnie The Pooh", "Italian", "Eggs Benedict", "the Taj Mahal", "Ayn Rand", "Brahma", "Jon Stewart", "The Sweet", "Tiger Woods", "the Amazon", "Harry Houdini", "Falconer", "Latifah", "Ezra Cornell", "Strawberry Fields", "The Hague", "Geena Davis", "pharmacy", "( Kit) White", "the NFL", "Jimmy Doolittle", "air", "the Curtain", "Floyd Mayweather Jr", "ABBA", "the League of Nations", "Marlee Matlin", "the money changers", "The X-Files", "Animals", "Mensa", "Edward Hopper", "oratorios", "steak", "a voodoo sorcerer", "a toddler", "Salt Lake City", "Veneto", "a watermelon", "the North Atlantic Treaty Organization", "Sparta", "the Sunday New York Times", "anode", "boldly go", "the National Teachers Hall of Fame", "the Bicentennial", "the Cherokee", "the epidermis", "the Texas Rangers", "Fluoxetine", "H CO ( equivalently OC (OH ) )", "Middle Eastern alchemy", "Brooklyn, New York", "Eton College", "Leeds", "Bexar", "Dwight D. Eisenhower", "Battleship", "\"Shake It Off\"", "pizza, the other for the drug ketamine.", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "Why he's more American than a German,", "Charles Sherrington"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6067708333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4481", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14183", "mrqa_searchqa-validation-7391", "mrqa_searchqa-validation-5581", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-6109", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-2458", "mrqa_searchqa-validation-13169", "mrqa_searchqa-validation-610", "mrqa_searchqa-validation-1894", "mrqa_searchqa-validation-16201", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-7363", "mrqa_searchqa-validation-3434", "mrqa_searchqa-validation-12882", "mrqa_searchqa-validation-11773", "mrqa_searchqa-validation-10056", "mrqa_searchqa-validation-14355", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-2999", "mrqa_hotpotqa-validation-5190", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2692"], "SR": 0.546875, "CSR": 0.5314316860465116, "EFR": 0.7586206896551724, "Overall": 0.6576198501403369}, {"timecode": 86, "before_eval_results": {"predictions": ["Happy feet", "a short race", "a Yeti", "Joseph", "Chicago", "Aphrodite", "Cannery Row", "Palatine", "California", "Mississippi", "Alpha", "Quebec", "nacre", "Texas Chainsaw Massacre III", "the rotunda", "a Medal of Honor", "Manet", "Plutarch", "Mediolanum", "Corin", "Shropshire", "a kidney", "Afghanistan", "satin", "Godiva", "the Sadler", "Vasco da Gama", "Millard", "a chino", "Finnegans Wake", "salamu", "the black market", "professor of higher education", "S-waves", "Maastricht", "Delilah", "synapses", "a croissant", "Rocky Down Mexico Way", "lungs", "fuchsia", "metacarpal", "a pool", "Bern", "a trowel", "Mercury", "China", "Gettysburg", "the 2010 Olympics", "trout", "slow", "1959", "season two", "$75,000", "(1923 - 1953)", "15", "stonemason's Yard", "Agent Carter", "Orson Welles", "Manhattan", "56", "Secretary of State", "Bright Automotive,", "James Hogg"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6186011904761906}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16474", "mrqa_searchqa-validation-1883", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-16459", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-7139", "mrqa_searchqa-validation-2960", "mrqa_searchqa-validation-4065", "mrqa_searchqa-validation-861", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-3402", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-9319", "mrqa_searchqa-validation-2509", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5239", "mrqa_searchqa-validation-6883", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-16661", "mrqa_searchqa-validation-2311", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-1095", "mrqa_triviaqa-validation-5762", "mrqa_hotpotqa-validation-24"], "SR": 0.546875, "CSR": 0.5316091954022988, "EFR": 0.896551724137931, "Overall": 0.685241558908046}, {"timecode": 87, "before_eval_results": {"predictions": ["Tim Russert", "the Alfonsists and the Carlists", "Michael Crawford", "Sonu Nigam", "Pat McCormick", "Louis Mountbatten", "David Ben - Gurion", "April 6, 1917", "assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "close to 5,770 guaranies", "Geoffrey Zakarian", "$72", "Walmart", "Jeff Gillen", "Milan, Italy", "homicidal thoughts of a troubled youth", "Florida", "Husrev Pasha", "Patrick Warburton", "Harry Kane", "Francis Ford Coppola", "$66.5 million", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "slavery", "Donny Osmond", "a political pamphlet written by the Russian revolutionary Vladimir Lenin in 1901 and published in 1902", "Australia", "Anakin Luke", "Jeff East", "one", "Thomas Lennon", "Ed Roland", "Kevin Garnett", "a star", "Brazil", "Instagram's own account", "the U.S. state of Washington", "by the early 3rd century", "Triple threat", "in his first year at the Hogwarts School of Witchcraft and Wizardry", "shared", "foreign investors", "Napoleon", "the inverted - drop - shaped icon that marks locations in Google Maps", "James Ray", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "2004", "User State Migration Tool ( USMT )", "Robber baron", "December 20, 1951", "Crick", "Aconcagua", "Bake Off", "2002", "Noah Levenstein", "Aldosterone", "Nicole Kidman", "last summer.", "speaking out about a cause", "fourth", "banker", "eyelid", "the Cubs", "thief"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6141345005159614}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.782608695652174, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7741935483870968, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.4, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9825", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-2524", "mrqa_naturalquestions-validation-1656", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-2008", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-3281", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-9712", "mrqa_triviaqa-validation-1386", "mrqa_triviaqa-validation-3335", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-397", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-152", "mrqa_searchqa-validation-14144", "mrqa_searchqa-validation-14176", "mrqa_triviaqa-validation-4676"], "SR": 0.515625, "CSR": 0.5314275568181819, "EFR": 0.8709677419354839, "Overall": 0.6800884347507331}, {"timecode": 88, "before_eval_results": {"predictions": ["Cologne, Germany", "Philip Markoff", "a bag", "World number two Roger Federer", "Veracruz, Mexico,", "Diego Milito's", "\"Mammograms are known to be uncomfortable,\"", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "Salt Lake City, Utah,", "normal maritime traffic", "Saluhallen, the covered market on Lilla Torg", "to protect ocean ecology, address climate change and promote sustainable ocean economies.", "Rocky Ford brand cantaloupes", "\"The oceans are kind of the last frontier for use and development,\"", "Two suspects are in custody.", "\"We want to reset our relationship and so we will do it together.'\"", "club managers", "Long Island", "90", "FBI", "Reggae legend Lucky Dube,", "the Kurdish militant group in Turkey", "At least 14", "\"It hurts my heart to see him in pain, but it enlightenedens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Kerstin Fritzl,", "Justice Department motion filed last week in support of the Defense of Marriage Act -- which effectively bars the federal government from recognizing same-sex unions.", "Europe", "\"I believe it's discriminatory. I think it interferes with state's rights, and we will work with Congress to overturn it,\"", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Greeley, Colorado", "Festival Foods", "military fired warning shots into the air and sprayed water cannons to disperse the crowd.Shiha said nearly 100 people were hurt, including nine who were shot by live ammunition. Saeed", "drugs", "Daniel Radcliffe", "1.2 million", "\"It was a wrong thing to say, something that we both acknowledge,\"", "12.3 million", "Krishna Rajaram,", "a rocket", "Patrick McGoohan,", "saying Chaudhary's death was warning to management.", "Hamas ministry spokesman Taher Nunu", "state senators", "2,000 euros ($2,963)", "Anil Kapoor", "Turkey can play an important role in Afghanistan as a reliable NATO ally.", "Yemeni port city of Aden", "12 off-duty federal agents in southwestern Mexico,", "dogfights.", "central business district of Bangkok", "journalists and the flight crew will be freed,", "writ of certiorari", "pigs", "James Corden", "Norway", "Hamlet", "The Hague Conventions", "Robert Downey, Jr.", "Viscount Cranborne", "England", "beef", "Sleyman I", "the rainforest", "Ramadan"], "metric_results": {"EM": 0.46875, "QA-F1": 0.577513354362136}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.4444444444444445, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.13333333333333333, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.9655172413793104, 0.6666666666666666, 0.24000000000000002, 0.0, 0.0, 0.0, 1.0, 0.0, 0.11999999999999998, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.17391304347826086, 0.0, 0.2, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-834", "mrqa_newsqa-validation-1361", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-1534", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-1662", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-1212", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-3169", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-3763"], "SR": 0.46875, "CSR": 0.5307233146067416, "EFR": 0.6470588235294118, "Overall": 0.6351658026272307}, {"timecode": 89, "before_eval_results": {"predictions": ["133d Air Refueling Squadron", "Kim So-hyun", "president of Guggenheim Partners", "Comedy Film Nerds", "9\u201310 March 1945", "2011", "John D Rockefeller's Standard Oil Company", "early 1970s", "Asiana Town building", "American R&B singer, guitarist, songwriter and music producer", "Rockland County", "Manitowoc County, Wisconsin", "34.9 kilometres", "1967", "alcoholic drinks", "Fiat Chrysler Automobil NV", "Chrysler", "Australian", "gorillas", "\"Traumnovelle\" (\"Dream Story\")", "Joshua Rowley", "National Archives and Records Administration (NARA)", "Yitzhak Edward Asner", "the Beatles", "Baden-W\u00fcrttemberg", "2001 NBA All-Star Game", "\" rated R\"", "95 AD", "1614", "French", "\"The Manhunter from Mars\" in \" Detective Comics\" #225 (Nov. 1955)", "Mondays", "Michael Jordan", "\"I, D'oh- Bot\"", "HSBC Building", "1987", "Kalokuokamaile", "17 October 2006", "melodic hard rock", "The Home Rule League", "Anne Fletcher", "1822", "Mulberry", "Suspiria", "The Independent", "Kansas\u2013Nebraska Act", "Scandinavian design", "Buck Owens", "Big Machine Records", "postal delivery", "Flaw", "June 5, 2017", "1972", "an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "Daniel Boone", "forearm", "Tigris", "African National Congress Deputy President Kgalema Motlanthe,", "Piers Morgan Tonight", "misdemeanor assault charges after a fight at his Texas high school", "Florida", "The Partridge Family", "Mickey Spillane", "for housing, business and infrastructure repairs, federal authorities said."], "metric_results": {"EM": 0.625, "QA-F1": 0.6844211344211344}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.9, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-1509", "mrqa_hotpotqa-validation-3795", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5233", "mrqa_hotpotqa-validation-5188", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-3832", "mrqa_naturalquestions-validation-9812", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-702", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3369"], "SR": 0.625, "CSR": 0.5317708333333333, "EFR": 0.875, "Overall": 0.6809635416666666}, {"timecode": 90, "before_eval_results": {"predictions": ["Sharon Sheeley", "Ardeth Bay", "2009", "singer", "Pakistan", "1754", "Confessions of a Teenage Drama Queen", "Bundesliga", "valkyries", "David Villa S\u00e1nchez", "Adrian Peter McLaren", "2013", "an early colonist of South Australia, remembered as a schoolmaster at J. L. Young's Adelaide Educational Institution and at Saint Peter's College.", "Cleopatra", "Leon Marcus Uris", "Knoxville, Tennessee", "cancer", "Kim Yoon-seok and Ha Jung-woo", "Fuli", "25 November 2015", "Craig William Macneill", "January 14, 2010", "2,664", "Tamil", "Objectivism", "Chicago", "London Heathrow", "Riot Act", "The Gold Coast", "January 30, 1930", "Soma", "October 29, 1985", "35,124", "Estadio de L\u00f3pez Cort\u00e1zar", "Sir Seretse Khama", "Scandinavian design", "Mike Pence", "Barack Obama", "Flexible-fuel", "Bulgarian", "1949", "Trappist beer", "\"Waiting for Guffman\"", "Presbyterian", "138,535", "Ry\u016bky\u016b", "1972", "Stern-Plaza", "Life Is a Minestrone", "Columbia Records", "The Spiderwick Chronicles", "Jewel Akens", "gravitation", "ensures consistency within a document and across multiple documents and enforces best practice in usage and in language composition, visual composition, orthography and typography", "South Africa", "Ann Widdecombe", "Jennifer Eccles", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "sailor", "Robert Barnett,", "Barbara Explorer", "Irving Stone", "CO2", "Shout"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6619578677662917}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.787878787878788, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.30434782608695654, 1.0, 1.0, 1.0, 0.2564102564102564, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3064", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-1539", "mrqa_hotpotqa-validation-4501", "mrqa_hotpotqa-validation-3907", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-598", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1457", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-4050", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-4335", "mrqa_searchqa-validation-10988", "mrqa_searchqa-validation-11743"], "SR": 0.5625, "CSR": 0.5321085164835164, "EFR": 0.7142857142857143, "Overall": 0.6488882211538461}, {"timecode": 91, "before_eval_results": {"predictions": ["Close Encounters of the Third Kind", "Apple", "Jaguar S-Type R", "the Arch", "Friday", "Sabino Canyon", "Depp", "Babe Ruth", "Knots Landing", "Arkansas", "Vince Lombardi", "Virgo", "Contemporary", "Steppenwolf", "bcolicus", "Tito Puente", "hydrogen", "\"...Drink to me only with thine eyes, And I will pledge with mine", "Hodgkin's", "Margaret, Countess of Snowdon", "Las Vegas", "San Francisco", "phosphates", "Mary Baker Eddy", "Bank One Corp.", "the College of William & Mary", "The Wright Brothers", "Badminton", "John Deere", "43", "Pontiac", "reptiles", "Georgia Bulldogs", "Key lime pie", "Lettuce", "Arabian Nights", "bumblebee", "Savannah", "Rickey Henderson", "parquet", "Alice Walker", "F Troop", "Russia", "Lincoln", "Eva Pern", "Port Royal", "a key", "Ghost", "Francisco Pizarro", "Iraq", "Marat", "Cetshwayo", "Bay of Montevideo", "the bank, rather than the purchaser, is responsible for paying the amount", "special sauce lettuce cheese pickles onions", "Bobby Brown", "bath", "1.5 million households", "Macomb County", "Kristoffer Rygg", "four university students and a safety officer -- told the Coast Guard they were forced off their sailboat after it took on water and capsized.", "Monday", "eight", "minister and biographer"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6657852564102564}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-5873", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-7846", "mrqa_searchqa-validation-5927", "mrqa_searchqa-validation-13146", "mrqa_searchqa-validation-13929", "mrqa_searchqa-validation-595", "mrqa_searchqa-validation-254", "mrqa_searchqa-validation-14099", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-955", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-10442", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-1574", "mrqa_searchqa-validation-14501", "mrqa_naturalquestions-validation-3303", "mrqa_triviaqa-validation-7696", "mrqa_newsqa-validation-4011", "mrqa_hotpotqa-validation-4539"], "SR": 0.5625, "CSR": 0.5324388586956521, "EFR": 0.8214285714285714, "Overall": 0.6703828610248447}, {"timecode": 92, "before_eval_results": {"predictions": ["Luzon", "Brancusi", "Quantico, Virginia", "the East River", "Shakespeare", "William Shakespeare", "abscesses", "Alaska", "Sputnik I", "Richmond, Virginia", "the early predecessors of program music", "Java", "baritone", "Reginald", "Indivisible", "Room-temperature vulcanization", "Wuthering Heights", "Muhammad", "Astrology", "Dead Man's Chest", "Frederick Forsyth", "Chesterfield, Virginia", "a chipmunk", "Rose", "salt", "a disorderly Conduct", "Gioachino Rossini", "Oman", "Spmi", "Tom Canty", "Roman Polanski", "Didion", "Chesapeake", "Baltimore", "the Bay of Bengal", "John Morton", "Hillary Clinton's", "Terrific", "time scale", "six sides", "Olympia", "Ship of Fools", "scare zone", "Research Proposal", "fluid", "Margaret Mitchell", "Frances Farmer", "Toorop", "Cremation", "French and Indian War", "a manic episode", "the Hudson Bay", "lighter fluid", "a scuffle with the Beast Folk", "Judi Dench", "Germany", "paernarfon", "Dar es Salaam", "Love Streams", "My Beautiful Dark Twisted Fantasy", "between June 20 and July 20", "Michael Krane,", "Virgin America", "President Paul Biya, left, walks with Pope Benedict XVI at the airport in Yaounde, Tuesday."], "metric_results": {"EM": 0.40625, "QA-F1": 0.5489831349206349}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1244", "mrqa_searchqa-validation-16790", "mrqa_searchqa-validation-6225", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4155", "mrqa_searchqa-validation-11106", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6502", "mrqa_searchqa-validation-173", "mrqa_searchqa-validation-2299", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-13171", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-3466", "mrqa_searchqa-validation-20", "mrqa_searchqa-validation-7466", "mrqa_searchqa-validation-4824", "mrqa_searchqa-validation-11893", "mrqa_searchqa-validation-15704", "mrqa_searchqa-validation-15802", "mrqa_searchqa-validation-13701", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-315", "mrqa_searchqa-validation-14800", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-6507", "mrqa_searchqa-validation-15281", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-4465", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-6867", "mrqa_hotpotqa-validation-3557", "mrqa_newsqa-validation-1663"], "SR": 0.40625, "CSR": 0.5310819892473118, "EFR": 0.8421052631578947, "Overall": 0.6742468254810413}, {"timecode": 93, "before_eval_results": {"predictions": ["Mesopotamia", "Gettysburg College", "Tim McGraw", "provides the public with financial information about a nonprofit organization", "Lulu", "Tony Orlando", "Mel Gibson", "1947, 1956, 1975, 2015 and 2017", "drivers who were 2016 Pole Award winners, former Clash race winners, Former Daytona 500 pole winners who competed full - time in 2016, and drivers who qualified for the 2016 Chase", "over the next seven years until the last coin, a gold sovereign, was struck in London in November 1975", "Pacific Grove", "while in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form, he made the hair more `` wild '' and covered Goku's body in red fur", "Audrey II", "January 2017", "NIRA", "1922", "Jacqueline Bouvier", "Justin Timberlake", "American singer Daya", "13 May 1787", "Prince James, Duke of York and of Albany ( later King James II & VII )", "his brother", "Seattle, Washington", "honey bees", "Article 1, Section 2, Clause 3", "Robin Williams", "Napoleon", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "September 27, 2017", "Fusajiro Yamauchi", "March 31 to April 8, 2018", "Tbilisi", "Tiffany Adams Coyne", "Jupiter", "hyperinflation", "1939", "Richard Masur", "Kyla Pratt", "Spanish", "Sauron", "Lana Del Rey", "statistical advantage for the casino that is built into the game", "159", "Third Five - year Plan", "The chant was first adopted by the university's science club in 1886", "makes Maria a dress to wear to the neighborhood dance", "increases the life of the pump, allows a smaller and lighter device to be used, and reduces electrical load", "limited period of time", "commemorating fealty and filial piety", "inwards towards the pith", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "euro", "Robin Hood's A Holy Grail", "Snowshoe", "February 13, 1946", "Crystal Dynamics", "Congo River", "CNN.com", "The Da Vinci Code", "humiliate herself by standing next to a story,\"", "Khrushchev", "Julie Andrews", "Ichabod Crane", "Leo Frank,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6252487661794727}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.13333333333333333, 0.17391304347826086, 0.30769230769230765, 1.0, 0.0, 0.0909090909090909, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3305", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3374", "mrqa_searchqa-validation-105"], "SR": 0.53125, "CSR": 0.5310837765957447, "EFR": 0.7666666666666667, "Overall": 0.6591594636524822}, {"timecode": 94, "before_eval_results": {"predictions": ["the theory of direct scattering and inverse scattering", "ThonMaker", "Battle of Chester", "youngest TV director ever", "19 February 1927", "on the shore, associated with \"the Waters of Death\" that Gilgamesh had to cross to reach Utnapishtim, the far-away", "playback singer, director, writer and producer", "L\u00edneas A\u00e9reas", "English", "National Basketball Development League", "Neville Chamberlain, then Minister of Health,", "Boulder High School in Boulder, Colorado", "Revengers Tragedy", "Japan", "rural", "8", "Larry Alphonso Johnson Jr.", "Gabriel Iglesias", "August 28, 1774", "CMYKOG process", "Las Vegas Boulevard", "The Bridge Between Science and Theology", "Anthony Herrera", "Jack Elam", "Adelaide Botanic Garden, Hutt Street, and Victoria Park", "Uzumaki", "Kansas", "nearly 80 years", "Chevy Corvette Stingrays", "Field of Dreams", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music.", "The Wachowskis", "Pour le M\u00e9rite", "mastered recordings for many well known musicians, including David Bowie, The Subways, Foo Fighters, Lou Reed, Paul McCartney, Sin\u00e9ad O'Connor, Natalie Merchant, Marianne Faithfull, and Madonna.", "Drowning Pool", "typically found within a casino, ranging from card to slot machines.", "Food and Agriculture Organization", "dance partner", "Bharat Ratna", "Cesar Millan", "Eurasia", "Beauty and the Beast", "Bardney", "Holinshed's Chronicles", "August 9, 2017", "Bangalore University", "25 October 1921", "Australian", "Bonkyll Castle", "February 5, 2015", "Zeffirelli", "giant planet", "alveolar process", "Dortmund - Ems Canal", "Hugh Quarshie", "Alexandra", "Tokyo", "Utah Valley Regional Medical Center", "Madonna", "Fareed Zakaria", "Easter Island", "Eli Whitney", "Today", "25"], "metric_results": {"EM": 0.625, "QA-F1": 0.7347787872458924}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.3157894736842105, 0.2857142857142857, 0.8, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.47619047619047616, 1.0, 1.0, 0.14814814814814814, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-839", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-3292", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2344", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-1206", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-4047", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-2703", "mrqa_triviaqa-validation-514", "mrqa_searchqa-validation-2056"], "SR": 0.625, "CSR": 0.5320723684210527, "EFR": 0.8333333333333334, "Overall": 0.6726905153508772}, {"timecode": 95, "before_eval_results": {"predictions": ["more than 250 million copies worldwide", "Ben Ainslie", "1978", "Spoorloos", "Scott Mosier", "1950", "Roy Spencer", "1964", "Shawnee Mission Parkway", "VH1", "March", "Russian", "Jack Ryan", "July 25 to August 4", "Claude Mak\u00e9l\u00e9l\u00e9 Sinda", "singer, songwriter, actress", "Northern Lights", "non-alcoholic", "Mach number", "Days of Our Lives", "Maine", "Encore Las Vegas", "Baa, Black sheep", "It's Always Sunny in Philadelphia", "John Francis Kelly", "Madeleine L' Engle", "1972", "President John F. Kennedy", "paracyclist", "Mandarin", "Kevin Spacey", "a pro-vice-chancellor at some institutions in the United Kingdom and Ireland, or a Deputy Vice-Chancellor (Academic) at most Australian universities", "Song Il-gon", "Teenitans Go!", "Mickey Mouse Cup", "Grammy Award", "right-hand", "Sheen Michaels Entertainment", "Sela Ann Ward", "seal hunting", "Houston Rockets", "DI Humphrey Goodman", "Daphnis et Chlo\u00e9", "Nebraska Cornhuskers women's basketball", "Metro-Goldwyn-Mayer", "P.O.S,", "My Backyard", "Sun Woong", "boxer", "Aloe Vera of America", "creeks", "1992", "the Second Continental Congress", "Roger Dean Stadium", "cirrocumulus", "Compiegne", "La traviata", "April", "The Da Vinci Code", "Dog patch Labs", "iceberg", "a battery", "Queen Victoria", "Venus Williams"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6666914682539683}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.8571428571428571, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-74", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-5809", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-4196", "mrqa_hotpotqa-validation-1151", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-5665", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-2388", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-191", "mrqa_searchqa-validation-14503"], "SR": 0.578125, "CSR": 0.5325520833333333, "EFR": 0.8518518518518519, "Overall": 0.676490162037037}, {"timecode": 96, "before_eval_results": {"predictions": ["an obsessed and tormented king", "1927", "16,116", "the 2012 Summer Olympics", "the end of the 18th century", "1942", "Johnny Cash and Waylon Jennings", "Estadio de L\u00f3pez Cort\u00e1zar", "Syracuse", "Kim Jong-hyun", "Bears", "Gillian Leigh Anderson", "the alternative rock band R.E.M.", "Ice Princess", "the Games of the Olympiad", "Oldham County", "1896", "Oracle Corporation", "143,007", "SARS", "5.3 million", "chocolate-colored", "Norman Graham Hill", "1952", "Neneh Mariann Karlsson", "American rapper Eminem", "Love Streams", "In a Better World", "Shropshire Union Canal", "Easy", "The Killer", "2015", "Dutch", "Lowestoft, Suffolk", "Trey Parker", "\"Pimp My Ride\"", "Big 12 Conference", "Hillsborough County", "Dancing with the Stars", "an Indian", "John Francis Kelly", "early Romantic period", "$700 million", "the Sun", "Bhushan Patel", "24 December 1692", "interstate commerce", "The Wu-Tang Clan", "Kids", "Mortal Kombat X", "Kew Gardens", "third", "May 2016", "Kristy Swanson", "colonel", "greece (or joel) leckie)", "scalene", "to put a lid on the marking of Ashura", "Pakistan", "homicide", "bread pudding", "leather", "cornea", "Eleanor Roosevelt"], "metric_results": {"EM": 0.625, "QA-F1": 0.794248112998113}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.3333333333333333, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.18181818181818182, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-4514", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-1887", "mrqa_triviaqa-validation-2789", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1218"], "SR": 0.625, "CSR": 0.5335051546391752, "EFR": 0.9583333333333334, "Overall": 0.6979770725945017}, {"timecode": 97, "before_eval_results": {"predictions": ["The Theory of Everything", "the Caucasus range", "David Bowie", "Steve Davis", "Granada", "Treaty of Brest-Litovsk", "Georg Wilhelm Friedrich", "Procol Harum", "Wallis Warfield", "feisty", "1957", "1912", "Transvestite/Cross dresser", "Fred Astaire", "Southampton", "Scotland Yard detective", "the island does not have a common name in either English or Scottish Gaelic and is referred to as 'Lewis and Harris", "fair", "doinnie Mae", "Rudyard Kipling", "1921", "The Full Monty", "Emilia", "avocado", "Frans Hals", "Syriza", "Ford", "garbanzo", "Cole Porter", "1826", "w WJacobs", "the Parthenon", "Paddy Dooley", "Thomas Aquinas", "Dubonnet", "elephant", "Tigran Petrosyan", "artha", "Westminster Abbey", "Canada", "Seal", "Edward VII", "Tombstone", "Santo Ant\u00e3o", "Mr. Tickle", "Worcester Cathedral", "Mercury", "December 7, 1941", "the ear", "Nadia Comaneci", "Buzz Aldrin", "Ant & Dec", "John Ernest Crawford", "March 1930", "Gillian Anderson", "VAQ-135", "95 AD", "more than 170", "Gossip Girl", "Karen Floyd", "awe-inspiring", "the Caspian Sea", "Francisco Pizarro", "1922 to 1991"], "metric_results": {"EM": 0.5, "QA-F1": 0.5854166666666667}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7127", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5066", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3538", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-5204", "mrqa_triviaqa-validation-6617", "mrqa_triviaqa-validation-2651", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-4876", "mrqa_triviaqa-validation-3730", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-1547", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-174", "mrqa_hotpotqa-validation-2058", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3100", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-15233", "mrqa_naturalquestions-validation-7080"], "SR": 0.5, "CSR": 0.5331632653061225, "EFR": 0.84375, "Overall": 0.6749920280612245}, {"timecode": 98, "before_eval_results": {"predictions": ["sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "Afghanistan", "deutschneudorf", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "his health and about a comeback.", "poems telling of the pain and suffering of children just like her; girls banned from school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan.", "then-Sen. Obama", "women", "581 points", "The Everglades, known as the River of Grass,", "Herman Cain", "Brett Cummins,", "3-0", "what caused the collapse of the building which contained the city's historical archives, bringing down parts of the two nearby structures.", "Former Mobile County Circuit Judge Herman Thomas", "celebrities and ministers, ranging from Yolanda Adams to Bishop T.D. Jakes to Kirk Franklin.", "generate job opportunities for nearly 200,000 Iraqi citizens in infrastructure, industrial projects, support services and other business activities.", "Phillip A. Myers.", "\"Swingin' Down the Lane\"", "to share personal information.", "Argentina", "a Yemeni cleric and his personal assistant,", "Iraqi Prime Minister Nouri al-Maliki", "France", "did not speak to those who had gathered but shadow-boxed to spectators and cameras before meeting his distant relatives.\"It was incredible. We've had so much rain, and yet today it was beautiful.", "\" Few people are so lucky to have that from the moment you meet that one person, and everything we add,\"", "WBO welterweight title", "Austin, Texas,", "15-month investigation, at least a part of which was conducted undercover.\"", "to pay him a monthly allowance,", "Manmohan Singh's Congress party,", "the war of words in the Republican Party centered around Rush Limbaugh.", "for death squad killings carried out during his rule in the 1990s.", "100 meter", "sniff out cell phones.", "Fayetteville, North Carolina,", "American Bill Haas", "Consumer Reports", "28", "step up.\"", "42 years old", "since 1983", "particular health ailment or beauty concern.", "almost 100", "Al Alberto Espinoza Barron", "Derek Mears", "\"By working together, we will set wise and effective policies.\"", "fastest circumnavigation of the globe in a powerboat", "$106,482,500", "18th", "Haeftling", "left of the dinner plate", "Asuka", "Bart Millard", "Nissan", "stone arch bridges", "jMW Turner", "Marx Brothers film", "Indian", "early 20th-century Europe", "\"No hostage will be released until all our demands are met,\"", "Shakespeare in Love", "w. Somerset Maugham", "leicestershire"], "metric_results": {"EM": 0.453125, "QA-F1": 0.608669855293912}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.8918918918918919, 0.0, 0.0625, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 0.38095238095238093, 0.8333333333333333, 0.0, 0.09999999999999999, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.11764705882352941, 0.07692307692307693, 1.0, 1.0, 0.18181818181818182, 0.1818181818181818, 0.6666666666666666, 0.0, 0.2105263157894737, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 0.13333333333333333, 0.2857142857142857, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.16666666666666669, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2182", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-3073", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-1336", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2745", "mrqa_newsqa-validation-2541", "mrqa_naturalquestions-validation-2024", "mrqa_triviaqa-validation-3928", "mrqa_triviaqa-validation-5307", "mrqa_hotpotqa-validation-3326", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-14191"], "SR": 0.453125, "CSR": 0.532354797979798, "EFR": 0.6285714285714286, "Overall": 0.6317946203102454}, {"timecode": 99, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1002", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-136", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1834", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-2132", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2463", "mrqa_hotpotqa-validation-2489", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2652", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2844", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3278", "mrqa_hotpotqa-validation-3289", "mrqa_hotpotqa-validation-3301", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-377", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3996", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4169", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4435", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5283", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5630", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-969", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-228", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9208", "mrqa_naturalquestions-validation-9219", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9595", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-9987", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1475", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-189", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2379", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3077", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-3118", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3464", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3925", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-965", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-10146", "mrqa_searchqa-validation-10231", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10763", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-11077", "mrqa_searchqa-validation-11089", "mrqa_searchqa-validation-11111", "mrqa_searchqa-validation-11151", "mrqa_searchqa-validation-11196", "mrqa_searchqa-validation-11599", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12092", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12942", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13182", "mrqa_searchqa-validation-13352", "mrqa_searchqa-validation-13625", "mrqa_searchqa-validation-13654", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-1371", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14001", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-14740", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-14805", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15157", "mrqa_searchqa-validation-15235", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-15843", "mrqa_searchqa-validation-15883", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-16140", "mrqa_searchqa-validation-16335", "mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-1655", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-1741", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-1897", "mrqa_searchqa-validation-2116", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-2228", "mrqa_searchqa-validation-2392", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-3087", "mrqa_searchqa-validation-334", "mrqa_searchqa-validation-3347", "mrqa_searchqa-validation-3469", "mrqa_searchqa-validation-3496", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-3825", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-4023", "mrqa_searchqa-validation-4481", "mrqa_searchqa-validation-4512", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4808", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-5625", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5906", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-6736", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-6941", "mrqa_searchqa-validation-7139", "mrqa_searchqa-validation-7166", "mrqa_searchqa-validation-7440", "mrqa_searchqa-validation-746", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7753", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-8239", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8383", "mrqa_searchqa-validation-8459", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-861", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-8933", "mrqa_searchqa-validation-9119", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9942", "mrqa_squad-validation-10026", "mrqa_squad-validation-10227", "mrqa_squad-validation-112", "mrqa_squad-validation-1204", "mrqa_squad-validation-1454", "mrqa_squad-validation-1758", "mrqa_squad-validation-1759", "mrqa_squad-validation-2225", "mrqa_squad-validation-2365", "mrqa_squad-validation-2466", "mrqa_squad-validation-2784", "mrqa_squad-validation-3080", "mrqa_squad-validation-3110", "mrqa_squad-validation-3130", "mrqa_squad-validation-3581", "mrqa_squad-validation-3632", "mrqa_squad-validation-4259", "mrqa_squad-validation-457", "mrqa_squad-validation-4621", "mrqa_squad-validation-4770", "mrqa_squad-validation-5010", "mrqa_squad-validation-5651", "mrqa_squad-validation-5784", "mrqa_squad-validation-5913", "mrqa_squad-validation-6166", "mrqa_squad-validation-6694", "mrqa_squad-validation-6789", "mrqa_squad-validation-6947", "mrqa_squad-validation-7214", "mrqa_squad-validation-7269", "mrqa_squad-validation-7521", "mrqa_squad-validation-7547", "mrqa_squad-validation-7596", "mrqa_squad-validation-7848", "mrqa_squad-validation-8052", "mrqa_squad-validation-8151", "mrqa_squad-validation-8733", "mrqa_squad-validation-8830", "mrqa_squad-validation-9233", "mrqa_squad-validation-930", "mrqa_squad-validation-9311", "mrqa_squad-validation-962", "mrqa_squad-validation-9816", "mrqa_squad-validation-9859", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1216", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-1450", "mrqa_triviaqa-validation-1547", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-1923", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-2200", "mrqa_triviaqa-validation-2208", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3226", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3471", "mrqa_triviaqa-validation-3707", "mrqa_triviaqa-validation-3796", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-4660", "mrqa_triviaqa-validation-4737", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4876", "mrqa_triviaqa-validation-4890", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-5457", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5820", "mrqa_triviaqa-validation-5832", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6239", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6329", "mrqa_triviaqa-validation-642", "mrqa_triviaqa-validation-6540", "mrqa_triviaqa-validation-6636", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-6985", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7219", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7350", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-958"], "OKR": 0.716796875, "KG": 0.51015625, "before_eval_results": {"predictions": ["Mount Rainier, Washington", "Douglas Jackson", "Austral L\u00edneas A\u00e9reas", "2", "Craig William Macneill", "USS Essex", "8,648", "three", "Jeffrey Adam \"Duff\" Goldman", "Minnesota", "global peace", "Oregon State Beavers", "Arkansas", "the 2011 Pulitzer Prize in General Nonfiction", "Golden Gate National Recreation Area", "Pain Language", "Broadcasting House in London", "London Tipton", "Barney Miller", "Lily Hampton", "41st President of the United States", "Big Machine Records", "constant support from propaganda campaigns", "The Heirs", "Saturday Night Live", "strongly associated with Gaia and Cybele", "Tumi Holdings, Inc.", "Black Ravens", "Lifestyle cities", "Suspiria", "Silvia Navarro", "22,500 acres", "Warsaw, Poland", "Nelson County", "Kang", "25 million", "Cleopatra VII Philopator", "James G. Kiernan", "Fred &quot;Sonic&quot", "James City County", "Tunisian", "Linda Ronstadt", "the United Kingdom", "21st birthday", "the Americas and the entire South American temperate zone", "The Omega Man", "five", "Alex Turner, Bill Bailey, Pete Shelley, Paul Farley,", "The 2017\u201318 Premier League", "Kevin Spacey", "Stalybridge Celtic", "The term was first used in tennis", "Frank Zappa", "1991", "apples", "Fred Trueman", "Scotland", "Chesley \"Sully\" Sullenberger", "Fernando Torres", "Friday,", "Lifeboat", "a kilobyte", "stock-broker", "Benazir Bhutto"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6738047542735043}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.05128205128205128, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-1798", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-3197", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-2156", "mrqa_naturalquestions-validation-10325", "mrqa_triviaqa-validation-7095", "mrqa_searchqa-validation-572"], "SR": 0.578125, "CSR": 0.5328125, "EFR": 0.8518518518518519, "Overall": 0.6774016203703704}]}