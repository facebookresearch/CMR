{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8', diff_loss_weight=0, ewc_gamma=0.9, ewc_lambda=250.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4370, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["polynomial algebra", "Sydney", "Von Miller", "Kalenjin", "thymus and bone marrow", "Black's Law Dictionary", "building construction, heavy and civil engineering construction, and specialty trade contractors", "3.5 billion", "John Elway", "1798", "Albert Einstein", "monophyletic", "Xingu", "the New York Times", "white flight", "males", "uncivilized", "Greenland", "the equality of forces between two objects", "to encourage investment", "Grissom, White, and Chaffee", "N\u2013S", "Tolui", "18 million volumes", "since the Sui and Tang dynasties", "Ps. 31:5", "a new magma", "in an adult plant's apical meristems", "the Lisbon Treaty", "cysteine and methionine", "12", "17", "Honorary freemen", "Seine", "1072", "Academy of the Pavilion of the Star of Literature", "1969", "cilia", "main porch", "Denver Broncos", "electric eels", "San Jose Marriott", "Half of Paris's population of 100,000 people died. In Italy, the population of Florence was reduced from 110\u2013120 thousand inhabitants in 1338 down to 50 thousand in 1351", "Richard Leakey", "\u00dcberseering BV v Nordic Construction GmbH", "regeneration", "Wisconsin v. Yoder", "MODES", "1943", "Thomson", "1950s", "2010", "two", "Genghis Khan", "Serbian", "Warsaw", "Skylab", "The United Methodist Church", "Jerome Schurf", "a computer network funded by the U.S. National Science Foundation (NSF)", "14th to 17th centuries", "through homologous recombination", "University of Washington", "Falls"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9189338235294118}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4117647058823529, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8544", "mrqa_squad-validation-10333", "mrqa_squad-validation-6099", "mrqa_squad-validation-5065", "mrqa_squad-validation-7666", "mrqa_squad-validation-7049"], "SR": 0.90625, "CSR": 0.90625, "EFR": 1.0, "Overall": 0.953125}, {"timecode": 1, "before_eval_results": {"predictions": ["the poor", "higher", "9", "Canterbury", "Mediterranean", "Brown v. Board of Education of Topeka", "A turlough", "three other cricketers have taken more than one Test hat - trick", "Gladys Knight & the Pips", "In Time", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "The main television coverage of Celebrity Big Brother was screened on CBS during the winter of the 2017 -- 18 network television season", "Dawn ( Lizzy Greene ) is the oldest of the quadruplets", "W. Edwards Deming", "Langdon was more frequently seen on the small screen in guest spot roles such as Kitty Marsh during the NBC portion ( 1959 -- 1961 ) of Bachelor Father", "iron", "first message was sent over the ARPANET in 1969", "The Methodist revival began with a group of men, including John Wesley ( 1703 -- 1791 ) and his younger brother Charles ( 1707 -- 1788 )", "the show has run on BBC One since 15 May 2004, primarily on Saturday evenings with a following Sunday night results show", "Henry Gibson as Wilbur, a pig who was almost killed due to being a runt", "the provisional Indian Olympic Committee formally became the Indian Olympic Association ( IOA )", "iron ore production in Western Australia, and Australia as a whole, was negligible, in the range of less than 10 million tons a year", "multinational retail corporation", "Pakhangba", "the ancestral virus, of avian origin, crossed the species boundaries and infected humans as human H1N1", "A mama don't like you, and she likes everyone", "Speaker of the House of Representatives shall, upon his resignation as Speaker and as Representative in Congress, act as President", "It now plays a central role in the management of balance of payments difficulties and international financial crises", "by December 1349 conditions were returning to relative normalcy", "Sakshi Malik", "2017 Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "the Baltimore teenagers Ivan Ashford, Markel Steele, Cameron Brown, Tariq Al - Sabir and Avery Bargasse", "Jacob Packer is the Ripper", "Jerry Leiber and Mike Stoller", "TLC - All That", "muscle contraction", "It is the heaviest fully enclosed armoured fighting vehicle ever built", "1700 Cascadia earthquake", "April and Andy ask Leslie and Ben for advice regarding the prospect of having children", "Wednesday, May 24, 2017", "With an initial worldwide gross of over $1.84 billion, Titanic was the first film to reach the billion - dollar mark", "the algebraic rules for the elementary operations of arithmetic with such numbers", "Gustav Bauer, the head of the new government, sent a telegram stating his intention to sign the treaty if certain articles were withdrawn", "No. 23 retired by Chicago Bulls", "May 3, 2005", "Bieber's R&B vocals over a backdrop containing a dance infused beat", "Tami Lynn", "90 \u00b0 N 0 \u00b0 W", "A substitute good is one good that can be used instead of another", "1985", "the chant was first adopted by the university's science club in 1886", "It intertwines with the rete ridges of the epidermis and is composed of fine and loosely arranged collagen fibers", "J. S. Bach", "Christmas", "Westminster Abbey", "Liverpool", "Gracie Mansion", "2004 Paris Motor Show", "order and its tactics", "a rally", "dual nationality", "Burt's Bees is an \"Earth Friendly, Natural Personal Care Company.", "the Indiana Republican Party", "Beautiful Swimmers: Watermen, Crabs"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4559287094484462}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1, 0.4444444444444445, 1.0, 0.07999999999999999, 0.0, 0.2222222222222222, 0.12121212121212122, 0.18181818181818182, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 0.10526315789473684, 0.3636363636363636, 1.0, 0.25, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.10526315789473684, 0.0, 0.1904761904761905, 0.0, 1.0, 0.0, 1.0, 0.6, 0.2857142857142857, 1.0, 0.5, 0.09523809523809523, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.4, 1.0, 0.3636363636363636, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-3971", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6130", "mrqa_hotpotqa-validation-2137", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-4059", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-748"], "SR": 0.34375, "CSR": 0.625, "EFR": 0.9285714285714286, "Overall": 0.7767857142857143}, {"timecode": 2, "before_eval_results": {"predictions": ["Oxygen therapy", "Lou Diamond Phillips", "Fran", "North Holland", "International Hotel", "21 July 2015", "Porto of Portugal", "University of Texas at Austin", "Blake Shelton", "Coalwood, West Virginia", "G\u00e9rard Depardieu", "tailless", "Francophone and French", "Polish", "Jim Davis", "Lockhart", "1946", "34.9 kilometres", "South African", "Christopher Tin", "Los Angeles Dance Theater", "Anglo-Frisian", "South Africa", "clapping", "Macau", "Premier League", "Rule of three", "\"media for the 65.8 million", "2009", "Bhaktivedanta Manor", "Vyd\u016bnas", "47,818", "The Catcher in the Rye", "Fyvie Castle", "Gerard Marenghi", "Oracle Corporation", "Dennis Kux", "The Spiderwick Chronicles", "Lerotholi Polytechnic", "La Familia Michoacana", "Nikolai Alexandrovich Morozov", "Rural Electrification Act", "Golden Gate National Recreation Area", "July 22, 1946", "Thored", "John of Gaunt", "Treaty of Trianon", "Sturt", "Kind Hearts and Coronets", "Conservative Party", "2015 Monaco GP2 Series round", "Tilak Raj", "four volumes", "it is the oldest city in Highlands County, and was named after Stratford - upon - Avon, England", "the tallest building in the world", "Pearl Slaghoople", "Downton Abbey", "MS Columbus", "the skull", "Swedish Prime Minister Fredrik Reinfeldt", "Twenty three", "the total binding energy released in fission of an atomic nucleus", "a small measuring cup for... Imagine your college-era friends screaming: alcohol abuse", "\"Merchant's Harbor\""], "metric_results": {"EM": 0.46875, "QA-F1": 0.5661830357142856}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 0.0, 0.6666666666666666, 0.2, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-2307", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1462", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-10098", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-523", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-2045", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-11990"], "SR": 0.46875, "CSR": 0.5729166666666667, "EFR": 0.9705882352941176, "Overall": 0.7717524509803921}, {"timecode": 3, "before_eval_results": {"predictions": ["Frankfort", "in the waning days of the American Civil War", "Priscilla Presley", "B.B. King", "the Yolngu (Aboriginal) homelands", "wcralston", "trespassing", "King Menkaure (Mycerinus)", "the KKK", "Anja Prson", "John Sevier", "Max Planck", "Polar bear", "wildebeests", "1st February,1709", "in the Ozark Mountains of Oklahoma", "The British Earl of Yankerville", "St. Petersburg", "Poseidon", "'68", "Geraldine A. Ferraro", "the pharynx", "the Chesapeake", "games named for the skull", "Seoul", "Gustav", "Alger Hiss", "New Zealand's Minister of Economic Development", "Chrome & iron", "Jordin Sparks", "Ecuador", "\"The Gopher State\"", "admiral", "Baha Bariay", "Arroyo", "W. C. Fields", "pnctulis", "Fleetwood Mac", "white blood cells", "Meryl Streep", "March 1888", "is the bulging (or prolapse) of one or both of the mitral", "Marlon Brando", "Sierra Leone", "Wilbur Wright", "the Pelican", "littrature", "the 1897 Greco-Turkish War", "the tail", "Elaine Risley", "a young William Shakespeare", "Dougie MacLean", "Claims adjuster", "Geophysicists", "Columbus", "Omid Djalili", "red", "concentration camp", "79 AD", "1998", "2-0", "you can go from rags to riches", "the Kurdish militant group in Turkey", "Five Summer Stories"], "metric_results": {"EM": 0.453125, "QA-F1": 0.48124999999999996}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10393", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-11331", "mrqa_searchqa-validation-16135", "mrqa_searchqa-validation-7799", "mrqa_searchqa-validation-9678", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-3001", "mrqa_searchqa-validation-5662", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-12812", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-9842", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-2811", "mrqa_searchqa-validation-3798", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-6857", "mrqa_triviaqa-validation-96", "mrqa_hotpotqa-validation-3406", "mrqa_newsqa-validation-1508"], "SR": 0.453125, "CSR": 0.54296875, "EFR": 1.0, "Overall": 0.771484375}, {"timecode": 4, "before_eval_results": {"predictions": ["Woodstock", "Dartmouth College", "1789", "Rio de Janeiro", "a brain injury", "\"Big Dipper\"", "Siamese", "Ken Burns", "two", "Lady Gaga", "George I", "Milwaukee, Wisconsin", "aardvark", "Copenhagen", "George W. Bush", "George IV", "Aidan Crawley", "bitter liqueurs", "an isosceles", "Luke", "Tears for Fears", "Polish", "bauxite", "the Hamburgian Bach", "the Hooded Claw", "New Years Day", "1984", "tequila", "El Aneto", "Otto von Bismarck", "Mathematics", "Edwina Currie", "Wellington", "Pisces", "Montezuma", "\"Ana Brazil\"", "French Guiana", "Prime Minister of the Commonwealth of Australia", "France Polynesia", "East Berlin and West Berlin", "Burkina Faso", "Kiki the frog", "Daedalus", "Illinois", "Rosa Parks", "San Francisco", "Aeschylus", "\"Bellyhands\"", "Hair", "Harry Potter", "a company that provides Internet services", "China", "qualitative data, quantitative data", "those at the bottom of the economic government", "Sharmi Albrechtsen", "1992", "the ExCeL Exhibition Centre", "the Dalai Lama", "Hudson, Wisconsin", "(l-r)", "The Republic", "Megan Fox", "Leontyne Price", "bass"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5952132936507937}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-4899", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-415", "mrqa_triviaqa-validation-4566", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-2055", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-5215", "mrqa_hotpotqa-validation-1777", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-2133"], "SR": 0.53125, "CSR": 0.540625, "EFR": 0.9666666666666667, "Overall": 0.7536458333333333}, {"timecode": 5, "before_eval_results": {"predictions": ["revenge and karma", "functions", "triglycerides ( lipids )", "Cyanea capillata", "Roger Dean Stadium", "1936", "to help bring creative projects to life", "Billy Idol", "Cairo, Illinois", "Idaho", "October 12, 1979", "Speaker of the House of Representatives", "1600 BC", "what is to be done", "the person compelled to pay for reformist programs", "April 6, 1917", "Andy Serkis", "Narendra Modi", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "eurula", "Abid Ali Neemuchwala", "Steve Hale", "Brooklyn Heights fortifications", "2017 season", "nucleotides", "Ethiopia and Liberia", "it is an expression of at least a moderate amount of manual dexterity", "2018", "Vice President", "Killer Within", "balsam", "Javier Fern\u00e1ndez", "1959", "the ground", "2013", "games", "Major Molineux", "Cedric Alexander", "1985", "May 31, 2012", "Gareth Barry", "Udhampur - Srinagar - Baramulla", "Dirk Benedict", "an unknown recipient", "molar concentration", "Joseph Heller", "May 1979", "in the 1820s", "Javier Fern\u00e1ndez", "the septum", "1799", "Iwo Jima", "gums", "east", "the Harpe brothers", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "The Three Caesars' Alliance", "4 meters (13 feet) high", "Europe's second-tier club competition", "Pakistan", "tango", "j Jacob Marley", "jedoublen/jeopardy", "tennis"], "metric_results": {"EM": 0.5, "QA-F1": 0.5701405238819666}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0689655172413793, 0.5, 0.11764705882352942, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.25, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-7047", "mrqa_triviaqa-validation-4912", "mrqa_hotpotqa-validation-1704", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1173", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-12461"], "SR": 0.5, "CSR": 0.5338541666666667, "EFR": 1.0, "Overall": 0.7669270833333334}, {"timecode": 6, "before_eval_results": {"predictions": ["Arundel", "rounders", "Spain", "gooseberry", "horse racing", "the Vietnam war", "Australia", "The Free Dictionary", "1982", "Calvors", "three", "capicum", "Edward Lear", "the failure of the duke of Monmouth\u2019s rebellion", "Chief Inspector of Prisons", "a style of speech that is excessively impressionistic", "Peter Paul Rubens", "Gerber Technology", "tomatoes", "England's smallest ceremonial county", "Sneezy", "aurochs", "Athina Onassis", "\"Say You'll Be There,\"", "Robert F. Kennedy", "American West", "oregon CARLSON", "Armley", "guitar", "obridge", "Stockton-on-Trent", "equinoxes", "maqui berry", "Michael Faraday", "SuperiorPics.com", "a grayhound, gazelle hound or tazi", "American", "Blue Ivy", "John Nash", "Miranda v. Arizona", "Judas Iscariot", "The Big Bopper", "a downtown restaurant in New York\u2019s Greenwich Village.", "ollie Williams", "tamale", "The romantic story of the Highland garb", "1994", "Jules Verne", "Strabo", "Newcastle Falcons", "The Fortune cookie", "74", "it connects with the Arafura Sea through the Torres Strait", "Treaty of Paris between France and the Sixth Coalition, and the Treaty of Kiel that covered issues raised regarding Scandinavia", "Elizabeth Claire Kemper", "December 19, 1967", "Marco Fu", "$250,000 for Rivers' charity: God's Love We Deliver", "Zac Efron", "island stronghold of the Islamic militant group Abu Sayyaf", "a desktop microcomputer", "copper", "an earthquake", "12\u201318"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4796875}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-7569", "mrqa_triviaqa-validation-4942", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-211", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3349", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-3853", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-28", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4367", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-4384", "mrqa_newsqa-validation-3406", "mrqa_searchqa-validation-8172", "mrqa_hotpotqa-validation-244"], "SR": 0.421875, "CSR": 0.5178571428571428, "EFR": 1.0, "Overall": 0.7589285714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["french chef Auguste Escoffier", "Edward leeds", "nUT", "Churchill Downs", "Fotheringhay Castle in England", "low crime and high spirituality", "Looney Tunes", "Gal\u00e1pagos Islands", "bactrian", "Narendra Modi", "FBI", "z", "Maine", "vitamin C", "Vincent Gene", "Dilbert", "Lorenzo da Ponte", "hair", "Celtic", "Taggart", "Mexico", "Zia ul- Haq", "China", "Brian Close", "jumanji", "Rocky Horror", "wainwright's", "Holy Roman Empire", "maverick", "krak\u00f3w", "Venice", "aristotle", "Robert Devereux", "southern Rhodesia", "Fermanagh", "candelabrum", "Oklahoma City", "Fancy Dress Shop", "berlin", "iron", "Blackburn", "Antonia Fraser", "1938", "french connection", "huckleberry", "New Netherland Institute", "a homeopathic physician", "Rabin", "powerpoint", "jean", "chile juice", "Border Collie", "December 1, 2009", "60", "raven jean paltrow", "2006", "Ang Lee", "fighting charges of Nazi war crimes", "Michael Jackson", "Nineteen", "baroque", "ben Affleck", "robert chvez", "pop and R&B"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5197916666666667}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false], "QA-F1": [0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-1185", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-2078", "mrqa_triviaqa-validation-4954", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-6815", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-4132", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5412", "mrqa_newsqa-validation-2052", "mrqa_searchqa-validation-12069", "mrqa_searchqa-validation-11191", "mrqa_hotpotqa-validation-4299"], "SR": 0.453125, "CSR": 0.509765625, "EFR": 1.0, "Overall": 0.7548828125}, {"timecode": 8, "before_eval_results": {"predictions": ["Douglas MacArthur", "w.C. Handy", "Don Garlits", "elton jean", "belle", "belle", "seabiscuit", "Joan", "turbojet", "Weimar", "horse", "charlie conan", "godot", "l", "Sikhism", "blond", "George Frideric Handel", "Mendenhall Glacier", "french toast", "badger", "the Army of the Potomac", "salmon", "man o' Lantern", "phlegethon", "Alexander Graham Bell", "belle", "belle doyle", "belle throckmorton", "charlie conan", "Denali", "Mars", "", "a jigger", "rocky", "a rail", "Nero Wolfe", "Almond Joy", "charlie donofrio", "Rachel Carson", "stroke", "the American Kennel Club", "belle", "Yuri Gagarin", "Mary (Wollstonecraft) Shelley", "fiber", "Bastille", "Sam Houston", "", "Dick Tracy", "drone", "Ethiopia", "Pradyumna", "Bonhomme Carnaval", "April 29, 2009", "Robert Stroud", "moem Hunter", "David Beckham", "arthur the Tomboy", "1976", "German", "Tennessee", "fake his own death", "English", "watchmen"], "metric_results": {"EM": 0.484375, "QA-F1": 0.53125}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-5680", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-501", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-3759", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-3365", "mrqa_searchqa-validation-4714", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-14732", "mrqa_searchqa-validation-7439", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-8794", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-11757", "mrqa_searchqa-validation-7445", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-12805", "mrqa_naturalquestions-validation-5611", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-6521", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-4388", "mrqa_newsqa-validation-2689"], "SR": 0.484375, "CSR": 0.5069444444444444, "EFR": 1.0, "Overall": 0.7534722222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["anti- strike", "37", "greater quantities of oil and gas", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "allergic reaction to peanuts", "President Obama", "27 Awa", "Alexey Pajitnov", "a rising tiger", "2001", "stimulus package", "Egypt", "Casalesi Camorra clan", "the Pakistani Taliban's chief in Punjab", "Kuwait", "Monday", "KCNA", "the U.S. Holocaust Memorial Museum", "Brian David Mitchell", "105", "the strength of its brand name and the diversity of its product portfolio", "80", "Dennis Davern", "123 pounds of cocaine and 4.5 pounds of heroin", "club managers", "15,000", "a sexual assault on a child", "Turkish President Abdullah Gul", "a radical Muslim sheikh called Friday for the creation of an Islamic emirate in Gaza", "propofol", "Hillary Clinton", "a monthly allowance", "40", "Aryan Airlines Flight 1625", "Swansea Crown Court,", "Spanish", "Roma", "Tulsa, Oklahoma", "Sporting Lisbon", "Rima Fakih", "Wicked", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States", "Yemen", "three", "July 18, 1994", "Eric Besson", "Susan Atkins", "Mad Men", "July for A Country Christmas", "Arthur E. Morgan III", "Draquila -- Italy Trembles", "Dan Stevens", "Alex Skuby", "in a counterclockwise direction", "a person born within hearing distance of the sound of Bow bells", "jimmy hiraga", "Harnoncourt", "kahului, Hawaii", "9 February 1971", "Fatih Ozmen", "Kenny G", "Cuba Gooding Jr.", "jade", "Shakespearean Heroines"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4947036310728745}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.10526315789473684, 0.07692307692307693, 0.4, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.4, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-627", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4647", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5354", "mrqa_searchqa-validation-6241"], "SR": 0.4375, "CSR": 0.5, "EFR": 0.9722222222222222, "Overall": 0.7361111111111112}, {"timecode": 10, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-103", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1170", "mrqa_hotpotqa-validation-1354", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1462", "mrqa_hotpotqa-validation-1623", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1970", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2292", "mrqa_hotpotqa-validation-2307", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3083", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3406", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-949", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-115", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3525", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-171", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1954", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2064", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3057", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-862", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10805", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11331", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-11908", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13362", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14219", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14732", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-16135", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-16871", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2590", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2811", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-3001", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3428", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-3759", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4714", "mrqa_searchqa-validation-4823", "mrqa_searchqa-validation-501", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5332", "mrqa_searchqa-validation-5435", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-5680", "mrqa_searchqa-validation-6059", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6603", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7439", "mrqa_searchqa-validation-7445", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7799", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8853", "mrqa_searchqa-validation-8974", "mrqa_searchqa-validation-9842", "mrqa_squad-validation-10333", "mrqa_squad-validation-10378", "mrqa_squad-validation-1115", "mrqa_squad-validation-1155", "mrqa_squad-validation-1158", "mrqa_squad-validation-1975", "mrqa_squad-validation-2240", "mrqa_squad-validation-2550", "mrqa_squad-validation-2655", "mrqa_squad-validation-2832", "mrqa_squad-validation-317", "mrqa_squad-validation-3705", "mrqa_squad-validation-384", "mrqa_squad-validation-3913", "mrqa_squad-validation-3986", "mrqa_squad-validation-4140", "mrqa_squad-validation-4183", "mrqa_squad-validation-4239", "mrqa_squad-validation-4436", "mrqa_squad-validation-4484", "mrqa_squad-validation-4711", "mrqa_squad-validation-4953", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5095", "mrqa_squad-validation-5174", "mrqa_squad-validation-5311", "mrqa_squad-validation-5315", "mrqa_squad-validation-5503", "mrqa_squad-validation-6099", "mrqa_squad-validation-6763", "mrqa_squad-validation-679", "mrqa_squad-validation-6792", "mrqa_squad-validation-7021", "mrqa_squad-validation-7049", "mrqa_squad-validation-7060", "mrqa_squad-validation-7080", "mrqa_squad-validation-7257", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-7455", "mrqa_squad-validation-7552", "mrqa_squad-validation-7666", "mrqa_squad-validation-7903", "mrqa_squad-validation-7917", "mrqa_squad-validation-8064", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-8313", "mrqa_squad-validation-8544", "mrqa_squad-validation-8846", "mrqa_squad-validation-8954", "mrqa_squad-validation-9149", "mrqa_squad-validation-9782", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1122", "mrqa_triviaqa-validation-1185", "mrqa_triviaqa-validation-1323", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-1724", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2055", "mrqa_triviaqa-validation-2078", "mrqa_triviaqa-validation-211", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2893", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3349", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-3523", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3638", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3900", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-415", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4215", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-4487", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4560", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-4899", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4942", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-5224", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-5303", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-5712", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-633", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6815", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6852", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7084", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7525", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-905", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-995"], "OKR": 0.857421875, "KG": 0.46484375, "before_eval_results": {"predictions": ["24", "$60 billion", "Ike", "Stella McCartney", "Newark's Liberty International Airport,", "Obama", "seven", "At least 40", "pilot", "leftist", "Zuma", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place,", "Bayern Munich", "the same drama that pulls in the crowds", "Russia", "Depp", "two-day", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Sonia Sotomayor", "intricate Flemish tapestries in an east-facing sitting room called the Morning Room.", "Steven Chu,", "Two pages", "opposition party members.", "Joel \"Taz\" Di Gregorio", "Nico Rosberg", "Mandi Hamlin", "boats", "the insurgency", "Obama", "enjoys a cold shower in his home in New Zealand.", "Josef Fritzl", "a bag", "17", "a", "the \"race for the future...", "American Airlines", "40", "served in the military, yet they fought on opposing sides.", "Obama", "\"Twilight\"", "\"To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect.\"", "the fact that the teens were charged as adults.", "Leo Frank", "the Ku Klux Klan", "China", "Sarah", "\"perezagruzka\"", "the legitimacy of that race.", "Brian Thomas, who killed wife Christine while they were on vacation in 2008, be dropped due to a \"unique set of circumstances.\"", "a sort of robot living inside.", "Apple employees", "Elizabeth Dean Lail", "for control purposes", "HTTP / 1.1", "Eddie Murphy", "islands", "Belgium", "Bourbon County", "Pinellas", "\"Winnie the Pooh\"", "September 8, 2008", "Torah", "Hartford", "tissues in the vicinity of the nose"], "metric_results": {"EM": 0.359375, "QA-F1": 0.43411539968652035}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.13793103448275862, 0.0, 0.16666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.14545454545454542, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4140", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-674", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-887", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-1702", "mrqa_hotpotqa-validation-2818", "mrqa_hotpotqa-validation-2600", "mrqa_searchqa-validation-10607", "mrqa_searchqa-validation-4162"], "SR": 0.359375, "CSR": 0.48721590909090906, "EFR": 1.0, "Overall": 0.7068181818181818}, {"timecode": 11, "before_eval_results": {"predictions": ["Florida", "Kim Wilde", "the mafia", "Patrick Henry", "Venezianisches Gondellied", "Louis XV", "\"In God We Trust\"", "the Pavillon de Breteuil in S\u00e8vres, France", "Mormon", "Couscous", "plants", "Handel", "Stephenie Meyer", "Ralph Vaughan Williams", "Willie Nelson", "Ridley Scott", "The National Council for the Unmarried Mother", "Tom Hanks", "Sir Walter Scott", "a system of recording important things.", "Angelina Jolie", "United Republic of Tanzania", "Blue Ivy", "6 in F Major", "the solar system.", "Jupiter", "Genesis", "The Quatermass Experiment", "the Aureus", "Giorgio Armani", "Jack Ruby", "Rugby", "Model T", "Il Divo", "the narwhal", "Marsa", "a machine that cuts the bread finely", "Real Madrid", "\"Duke\"", "\"mild-mannered\"", "Fernando Lamas", "Silverstone", "McDonalds", "Longchamp", "Kansas City", "Harley Cobblepot", "Frenchman", "Mars", "Salt Lake City", "Jeroboam", "Henkel", "Giancarlo Stanton", "2011", "Houston Dynamo", "Rolling Stones", "mid-engine sports car", "Frank Lampard", "2011", "country", "\"The train ride up there is spectacular.", "The Great Paris", "Toni Morrison", "Star Wars: The clone Wars", "business."], "metric_results": {"EM": 0.515625, "QA-F1": 0.5844494047619048}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-4205", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-1608", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-7520", "mrqa_triviaqa-validation-2558", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-6682", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-2620", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-3376", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-178", "mrqa_searchqa-validation-9383"], "SR": 0.515625, "CSR": 0.48958333333333337, "EFR": 0.967741935483871, "Overall": 0.7008400537634409}, {"timecode": 12, "before_eval_results": {"predictions": ["Lady Gaga", "Amundsen", "sugarloaf", "Act II of Madama Butterfly", "Anita Roddick", "Gulf of Aden", "green tambourine", "Four Tops", "Till Death Us Do Part", "Mark Rothko", "humphrey Lyttelton", "Byker Grove", "Hampton Court Palace", "holly", "sound and light", "loadsamoney", "robert smith", "albatrosses", "phillies", "flannel", "kitunes", "lager", "billy crystal", "\"the meadows,\"", "Kenya", "power station", "phoenician", "Vince Cable", "Uncle Tom\u2019s Cabin", "Luxor", "Love Never Dies", "Adam Faith", "holly", "blood", "partridge", "serenade no. 7", "diffusion", "2", "cutters", "Beaujolais", "phillies", "George Fox", "denarii", "Sheffield", "maggie dent", "Devonport", "1920", "baulieu", "end", "Novak Djokovic", "omerta", "Orange Juice", "love of Neighbour", "Super Bowl XXXIX", "Oklahoma Sooners", "Capture of the Five Boroughs", "world", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "maggie phillies", "niko Savoie", "blood type", "knock on wood", "Calamity Jane", "Barcelona"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5750202186853002}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.17391304347826086, 0.0, 0.125, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4128", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-222", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-5773", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-2321", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-1946", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-1899", "mrqa_searchqa-validation-4281"], "SR": 0.53125, "CSR": 0.49278846153846156, "EFR": 1.0, "Overall": 0.7079326923076923}, {"timecode": 13, "before_eval_results": {"predictions": ["Duck", "Pink Floyd", "in the Chicago metropolitan area in 1982", "Doug Pruzan", "Nodar Kumaritashvili", "1881", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "letters about his life to an unknown recipient", "ase", "the Prince - Electors", "a place of trade, entertainment, and education", "Ben Willis", "notorious Welsh pirate Edward Kenway", "6 January 793", "Hellenism", "Gorakhpur Junction", "Jurriaen Aernoutsz", "Ptolemy", "transmissions", "in Poems : Series 1", "1931", "Michael Clarke Duncan", "East India Company", "InterContinental Hotels Group", "the Jos Plateau", "2018", "gravity", "IX", "Charles Perrault", "1926", "at birth", "Branford College", "Tim McGraw", "Orcrist", "A complex sentence", "Marty Robbins", "two", "Mexico", "season seven", "nominally a civil service post", "ninth", "2 %", "maquiladora", "1936", "$2 million", "state legislators of Assam", "Old Trafford", "Thomas Jefferson", "regulatory", "Fall 1998", "USS Chesapeake", "ozone", "naveh", "McDonnell Douglas", "Hermione Baddeley", "John Andr\u00e9", "Guardians of the Galaxy Vol.  2", "\"Up,\"", "walk on ice in Alaska.", "a passenger's name", "better safe than sorry", "k Kyrgyzstan", "submarine", "the insurgency"], "metric_results": {"EM": 0.453125, "QA-F1": 0.574372329059829}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true], "QA-F1": [0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3076923076923077, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-8460", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-2506", "mrqa_triviaqa-validation-1651", "mrqa_hotpotqa-validation-4345", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-826", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-13535"], "SR": 0.453125, "CSR": 0.4899553571428571, "EFR": 0.9714285714285714, "Overall": 0.7016517857142857}, {"timecode": 14, "before_eval_results": {"predictions": ["the efferent nerves that directly innervate muscles", "Marcus Atilius Regulus", "Omar Khayyam", "graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme,", "the Han", "an iron -- nickel alloy", "Giancarlo Stanton", "Acts passed by the Congress of the United States and its predecessor, the Continental Congress, that were either signed into law by the President or passed by Congress after a presidential veto.", "late January or early February", "when the forward reaction proceeds at the same rate as the reverse reaction", "Andy Serkis", "half - giant", "Masha Skorobogatov", "the source of the donor organ", "Human fertilization", "Alastair Cook", "the first four caliphs ( successors )", "a transformation change of heart", "members of the gay ( LGBT ) community", "18", "Robber Barons", "up to 100,000 write / erase cycles", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Los Angeles", "13 February", "J. Presper Eckert", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "if the concentration of a compound exceeds its solubility", "land, fresh water, air, rare earth metals and heavy metals", "Dadra and Nagar Haveli", "961", "on the continent of Antarctica", "January 2004", "Currington", "in classes with his friends since he's carrying all honors and college prep classes.", "Bon Jovi", "in the books of Exodus and Deuteronomy", "The Union", "Xiu Li Dai", "W. Edwards Deming", "the referee", "Marshall Sahlins", "Robyn", "Joel", "thick skin", "The federal government", "Terry O'Neill", "Muhammad Yunus", "a two - layer coat which is close and dense with a thick undercoat", "a habitat", "longer span than another", "Evening Prayer", "Richard Wagner", "the rent doesn't include additional costs such as insurance or business rates.", "the nuclear fission products", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "the Mayor of the City of New York", "1918-1919", "three", "Steven Gerrard", "the Rock and Roll Hall of Fame", "Marie Osmond", "jedoublen", "Hamas forces"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5422055069993394}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false], "QA-F1": [0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.06896551724137931, 0.5000000000000001, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.3333333333333333, 0.5454545454545454, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.35294117647058826, 0.923076923076923, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.12500000000000003, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3896", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-7440", "mrqa_triviaqa-validation-2506", "mrqa_hotpotqa-validation-3972", "mrqa_newsqa-validation-3356", "mrqa_searchqa-validation-13663", "mrqa_newsqa-validation-2732"], "SR": 0.453125, "CSR": 0.48750000000000004, "EFR": 1.0, "Overall": 0.7068749999999999}, {"timecode": 15, "before_eval_results": {"predictions": ["Eleanor Roosevelt", "a sidereal day", "Baltimore Steelers", "the ship's five anchors", "Italy", "Gang of Four", "the Capulets", "Viggo Mortensen", "Mason-Dixon Line Segment", "Green Party", "ark of acacia", "Qing", "CLOTHES-M INDED", "Little Red Riding Hood", "Grover Cleveland", "iguanas", "gershwin", "olly Ringwald", "bebop", "Nikolai Gogol", "Vietnam War", "oxygen", "Nashville", "Abraham Lincoln", "BRIGHTTON DEBUTS", "Canada", "savanna", "Tuscany", "Dada", "Ray Kroc", "Morticia Addams", "the signature song from the musical wicked,", "Virginia", "Dairy Queen", "New Jersey", "West Virginia", "Lewis and Clark", "John Glenn", "Transformers", "beans", "Norway", "Hawaii", "pound sterling", "Mario Puzo", "Wilkie Collins", "George C. Marshall", "three", "Nile", "Kermit Roosevelt", "Richie Rich", "Africa", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "2013", "1995", "nijinsky", "Thailand", "matricide", "Can't Be Tamed", "a schoolmaster", "Singapore", "Nechirvan Barzani", "allegedly involved in forged credit cards and identity theft", "At least 15", "Lorazepam"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5778318903318902}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.22727272727272727, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-9127", "mrqa_searchqa-validation-92", "mrqa_searchqa-validation-13101", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-8642", "mrqa_searchqa-validation-7306", "mrqa_searchqa-validation-1336", "mrqa_searchqa-validation-10600", "mrqa_searchqa-validation-15570", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-7198", "mrqa_searchqa-validation-14038", "mrqa_searchqa-validation-4546", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-14765", "mrqa_searchqa-validation-7874", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7554", "mrqa_triviaqa-validation-1555", "mrqa_hotpotqa-validation-1539", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3200", "mrqa_naturalquestions-validation-4463"], "SR": 0.46875, "CSR": 0.486328125, "EFR": 0.9705882352941176, "Overall": 0.7007582720588236}, {"timecode": 16, "before_eval_results": {"predictions": ["watermark", "the word at the top of... 11a was interesting", "the Monitor", "the Himalayan mountain view", "J.K. Rowling's", "The Thin Red Line", "Smith & Wesson", "the hussar busby", "Kindergarten Cop", "National French Fried Clam Day", "masks", "Jean-Paul", "Matt Dillon", "Anne Frank", "Endymion", "the masses", "a sandstorm", "Ben & Jerry", "1,000,000", "Just the Way You Are", "Abigail Breslin", "Avril Lavigne", "Robin Williams", "Hephaestus", "Arroyo", "(NUN)", "Hallmark Cards", "the redbird", "San Francisco", "Bloomingdale's", "gravitational field", "Lady Macbeth", "Indiana Jones", "(24 km)", "a colonel", "Frank Lloyd Wright", "Josephine (de Beauharnais)", "Jack Johnson", "cytokinesis", "Beau Bridges", "Tudor", "James Cook", "Crayola", "Neil Simon", "a 919mm Parabellum pistol", "San Francisco", "Princess Leia", "the lithosphere", "Gibraltar", "Vichy France", "ball-point pen", "The 111th edition of the World Series", "100", "Burbank, California", "Rugby School", "an acid phosphate", "a scaly material shed from the scalp, due to excessive or normal branny exfoliation of the epidermis.", "1989 until 1994", "the Golden Gate National Recreation Area", "Nayvadius DeMun Wilburn", "\"CNN Heroes: An All-Star Tribute\"", "Washington State's decommissioned Hanford nuclear site,", "Les Bleus", "(Annales of Chemistry and of Physics)"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5411172161172161}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.3076923076923077, 0.6666666666666666, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.8, 1.0, 0.0, 0.8333333333333334]}}, "before_error_ids": ["mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-14068", "mrqa_searchqa-validation-14048", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-13258", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-14646", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-4679", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-1464", "mrqa_searchqa-validation-3548", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-3201", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-8830", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-2160", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-956", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1229", "mrqa_hotpotqa-validation-4813"], "SR": 0.4375, "CSR": 0.4834558823529411, "EFR": 1.0, "Overall": 0.7060661764705882}, {"timecode": 17, "before_eval_results": {"predictions": ["James Brown", "October 29, 2015", "Lorenzo Lamas", "230 million kilometres ( 143,000,000 mi )", "sperm and ova", "the Philippines and Guam", "December 2, 2013, and the third season concluded on October 1, 2017", "Lionel Hardcastle", "at least 28", "Rockwell", "Lady Gaga", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law", "February 27, 2007", "South Carolina state militia threatened to seize Fort Sumter, an island fortification in Charleston harbor from the small U.S. Army garrison", "Atharvaveda and Taittiriya Samhita", "Lee Freedman", "13 June 1990", "John Young", "Brazil", "January 15, 2007", "Montevideo", "2012", "Australia", "the only way to go forward is to just keep living her life.", "December 25", "full - Frontal", "Games", "22 November 1970", "sex hormones", "Fix You", "a contemporary drama in a rural setting", "tectonic plates", "The team", "between the stomach and the large intestine", "the Director of National Intelligence", "Duisburg", "an explosion", "the U.S. Electoral College", "October 1941", "111", "Paracelsus", "Mohammad Reza Pahlavi", "rabbinic sources", "Peggy Lipton", "Afghanistan", "775", "Carol Worthington", "a region in Greek mythology", "Matt Monro", "1890s", "Iran", "Hibernian", "T.S. Eliot", "Tartar", "its eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music.", "Tel Aviv University", "Cumberland Gap", "Monday", "the North Korean regime intends to fire a missile toward Hawaii", "Missouri", "protons", "Mount Kilimanjaro", "the Himalaya mountains", "February 26, 1948"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5273482150010682}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.9090909090909091, 0.5, 0.0, 0.0, 0.0, 0.26666666666666666, 1.0, 1.0, 0.7837837837837839, 1.0, 0.08695652173913043, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.125, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.20000000000000004, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-9723", "mrqa_triviaqa-validation-4099", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-4624", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3300", "mrqa_searchqa-validation-16690", "mrqa_searchqa-validation-8548"], "SR": 0.4375, "CSR": 0.4809027777777778, "EFR": 0.9444444444444444, "Overall": 0.6944444444444444}, {"timecode": 18, "before_eval_results": {"predictions": ["somatic", "the human hands and face", "Tara", "the Munchkin maiden", "a federal republic", "3 lines of reflection", "Kyla Coleman", "1939", "Millennium Tower", "1876", "winter", "three high fantasy adventure films", "The fifth season of Chicago P.D.", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "the Finch family's African - American housekeeper", "the British rock group Coldplay", "Warren Hastings", "April 26, 2005", "12 November 2010", "Babe Ruth", "the season seven finale", "The Spanish brought the European tradition to Mexico, although there were similar traditions in Mesoamerica, such as the Aztecs'honoring the birthday of the god Huitzilopochtli in mid December", "the New Jersey Devils", "to collect menstrual flow", "early Land Rover Series which was launched at the Amsterdam Motor Show in April 1948", "cartilage", "Spektor", "East River", "a federal republic", "Yugoslavia was set up as a federation of six republics, with borders drawn along ethnic and historical lines : Bosnia and Herzegovina, Croatia, Macedonia, Montenegro, Serbia and Slovenia", "President of the United States", "an AL team", "cells", "October 14, 2017", "2010", "961", "Bidar", "an idiom for the most direct path between two points", "September 9, 2010", "30 October 1918", "2018", "in the napkin ), napkin, and flatware ( knives and spoons to the right of the central plate, and forks to the left )", "the spoiled, bedridden daughter of wealthy businessman James Cotterell ( Ed Begley )", "After World War II", "there are no repeated data values", "Alaska, Arizona, California, Colorado, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington and Wyoming", "sovereign states", "bicameral", "Lewis Carroll", "2018", "the government - owned Panama Canal Authority", "Microsoft", "Charlie Chaplin", "Michigan", "Gal Gadot", "Federal Minister for Transport, Innovation and Technology in the government of Chancellor Christian Kern", "Schutzstaffel", "the Obama chief of staff", "Gaylord Opryland", "a German citizen", "James Watt", "Casey Stengel", "sake", "B\u00e9la Bart\u00f3k"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5057060121088655}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false], "QA-F1": [0.4, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.5454545454545454, 0.8571428571428571, 0.125, 0.0, 1.0, 0.5, 1.0, 0.07407407407407407, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333334, 0.6666666666666666, 0.08695652173913042, 1.0, 0.3846153846153846, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-3884", "mrqa_hotpotqa-validation-3320", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-150", "mrqa_triviaqa-validation-5761"], "SR": 0.390625, "CSR": 0.4761513157894737, "EFR": 0.8974358974358975, "Overall": 0.6840924426450743}, {"timecode": 19, "before_eval_results": {"predictions": ["January 15, 2007", "up to 100,000", "Nancy Jean Cartwright", "60 by West All - Stars", "the Gold Rush", "birth", "July 25, 2017", "786 -- 802", "7 correct numbers", "A simple majority", "United Nations Peacekeeping Operations", "treats a specific ligand, a transmembrane domain, and an intracellular catalytic domain, which is able to bind and phosphorylate selected substrates", "the brain", "in each state's DMV, which is required to drive", "Confederate victory", "on the slopes of Mt. Hood in Oregon", "the inverted - drop - shaped icon that marks locations in Google Maps", "Rock Island, Illinois", "Quantitative psychological research", "March 2016", "Dan Bern and Mike Viola", "pneumonoultramicroscopicsilicovolcanoconiosis", "Laura Jane Haddock", "Casino promotions such as complimentary matchplay vouchers or 2 : 1 blackjack payouts", "May 18, 2010", "nearly 92 %", "May 1963", "Mirzapur", "A diastema ( plural diastemata )", "January 2018", "growing faster than the rate of economic growth", "London", "April 6, 1917", "Anglican", "Malina Weissman", "23 % of GDP", "the trunk", "Kaley Christine Cuoco", "June 27, 2008", "treats on 20 May 2011, Trinamool leader Mamata Banerjee is the current incumbent, the state's first woman chief minister", "Zhu Yuanzhang", "Chernobyl Nuclear Power Plant", "Sedimentary rock", "Woodrow Wilson", "the main type of cell found in lymph", "Johannes Gutenberg", "Humphrey Bogart, Ingrid Bergman, and Paul Henreid", "Muhammad's grandson Hussein ibn Ali", "Valmiki", "the chryselephantine statue of Athena Parthenos", "2004", "karkaroff", "Sparks", "George Eliot", "Coronation Street", "Tom Jones", "David Kossoff", "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown", "13", "a rabbit hole", "Lumiere", "Cars", "Conjunction Junction", "his deputy Thabo Mbeki"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6325582837301587}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.125, 0.0, 0.19999999999999998, 0.6666666666666666, 0.8750000000000001, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.125, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.4444444444444445, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-4930", "mrqa_triviaqa-validation-436", "mrqa_newsqa-validation-702", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-4349", "mrqa_triviaqa-validation-5245"], "SR": 0.53125, "CSR": 0.47890625, "EFR": 0.9666666666666667, "Overall": 0.6984895833333333}, {"timecode": 20, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1539", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2292", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-617", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-949", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-22", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2703", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-4966", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-5026", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6292", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-730", "mrqa_naturalquestions-validation-7374", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-8147", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-846", "mrqa_naturalquestions-validation-858", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9297", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-171", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4140", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-674", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-887", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10600", "mrqa_searchqa-validation-10811", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-11752", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-12812", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13101", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-1327", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14732", "mrqa_searchqa-validation-14765", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2071", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-3699", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3759", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-4187", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-46", "mrqa_searchqa-validation-4823", "mrqa_searchqa-validation-493", "mrqa_searchqa-validation-5197", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7198", "mrqa_searchqa-validation-7306", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-7631", "mrqa_searchqa-validation-8015", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-8642", "mrqa_searchqa-validation-8696", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9447", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-9842", "mrqa_searchqa-validation-996", "mrqa_squad-validation-2240", "mrqa_squad-validation-2550", "mrqa_squad-validation-2832", "mrqa_squad-validation-3", "mrqa_squad-validation-30", "mrqa_squad-validation-317", "mrqa_squad-validation-3781", "mrqa_squad-validation-3913", "mrqa_squad-validation-4059", "mrqa_squad-validation-4484", "mrqa_squad-validation-4752", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5174", "mrqa_squad-validation-5315", "mrqa_squad-validation-6792", "mrqa_squad-validation-7006", "mrqa_squad-validation-7257", "mrqa_squad-validation-7333", "mrqa_squad-validation-7903", "mrqa_squad-validation-7917", "mrqa_squad-validation-8157", "mrqa_squad-validation-8187", "mrqa_squad-validation-8224", "mrqa_squad-validation-8733", "mrqa_squad-validation-8846", "mrqa_squad-validation-9149", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1570", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1640", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-211", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2164", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3355", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-3884", "mrqa_triviaqa-validation-3900", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4739", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-5242", "mrqa_triviaqa-validation-5245", "mrqa_triviaqa-validation-5303", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6386", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6390", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-6902", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-953"], "OKR": 0.8203125, "KG": 0.46640625, "before_eval_results": {"predictions": ["Haggith", "Queen", "Shoeless", "Back to Bedlam", "Lady Chatterley's lover", "mako shark", "king Claudius", "al-Umari", "Louis C. Tiffany", "Dwight D. Eisenhower", "salt", "alopithecus afarensis", "Harvard", "Stephen Hawking", "The Grand Ole Opry", "Disney", "electors", "Poseidon", "pituitary apoplexy", "men and women", "Snickers", "a fish", "War and Peace", "traditionally", "Collage", "Boxing Day", "calendar", "hungarian rhyme lad, he's under the haystack fast # Quiz", "the manatee", "Alfred Hitchcock", "Blondie Bumstead", "ROYALTY", "A Midsummer Night's Dream Police", "Yalta Conference", "Jane Austen", "4", "Maravich", "Christopher Wren", "insulin", "Adam", "George Mason", "grease", "dingo", "The Sun Also Rises", "The Evil Dead", "pragmatism", "ovulation", "Hugh Laurie", "Helen of Troy", "Postbulletin", "Cleopatra VIII", "In Time", "Jackie Robinson", "Cozonac", "garbanzo", "Morgan Spurlock", "Baking Beauty  Copycat Big Mac", "1986", "\"50 best cities to live in.\"", "Melbourne Storm", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Arroyo and her husband", "her decades-long portrayal of Alice Horton", "Sammi Smith"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5315919757326008}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.375, 1.0, 0.923076923076923, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12958", "mrqa_searchqa-validation-939", "mrqa_searchqa-validation-16359", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-7413", "mrqa_searchqa-validation-3576", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-11357", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-11218", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-12403", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-10522", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-15081", "mrqa_searchqa-validation-7001", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-10762", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-7707", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2112", "mrqa_naturalquestions-validation-7651"], "SR": 0.421875, "CSR": 0.47619047619047616, "EFR": 1.0, "Overall": 0.6959412202380952}, {"timecode": 21, "before_eval_results": {"predictions": ["James Jeffords", "Germany", "Mississippi River", "A Clockwork Orange", "jockey", "Swan Lake", "Benito Mussolini", "South Korea", "Amazon River", "a pearl", "John Donne", "laryngitis", "Synchronicity", "Emma Stone", "Two Gentlemen of Verona", "Fairbanks", "(John) Locke", "Nancy Sinatra", "(Bill) Clinton", "Tupelo", "the French Legion of Honour", "Tumbler", "a bee", "Arethusa", "Planet of the Apes", "pajamas", "Kermit", "England", "polio", "Sweden", "(Ar Edgar Rice Burroughs", "Madagascar", "Tuna", "hock", "the electromagnetic spectrum", "The Big Easy", "Wobblies", "a magnetic compass", "Slovakia", "peanuts", "Joe Lieberman", "a sharpener", "Corcoran", "\"Vega$\"", "Midas", "at the top", "Lake Coeur d'Alene", "Council of Better", "Opossum", "a short circuit", "totalitarianism", "Brobee", "1987", "tomato pur\u00e9e generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Sam Cooke", "\"major science finding from the agency's ongoing exploration of Mars.\"", "Ankh-Morpork", "a suburb of Adelaide in the City of Port Adelaide Enfield", "Marilyn Martin", "\"The Process\"", "Picasso's muse and mistress, Marie-Therese Walter", "Kevin Kuranyi", "Hapag-Lloyd Cruises", "Belleville, Illinois"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6265354437229438}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9696969696969697, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-5321", "mrqa_searchqa-validation-4754", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-5552", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-8527", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-14957", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-4549", "mrqa_searchqa-validation-16591", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-3440", "mrqa_naturalquestions-validation-2945", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-6048", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-4021", "mrqa_newsqa-validation-637", "mrqa_hotpotqa-validation-2637"], "SR": 0.53125, "CSR": 0.47869318181818177, "EFR": 0.9666666666666667, "Overall": 0.6897750946969696}, {"timecode": 22, "before_eval_results": {"predictions": ["Marie Van Brittan Brown", "saecula saeculorum in Ephesians 3 : 21", "around 1872", "George Halas", "the focal point", "1997", "Identification of alternative plans / policies", "Del and Rodney", "1963", "Kaley Christine Cuoco", "a divergent tectonic plate boundary", "Spanish missionaries, ranchers and troops", "the pace car circles the track at pit road speed during the warm - up laps", "Toto", "The Lightning thief", "1977", "the coffee shop Monk's", "Johnny Logan", "Carroll O'Connor", "Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "span", "Christianity", "Billy Idol", "in a thousand years", "( Tavish Crowe )", "Veronica", "247.3 million", "the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "the raconteur ( Austin Winkler ) and his former lover ( Emmanuelle Chriqui )", "Robert Kirkman, Tony Moore, and Charlie Adlard", "Lana Del Rey", "meaning", "1992", "her mother and daughter characters", "the mid-1980s", "1871", "Paradise", "Partial", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen )", "the ACU", "Charles Sherrington", "October 30, 2017", "US $24,250", "Gunpei Yokoi", "the New York Yankees", "the inner core", "more like historical fiction than contemporary fantasy", "the 4th century", "Florida's Natural Growers", "before the first year begins", "Thorleif Haug", "role", "the blackcurrant liquor", "ghee", "Polish-Jewish", "\" Cleopatra\"", "EQT Plaza in Pittsburgh, Pennsylvania", "269,000", "January 24, 2006", "March 4", "pew", "the Bean Sidhe", "Our Country", "ice cream"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5685670141262247}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.631578947368421, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.33333333333333337, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.9714285714285714, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.5, 0.5454545454545454, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-3482", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-7477", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-2509", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4210", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-9572"], "SR": 0.4375, "CSR": 0.47690217391304346, "EFR": 0.9166666666666666, "Overall": 0.679416893115942}, {"timecode": 23, "before_eval_results": {"predictions": ["Boyd Gaming", "local South Australian and Australian produced content", "1919", "Ted Nugent", "1998", "Everbank Field", "Alex Song", "Chrysler", "Carlos Santana", "Jack Posobiec", "7 October 1978", "a basilica", "white", "1988", "2017", "Enkare Nairobi", "the Durban International Convention Centre (ICC Arena)", "between 11 or 13 and 18", "\"The Longest Yard\"", "Elijah Wood", "the Military Band of Hanover", "C. J. Cherryh", "Dame Harriet Walter", "The Golden Egg", "Ais", "Ben Ainslie", "Dutch", "a royal concubine against her will", "the Bank of China Tower", "the 28th season", "Barcelona", "Albert Park", "sim", "Pablo Escobar", "1941", "Hazel Keech", "DJ Scotch Egg", "June 24, 1935", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Chad", "Derry City F.C.", "Charles de Gaulle Airport", "the \"Anglo-Saxon Chronicle\"", "Friday", "boar-crested helm", "Bury St Edmunds, Suffolk, England", "The Lancia Rally", "Michael Jordan", "Harold Lipshitz", "nine", "the Morgaine Stories", "the southeastern coast of the Commonwealth of Virginia in the United States", "the defendant owed a duty to the deceased to take care", "( IIII ) and 9 ( VIIII )", "Downton Abbey", "Westminster Abbey", "Eminem", "28", "Saturday", "the Illinois Reform Commission", "aroucho", "a centiliter", "the Lexus louis", "arranged for the bodyguard to pick up $12,000 in cash from a bank to buy the guns"], "metric_results": {"EM": 0.421875, "QA-F1": 0.518671679197995}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.9473684210526316, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.38095238095238093]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-1699", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-1226", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2955", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-1195", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3629", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-5409", "mrqa_newsqa-validation-1242"], "SR": 0.421875, "CSR": 0.474609375, "EFR": 1.0, "Overall": 0.6956249999999999}, {"timecode": 24, "before_eval_results": {"predictions": ["New Years, and consummate their relationship near the end of the school year.", "Morgan Freeman", "The Bangladesh -- India border", "Richard Carpenter", "Idaho's Snake River Valley", "Nick Wilton", "A Turtle's Tale : Sammy's Adventures", "Nathan Hale", "Emma Watson", "The Chesapeake", "1983", "Yugoslavia", "George Harrison", "New England", "from 1922 to 1991", "total cost", "the nasal septum", "The player named on 75 % or more of all ballots cast", "During Super Saiyan 4 is brought about while in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "Stefanie Scott", "Mitch Murray", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "Paul", "Erica Rivera", "Labour", "in a Norwegian town circa 1879", "Thomas Edison", "1959", "at the intersection of Del Monte Blvd and Esplanade Street", "a pole", "Hendersonville, North Carolina", "aorta", "Atlanta, Georgia", "Sonu Nigam", "a flood defense system", "Donna Reed", "Total Drama World Tour", "Staci Keanan", "18th century", "Debbie Gibson", "Masha Skorobogatov", "House of Representatives", "The reservation in India comprises a series of action measures, such as reserving access to seats in the various legislatures, to government jobs, and to enrollment in higher educational institutions", "Richard Stallman", "Maya Rudolph", "Quantitative psychological research", "1603", "September 27, 2017", "Nepal", "BC Jean and Toby Gad", "six - hoop game", "Sir Robert Walpole", "Lee Harvey Oswald", "Mauritania", "guitar feedback", "John Robert Cocker", "Isla de Xativa", "January 24, 2006", "March 8", "terminal brain cancer.", "the Panama Canal", "mezcal", "Solidarity", "heads"], "metric_results": {"EM": 0.5, "QA-F1": 0.590749614968365}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.1904761904761905, 0.0, 1.0, 1.0, 0.7692307692307692, 0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 0.0, 0.13636363636363638, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-1831", "mrqa_triviaqa-validation-6158", "mrqa_hotpotqa-validation-4926", "mrqa_searchqa-validation-11851", "mrqa_triviaqa-validation-4719"], "SR": 0.5, "CSR": 0.47562499999999996, "EFR": 0.90625, "Overall": 0.677078125}, {"timecode": 25, "before_eval_results": {"predictions": ["one", "\"Rudolpho Valentino\"", "Crete", "Churchill Downs", "\"Un Giorno di Regno\"", "Robert Hooke", "1963", "\"Corse of Scotland\"", "sea shells", "the Daily Herald", "\"Trevor John Eve (born 1 July 1951)", "a zoom lens", "a mansard roof", "armada", "Moldova", "Florida", "\"Mighty Wurlitzer\"", "Switzerland", "The Hague Conventions of 1899 and 1907", "j.B.Priestley", "Utrecht", "Washington", "orange", "(Willem de Zwijger (William the Silent)", "Bunratty Castle", "Eddie Cochrane", "trumpet", "The Merchant of Venice", "\"Rivers of Blood\"", "the Temple of Artemis", "Lindisfarne", "a muffin", "The Undertones", "George Santayana", "Philistine", "Doctor Who", "\"Moshe Rabbeinu,\"", "Andrew Jackson", "Italy", "Bihari cuisine", "nose", "Sandi Toksvig", "Guinea", "Los Angeles", "luster", "Margaret Thatcher", "the Reform Club", "The Cathedral,", "pyrotechnics", "a gorilla", "polyhedrons", "937 total weeks", "Ed Sheeran", "over 800", "over 281", "Charles L. Clifford", "23 July 1989", "U.S. government", "Roger Federer", "U.S. State Department and British Foreign Office", "the Diners' Club Card", "\"The Space Primevals,\"", "Calcium", "punish participants in this week's bloody mutiny, which killed nearly 100 army officers and civilians,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5964015151515152}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.4, 1.0, 0.6363636363636364]}}, "before_error_ids": ["mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-2986", "mrqa_triviaqa-validation-2863", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-6100", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-7284", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-3174", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-5049", "mrqa_hotpotqa-validation-1149", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-636", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-12653", "mrqa_newsqa-validation-1619"], "SR": 0.53125, "CSR": 0.47776442307692313, "EFR": 1.0, "Overall": 0.6962560096153847}, {"timecode": 26, "before_eval_results": {"predictions": ["1,776", "Bemis Heights", "carbonated water", "Paradise, Nevada", "thirteen American colonies", "2017 season", "Charles Darwin", "Coordinates : 26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff / \ufebb\ufffd 26.617 \u00b0 N 81.", "Dr. Rajendra Prasad", "at the 1964 Republican National Convention in San Francisco, California", "Hakeem Olajuwon of Nigeria, Tim Duncan of the U.S. Virgin Islands, Steve Nash of Canada and Dirk Nowitzki of Germany", "Annette", "Total Drama World Tour", "more than 80", "Brenda", "Tony Orlando and Dawn", "Marie Fredriksson", "1830", "99.57 %", "approximately 11 %", "786 -- 802", "milling", "Andrew Gold", "Henry Purcell", "either two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "General George Washington", "2017 season", "`` Have I Told You Lately ''", "in Middlesex County, Province of Massachusetts Bay", "April 12, 2017", "either the Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee,", "The Ministry of Corporate Affairs", "House of Representatives", "Ohio and briefly attended the University of Pittsburgh before transferring to Mount Union, where he played defensive line.", "Abid Ali Neemuchwala", "seven", "2 pages", "digitization of social systems", "1975", "James Brown", "Kristy Swanson", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "around 10 : 30am", "1966", "Glen W. Dickson", "songs", "Manhattan", "various newspaper reporters, including Sylvia F. Porter", "John J. Flanagan", "Kimberlin Brown", "1955", "benedicti", "hierarchical web portal", "Scotland", "villanelle", "Bayern Munich", "Atlanta, Georgia", "Barbara Streisand", "Lindsey Vonn", "netherlands", "bacon", "cINEMA SHORTHAND", "Philippines", "a basilica"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6029621967944336}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7999999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 0.8, 1.0, 0.5384615384615384, 1.0, 0.8571428571428572, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-8582", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-1144", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-1384", "mrqa_searchqa-validation-14755", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-16051"], "SR": 0.515625, "CSR": 0.47916666666666663, "EFR": 0.9354838709677419, "Overall": 0.6836332325268817}, {"timecode": 27, "before_eval_results": {"predictions": ["Ogoki diversion", "scotland", "the Ronald Reagan Presidential Library", "Pe peanut butter", "Treasure Island", "Gloria Steinem", "Luxembourg", "curly", "\"7 and 7 Is\"", "\" Timothy\" Geithner", "oven", "volcanic cones", "scotland", "Adam Trask", "the kitchen sink", "arizonensis", "\"First-rate second-rate men\"", "dressage", "lipos", "A Series of Unfortunate Events", "a bus tour", "beautiful black and white", "Misery Chastain", "the Rolling Stones", "malaria", "Hamlet", "apples", "aortic valve", "caribou", "Jabberwocky", "Making the Band 3", "Virgin Atlantic Airways Limited", "Olympia", "Federalist Papers", "the Lone Ranger", "The Boeing Everett Factory", "The Ladies' Singles Trophy", "The Oscars Show - Seeing Stars", "nautilus", "Richelieu", "Gwalior", "in the southeastern Pacific Ocean,", "madden NFL 06", "Carrie Underwood", "their common enemy", "eustachian tube", "Florida State University", "hiccups", "anepidural injection", "Brigham Young", "U.S. Department of Transportation", "Secretary of State John M. Hay and Colombian Charg\u00e9 Dr. Tom\u00e1s Herr\u00e1n", "just after the Super Bowl", "16,801", "between Barcelona and the French border.", "\"The Greatest Generation\"", "Basel, Switzerland", "the 2007 Formula One season", "in San Francisco, California with offices in New York City and Atlanta", "The Hard Way", "The Glasgow, Scotland concert", "gang rape of a 15-year-old girl", "scardia", "cations"], "metric_results": {"EM": 0.34375, "QA-F1": 0.45930059523809524}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-8261", "mrqa_searchqa-validation-1867", "mrqa_searchqa-validation-2008", "mrqa_searchqa-validation-11353", "mrqa_searchqa-validation-13722", "mrqa_searchqa-validation-2490", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3723", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-7995", "mrqa_searchqa-validation-6022", "mrqa_searchqa-validation-6617", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-14352", "mrqa_searchqa-validation-16897", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-4526", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-11202", "mrqa_searchqa-validation-14499", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-692", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-3547", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-1918"], "SR": 0.34375, "CSR": 0.4743303571428571, "EFR": 1.0, "Overall": 0.6955691964285714}, {"timecode": 28, "before_eval_results": {"predictions": ["USA Today", "flytrap", "Dr Humbert", "Red Cross", "1919", "Ty Hardin", "gizzard", "denier", "March 19", "Republic of Biafra", "engine", "Colombia", "Lewis Carroll", "a robe", "Rutger Hauer", "the moon", "the Crystal Palace", "Scotland", "Milton Keynes", "Leadbetter", "a lie detector", "Italy", "Ivy", "Law & Order Criminal Intent", "Dundee", "the Colossus", "60 or more Points", "the President", "Hyperbole", "bone", "Manchester", "Delilah", "Bridge", "the Black Sea", "the Reform Club", "U.K.", "West Point", "The Rumble in the Jungle", "phosphorus", "worked", "ocho", "Ross Bagdasarian", "Massachusetts", "Mercedes-Benz", "Alexandria", "Wordsworth", "a bronze", "Joshua Tree National Park", "Trainspotting", "anutan", "the Black Death", "1967", "the Atlantic Ocean", "Tim Rice", "The Royal Albert Hall", "Philip K. Dick", "2006", "menstruation", "the American Civil Liberties Union", "(Dr. Albert Reiter,", "\"Little Boy\"", "Thurgood Marshall", "Gainsborough", "four"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6536458333333333}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-510", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2123", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-2051", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5893", "mrqa_triviaqa-validation-901", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-6976", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-2901", "mrqa_searchqa-validation-9687"], "SR": 0.609375, "CSR": 0.47898706896551724, "EFR": 0.96, "Overall": 0.6885005387931035}, {"timecode": 29, "before_eval_results": {"predictions": ["along a narrow stretch of road through Florida's Everglades", "lula da Silva", "three aid workers", "UNICEF", "in Hong Kong's Victoria Harbor", "Uighurs", "used car", "about 70,000", "Asashoryu", "President Bush of a failure of leadership at a critical moment in the nation's history", "last week", "Rawalpindi", "22", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Ashley \"A.J.\" Jewell", "Mikkel Kessler", "\"Itsy bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "in their Naples home.", "acute stress disorder", "nivose", "one of the most sought-after fugitive outside the country's rebel leaders.", "humbert humbert", "Africa", "Omar Bongo", "the Dutch National Police Services Agency", "Rwandan militia led by Kagame defeated the Hutu rebels and took control of the government.", "gig", "Dr. Jennifer Arnold and husband Bill Klein,", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "gasoline", "chadian president Idriss Deby", "At least 14", "on Anjuna beach in Goa", "Larry Ellison", "murder", "his injuries", "Barack Obama:", "garth Brooks", "gang rape", "\"Empire of the Sun,\"", "thabo mbeki", "\"They don't go through safety is the paramount concern,", "the Genocide Prevention Task Force", "\"private client\" list", "Ricardo Valles de la Rosa", "colonel in the Rwandan army", "\" Raidersers of the Lost Ark.\"", "bipartisan", "two", "one", "Kerstin and two of her brothers", "A footling breech", "During Hanna's recovery masquerade celebration", "planet Name Game", "vincent van Gogh", "fourteen", "\"In God we Trust\"", "Katherine Harris", "Port Clinton", "gueuze", "colossus humbert", "give love a bad name", "ivory", "Psy"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5672327263496427}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, true, true, true], "QA-F1": [0.2222222222222222, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.10526315789473684, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.7659574468085107, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.33333333333333337, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.7272727272727272, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-2105", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2946", "mrqa_newsqa-validation-2902", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-5631", "mrqa_triviaqa-validation-524", "mrqa_hotpotqa-validation-3487", "mrqa_searchqa-validation-16692"], "SR": 0.453125, "CSR": 0.478125, "EFR": 0.9714285714285714, "Overall": 0.6906138392857143}, {"timecode": 30, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1631", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3498", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-949", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-115", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2138", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-22", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5242", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7074", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-730", "mrqa_naturalquestions-validation-7374", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7802", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9297", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-674", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-925", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10522", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10805", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11189", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-11486", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12301", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13535", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-13722", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14499", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14755", "mrqa_searchqa-validation-14957", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15051", "mrqa_searchqa-validation-1509", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-15260", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16690", "mrqa_searchqa-validation-16692", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4349", "mrqa_searchqa-validation-493", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5218", "mrqa_searchqa-validation-5248", "mrqa_searchqa-validation-5409", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6022", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-6300", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7001", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8483", "mrqa_searchqa-validation-8527", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8707", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-8794", "mrqa_searchqa-validation-8974", "mrqa_searchqa-validation-9046", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-996", "mrqa_searchqa-validation-9992", "mrqa_squad-validation-2550", "mrqa_squad-validation-2832", "mrqa_squad-validation-317", "mrqa_squad-validation-3781", "mrqa_squad-validation-4059", "mrqa_squad-validation-4752", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5174", "mrqa_squad-validation-6537", "mrqa_squad-validation-679", "mrqa_squad-validation-6792", "mrqa_squad-validation-7257", "mrqa_squad-validation-7903", "mrqa_squad-validation-8157", "mrqa_squad-validation-8187", "mrqa_squad-validation-8224", "mrqa_squad-validation-8733", "mrqa_squad-validation-8954", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-1277", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1640", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1685", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-2123", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-2863", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-3068", "mrqa_triviaqa-validation-3138", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3355", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4253", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-436", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4684", "mrqa_triviaqa-validation-4719", "mrqa_triviaqa-validation-4739", "mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-5242", "mrqa_triviaqa-validation-5245", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5712", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-5770", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6390", "mrqa_triviaqa-validation-642", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-6902", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7061", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-953"], "OKR": 0.80078125, "KG": 0.43046875, "before_eval_results": {"predictions": ["4", "saying Chaudhary's death was warning to management.", "The Casalesi Camorra clan", "\"Dancing With the Stars\"", "\"The voice of change,\"", "Lance Cpl. Maria Lauterbach", "Daryeel Bulasho Guud", "an angel", "Metro transit trains that crashed the day before, killing nine", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "boyhood experience in a World War II internment camp", "Bodyguard Trevor Rees,", "to the southern city of Naples", "\"Taz\" DiGregorio,", "Iran's parliament speaker", "14", "Gary Player,", "the Dutch patent office", "The man ran out of bullets and blew himself up.", "a member of the Sodra nongovernmental organization,", "246", "about 3,000 kilometers (1,900 miles)", "five", "Chile", "\"scared I won't be able to go home.", "outside influences in next month's run-off election,", "1981", "Susan Boyle", "forgery and flying without a valid license", "the way their business books were being handled.", "jazz", "Janet and La Toya", "100 percent", "Ashley \"A.J.\" Jewell", "well over 1,000 pounds", "\"iKini\"", "Little Rock Central High School", "to focus on the space between two thoughts, because it prevents me from getting lost,\"", "a birdie four at the last hole", "12-1", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Chinese President Hu Jintao", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "four", "Arizona's bill orders immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "Roger Federer", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials,", "Zoe's Ark", "\"wider relationship\"", "a mode of transport, flying is regarded as being particularly polluting because of the amount of fuel used at high altitude.", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "Hold On", "Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida", "special economic zones", "Zsa Zsa Gabor", "riyadh", "benedictus", "River Clyde", "on the north bank of the North Esk,", "Debbie Harry", "the Indians", "Fort Sam Houston", "\"RUBAI'I\"", "an open field northwest of Bemis Heights"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5587513160858749}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.16666666666666666, 0.0, 0.8, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.23529411764705882, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0606060606060606, 0.0, 0.0, 0.07692307692307693, 0.14285714285714288, 1.0, 0.9600000000000001, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.7058823529411764]}}, "before_error_ids": ["mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3188", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3706", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-921", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-392", "mrqa_naturalquestions-validation-6670", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-2324", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-5435", "mrqa_searchqa-validation-1592", "mrqa_naturalquestions-validation-6461"], "SR": 0.40625, "CSR": 0.47580645161290325, "EFR": 0.9210526315789473, "Overall": 0.6603874416383702}, {"timecode": 31, "before_eval_results": {"predictions": ["Lonnie", "Cpl. Richard Findley,", "saving and planning for retirement long before his career neared its end.", "the District of Columbia National Guard", "\"He wanted to kill all of us,\"", "Gavin de Becker", "Brian Smith", "well over 1,000 pounds", "31 meters (102 feet) long and 15 meters (49 feet) wide", "3,000 kilometers (1,900 miles),", "China, Taiwan, Hong Kong and Mongolia", "monarchy", "Gary Player", "Mohamed Mohamud Qeyre", "Kurdistan Freedom Falcons,", "30", "the United States can learn much from Turkey's many complex issues and identities, but also handled tough issues with great skill.", "All three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "Sharp-witted. Direct. In control. Loyal.", "UNICEF", "Little Rock military recruiting center", "the Dalai Lama", "Susan Atkins,", "two", "a massive electrical problem", "North Korea intends to launch a long-range missile in the near future,", "CEO of an engineering and construction company with a vast personal fortune.", "'We want to reset our relationship and so we will do it together.'\"", "\"Five of us for the United States and two against us because they were stranded in Japan\"", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "India", "at least 25", "misdemeanor assault", "19-year-old", "the \" Michoacan Family,\"", "$5.5 billion to build", "near the Somali coast", "Russia and China", "his comments", "Nearly eight in 10", "jobs up and down the auto supply chain: from dealers to assembly workers and parts markers.", "she's in love,", "orange", "Philippines", "Italy", "the refusal or inability to \"turn it off\"", "Dan Brown", "mental health and recovery.", "the IAAF", "\"Dancing With the Stars\"", "1940's", "after losing in the Finals in 1969 and 1970", "Acid rain", "Dunedin, Port Chalmers and on the Otago Peninsula, Saint Bathans in Central Otago and at the Cape Campbell Lighthouse in Marlborough", "Pandore", "Paul Cezanne", "Gandalf", "three", "Mazda's", "Mountain West Conference", "the end of the War of 1812", "phylum Arthropoda", "Paul Gauguin", "St. George"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5439031862745098}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.1, 0.888888888888889, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.058823529411764705, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.7499999999999999, 0.0, 0.11764705882352941, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-990", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-3401", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2738", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-1304", "mrqa_triviaqa-validation-3723", "mrqa_triviaqa-validation-2273", "mrqa_hotpotqa-validation-4710", "mrqa_searchqa-validation-10219", "mrqa_searchqa-validation-5260", "mrqa_searchqa-validation-5038"], "SR": 0.40625, "CSR": 0.4736328125, "EFR": 1.0, "Overall": 0.6757421875}, {"timecode": 32, "before_eval_results": {"predictions": ["more than 78,000 parents of children ages 3 to 17 at 110 per 10,000 -- slightly more than 1 percent.", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating.", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "the club's board", "in 1971,", "Kim Jong Il's", "for his efforts to help male veterans struggling with homelessness and addiction.", "will not support the Stop Online Piracy Act,", "Nashville suburb of Franklin, Tennessee,", "Larry Zeiger", "four", "consumer confidence", "Queen Elizabeth's birthday", "United States, NATO member states, Russia and India", "for an independent homeland since 1983.", "The show allows 10 boys and 10 girls between the age of eight and 11 to create their own mini-societies, organizing everything from what they eat to how they should entertain themselves.", "the insurgency,", "The United States", "the two-state solution", "Stanford University", "Jeanne Tripplehorn", "Dennis Davern,", "breast cancer", "it is provocative action.", "\"We have duty to keep cases under continuous review, and following expert evidence from a psychiatrist it was suggested no useful purpose would be served by Mr Thomas being detained and treated in a psychiatric hospital,\"", "More than 150,000", "finance", "The son of", "the FARC had targeted the Awa because the Indians don't want to get involved in the armed struggle and refuse to reveal information on government troop actions.", "Washington State's", "11", "the banned substance cortisone.", "D-Day on June 6, 1944,", "the program exists for peaceful purposes, but the United States and other Western nations are concerned by Iran's refusal to halt uranium enrichment activities.", "Juliet", "NATO's Membership Action Plan, or MAP,", "a certain carrier based in Texas.", "for strategy, plans and policy on the Army staff.", "Indian Army", "Obama", "supplies power to almost 9 million Americans,", "1913,", "Turkey", "the game", "small carmaker from Anderson, Indiana, to showcase its IDEA, a new, 100-mpg plug-in hybrid electric vehicle that it hopes to market for government and commercial fleets.", "Myanmar's military rulers", "motor motorcycle accident.", "expanded legal protections", "U.S. President-elect Barack Obama", "4, the highest ever position", "The stability, security, and predictability of British law and government", "Matt Jones", "Costa Rica, Brazil, and the Philippines", "Albert Finney", "Kosovo", "the Violin", "Cyclic Defrost", "Peter O'Toole, Eli Wallach and Hugh Griffith.", "Illinois", "possessed Man: Fyodor Dostoevsky", "Blackbird", "General Hospital", "the final episode of the series"], "metric_results": {"EM": 0.328125, "QA-F1": 0.46458032708032704}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, true], "QA-F1": [0.5185185185185185, 0.4, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407408, 0.6666666666666666, 0.0, 0.5, 0.7499999999999999, 0.08333333333333334, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-2655", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-372", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-2167", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-2740", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-7398", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-5856", "mrqa_searchqa-validation-4039"], "SR": 0.328125, "CSR": 0.46922348484848486, "EFR": 1.0, "Overall": 0.674860321969697}, {"timecode": 33, "before_eval_results": {"predictions": ["northern Israel", "Garth Brooks", "off the coast of Dubai", "how it will proceed.", "a long-range missile", "a \"new chapter\" of improved governance in Afghanistan", "he fears a desperate country with a potential power vacuum that could lash out.", "Japan", "The United Nations", "133 people", "the Southern Baptist Convention,", "kept the details on both the timing and selection of the running mate under wraps.", "lightning strikes", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "August 4, 2000", "Wednesday.", "Marc Jacobs", "funded by a German company and affiliated with the group Bread for the World.", "Bangladesh", "Haleigh Cummings,", "\"Zionist\" and \"a zealous supporter of the usurper entity, and a prominent member of a number of Zionist hate organizations,\"", "Sabina Guzzanti", "the Southeast,", "137", "Alexandros Grigoropoulos,", "the insurgency,", "end her trip in Crawford and hoped to arrive on September 15 after hitting the road from the White House in July.", "Elizabeth Birnbaum", "heavy flannel or wool", "school,", "1913,", "Portuguese water dog", "Iran", "2011.", "Misty Cummings,", "took an obscure story of flowers and turned it into the masterful 1998 best-seller", "Thailand", "glamorous, sexy and international.", "September,", "an acid attack", "\"People have lost their homes, their jobs, their hope,\"", "a public-television show", "The oceans", "August 19, 2007.", "Haiti", "March 24,", "in his 60s,", "1616.", "Lee Myung-Bak", "Ralph Lauren", "about 5:20 p.m. at Terminal C", "~ 0.058 - 0.072 mm", "In the original Star Wars film in 1977", "the late 16th century", "Gregory v. Helvering,", "Liechtenstein", "the Coalition of the Radical Left (Syriza)", "Broad Top Township, Bedford County", "Giacomo Puccini", "whale lice.", "Denmark", "Newfoundland", "Blue Hawaii", "Kirk Douglas"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6016089795961119}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-3689", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3480", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-6881", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-5738", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-1332", "mrqa_searchqa-validation-5234"], "SR": 0.578125, "CSR": 0.4724264705882353, "EFR": 0.9629629629629629, "Overall": 0.6680935117102397}, {"timecode": 34, "before_eval_results": {"predictions": ["$22 million", "his native Philippines", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "way to spread the love", "the player", "L'Aquila earthquake,", "young girl being sexually assaulted.", "at least two and a half hours.", "A Brazilian supreme court judge on Tuesday", "Wednesday at home in Stanford Alto, California,", "40 lash after he was convicted of drinking alcohol in Sudan where he plays for first division side Al-Merreikh of Omdurman.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential", "Fullerton, California,", "Sarah,", "one American diplomat to a \"prostitute\" and threatening to oust another from his country.", "Hillary Clinton", "\"They were nothing,\"", "United States", "the Rockies", "the leader of a drug cartel", "way their business books were being handled.", "state senators", "4.6 million", "New York Philharmonic Orchestra in North Korea to Dharamsala, India.", "Haiti", "Austin Wuennenberg,", "learn in safer surroundings.", "a reward car.", "$14.1 million.", "Christmas", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Nineteen", "the Brundell family in Deene Park, England,", "the Beatles", "Iran's", "President Paul Biya,", "a 12-year veteran of the Utah state police,", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "fight back against Israel in Gaza.", "Six", "the first five Potter films", "Sunday's", "\"The Kirchners have been weakened by this latest economic crisis,\"", "Haiti.", "80,", "her home for 12 of the past 18 years.", "can play an important role in Afghanistan as a reliable NATO ally. The question is: How can", "Authorities in Fayetteville, North Carolina,", "\"We Found Love\"", "Bastian Schweinsteiger", "British Prime Minister Gordon Brown's", "end aggressive militarism", "Since 1979 / 80", "between $10,000 and $30,000", "Maine", "cars", "sylvesley Lawson (n\u00e9e Hornby; born 19 September 1949),", "Oklahoma", "\"Aloha \u02bbOe\"", "Boyd Gaming.", "support", "NYC", "the Pacific Ocean", "Mississippi"], "metric_results": {"EM": 0.375, "QA-F1": 0.5039651553474493}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.5, 0.4210526315789474, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 0.2, 0.08695652173913042, 1.0, 1.0, 1.0, 0.5263157894736842, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.09523809523809525, 0.6666666666666666, 1.0, 0.36363636363636365, 0.15384615384615383, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 1.0, 0.32, 0.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2143", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-4768", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-3347", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-3679", "mrqa_searchqa-validation-6092", "mrqa_searchqa-validation-14603"], "SR": 0.375, "CSR": 0.46964285714285714, "EFR": 0.975, "Overall": 0.6699441964285715}, {"timecode": 35, "before_eval_results": {"predictions": ["Partial weight - bearing", "in the season - five premiere episode `` Second Opinion ''", "Database - Protocol driver", "the governor of West Virginia", "Charles Lebrun", "cella", "John Bull", "Rip Van Winkle", "January 1923", "Noel Kahn", "7000301604928199000 \u2660 3.016 049 281 99 ( 23 ) u )", "James Earl Jones", "biblical", "Thomas Lennon", "Robber Barons", "in people and animals", "10.5 %", "1996", "Mike Leeson and Peter Vale", "collect menstrual flow", "Leo Arnaud", "President pro tempore of the Senate", "September 2017", "since the early 20th century", "Latin liberalia studia", "Charles Path\u00e9", "Mel Tillis", "Florida", "four", "O'Meara", "demonstrations", "November 17, 2017", "Bobby Darin", "climate on the Earth", "Ben Findon, Mike Myers and Bob Puzey", "Donny Osmond", "system of state ownership of the means of production", "Napoleon Bonaparte", "distribution and determinants of health and disease conditions in defined populations", "the nerves and ganglia outside the brain and spinal cord", "hero of Tippecanoe", "24 November 1949", "Martin Lawrence", "In 1984", "September 29, 2017", "1976", "at", "legally neutral", "Ali Daei", "copper ( Cu )", "Colon Street", "Sir Francis Dashwood", "shoji screen", "peppers", "Sunday, November 2, 2003,", "Citizens for a Sound Economy", "Harry Robbins \"Bob\" Haldeman", "Sonia Sotomayor,", "between Israel and Hezbollah.", "Maersk Alabama", "Hawaii", "Derek Jeter", "sheep", "Lou and Wilson"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5872523743617494}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.923076923076923, 0.6666666666666666, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.8571428571428571, 1.0, 0.20000000000000004, 0.4444444444444445, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.4, 1.0, 0.625, 1.0, 0.42857142857142855, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.06060606060606061, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2818", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-9091", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-4064", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-1144", "mrqa_naturalquestions-validation-31", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-934", "mrqa_triviaqa-validation-3795", "mrqa_hotpotqa-validation-3489", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-16578", "mrqa_newsqa-validation-3307"], "SR": 0.421875, "CSR": 0.4683159722222222, "EFR": 1.0, "Overall": 0.6746788194444445}, {"timecode": 36, "before_eval_results": {"predictions": ["that they'd get to bring a new puppy with them to the White House in January.", "striker Tevez remained on the bench despite a rousing reception when he went on a touchline warm-up during the game.", "he'd begin sending the additional troops \"at the fastest pace possible\" starting in early 2010 \"with a goal of starting to withdraw forces from the country in July 2011.", "a Yemeni cleric", "Adriano", "central Cairo,", "300", "the club -- which he called \"very diverse\" -- invited camps in the Philadelphia area to use his facility because of the number of pools in the region closed due to budget cuts this summer.", "Red River", "Bialek", "Switzerland,", "Haiti", "more than 700 guests each year in George and Martha's time.", "Kris Allen,", "Akshay Kumar", "her experience and became a passionate advocate for early detection and helping other women cope with the disease.", "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.", "Joan Rivers", "Goa", "Robert Kimmitt.", "David Beckham", "1918-1919.", "Jim Anderson,", "Amstetten,", "the Internet", "137", "between 1917 and 1924 when he was in his late 30s and early 40s.", "John Lennon and George Harrison,", "Haeftling", "Christiane Amanpour", "the Muslim festival of Eid al-Adha.", "Alfredo Astiz,", "folding table", "beetles.", "upper respiratory infection.", "Bob Bogle,", "\"Beats digging ditches.\"", "rally at the State House", "Williams' body", "Al-Shabaab", "Dr. Maria Siemionow,", "Pope Benedict XVI", "the green grump", "ALS6,", "the FDA warned nine companies to stop manufacturing 14 unapproved narcotics that are widely used to treat pain.", "the allegations of MMS corruption \"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "\"She was focused so much on learning that she didn't notice,\"", "Her husband and attorney, James Whitehouse,", "heavy brush,", "frees up a place", "Sri Lanka,", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "Sandy Knox and Billy Stritch", "1961", "Leeds", "Backgammon", "Chongqing", "Paige O'Hara", "Champion Jockey", "Arthur Miller", "Sinclair Lewis", "fish.", "sand", "a muzzle-loading high-angle gun with a short barrel that fires shells at high elevations for a short range (syn: howitzer)"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6951299933514089}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false], "QA-F1": [0.962962962962963, 0.0, 0.11428571428571428, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.72, 0.3157894736842105, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.12121212121212122, 0.06896551724137931, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1263", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-3828", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2215", "mrqa_newsqa-validation-2293", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2895", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-3672", "mrqa_triviaqa-validation-206", "mrqa_hotpotqa-validation-206", "mrqa_searchqa-validation-7508"], "SR": 0.578125, "CSR": 0.47128378378378377, "EFR": 0.9629629629629629, "Overall": 0.6678649743493493}, {"timecode": 37, "before_eval_results": {"predictions": ["space shuttle Discovery", "Martin Aloysius Culhane", "five female pastors", "three-time road race world champion,", "the river will crest Saturday about 20 feet above flood stage.", "eight", "Tutsis the privileged ethnicity,", "2,000 euros", "outside the Iranian consulate in Peshawar on Wednesday.", "India", "Bob Dole", "terrorism.", "Victor Manuel Mejia Munera was a drug lord with ties to paramilitary groups,", "since 1983.", "Zuma", "to hold onto his land", "the Muslim community around the world the message we have been waiting for.", "Sunday", "the Somali coast", "Chinese", "southern city of Naples", "38", "meeting with the president to discuss her son.", "Marc Jacobs", "civilians,", "a crew of Grayback forest-firefighters walk up the sides of what most people would consider a chainsaw or the proverbial cigarette butt out the car window can set the entire region ablaze.", "Gen. Stanley McChrystal,", "grossed $55.7 million during its first frame,", "eteen", "jazz", "racial intolerance.", "he will represent himself.", "Felipe Calderon", "Wednesday", "a full facial transplant", "collaborating with the Colombian government,", "three", "just 58 minutes.", "Scarlett Keeling", "\u00a320 million ($41.1 million) fortune", "passengers on the Miva Marmara", "the man facing up, with his arms out to the side.", "George Washington", "be silent.", "Nigeria", "whether he should be charged with a crime,", "Maj. Nidal Malik Hasan,", "Manmohan Singh's", "Rany Freeman,", "last week", "closed on 366 for eight wickets on the opening day.", "Julia Ormond", "about 6 : 00 p.m.", "all transmissions", "Chicago", "Stephen King", "Paris", "the Canadian Football League (CFL)", "Miriam Margolyes", "Messiah Part II", "1.80652,", "Billy Pilgrim", "the Boers", "I Love You"], "metric_results": {"EM": 0.5, "QA-F1": 0.5809415932700993}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.5, 0.0, 0.8, 0.25, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.04878048780487805, 0.0, 1.0, 0.0, 1.0, 1.0, 0.14814814814814817, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-2898", "mrqa_naturalquestions-validation-3373", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4382", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-2921", "mrqa_searchqa-validation-6687", "mrqa_searchqa-validation-4318"], "SR": 0.5, "CSR": 0.4720394736842105, "EFR": 1.0, "Overall": 0.6754235197368421}, {"timecode": 38, "before_eval_results": {"predictions": ["acid", "11.4 million orphans", "Bryant Purvis", "Kurdistan Freedom Falcons, known as TAK,", "A portrait", "Halloween", "700", "the coalition seeks to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "criminals", "Jenny Sanford,", "near Fort Bragg in North Carolina.", "Manchester United.", "former U.S. secretary of state.", "helped nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "$3 billion", "Larry Ellison,", "the National September 11 Memorial Museum", "Kim Il Sung", "Brazil began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "Gary Brooker", "a baseball bat", "closing these racial gaps.", "Bollywood", "up to $50,000", "bankruptcies", "A witness", "Gary Coleman", "fifth season", "forgery", "fake his own death by crashing his private plane into a Florida swamp.", "to secure more funds from the region.", "Fukuoka,", "1994", "Rima Fakih", "a bucket truck used for repairing power lines to something resembling an enclosed golf cart", "Roy Foster's", "Ryan Adams.", "41,", "the immorality of these deviant young men", "an \"independent jurist\" with a \" sharp and agile mind\"", "the Zetas and Gulf cartel", "gun", "President Obama and Britain's Prince Charles", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "3,000 kilometers (1,900 miles),", "Oxbow,", "pirates", "\"There is a tiny sliver of good news -- the number of Americans who think things are going very badly has dropped from 40 percent in December to 32 percent now,\"", "Caster Semenya", "reducing greenhouse gas emissions.", "\"A Whiter Shade of Pale\"", "north", "Randy", "Jonathan Breck", "Achille Lauro", "Rio de Janeiro", "Jane Austen", "Loch Shiel", "Kennedy John Victor", "Kentucky, Virginia, and Tennessee", "foxes", "the Spanish-American War", "he skipped a vote honoring 28 soldiers who gave their lives in defense of our...", "Joe DiMaggio"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7013491723270335}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 0.25, 1.0, 0.9090909090909091, 1.0, 1.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-1595", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-2748", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-10724", "mrqa_triviaqa-validation-5307", "mrqa_hotpotqa-validation-5421", "mrqa_hotpotqa-validation-2833", "mrqa_searchqa-validation-1767", "mrqa_searchqa-validation-2738"], "SR": 0.59375, "CSR": 0.4751602564102564, "EFR": 1.0, "Overall": 0.6760476762820513}, {"timecode": 39, "before_eval_results": {"predictions": ["Sunday", "the \" Michoacan Family,\"", "Steven Green", "More than 150,000", "revelry", "April 24 through May 2.", "12-hour-plus", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "dental work", "House-passed bill that eliminates the 3% withholding requirement for government contractors", "Robert Park", "30", "calls for him to step down as majority leader.", "Meredith Kercher.", "10 percent", "being evicted", "Siemionow", "\"People of Palestine\"", "it will be the longest domestic relay in Olympic history,", "Muslim festival", "China", "Laura Ling and Euna Lee", "from Amsterdam, in the Netherlands, to Ankara, Turkey,", "Morgan Tsvangirai.", "many of those who haven't bought converters are poor, older than 55, rural residents or racial minorities,", "Muslim", "\"He was mentoring them. He was trying to get them to do right, to be productive citizens.\"", "about the shootings, handed over the AR-15 and two other rifles and left the cabin.", "consumer confidence", "South Africa", "Sunday,", "Iowa's critical presidential caucuses", "a bank", "Long Island convenience store", "\"Don't Ask, Don't Tell\"", "12 shades of violet, including a welcoming, bright blue-purple", "\"placenta pack\" is said to help rejuvenate and ease muscle stiffness.", "\"fusion teams,\"", "maintain an \"aesthetic environment\" and ensure public safety,", "the Russian air force", "CNN/Opinion Research Corporation", "U.S. Army scout", "between June 20 and July 20.", "opium", "to take the Rio Group to a new level by creating the organization.", "October 12", "twice", "Sharon Bialek", "in the southern Afghan province of Helmand,", "Abhisit Vejjajiva", "a man's lifeless, naked body", "Lula", "April 10, 2018", "the central plains", "Friday 13th 14th 15th May 2016 Grand Prix", "Anglesey", "Fancy Dress Shop", "1992", "American", "Ellie Kemper", "initiative", "owl", "New Orleans", "the Harpe brothers"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6821886446886447}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.6666666666666666, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3875", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-687", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1793", "mrqa_naturalquestions-validation-430", "mrqa_triviaqa-validation-2959", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-16859"], "SR": 0.640625, "CSR": 0.47929687499999996, "EFR": 1.0, "Overall": 0.6768750000000001}, {"timecode": 40, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1450", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2637", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-845", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1539", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2167", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-2224", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2501", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3579", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-572", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-687", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-90", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-951", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10811", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11486", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12745", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14404", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-1474", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16708", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-16906", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4069", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6300", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7033", "mrqa_searchqa-validation-723", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7731", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10344", "mrqa_squad-validation-1158", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4140", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-6763", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4487", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7078", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-978"], "OKR": 0.7890625, "KG": 0.4796875, "before_eval_results": {"predictions": ["panko", "home-grown potatoes", "Trinity Church", "Robert Todd Lincoln", "stanzas", "Thomas Edison", "the windup", "Cy Young", "Judith", "midfielder", "Frederic", "Frasier", "nightshirts", "Las Vegas", "Australian", "the Department of Justice", "Peter Goldmark", "Utah", "British Broadcasting Corporation (BBC)", "Chinese Meatballs", "Pope John Paul II", "Nancy Drew", "Wendys Wheres", "Google", "the stethoscope", "Israeli", "Harry Truman", "crystals", "a charging cable", "nails", "a lighthouse.", "Perthshire", "colored sands", "the American League", "wren", "Cubism", "shoes", "the Erie Canal", "All the President's Men", "Simon & Garfunkel", "The Silence of the Lambs", "the banjo", "The Exorcist", "Makar Dievushkin Alexievitch", "poetry", "Kick", "the Tigris", "the Leyden jar", "Bobby Jones", "the comet", "March", "orbit", "the Naturalization Act of 1790", "25 September 2007", "Chinese", "James Hook", "Lake Placid,", "Tom Werner,", "the B-17 Flying Fortress", "playwright", "Tuesday.", "Nineteen", "4.6 million", "the Hongwu Emperor of the Ming Dynasty"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5586309523809524}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 0.5, 0.0, 0.5, 1.0, 0.0, 0.7499999999999999, 0.25, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11946", "mrqa_searchqa-validation-7421", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-220", "mrqa_searchqa-validation-8762", "mrqa_searchqa-validation-5185", "mrqa_searchqa-validation-1404", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-11866", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-15206", "mrqa_searchqa-validation-15613", "mrqa_searchqa-validation-11090", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-7436", "mrqa_searchqa-validation-7203", "mrqa_searchqa-validation-5932", "mrqa_searchqa-validation-2705", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-10617", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-4531", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-15712", "mrqa_searchqa-validation-6835", "mrqa_searchqa-validation-13632", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-6972", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-7687", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-2044", "mrqa_naturalquestions-validation-8907"], "SR": 0.453125, "CSR": 0.47865853658536583, "EFR": 1.0, "Overall": 0.6928410823170731}, {"timecode": 41, "before_eval_results": {"predictions": ["the spiritual authority of Rome", "kinetic", "Bob Dylan", "Oz", "Lanai", "Dorothy", "American", "burying the dead.", "Wordsworth", "Mount Rushmore", "dressage", "(Geena) Davis", "The Browns", "a cross", "Christmas", "(John) Dulles", "(My Therapist Is Making Me Nuts)", "Goodyear", "low void", "gigantic", "a truck", "the Bill of Rights", "Philippines", "Dublin", "Tainted Love", "Doom 3", "the Frog", "3Com", "Ned", "sound", "Hungary", "Washington, DC", "Goldenrod", "head", "a hummingbird", "manager", "Wessex", "Tupsa", "candy cane", "(John) Maugham", "California", "New Orleans", "Shirley Schmidt", "Irving G. Thalberg Memorial Award", "the Colosseum", "The Simple Life", "the guitar", "language", "pharaoh", "Aesop", "Raymond Chandler", "fermenting dietary fiber into short - chain fatty acids ( SCFAs ), such as acetic acid and butyric acid, which are then absorbed by the host", "From 14 -- 20 April", "10 years", "Tasmania", "glycerol", "Alex Kramer", "Theo James Walcott", "the British Army.", "John John Florence", "Casey Anthony,", "that he believed he was about to be attacked himself.", "Afghanistan's restive provinces", "seven"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6090277777777777}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.9777777777777777, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12050", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-7308", "mrqa_searchqa-validation-5905", "mrqa_searchqa-validation-2002", "mrqa_searchqa-validation-10221", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-9410", "mrqa_searchqa-validation-14754", "mrqa_searchqa-validation-6892", "mrqa_searchqa-validation-7887", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-8700", "mrqa_searchqa-validation-6743", "mrqa_searchqa-validation-3344", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-1731", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1857"], "SR": 0.515625, "CSR": 0.47953869047619047, "EFR": 1.0, "Overall": 0.6930171130952381}, {"timecode": 42, "before_eval_results": {"predictions": ["University of Vienna", "music teacher, and conductor", "Speedway World", "June 11, 1986", "Mercury Records", "203 people", "Count Schlieffen", "German shepherd", "the Moselle", "United States", "Hawaiian", "\"Hendrix\"", "Friedrich Nietzsche", "Sex Drive", "Tel Aviv", "California", "philosopher, statesman, scientist, jurist, orator, and author", "Martha Wainwright", "David Irving", "2 November 1902 \u2013 27 August 1944", "American theoretical physicist and Nobel laureate in Physics", "Russian Ark", "\"The Land of Enchantment\"", "hunt", "Seoul", "Eve Hewson", "Liguria", "Virgin", "Kevin Peter Hall", "Black Panthers", "Newfoundland and Labrador", "NATO", "\"The Mask\"", "\"Tis the Fifteenth Season\"", "\"Bakusou Buggy Ippatsu\"", "water", "Alfred Preis", "Bayern Munich", "World Outgames", "capital of French Indochina", "Philip Quast", "841", "Gabrielle-Suzanne Barbot de Villeneuve", "Father Dougal McGuire", "Robert John Day", "2017", "the Lommel", "ITV", "Gweilo", "Canada Goose", "Strait of Gibraltar", "Bart Millard", "1931", "a cake", "the armpit", "rabies", "\"Sirpatrickmoore\" Moore", "\"Nu au Plateau de Sculpteur,\"", "March 22,", "Darrin Tuck,", "Grambling State University", "Russia", "\"skill\" or \"talent\"", "Ridge Forrester ( Ronn Moss, later Thorsten Kaye )"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5557291666666666}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-5658", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-248", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-984", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-2005", "mrqa_hotpotqa-validation-1634", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-4414", "mrqa_triviaqa-validation-5439", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-4267", "mrqa_searchqa-validation-2446", "mrqa_searchqa-validation-1023", "mrqa_naturalquestions-validation-8695"], "SR": 0.46875, "CSR": 0.47928779069767447, "EFR": 1.0, "Overall": 0.692966933139535}, {"timecode": 43, "before_eval_results": {"predictions": ["Pat Houston", "Bromley-By- Bowen", "\"two fossil fish vertebrae\"", "redheaded", "Dodo", "Duke Francis", "Parker Pyne", "Franz Liszt", "Janis Joplin", "July 20, 1969", "Gondwana", "rivers", "M69", "\"Bactrians and dromedaries Camels (one hump or two)", "Steptoe and Son", "2011", "Louis Le Vau", "Novak Djokovic", "London King's Cross", "The Centaurs", "Emily Dickinson", "vitamin D", "Usain Bolt", "Jimmy Perry and David Croft", "French", "kelp", "Allardyce", "1951", "Washington", "\"an-juh-vin\"", "Trainspotting", "Francisco de Goya", "absinthe", "Ganges", "Origami", "Buxton", "Ankhises", "tappuah ha\u2019adam", "twenty", "South Carolina", "Granada", "1969", "Paul Maskey", "French", "Fiat SpA", "parochial undertaker", "Charlie Brown", "mercury", "Tasmanian", "five", "Tony Cozier", "Rose Stagg ( Valene Kane )", "Isaiah Amir Mustafa", "Bonanza Creek Ranch", "Conservative", "1898", "\"The Dragon\"", "the Arab world", "public opinion in Turkey", "poems telling of the pain and suffering of children just like her; girls banned from school, their books burned,", "The Color Purple", "children's hospitals", "\"TSP\"", "anaphylaxis,"], "metric_results": {"EM": 0.5, "QA-F1": 0.5453947368421053}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-5893", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-3712", "mrqa_triviaqa-validation-6929", "mrqa_triviaqa-validation-7493", "mrqa_triviaqa-validation-6032", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-3677", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-3189", "mrqa_triviaqa-validation-6111", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-4746", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3073", "mrqa_searchqa-validation-2908", "mrqa_searchqa-validation-3059"], "SR": 0.5, "CSR": 0.4797585227272727, "EFR": 1.0, "Overall": 0.6930610795454545}, {"timecode": 44, "before_eval_results": {"predictions": ["meat man", "kaleidoscope", "socialism", "john beery, Jr.", "Charlotte Elizabeth Diana", "cush", "children of Israel", "Robin Hood Men in Tights", "The Blue Boy", "Fort Leavenworth", "Ascot", "big brother", "Japan", "venus", "co-  operative", "Spanish", "Tim Roth", "henry australa", "joanne Harris", "optical illusion toy", "mungo park", "vitamin C", "Irish Setter", "purple", "whey", "\"The Meadows\"", "copywriter", "x-Men Origins: Wolverine", "boxelder bug", "big brother", "adams", "amNESTY INTERNATIONAL", "Washington", "papelino", "1982", "John McCarthy", "Jennifer Lopez", "muffin man", "yichang", "Wat Tyler", "Blackstone", "basingstoke, Hampshire", "paddy doherty", "vincent van Gogh", "france australia", "Elizabeth I", "Robert Hooke", "Iain Banks", "brown", "brazil", "checkers", "Jason Flemyng", "mind your manners '', `` mind your language '',`` be on your best behaviour '' or similar", "arithmetic operations of addition, subtraction, multiplication, and division are represented by the +, -, *, and / keys, respectively", "Welterweight", "Bonkyll Castle", "21", "as soon as 2050,", "three", "Arnold Drummond", "claymore", "Lost in Yonkers", "Lake Champlain", "nanna popham Britton"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5542410714285715}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-2397", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-5599", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-796", "mrqa_triviaqa-validation-212", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-1705", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-2917", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-1472", "mrqa_naturalquestions-validation-10364", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2935", "mrqa_newsqa-validation-1828", "mrqa_searchqa-validation-605", "mrqa_hotpotqa-validation-2720"], "SR": 0.53125, "CSR": 0.4809027777777778, "EFR": 1.0, "Overall": 0.6932899305555555}, {"timecode": 45, "before_eval_results": {"predictions": ["james", "windmill", "spain", "spouts", "joshua Sawalha", "scurvy", "Tony Blair", "arthur", "cogs", "tonsure", "horses", "arthur", "spain", "Taiwan", "james", "cast", "pulsar", "cask", "t.S. Eliot", "river", "adolphe Adam", "\"A Metro\u2013Goldwyn Mayer Picture\u201d", "seattle", "Francis Matthews", "Mickey Mouse", "Peter Townsend", "gillingham", "the Underworld", "green", "spain", "books have been among the world's best-sellers", "james", "joshua", "reservoirs", "king of spartan", "stilts", "Gary Oldman", "square", "five", "disraeli", "gooseberry", "arthur", "raspberries", "arthur", "Tripoli", "Andrew Jackson", "Salvador Dal\u00ed", "tyres", "Mull", "turnip", "samovar", "altitude", "2015", "Elizabeth Dean Lail", "December 1974", "Lindsey Islands", "\"Gliding Dance of the Maidens\"", "56", "Obama", "in the neighboring country of Djibouti,", "Obama", "sandman", "Fred Rogers", "Republic of Ireland"], "metric_results": {"EM": 0.375, "QA-F1": 0.44270833333333337}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 0.5, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-4795", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-1160", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-3226", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-1588", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-1153", "mrqa_triviaqa-validation-5214", "mrqa_triviaqa-validation-569", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-4732", "mrqa_naturalquestions-validation-413", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-4284", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-2341", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-8212", "mrqa_searchqa-validation-11910", "mrqa_hotpotqa-validation-988"], "SR": 0.375, "CSR": 0.47860054347826086, "EFR": 1.0, "Overall": 0.6928294836956521}, {"timecode": 46, "before_eval_results": {"predictions": ["honey", "curious george", "harmonica", "Chicago", "alphabets", "Assault on Precinct 13", "entropy", "hickory", "bobby burke", "the Kentucky Derby", "silvergrass", "playwright willy Russell", "Thundercats", "27", "h Herbert Lom", "republic of Gambia", "table salt", "jones", "le hare", "phobias", "Annie Lennox", "henna", "Hungary", "graphite", "david Jason", "indonesia", "city of liberia", "mary Sue", "james Chadwick", "Old Sparky", "Newcastle Falcons", "bankside power station", "indonesia", "Aldi", "1969", "atrium", "Billy Fury", "hugh Laurie", "republic of tanzibar", "palladium", "cyclops", "Washington Irving", "denarii", "p Pablo Picasso", "Narendra Modi", "Prime Minister Yitzhak Rabin", "indonesia", "price cut", "Superstar", "centaur", "james pilate", "June 12, 2018", "Thomas Middleditch", "Mike Mushok", "Hayley Catherine Rose Vivien Mills", "\" Fearless\"", "October 3, 2017", "five", "Vertikal-T,", "poor families", "Jeopardy", "Twelve Days of Christmas", "otoberfest", "Juice Newton"], "metric_results": {"EM": 0.46875, "QA-F1": 0.559375}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.8, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-5422", "mrqa_triviaqa-validation-7313", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-6405", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-6202", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2078", "mrqa_triviaqa-validation-5764", "mrqa_naturalquestions-validation-1089", "mrqa_hotpotqa-validation-1487", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-3446", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9466"], "SR": 0.46875, "CSR": 0.4783909574468085, "EFR": 0.9705882352941176, "Overall": 0.6869052135481851}, {"timecode": 47, "before_eval_results": {"predictions": ["jingles", "a gland", "november", "364", "alba longa", "a modern Townsend Thoresen car and passenger ferry", "james blunt", "bright light", "jim criminals", "nupedia", "Vietnam", "australia", "bertrand Russell", "human rights lawyer", "australia", "salyut 1", "are you going to come quietly, or do I have to use earplugs", "Everly Brothers", "akkordeon", "wist", "n Nova Scotia", "c Cannes", "halloween", "nigeria", "Tombstone", "woodstock", "Parkinson\\'s", "washington", "language", "Arctic Monkeys", "an architect who, in the opinion of select Pritzker Prize jury,", "entropy", "algeciras", "halloween daye", "a police detective whom circumstances have set down in far Northerumberland, where he remains, both to recover himself and to mentor an ambitious, morally unformed detective sergeant (Lee Ingleby)", "Iceland", "eggs benedict", "kent", "Today", "halloween", "Jean-Paul Gaultier", "Cockermouth", "26", "david chipperfield", "halloween", "kirklees", "halloween", "indonesia", "Brian Clough", "Boston Legal", "zero mostel", "Shareef Abdur - Rahim", "a man who could assume the form of a great black bear", "Space is the Place", "the Rose Theatre", "Bob Gibson", "the Corps of Discovery,", "different women coping with breast cancer", "\"wacko.\"", "Transportation Security Administration", "akkordeon", "airbags", "khem", "Magnetically soft ( low coercivity ) iron"], "metric_results": {"EM": 0.421875, "QA-F1": 0.46822916666666664}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5190", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-3709", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-2643", "mrqa_triviaqa-validation-7632", "mrqa_triviaqa-validation-4918", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-2107", "mrqa_triviaqa-validation-3656", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-1266", "mrqa_triviaqa-validation-1729", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-939", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-5639", "mrqa_triviaqa-validation-2391", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-5235", "mrqa_triviaqa-validation-6764", "mrqa_triviaqa-validation-1552", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-8972", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-4751", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-417", "mrqa_naturalquestions-validation-5927"], "SR": 0.421875, "CSR": 0.47721354166666663, "EFR": 1.0, "Overall": 0.6925520833333333}, {"timecode": 48, "before_eval_results": {"predictions": ["the ISS", "Akon", "wake", "cycling", "wrigley", "3\u201311", "priam of troy", "wind turbines", "doe", "city", "group", "greece", "jacob epstein", "14", "3", "madonna", "William Randolph Hearst", "yaroshinskaya 1990", "samovar", "kievan Rus", "Pete Best", "leopons", "Challenger", "sedimentary", "George Lucas", "eyelids", "jacob epstein", "greece", "tango", "canmore", "st Moritz", "Christine Keeler", "j\u00f8rn Utzon", "jacob epstein", "lepus hare", "jacob kennedy", "japan", "narcolepsy", "james mason", "Dirty Dancing", "Saga Noren", "pain", "Cardigan", "jacob epstein", "scotia", "zimbabwe", "jacob epstein", "Egypt", "stoned to death", "Syriza", "numerals", "Missi Hale", "Salman Khan", "Battle of Antietam", "CBS", "Lewis Carroll", "Arsenal Football Club", "of the Movement for Democratic Change,", "The Rosie Show", "Eleven", "competition law", "Army Purple Heart", "a palace", "vicuna"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5098958333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-5779", "mrqa_triviaqa-validation-7161", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-65", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-7287", "mrqa_naturalquestions-validation-6806", "mrqa_hotpotqa-validation-1677", "mrqa_hotpotqa-validation-2335", "mrqa_newsqa-validation-661", "mrqa_searchqa-validation-8159", "mrqa_searchqa-validation-5661", "mrqa_searchqa-validation-15427"], "SR": 0.453125, "CSR": 0.47672193877551017, "EFR": 0.9714285714285714, "Overall": 0.6867394770408163}, {"timecode": 49, "before_eval_results": {"predictions": ["Lake Mead", "Jesus", "Jamie Lee Curtis", "a porter", "bridge on the River Kwai", "cuniculus", "Tajikistan", "Marty the Zebra", "Niccol Machiavelli", "space shuttle", "Sarah Bernhardt", "fermentation", "Simon Legree", "cicero", "Tragedy of Coriolanus", "Pinocchio", "the Battle of San Juan Hill", "Drag", "Henry Hudson", "Doctor Zhivago", "William Shakespeare", "Gemini", "alexia", "The Untouchables", "Boris Godunov", "Sam Malone", "a principality", "a fudge sundae", "Muhammad Ali", "a howitzer", "Mad Men", "a scrambled egg", "paleoconservatism", "Richard Branson", "Arlington", "Andorra", "a student loan", "endymion", "a disco", "centigrade", "Jimmy", "a steak tartare", "Captain Nemo", "Andrea del Sarto", "hydroelectric", "Charlie Bartlett", "a clock", "huckleberry Finn", "chive", "coral", "Robert Fulton", "frontal lobe", "electron donors", "the liver", "microwave oven", "james Foster", "birds", "August 10, 1933", "India", "boundary river", "Oaxacan countryside of southern Mexico", "super-yacht designers", "Wigan Athletic", "unalienable"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5808712121212121}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-968", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-4021", "mrqa_searchqa-validation-6407", "mrqa_searchqa-validation-3766", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-6276", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-15767", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-3705", "mrqa_searchqa-validation-6795", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-8703", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-16361", "mrqa_searchqa-validation-9008", "mrqa_searchqa-validation-6115", "mrqa_searchqa-validation-6380", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-14900", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-1179", "mrqa_triviaqa-validation-6070", "mrqa_newsqa-validation-3786", "mrqa_naturalquestions-validation-9299"], "SR": 0.515625, "CSR": 0.47750000000000004, "EFR": 1.0, "Overall": 0.692609375}, {"timecode": 50, "UKR": 0.681640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-1450", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4261", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-845", "mrqa_hotpotqa-validation-848", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-22", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10617", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12081", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12399", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13413", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14404", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-1474", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16906", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2786", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4021", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5469", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7033", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-8700", "mrqa_searchqa-validation-8703", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-9951", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10344", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1162", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2397", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2520", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-2953", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3738", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4935", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5190", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5344", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5473", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6836", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7108", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7313", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-889", "mrqa_triviaqa-validation-978"], "OKR": 0.775390625, "KG": 0.48515625, "before_eval_results": {"predictions": ["Armageddon", "Lackawanna Six", "Mark Darcy", "red", "Sparks", "Anne Frank", "Wembley London, Britain", "Uganda", "R\u00edo de la Plata", "Lady Gaga", "U", "axles", "\u201cSwan Lake\u201d", "Paraguay", "wales", "Nepal", "Bashir", "Steve Davis", "Crusades", "john Buchan", "Rick James", "Hebrew", "Barbadian", "steel", "David Bowie", "Midsomer Murders", "Some Like It Hot", "to celebrate the anniversary of a person's birth", "France", "Duke", "henry kelly", "golf", "ram", "john ford", "republic of Tajikistan", "Alaska", "le havre", "rhino", "Ace of Spades", "Big Ben", "Genesis", "Carmen", "red", "venus", "Whitney Houston", "igneous", "sweden", "jean passe", "venus", "the Marsland stream", "the Battle of Thermopylae", "Macon Blair", "Saphira", "Darlene Cates", "Archbishop of Canterbury", "Northern Transcon", "Two Pi\u00f1a Coladas", "about the shootings,", "Anil Kapoor", "a nuclear weapon", "hypothermia", "(NFL) Blitz", "Heroes", "local authorities, specifically London boroughs, Metropolitan boroughs ), unitary authorities, and district councils"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6700892857142857}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-4685", "mrqa_triviaqa-validation-4787", "mrqa_triviaqa-validation-2430", "mrqa_triviaqa-validation-214", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-3308", "mrqa_triviaqa-validation-3158", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-248", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-7289", "mrqa_hotpotqa-validation-3417", "mrqa_searchqa-validation-1408", "mrqa_naturalquestions-validation-6194"], "SR": 0.65625, "CSR": 0.48100490196078427, "EFR": 1.0, "Overall": 0.6846384803921569}, {"timecode": 51, "before_eval_results": {"predictions": ["australia", "g\u00e9rard taylor", "guanabara bay", "khaki uniforms", "Arthur Hailey", "coffee", "official Languages Act", "jack Nicholson", "dennis caffari", "LDV", "Julie Andrews Edwards", "Anita roddick", "dirkshire", "john keats", "john Mellencamp", "(Julius) Caesar", "gisbert", "european parliament", "titanium", "john taylor", "Margaret Beckett", "wanderers", "spacecraft", "guatamala", "beaver", "john dennis", "david hockney", "chloride", "nirvana", "crackfast time", "bacall", "bertrand russell", "leucippus", "memory-robbing disease", "gizace", "george best", "kathy passeley", "john kennedy", "mandible", "kazakhstan", "birmingham", "calcaneus", "passion", "goat Island", "placebo", "queen", "papal", "fluorine", "dirk", "albinism", "queen", "Jackie Robinson", "basketball player and amputee", "2010", "Eliot Cutler", "Lerotholi Polytechnic Football Club", "Wu-Tang Clan", "Fort Bragg in North Carolina.", "Mitt Romney", "suicides", "woodcarver", "Delaware", "maryshimov", "Fa Ze YouTubers"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5176282051282051}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.46153846153846156, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5136", "mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-7764", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-3320", "mrqa_triviaqa-validation-7211", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-357", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-2850", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2042", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-6935", "mrqa_searchqa-validation-14492", "mrqa_searchqa-validation-10843", "mrqa_naturalquestions-validation-3297"], "SR": 0.484375, "CSR": 0.48106971153846156, "EFR": 1.0, "Overall": 0.6846514423076924}, {"timecode": 52, "before_eval_results": {"predictions": ["John Mills", "Robert Marvin \"Bobby\" Hull, OC", "buck Boeser", "Hermione Baddeley", "supreme playwright and poet of the English language", "more than 40 million", "Kolkata", "Winecoff", "Warrington, England", "Windermere", "February 13, 1946", "atlas", "Luke Bryan", "Ghana Technology University College", "Yunnan-Fu (\u4e91\u5357\u5e9c, \"Y\u00fann\u00e1nf\u01d4\")", "Bigfoot", "Chick tract", "Monty Python", "Gainsborough Trinity", "G\u00f6ran Bror Benny Andersson", "Dana Andrews", "Las Vegas", "George Adamski", "1961", "Backstreet Boys", "DS Virgin Racing reserve driver in Formula E and G-Drive Racing", "Baugur Group", "KlingStubbins", "Christopher Tin", "Jack Kilby", "skerries", "Humberside", "Laertes", "my Beautiful Dark Twisted Fantasy", "June 11, 1986", "Kathleen O'Brien", "13", "AVN Adult Entertainment Expo", "Long Island", "1,696", "Michelle Anne Sinclair", "Alexander Lippisch", "Linda Ronstadt", "Richa Sharma", "Jack Murphy Stadium", "cricket fighting", "U.S. military", "Louis King", "gamecock", "from 1973 to 1996", "shopping malls", "Brian", "RAF Bovingdon", "to indicate agreement, acceptance, or acknowledgement", "a condor", "eight", "saskatoon", "a lizard-like creature from New Zealand", "Sunday,", "150", "red", "caper", "makeup", "Florence"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5098093083387201}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.15384615384615385, 0.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.2857142857142857, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5882352941176471, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-4131", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-3227", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-5479", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5653", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-1581", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-1359", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-7513", "mrqa_newsqa-validation-4028", "mrqa_searchqa-validation-4862", "mrqa_searchqa-validation-14559"], "SR": 0.390625, "CSR": 0.47936320754716977, "EFR": 1.0, "Overall": 0.684310141509434}, {"timecode": 53, "before_eval_results": {"predictions": ["Teriade", "five aerial victories", "first Circle-Vision show that was arranged and filmed with an actual plot and not just visions of landscapes, and the first to utilize Audio- Animatronics.", "Italian", "SM Lifestyle Cities", "2001 NBA All-Star Game", "Ringo Starr", "United States", "Sparky", "James Lofton", "Samantha Spiro", "New Orleans Saints", "Prof Media", "Scottish", "Igor Stravinsky, Carl Orff", "brigadier general", "Battle of Prome", "Fat Man", "Clovis I", "Sir Ahmed Salman Rushdie", "Mark Alan Dacascos", "45th Infantry Division", "a minor basilica", "Wilhelmus Simon Petrus Fortuijn", "Bardot", "King R\u00e6dwald of East Anglia", "American", "Charlie Wilson's War", "President Bill Clinton.", "Canadian", "a saint", "Australian", "Guthred", "Afghanistan", "1867, late in the regime of Emperor Napoleon III", "City of Peace", "State House in Augusta", "music producer from Wood Green, London.", "Sam Bettley", "Roman", "Manchester United", "David Abelevich Kaufman", "eight", "Robert Jenrick", "more than 230", "Texas's 27th congressional district", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia.", "Pennacook", "Valeri Bure", "New York Giants", "Old World fossil representatives", "a long line, called the main line", "1987", "1930s", "1941", "alberta", "Genghis Khan.", "to share personal information.", "David Beckham", "sportswear", "T rex", "General Hospital", "Joseph Lieberman", "San Francisco"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6004493464052287}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.22222222222222218, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5555555555555556, 1.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.11764705882352941, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-5079", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-2962", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3003", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4862", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-8028", "mrqa_triviaqa-validation-7348", "mrqa_newsqa-validation-1911", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-5642"], "SR": 0.484375, "CSR": 0.4794560185185185, "EFR": 1.0, "Overall": 0.6843287037037038}, {"timecode": 54, "before_eval_results": {"predictions": ["the Oyster Bay Guardian", "1902", "100 million", "AT&T", "January 23, 1898", "Cincinnati metropolitan area, informally known as Greater Cincinnati, is a metropolitan area that includes counties in the U.S. states of Ohio, Kentucky, and Indiana around the Ohio city of Cincinnati", "the 2010 film \"How to Train Your Dragon\"", "Columbus Crew Soccer Club is an American professional soccer club based in Columbus, Ohio.", "Ashanti Region", "Kenan Thompson", "County Executive", "Disco", "Virginia", "1730", "7 October 1978", "University of Missouri", "Stratfor", "129,007", "the Seasiders", "Symphony No. 7", "Burl Ives, Jim Reeves, Jerry Garcia, Dolly Parton, Emmylou Harris, and Linda Ronstadt", "boxer", "Liverpool Bay", "1991", "the Crips gang", "Edward Albert Heimberger", "848", "16\u201321", "Vilnius Old Town", "the Saint Petersburg Conservatory", "James Fell", "Newcastle upon Tyne, England", "Big Kenny", "Benjamin Andrew \"Ben\" Stokes", "around 3,500,000", "Los Angeles", "10 June 1921", "John D Rockefeller", "Norway", "the Moon's surface", "\"N.I.B.\"", "Sada Carolyn Thompson", "the Austro-Hungarian Army", "21 August 1986", "David Irving", "Neneh Mariann Karlsson", "Norsemen", "Afro-Russians", "Western District of Victoria, Australia", "Captain Marvel", "Hong Kong professional footballer playing for Hong Kong Premier League club HK Pegasus", "a Celtic people living in northern Asia Minor", "to capitalize on her publicity", "capillaries, alveoli, glomeruli", "Edward Lear", "Russia", "10", "Shenzhen in southern China.", "30", "carving in the middle of our Mountain View, California, campus.", "3800", "dynamite", "the Burrard inlet", "is being treated there after being admitted on Wednesday."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5918158740769035}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.47058823529411764, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444444, 0.0, 1.0, 1.0, 0.8, 1.0, 0.8571428571428571, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 0.1818181818181818, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-574", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-3641", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-5881", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-4314", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-1763", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-10098", "mrqa_newsqa-validation-1829"], "SR": 0.46875, "CSR": 0.4792613636363636, "EFR": 1.0, "Overall": 0.6842897727272728}, {"timecode": 55, "before_eval_results": {"predictions": ["UTC \u2212 09 : 00", "Milira", "Tsetse can be distinguished from other large flies by two easily observed features", "Norman origin", "the film was theatrically released in the United States on April 14, 2017", "the nasal septum", "prevent pressure on the public food supply", "cumin", "the development of electronic computers in the 1950s", "Australia's Sir Donald Bradman", "1975", "shortwave radio", "Eurasian Plate", "Jenny", "Edward G. Robinson", "March 2, 2016", "Glenn Close", "August 18, 1998", "the Serbian army", "skeletal muscle", "The UN General Assembly", "July 2017", "Mike Czerwien", "to ensure party discipline in a legislature", "cells", "24", "New Zealand", "2010", "It is slated to premiere on November 5, 2017", "February 7, 2018", "Manchuria", "Brooklyn, New York", "Lex Luger and Rick Rude", "the pouring rain", "number of games where the player played", "Paul Revere", "the American Revolutionary War", "2015", "Ian Hart", "prenatal development", "Ricky Nelson", "Service / Crown personnel serving in the UK or overseas in the British Armed Forces or with Her Majesty's Government", "On July 4, 1898", "Heather Stebbins", "red", "the only way to go forward is to just keep living her life", "Dorothy Gale", "William Strauss and Neil Howe", "to accomplish the objectives of the organization", "mid - to late 1920s", "six", "Rambo", "earth", "Fitzwilliam Darcy", "119", "Christopher Michael \"Chris\" DeStefano", "the State House of Representatives", "September 23,", "44th", "their \"Freshman Year\" experience", "vowel", "Galileo", "Pharmacy", "(Jack) Johnson"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5865042726370853}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0625, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7619047619047621, 0.8571428571428571, 0.0, 0.4, 0.125, 0.0, 0.0, 0.888888888888889, 0.888888888888889, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-3881", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-973", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-8346", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-4988", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-3011", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-3176", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-7391"], "SR": 0.4375, "CSR": 0.478515625, "EFR": 1.0, "Overall": 0.6841406250000001}, {"timecode": 56, "before_eval_results": {"predictions": ["Saturday,", "rural Tennessee", "Don Draper", "he was diagnosed with skin cancer.", "her fianc\u00e9,", "(Charles Reisner, 1928)", "Cyprus", "Stratfor,", "he was injected with drugs by ICE agents against his will.", "May 4", "that the U.S. might use interceptor missiles for offensive purposes.", "to reach car owners who have failed to comply,\"", "his former caddy,", "the worst snowstorm to hit Britain in 18 years", "one of its diplomats in northwest Pakistan", "three people", "six Africans dead.", "gains access to a reported \u00a320 million ($41.1 million) fortune", "he has helped finance the insurgency against U.S. troops", "a mammoth", "J. Crew", "the National Restaurant Association", "\"Watchmen\"", "over the Gulf of Aden,", "Ben Roethlisberger", "$40 and a bread.", "Bill Stanton", "a monthly allowance,", "cancer that strikes fewer than 2,000 men a year, compared with about 200,000 women.", "his comments to Rolling Stone magazine that he can \"t Totally understand\" O.J. Simpson -- the former football great found liable for the deaths of his wife and another man -- are being misunderstood.", "Ali Bongo received 41.73 percent of the votes in Sunday's election,", "Minerals Management Service Director Elizabeth Birnbaum", "Tim Masters,", "we Found Love", "they did not know how many people were onboard.", "aesthetic environment", "it was like going somewhere very special, far away, because under the Communist regime you didn't travel that much and Prague was \"wow.\"", "42 years old", "refusal or inability to \"turn it off\"", "the Transportation Security Administration", "not guilty", "in a public housing project,", "as many as 250,000", "Washington", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.iReport.com:", "tax incentives", "Niger Delta region", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "fighting charges of Nazi war crimes for well over two decades.", "5:20 p.m.", "new DNA evidence indicated someone else might have committed the crime.", "the 10th century", "t\u00e6n\u026ak", "During his epic battle with Frieza", "Imagine Dragons", "Pamplona", "Afghanistan", "Singapore", "consulting services", "2006", "Castle Rock Entertainment", "a pole", "hate crime Statistics Act", "Sheev Palpatine"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5675903050043876}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.9523809523809523, 1.0, 0.0, 0.43478260869565216, 0.0, 0.2222222222222222, 0.0, 0.6666666666666666, 0.5714285714285715, 0.8, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.28571428571428575, 0.13333333333333333, 0.5, 1.0, 1.0, 0.0, 0.4444444444444445, 0.06666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.35294117647058826, 0.6666666666666666, 0.0, 0.13333333333333333, 0.7058823529411764, 0.0, 0.4615384615384615, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.3333333333333333, 0.4444444444444445]}}, "before_error_ids": ["mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-762", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-3925", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-56", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-7115", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-16115", "mrqa_searchqa-validation-6610", "mrqa_naturalquestions-validation-5986"], "SR": 0.34375, "CSR": 0.4761513157894737, "EFR": 1.0, "Overall": 0.6836677631578947}, {"timecode": 57, "before_eval_results": {"predictions": ["allegedly involved in forged credit cards and identity theft", "misdemeanor assault charges", "Manchester United", "a skilled hacker", "We Found Love", "Too many glass shards", "at least 27", "baby wipe, baby food and boxed wine.", "citizenship", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "September 21.", "Two UH-60 Blackhawk helicopters", "Saturday,", "11", "Lebanese", "the remains of \"Zed,\" a Columbian mammoth whose nearly intact skeleton is part of what is being described as a key find by paleontologists at Los Angeles' George C. Page Museum.", "Larry King", "eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Symbionese Liberation Army", "Jezebel.com's Crap E-mail", "Kurt Cobain's", "$80,000", "around 10:30 p.m.", "Arizona", "Ryan Adams.", "The North Korean news agency KCNA", "auction off one of the earliest versions of the Magna Carta later this year,", "Monday,", "Copts", "Port-au-Prince, Haiti", "Harry Potter star Daniel Radcliffe", "the final resting place for many casualties of the wars in Iraq and Afghanistan.", "Atlanta, Georgia,", "2004.", "the man facing up, with his arms out to the side.", "al-Nour al-Maqdessi,", "by out-of-towners.", "cortisone.", "July 1999,", "mated", "in a Johannesburg church that has become a de facto transit camp,", "Arsene Wenger", "both food and inhalant", "on Saturday.", "At least 14", "one-of-a-kind navy dress with red lining", "JBS Swift Beef Company,", "from Amsterdam, in the Netherlands, to Ankara, Turkey,", "Toffelmakaren", "Jason Chaffetz", "Iran could be secretly working on a nuclear weapon", "Spektor", "Atlanta Hawks", "the Atlantic Ocean", "Los Angeles", "Las Vegas", "May 20, 2003", "The Summer Olympic Games", "people working in film and the performing arts", "The Royal Navy", "a comb", "Korean War", "Philadelphia, Pennsylvania", "The United Arab Emirates"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6120595397249808}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true], "QA-F1": [0.19047619047619047, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.058823529411764705, 0.5, 0.8, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.7272727272727273, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-2269", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-726", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-5567", "mrqa_searchqa-validation-8265"], "SR": 0.484375, "CSR": 0.4762931034482759, "EFR": 1.0, "Overall": 0.6836961206896552}, {"timecode": 58, "before_eval_results": {"predictions": ["intravenous vitamin \"drips\"", "club managers,", "city of romance, of incredible architecture and history.", "Olivia Newton-John", "no", "5", "Ricardo Valles de la Rosa,", "Los Angeles", "40 lashes", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "reached an agreement late Thursday", "E. coli", "the United States", "more than 100.", "may", "Museum-worthy pieces", "Susan Atkins", "Bright Automotive,", "Too many glass shards left by beer drinkers in the city center,", "Brazil", "American Muslim and Christian leaders", "\"release\" civilians,", "Bill Klein,", "acid attack", "one", "to host the Olympic Games in Rio de Janeiro.", "sailing", "help rebuild the nation's highways, bridges and other public-use facilities,", "attempt to make viewers feel like they're in good hands with him as Emmy host.", "they did not receive a fair trial.", "Saturday", "wings,", "Ross Perot.", "to put a lid on the marking of Ashura", "collaborating with the Colombian government,", "has to move out of her rental house because it is facing foreclosure", "James Osterberg", "17", "30-minute", "Sabina Guzzanti", "a hospital", "\"Steamboat Bill, Jr.\"", "billions", "North vs. South,", "ice jam", "Royal Navy servicemen who have been helping the Iraqis to protect oil fields around the port town of Umm Qasr,", "last month's", "Christopher Savoie", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "Sunday.", "Garth Brooks", "following the 2017 season", "New England Patriots", "Charlene Holt", "Albert Einstein", "Parsley the Lion", "a well", "The Rawlings Gold Glove Award", "perjury and obstruction of justice", "December 23, 1977", "orange", "Willa Cather", "Persian Gulf", "bats"], "metric_results": {"EM": 0.625, "QA-F1": 0.7229166386105724}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.9411764705882353, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.125, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1312", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3474", "mrqa_triviaqa-validation-6649", "mrqa_hotpotqa-validation-5293", "mrqa_hotpotqa-validation-410"], "SR": 0.625, "CSR": 0.47881355932203384, "EFR": 1.0, "Overall": 0.6842002118644068}, {"timecode": 59, "before_eval_results": {"predictions": ["Wayne Conley", "a barcode", "Michael Seater", "New York University School of Law", "2017", "Orange County, Florida, United States", "USC Marshall School of Business", "the increased risk of terrorist activity against Norwegian interests, including the oil platforms in the North Sea oil.", "New York City", "Max Kellerman", "September 30, 2017", "Adelaide's number one Newstalk radio station", "November 11, 1901", "David Mandel, and Jeff Schaffer", "Jahseh Dwayne Onfroy", "Boyd Gaming", "Sandusky, Ohio", "The Birds", "May 5, 1939", "1895", "DS Virgin Racing Formula E Team", "Miss Universe", "a domestic cat", "a vegetarian dish called Buddha's delight", "seventh", "gesellschaft mit beschr\u00e4nkter Haftung", "Joseph Raymond McCarthy", "Silvia Navarro", "Stravinsky's", "Richard Wayne Snell", "December 19, 1998", "1960", "about 26,000", "The United States presidential election of 2016", "Joshua Rowley", "English musician and songwriter", "the Austro-Hungarian Army", "beer", "Black Friday", "(Emilian: Frara)", "bioelectromagnetics", "British", "St. Louis Cardinals", "the Kalahari Desert", "The Gold Coast", "Danielle Fernandes Dominique Schuelein- Steel", "mastered recordings for many well known musicians,", "Julia Kathleen McKenzie", "Stephen Crawford Young", "Cody Miller", "1,521", "Caparra", "the National September 11 Memorial plaza", "Zeus", "tem\u00b7per\u00b7a", "John le Carr\u00e9", "Czech Republic", "Jeddah, Saudi Arabia,", "Harare", "Krishna Rajaram,", "Tuscany", "Spanish-American War", "a police car.", "the Westwall"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6122578197945845}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666665, 0.4444444444444445, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-3991", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-2663", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-3803", "mrqa_hotpotqa-validation-2711", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-4134", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-5449", "mrqa_hotpotqa-validation-1206", "mrqa_hotpotqa-validation-5832", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-10088", "mrqa_triviaqa-validation-5858", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-1137", "mrqa_triviaqa-validation-4861"], "SR": 0.515625, "CSR": 0.4794270833333333, "EFR": 1.0, "Overall": 0.6843229166666667}, {"timecode": 60, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1491", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-255", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2711", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3991", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4476", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5479", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-845", "mrqa_hotpotqa-validation-848", "mrqa_hotpotqa-validation-86", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-22", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-531", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-725", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10617", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-1121", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12081", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12399", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13054", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2786", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5469", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-8700", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-996", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1234", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-1285", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-1966", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2172", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-248", "mrqa_triviaqa-validation-2520", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-2799", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-2953", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3103", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-357", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5317", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5344", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-5473", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-5701", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6836", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7108", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7211", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7677", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7764", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-889", "mrqa_triviaqa-validation-912", "mrqa_triviaqa-validation-974", "mrqa_triviaqa-validation-978"], "OKR": 0.794921875, "KG": 0.4984375, "before_eval_results": {"predictions": ["Home Rule Party or the Home Rule Confederation", "Disha Patani", "Mineola", "Jimmy Ellis", "Vilius Storostas-Vyd\u016bnas", "Mario Lemieux", "a recently widowed former horse trainer and active horse farm owner, who becomes the guardian of Dwight Eisenhower", "Sugar Ray Robinson", "Pyotr Ilyich Tchaikovsky", "twin sister", "Todd Emmanuel Fisher", "Roger Thomas Staubach", "86 ft", "England", "David Patrick Griffin", "15 October 1988", "People v. Brock Allen Turner", "Lucille D\u00e9sir\u00e9e Ball", "Do Kyung-soo", "Cherokee River", "City of Westminster, London", "Lehmber Hussainpuri", "Eric Arthur Blair", "1974", "Old World fossil representatives", "U.S. Representative", "Twister...Ride it Out", "Rebirth", "Peter Chelsom", "Aaron Hall", "Elisha Nelson Denver", "1983", "Douglas Jackson", "40 million", "Valley Falls", "2012 Hong Kong action film", "Bisexuality", "May 5, 1939", "27 August 1959", "Ben Elton", "Secretary of Defense", "Ambroise Thomas", "Belgian", "the first Saturday in May", "Lochaber, Highland, Scotland", "Firth of Forth", "28,776", "SARS", "8,648", "Moselle", "Cristian S\u00e1ez Vald\u00e9s Castro", "USS", "Mockingjay -- Part 2 ( 2015", "in and around an unnamed village", "Toledo", "Eddie Shoestring", "Paraguay", "graduate from this school district.", "Daniel Wozniak,", "A witness", "a dust storm", "Dead Ringers", "ethanol", "April 2010."], "metric_results": {"EM": 0.515625, "QA-F1": 0.62265625}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-3237", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3167", "mrqa_hotpotqa-validation-2827", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3563", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-2558", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1570", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-4681", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-1427", "mrqa_naturalquestions-validation-3095", "mrqa_newsqa-validation-2695", "mrqa_searchqa-validation-3931"], "SR": 0.515625, "CSR": 0.48002049180327866, "EFR": 1.0, "Overall": 0.6980353483606557}, {"timecode": 61, "before_eval_results": {"predictions": ["\"new chapter\" of improved governance in Afghanistan", "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "Somali", "celebrities", "\"The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "the Government Accountability Office report", "an antihistamine", "\"tsk, tsk,\"", "22", "Lebanese", "CBS, CNN, Fox and The Associated Press.", "his father's", "Los Angeles", "Haiti", "mated", "apologized,", "$7.8 million", "the most high-profile amalgamation of Indian and western talent", "\"a striking blow to due process and the rule of law.\"", "the Airbus A330-200", "Hurricane Gustav", "A large concrete block is next to his shoulder, with shattered pieces of it around him.", "Japan", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "acid attack", "30,000", "opium", "Afghanistan,", "More than 15,000", "President Obama", "\"We've just lost count of how many demonstrations are taking place now,\"", "Ralph Lauren (Rizzoli)", "Euna Lee,", "Consumer Reports", "curfew in Jaipur", "Kurdistan Freedom Falcons, known as TAK", "the insurgency,", "President-elect Barack Obama", "\"I think protecting your family and giving to them is the most important achievement.\"", "Spaniard", "CNN", "Aniston, Demi Moore and Alicia Keys", "Dan Parris, 25, and Rob Lehr, 26,", "Coast Guard", "\"a strong work ethic, rock-solid conservative values, and a deep sense of service to others,\"", "drug cartels", "Philip Markoff,", "Marie-Therese Walter.", "they did not receive a fair trial.", "Anil Kapoor.", "October 29", "`` state capitalism ''", "a routing table, or routing information base ( RIB )", "the south", "Walt Whitman", "Benjamin Barker", "The Derby", "Leonard Cohen", "Charlie Wilson", "1905", "a screwdrivers", "Queen Elizabeth I", "Ground traction", "140 million"], "metric_results": {"EM": 0.5, "QA-F1": 0.6360572874970954}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.125, 0.8, 1.0, 1.0, 0.5, 0.2222222222222222, 1.0, 0.8571428571428571, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7272727272727273, 0.06451612903225806, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1848", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-1891", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-91", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-3784", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-6087", "mrqa_hotpotqa-validation-2151", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-13231", "mrqa_hotpotqa-validation-4810"], "SR": 0.5, "CSR": 0.4803427419354839, "EFR": 1.0, "Overall": 0.6980997983870968}, {"timecode": 62, "before_eval_results": {"predictions": ["he has helped finance the insurgency against", "punish participants in this week's bloody mutiny,", "legitimacy of that race.", "five", "at \"CNN Heroes: An All-Star Tribute\" as a", "$500,000", "Larry Zeiger", "Thabo Mbeki,", "Thailand", "cortisone.", "to provide security as needed.", "Miss USA Rima Fakih", "France's famous Louvre museum", "a man's lifeless, naked body", "June 6, 1944,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Climate Care,", "The European Commission", "In immigration judge with the U.S. Justice Department", "Sen. Barack Obama", "over 127 acres.", "Haiti", "Dr. Death in Germany", "Fullerton, California,", "the southern city of Naples", "company Polo", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "his past and his future", "Buenos Aires.", "three", "was killed in an attempted car-jacking", "3 p.m. Wednesday", "breast cancer.", "1969", "Thabo Mbeki,", "America's infrastructure.", "African National Congress Deputy President Kgalema Motlanthe,", "Meredith Kercher.", "Angola,", "Kurt Cobain", "between 1917 and 1924", "Japan:", "Jaipur", "the Juarez drug cartel.", "my recent 12-day trip to Iran to film a public-television show.", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "I think Arrington truly believes everything he has said about the tech world being a meritocracy.", "suicides", "An unopposed former Communist leader", "10 years", "the issue to a crowd at the White House,", "One day", "Sebastian Lund ( Rob Kerkovich )", "2001", "HMS Amethyst", "oceania", "worked", "I, (Annoyed Grunt)-bot", "Menace II Society", "Dancing with the Stars", "bovines", "termites", "Sir Winston Churchill", "turtles"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5955433238636363}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false], "QA-F1": [0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.04761904761904762, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6875000000000001, 0.7272727272727273, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-965", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3933", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-2401", "mrqa_triviaqa-validation-2132", "mrqa_hotpotqa-validation-3504", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-9042", "mrqa_searchqa-validation-4054"], "SR": 0.484375, "CSR": 0.48040674603174605, "EFR": 0.9696969696969697, "Overall": 0.6920519931457432}, {"timecode": 63, "before_eval_results": {"predictions": ["Sue Miller", "Aiden Gillen", "Zambezi River", "Hepatitis B", "anemia", "Washington", "a riddle", "Abraham Lincoln", "Amherst", "Rand McNally", "Jane Bolling", "Calumet Farm", "Jan Hus", "a nomenclature", "roulette", "Chesapeake Bay", "gold", "Betsy Ross", "the Royal Military Academy Sandhurst", "Westerns", "dogs", "the Battle of Verdun", "Carole King", "Rocco Anthony Mediate", "Betty the Ugly", "Zbigniew Brzezinski", "David Berkowitz", "Svengali", "Drumline", "Colorado", "a comet", "Hilary Swank", "These Boots Are Made For Walkin", "Vermont", "a paddock", "water levels", "Macbeth", "The Dying Swan", "the Cotton Bowl", "Lost", "Dracula", "Wind Gods", "Chemistry", "Disturbia", "a heartwood", "Frank Zappa", "hypothermia", "axel", "Cummings", "a crossbow", "Sydney, New South Wales, Australia", "1924", "1830", "rocks and minerals", "Sabena", "vodka", "the Black Sea", "Margaret Thatcher", "Hirsch index rating", "Massachusetts", "whether he should be charged with a crime,", "Palestinian Islamic Army,", "President Mohamed Anwar al-Sadat", "1972"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7175595238095238}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4433", "mrqa_searchqa-validation-11483", "mrqa_searchqa-validation-5360", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-13794", "mrqa_searchqa-validation-9353", "mrqa_searchqa-validation-6383", "mrqa_searchqa-validation-1905", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-11075", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-10426", "mrqa_searchqa-validation-9134", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-5136", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-13729", "mrqa_searchqa-validation-14572", "mrqa_naturalquestions-validation-1003", "mrqa_triviaqa-validation-6373", "mrqa_hotpotqa-validation-3165", "mrqa_newsqa-validation-2431"], "SR": 0.609375, "CSR": 0.482421875, "EFR": 1.0, "Overall": 0.698515625}, {"timecode": 64, "before_eval_results": {"predictions": ["The Bomb Factory", "Captain Cook's Landing Place", "Oneida Limited", "Naomi Campbell", "1967", "the Tallahassee City Commission", "John Mark Galecki", "Titus Lucretius Carus", "23 July 1989", "Jericho Union Free School District", "in photographs, film and television", "six", "12", "Northrop P-61 Black Widow", "Chief Strategy Officer", "the Beatles", "Italy", "controversial public figure", "cruiserweight", "Pac-12", "People!", "Kane Meadows", "tempo", "Pope John X", "Mollie Elizabeth King", "\"Home of the Submarine Force\"", "1957", "three", "\"Grimjack\"", "Battle of Dresden", "Apsley George Benet Cherry-Garrard", "the Missouri Compromise", "1970", "the Sinclair Oil and Refining Corporation", "November 10, 2017", "Joachim Trier", "11,163", "The Sound of Music", "Disha Patani", "Trey Parker and Matt Stone", "elderships", "the Ruul", "Beno\u00eet Jacquot", "Todd Fisher", "Sir Charles Benedict Ainslie", "James Dean", "Brazil", "Robert Arthur Mould", "Cartoon Network Studios", "Michael Edwards", "Longford", "`` new version ''", "location", "September 4, 2000", "South Pacific", "the Iron Age", "Venezuela", "Ewan McGregor", "1-1", "Brian Smith.", "Bank One", "vertices", "slavery", "Dr. Lexie Grey"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7373263888888889}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3500", "mrqa_hotpotqa-validation-3403", "mrqa_hotpotqa-validation-2879", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-837", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-606", "mrqa_hotpotqa-validation-3595", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-5526", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2472", "mrqa_searchqa-validation-16522", "mrqa_searchqa-validation-8371"], "SR": 0.640625, "CSR": 0.48485576923076923, "EFR": 0.9565217391304348, "Overall": 0.6903067516722408}, {"timecode": 65, "before_eval_results": {"predictions": ["United States", "Russell T Davies", "Squam Lake", "in simple language", "1982", "racehorse breeder", "Peel Holdings", "Dialogues of the Carmelites", "Red", "Ding Sheng", "at the small forward position", "Homer Hickam, Jr.", "Nicholas \" Nick\" Offerman", "Team Penske", "the Commanding General of the United States", "Apalachees", "Bill Lewis", "1993", "Wu-Tang Clan", "David Pajo", "his virtuoso playing techniques and compositions in orchestral fusion", "Philip Quast", "Magnate", "Commerce", "Axl Rose", "Albert Bridge", "Laurel, Mississippi", "Vladimir Valentinovich Menshov", "Sergeant First Class", "Minnesota", "Dupont Plaza Hotel", "#364", "Kentucky, Virginia, and Tennessee", "White Horse", "David Irving", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan to the south", "Waylon Albright", "Bit Instant", "Tulsa, Oklahoma", "Kevin Spacey", "Nikita Sergeyevich Khrushchev", "Algernod Lanier Washington", "Sky News", "17", "Karakalpak", "the fourth Thursday", "Larry Eustachy", "Richard Arthur, former owner of the land on which it was built,", "the first and only U.S. born", "University of Nevada, Reno", "Leona Louise Lewis", "10.5 %", "John Goodman", "Rodney Crowell", "china", "dove", "a marble campanile, or bell tower", "super-yacht designers", "Brian Mabry", "a man's lifeless, naked body", "Dean Cain", "20", "hospitals they consider best in their specialty", "River Usk"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7283940018315018}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.3076923076923077, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4715", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-2616", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4402", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-1004", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-496", "mrqa_triviaqa-validation-7155", "mrqa_searchqa-validation-10998", "mrqa_triviaqa-validation-4272"], "SR": 0.59375, "CSR": 0.48650568181818177, "EFR": 1.0, "Overall": 0.6993323863636364}, {"timecode": 66, "before_eval_results": {"predictions": ["44,300", "Walldorf", "Franklin, Indiana", "Levittown", "William Cavendish", "Peter 'Drago' Sell", "three", "Forbes", "2009", "CBS News", "Jeanne Tripplehorn", "Philip K. Dick", "Analysis of the Alinsky Model", "Indooroopilly Shopping Centre", "zoonotic", "Dan Conner", "Westland Mall", "187th", "twice", "High school", "Yasiin Bey", "Graham Payn", "born 28 November 1973", "Charles Quinton Murphy", "Caesars Palace", "Yekaterinburg", "12\u201318", "Dorothy", "zero-g-roll", "Wayne Rooney", "1st Earl Grosvenor", "Bill Curry", "actress and singer", "Marco Hietala", "Mani", "Everton", "4 April 1963", "on the western end of the National Mall in Washington, D.C.", "Tampa Bay Storm", "Emad Hashim", "Washington", "Ronnie Schell", "Alan Tudyk", "heavier than a feather", "New Orleans Saints", "London", "video", "Lommel differential equation", "1938", "Indian", "in the state of Maryland", "Washington", "203", "in the vascular bundles", "Bridgeport", "copper", "Adios", "Kurdistan Workers' Party,", "Lashkar-e-Tayyiba", "autonomy", "trenchcoat", "Tugboat", "Stars", "Norfolk Island"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6463338744588745}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.28571428571428575, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.4, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4204", "mrqa_hotpotqa-validation-4578", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-809", "mrqa_hotpotqa-validation-5144", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3255", "mrqa_naturalquestions-validation-8220", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-1038", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-481", "mrqa_searchqa-validation-32", "mrqa_triviaqa-validation-3945"], "SR": 0.53125, "CSR": 0.4871735074626866, "EFR": 1.0, "Overall": 0.6994659514925373}, {"timecode": 67, "before_eval_results": {"predictions": ["7.6 mm", "Dr. Sachchidananda Sinha", "the International Campaign to Abolish Nuclear Weapons ( ICAN )", "Sumitra", "thicker consistency and a deeper flavour than sauce", "the Japanese government", "Puerto Rico ( Rich Port )", "John Locke", "premalignant flat ( or sessile ) lesion", "a virtual reality simulator", "October 29 - 30, 2012", "2003", "dispense summary justice or merely deal with local administrative applications in common law jurisdictions", "Kansas and Oklahoma", "Coconut Cove", "The Enchantress", "Steve Buscemi", "2017", "Tom Selleck", "`` kind of a'pick yourself up and dust yourself off and keep going ', female - empowerment song", "capillaries, alveoli, glomeruli, outer layer of skin and other tissues where rapid diffusion is required", "gravitation", "Mike Czerwien", "Cheryl Campbell", "from 13 to 22 June 2012", "Peter Klaven ( Paul Rudd )", "X-ray image of the chest", "`` Can't Change Me, ''", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "Nashville, Tennessee", "Whiskey Shivers as Saddle Up", "the concentration of a compound exceeds its solubility", "Darth Sidious and The Emperor", "seven", "1988", "Missi Hale", "1773", "U.S. Bank Stadium", "1997", "British Army soldiers shot and killed people while under attack by a mob", "scythe", "the right to be served in facilities which are open to the public -- hotels, restaurants, theaters, retail stores, and similar establishments", "Angola", "more than 420", "1948", "more than a million", "the 1840s", "California, Utah and Arizona", "pit road speed", "deceased - donor ( formerly known as cadaveric )", "`` speed limit ''", "Crimean Tatar", "chop suey", "a\u00b7meliorate", "National Association for the Advancement of Colored People", "Clarence Nash", "Roslin Castle", "trading goods and services without exchanging money", "state senators", "Kyra and Violet", "a bass drum", "porcelain ware", "Winston Rodney", "in the Muslim north of Sudan"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5561463168740184}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.3333333333333333, 1.0, 0.0, 0.25, 0.0, 0.6666666666666666, 1.0, 0.8333333333333333, 0.35294117647058826, 1.0, 1.0, 0.782608695652174, 0.5, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.8484848484848485, 0.6363636363636364, 0.0, 0.5, 1.0, 0.9090909090909091, 0.0, 0.0, 0.5714285714285715, 0.4, 1.0, 0.5714285714285715, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5454545454545454, 1.0, 0.4, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-7270", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5674", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-10271", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-2262", "mrqa_hotpotqa-validation-1720", "mrqa_hotpotqa-validation-3019", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-10401", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-4479", "mrqa_newsqa-validation-1755"], "SR": 0.359375, "CSR": 0.4852941176470589, "EFR": 0.975609756097561, "Overall": 0.694212024748924}, {"timecode": 68, "before_eval_results": {"predictions": ["the family, which remains united and strong despite the \"tremendous hardship,\" will release more information soon.", "U.S. Consulate in Rio de Janeiro,", "different women coping with breast cancer", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "burned over 65 percent of his body", "AbdulMutallab,", "1994", "opium", "partying", "183", "Stoke City", "Los Angeles Angels", "between 1917 and 1924", "calls for Reid's dismissal.", "six", "Ameneh Bahrami", "President Obama and Britain's Prince Charles", "Columbia, Illinois,", "Lake Washington", "Unseeded Frenchwoman Aravane Rezai", "Wednesday.", "The station", "brutal choice to step up attacks against innocent civilians.", "Bright Automotive,", "the Russian air force,", "tie salesman", "identity documents", "Italian government", "$8.8 million", "Movahedi", "five", "Elena Kagan", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "\"a striking blow to due process and the rule of law.\"", "her son has strong values.", "a cancer-causing toxic chemical.", "a baseball bat", "64,", "gasoline", "urgently to be rescued, fearing the crew could be harmed or killed,", "Appathurai", "Fort Bragg in North Carolina.", "France's famous Louvre museum", "Ashley \"A.J.\" Jewell,", "prison inmates.", "up three", "Jenny Sanford,", "Saturday.", "Canada.", "34", "Drew Kesse,", "by chlorine and bromine from manmade organohalogens", "compound sentence", "2010", "1802", "caspian phosphate", "Muriel Bing", "Statue of Liberty", "XXIV Summer Universiade", "Mickey\\'s PhilharMagic", "Captain Kangaroo", "Adlai Stevenson", "White Tiger", "Allan McNish"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7200902396214897}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.8571428571428571, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.4, 0.4, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-3331", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-2682", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6986", "mrqa_hotpotqa-validation-1870", "mrqa_searchqa-validation-9019", "mrqa_hotpotqa-validation-2473"], "SR": 0.578125, "CSR": 0.48663949275362317, "EFR": 0.9629629629629629, "Overall": 0.6919517411433171}, {"timecode": 69, "before_eval_results": {"predictions": ["VoteWoz.com", "Thomas", "Fred Bright, the district attorney in Milledgeville, Georgia,", "Mutassim,", "tenement in the Mumbai suburb of Chembur,", "The sailboat, named Cynthia Woods,", "the administration's progress,", "Dr. Jennifer Arnold and husband Bill Klein,", "Alison Sweeney,", "shoreline of the city of Quebradillas.", "skeletal remains", "Dr. Cade", "Arsene Wenger can expect an apology from Premier League referees chief Keith Hackett following his dismissal in the closing seconds of Saturday's 2-1 English Premier League defeat to Manchester United.", "Ronnie White,", "$40 and a loaf of bread.", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "keystroke", "Spaniard", "develop a common approach to combat global warming", "intention to set up headquarters in Dublin.", "buckling under pressure from the ruling party.", "highest ranking former member of Saddam Hussein's regime still at large,", "southern port city of Karachi,", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Cash for Clunkers", "between June 20 and July 20.", "Nicole", "Congress", "Shanghai", "18", "a homicide.", "gasoline", "The Stooges comedic farce entitled \"Three Little Beers,\"", "Sheikh Sharif Sheikh Ahmed", "heavy turbulence", "bartolom\u00e9 de las Casas", "eight.", "11th year in a row.", "0-0 draw", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony", "Abdullah Gul,", "2009", "in her home", "writing and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "Max Foster,", "Anne Frank,", "Web", "UNICEF", "Citizens are picking members of the lower house of parliament,", "Secretary of State", "Basel", "Rafael Nadal", "Number 4, Privet Drive, Little Whinging in Surrey, England", "International System of Units", "bantu", "George Washington", "hydrogen", "Perth, Western Australia", "Rocky Boy\\'s Indian Reservation", "gwailou", "FRAM", "Golden Girls", "R", "Hillsborough"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6611344818376068}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8750000000000001, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3228", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2070", "mrqa_naturalquestions-validation-5820", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-834", "mrqa_hotpotqa-validation-2241", "mrqa_searchqa-validation-9679", "mrqa_triviaqa-validation-1046"], "SR": 0.5625, "CSR": 0.4877232142857143, "EFR": 0.9642857142857143, "Overall": 0.6924330357142858}, {"timecode": 70, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1894", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-1939", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2827", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3498", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-368", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4205", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5634", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-927", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3281", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-3881", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1213", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3445", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-603", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-998", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10038", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-1020", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13535", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2908", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-3201", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-3868", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4796", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5332", "mrqa_searchqa-validation-5409", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-7455", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3523", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3559", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3813", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5411", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-7525", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-995"], "OKR": 0.8203125, "KG": 0.47421875, "before_eval_results": {"predictions": ["chess", "Boer War", "Louisiana", "A Christmas Story", "Golden Hind", "astronomer", "Montana", "Marlon Brando", "Hilary Swank", "Return of the Jedi", "Dale Earnhardt, Jr.", "Ned Flanders,", "lDiUdS1", "a rice measuring cup", "I Am Legend", "October", "Swamp Thing", "John Hancock", "Matt Leinart", "Nirvana", "a gulls", "Nitrous oxide", "Martin Luther", "Swaziland", "Nutty Professor II", "Donna Summer", "the Dominican Republic", "\"Let there be light\"", "Carver", "the Bastille", "animal trainer", "jr", "speed", "St. Francis of Assisi", "fast", "china", "green cards", "Long Island Sound", "The Great Gatsby", "a war bond", "Bryant", "Death Watch", "Boss 429 Lawman", "Harvard University", "Claire Chennault", "Captain Jack Sparrow", "Foucault", "Christmas", "third-degree", "High Tor", "Frank Sinatra", "Thirty years after the Galactic Civil War", "al - khimar", "15", "Vancouver Island", "j. M. Coetzee", "sand", "Humberside Airport", "Don Johnson", "Premier League club Liverpool and the England national team", "killing rampage.", "braces.", "issued his first military orders as leader of North Korea", "sewing machines"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5734739219114219}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.18181818181818182, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-5502", "mrqa_searchqa-validation-7325", "mrqa_searchqa-validation-9352", "mrqa_searchqa-validation-16807", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-11506", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-13391", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-11411", "mrqa_searchqa-validation-10378", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-13563", "mrqa_searchqa-validation-3241", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-14784", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-9467", "mrqa_triviaqa-validation-6847", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-3862", "mrqa_newsqa-validation-2778", "mrqa_triviaqa-validation-3882"], "SR": 0.484375, "CSR": 0.48767605633802813, "EFR": 1.0, "Overall": 0.6966758362676055}, {"timecode": 71, "before_eval_results": {"predictions": ["Laodicea", "Evermoist", "the region of the thorax between the neck and diaphragm in the front of the body", "Mitch Murray", "Kenny Gamble & Leon Huff", "Luke 6 : 12 -- 16, and Acts 1 : 13", "Hellenism ( \u0394\u03c9\u03b4\u03b5\u03ba\u03b1\u03b8\u03b5\u03ca\u03c3\u03bc\u03cc\u03c2 )", "Patrick Swayze", "letter series", "New England Patriots XX, XXXI, XXXVI, XXXVIII, XXXIX, XLII, XLVI, LI, LII", "Jurchen Aisin Gioro clan", "Laura Jane Haddock", "Justin Bieber", "1996", "Pasek & Paul", "1945", "the Viet Minh", "Colman", "states", "11 : 15 p.m.", "claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "the north end", "Cody Fern", "247.3 million", "The vapor pressure chart ( right hand side )", "New Zealand, Port Chalmers and on the Otago Peninsula, Saint Bathans in Central Otago and at the Cape Campbell Lighthouse in Marlborough", "Scarlett Johansson", "Article 1, Section 2", "908 mbar ( hPa ; 26.81 inHg )", "fresh nuclear fuel", "a premalignant flat ( or sessile ) lesion of the colon", "Andreas Vesalius", "JackScanlon", "Sachin Tendulkar", "Squamish, British Columbia, Canada", "2003", "Chandan Shetty", "Germany", "October 2008", "1983", "James Brown", "Kyla Coleman", "UTC \u2212 09 : 00", "the International Border ( IB )", "Andy", "Ernest Rutherford", "Majo to Hyakkihei 2", "June 12, 2018", "Pittsburgh", "775", "Amitabh Bachchan, Akshay Kumar, Bobby Deol, Sandali Sinha and Nagma", "Norwich", "Tina Turner", "Taka", "the 924", "Karl Kraus", "Smith Act", "22-year-", "suicides", "alert patients of possible tendon ruptures and tendonitis.", "Jonah", "a puppy", "addin", "The St Andrews Agreement"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6921536796536796}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.5333333333333333, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.28571428571428575, 0.18181818181818182, 1.0, 0.8, 0.6, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6448", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1304", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-4993", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5782", "mrqa_newsqa-validation-841", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-9123"], "SR": 0.59375, "CSR": 0.4891493055555556, "EFR": 1.0, "Overall": 0.696970486111111}, {"timecode": 72, "before_eval_results": {"predictions": ["Amanda Barrie", "germany", "Helen Gurley Brown", "Sting", "Burma", "Maerten Tromp", "Isaac Newton", "the Etruscans", "raven", "10-inch", "Franz Joseph Haydn", "Space Invader", "Charles I", "Athina", "March", "the Local Defence Volunteers (LDV)", "Julius Caesar", "Theresa May", "a couper", "us alone", "Unite", "David Bowie", "Liberator", "hedgehog", "Aaron", "Dik Browne", "Lyoness e", "Pears soap", "Kevin Spacey", "Yalta", "shoji", "blackhead", "in the British city of Leicester,", "snapdragons", "hongi", "Angela Merkel", "scottish", "chess", "germany", "the pea", "Republic of Fiji", "Ireland", "Essex Eagles", "a turtle", "\"Archer\"", "The Longest Day", "1619", "President Jimmy Carter", "Jimmy Carter", "Max Planck", "gay nightlife", "The Church of England", "1960", "atomic number 53", "Knoxville, Tennessee", "Patricia Veryan", "Melbourne Storm", "\"The Lost Symbol,\"", "sovereignty over them.", "May 4", "Song of Solomon", "Hawks", "Anthony Minghella", "Mark O'Connor"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6417782738095238}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-7413", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-6960", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-2810", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-4089", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4359", "mrqa_triviaqa-validation-3422", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-123", "mrqa_naturalquestions-validation-9054", "mrqa_hotpotqa-validation-626", "mrqa_newsqa-validation-3641", "mrqa_searchqa-validation-8757"], "SR": 0.578125, "CSR": 0.49036815068493156, "EFR": 1.0, "Overall": 0.6972142551369862}, {"timecode": 73, "before_eval_results": {"predictions": ["DTS-HD Master Audio", "\"American Idol\"", "Anne of Green Gables", "1986", "MGM Resorts International", "2,627", "Matt Groening", "Forbidden Quest", "Ronald Joseph Ryan", "Carson City", "Mossad", "Meghan Markle", "Carrefour", "ABC", "Memphis", "England", "a seabird", "\"godfather\"", "\"Darconville\u2019s Cat\"", "November of that year", "Jos\u00e9 Bispo Clementino dos Santos", "Swiss", "torpedoes", "PG pornographic", "heaviest album of all", "850 m", "Biola University in La Mirada, California", "various names", "\"Shaun of the Dead\"", "more than 40 million", "Rotterdam", "September 21, 2014", "Fox Broadcasting Company", "Attack the Block", "Isabella II of Jerusalem", "11,791", "seven", "Shane Meadows", "David \"Zeb\" Cook", "win world titles", "Leslie James \"Les\" Clark", "Montreal", "Plantation", "bass", "southwestern", "Band of Hanover", "Borwick railway station", "chiltern Hills", "The Gold Coast in Queensland", "Luigi Segre", "Mark \"Chopper\" Read", "fat or fatty acid", "food and clothing", "Bhupendranath Dutt", "HMS Conqueror", "an avocado", "henry v", "Thursday", "Spain,", "FBI Special Agent Daniel Cain,", "(Frederick) Douglass", "Galileo", "Rudolph Valentino", "a bolt-action rifle"], "metric_results": {"EM": 0.5, "QA-F1": 0.6403459821428572}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.125, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.4, 0.6, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-50", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-4911", "mrqa_hotpotqa-validation-4957", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-2026", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1122", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-2744", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-8163", "mrqa_triviaqa-validation-6185", "mrqa_newsqa-validation-646", "mrqa_searchqa-validation-3369"], "SR": 0.5, "CSR": 0.49049831081081086, "EFR": 1.0, "Overall": 0.6972402871621621}, {"timecode": 74, "before_eval_results": {"predictions": ["William Shakespeare", "three", "Oklahoma", "Santa Fe", "seven", "La vendedora de rosas", "Yellow fever", "14,652 at the 2010 census,", "Crown Holdings", "E22", "Bea Arthur", "Haleiwa, Hawaii", "1912", "quarterly", "homebrewer", "Anna Clyne", "Debbie Harry", "Cuban", "Kneeland Street", "May 4, 2004", "NATO", "seven", "four", "69.7 million litres", "Lego", "Football Bowl Subdivision", "MGM Grand Garden Special Events Center", "Acela Express", "the second", "Abidjan, Ivory Coast", "Imelda Marcos", "Perfect Strangers", "Duke of Westminster is a title in the Peerage of the United Kingdom", "1985", "Melissa Rauch (born June 23, 1980) is an American actress and comedian.", "The Cosmopolitan", "Vince Guaraldi", "Sergeant First Class", "1.5 million households", "The Times Higher Education Guide", "1936", "Tim Allen", "Rome", "World Famous Gold & Silver Pawn Shop", "New York Giants", "1932", "Scotiabank Saddledome", "Wolf Creek", "Electronic Attack Squadron 135 (VAQ-135), known as the \"BlackRaven\"", "Leona Lewis", "An All-Colored Vaudeville Show", "reared", "Institute of Chartered Accountants of India ( ICAI )", "John Adams", "Playboy", "Perth", "Puerto Rico", "The Tupolev Tu-160 strategic bombers", "45 minutes, five days a week.", "Republican senators", "Re-Animator", "The fifth chapter", "Coach", "muskets, fowling pieces or no weapon at all."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6699298964923965}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 0.3076923076923077, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5714285714285715, 0.22222222222222224, 1.0, 0.0, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-1471", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-3545", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-4078", "mrqa_hotpotqa-validation-4637", "mrqa_hotpotqa-validation-2552", "mrqa_hotpotqa-validation-668", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-2058", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4737", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4055", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-6962", "mrqa_triviaqa-validation-3975"], "SR": 0.5625, "CSR": 0.49145833333333333, "EFR": 1.0, "Overall": 0.6974322916666666}, {"timecode": 75, "before_eval_results": {"predictions": ["December 23, 1977", "Ars Nova Theater", "Venice", "Dave Cook", "\"Slaughterhouse-Five\"", "Two escorts", "Marine Corps Air Station Kaneohe Bay", "paternalistic policies", "Philadelphia", "1916", "South Park Mexican", "The Fault in Our Stars", "Robert Sylvester Kelly", "Eucritta melanolimnetes", "Catwoman", "Revengers Tragedy", "Al D'Amato 55% to 44%", "United States Navy", "National Hockey League", "February", "Oklahoma Sooners", "Seito Shokun!", "over 3,000 people", "a Golden Globe Award", "United Healthcare", "zoonotic", "1987", "the Dwarka Kingdom", "Perth", "Kentucky Wildcats", "I Am Furious", "Trey Parker", "Tamara Ecclestone Rutland", "North Dakota and Minnesota", "the State House in Augusta", "first baseman", "John Boyd Dunlop", "Merrimack County", "Mexico", "Peter Townsend", "Elbow", "five times", "2012", "Bonny Hills", "Wandsworth, London", "Alexander Martin Lippisch", "Univision", "more than 110 films", "1874", "statistics", "Mani", "the Election Commission of India", "Mel Gibson", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor ) is a fictional character and one of the primary antagonists of the Star Wars franchise", "Lehman Bros International", "Bedser", "William Lamb", "participate in Iraq's government.", "drama of the action in-and-around the golf course", "$40 and a loaf of bread.", "Alexander the Great", "Greece", "mass", "eight"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7264442155067156}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5714285714285715, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4718", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3226", "mrqa_hotpotqa-validation-3529", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-1599", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-1240", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-3086", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-4796", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-5986", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-6384", "mrqa_naturalquestions-validation-225"], "SR": 0.609375, "CSR": 0.49300986842105265, "EFR": 1.0, "Overall": 0.6977425986842105}, {"timecode": 76, "before_eval_results": {"predictions": ["third studio album, \"Nina\"", "Jim Harrison", "Paula D'Alessandris", "Guardians of the Galaxy Vol. 2", "Floyd Casey Stadium", "1948", "Stage Stores,", "Jane Mayer", "Thomas Cromwell, 1st Earl of Essex", "Talib Kweli", "the Commonwealth Naval Forces", "Lowestoft, Suffolk", "Chrysler", "banjo player", "The MGM Grand Las Vegas", "Michael Phelps", "2006 St. Louis Cardinals", "Maurice Ravel", "C. H. Greenblatt", "48,982", "erotic romantic comedy", "Africa", "Neon City", "Althea Rae Janairo", "Roc-A-Fella Records and Priority Records", "The Catholic Church in Ireland", "the Netherlands", "The Division of Cook", "1919", "Serial (Bad) Weddings", "1970", "Canadian-American Association of Professional Baseball", "City and County of Honolulu", "Hirsch index rating", "Sydney", "1,925", "the Las Vegas Strip in Paradise, Nevada", "Battleship", "Austrian", "Sam Rockwell", "Chief Strategy Officer", "John M. Dowd", "Bruce R. Cook", "Eisstadion Davos", "4,000", "Neighbours", "Super Junior", "Tennessee", "Dabestan-e Mazaheb", "\"Sh Shakespeare Wallah\"", "Trey Parker and Matt Stone", "The Forever People", "the alluvial plain", "Lord Banquo", "red", "Daedalus", "trout", "Juan Martin Del Potro.", "Tom Hanks", "gym to work out.", "\"St. Patrick's... -and that he had been married one year. His wife", "The Jefferson Airplane", "The Dallas-Fort Worth-Arlington MSA", "COHSE"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6708265692640693}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-5019", "mrqa_hotpotqa-validation-272", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-3286", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-3946", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-4159", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-4646", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-6519", "mrqa_triviaqa-validation-1300", "mrqa_newsqa-validation-1153", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-5822"], "SR": 0.578125, "CSR": 0.4941152597402597, "EFR": 1.0, "Overall": 0.697963676948052}, {"timecode": 77, "before_eval_results": {"predictions": ["\"Odorama\"", "Ryan Babel", "15 congressional district", "Ginger Rogers", "The Lord of the Rings", "Timothy Allen Dick", "St Augustine's Abbey", "My Father", "fennec", "Civic Arena", "VH1's \"100 Greatest Artists of Hard Rock\"", "Lynyrd Skynyrd", "three years", "1995 to 2012", "middleweight", "Tudor City", "Sleeping Beauty", "various registries", "clockwise", "Prada", "1963", "The Bomb Factory", "India", "Albert Park", "Angel Parrish", "The Chamber", "1,467 rooms", "The Nassau Herald", "Christian", "915,000", "Pittsburgh, Pennsylvania", "51,271", "James Brayshaw", "Chris Hemsworth", "FBI", "London", "1951", "Donald Sterling", "American", "Hanna", "Sleepy Hollow", "Westfield Old Orchard", "German", "indigenous flutes, panpipes and drums, as well as stringed instruments", "Aircraft maneuvering", "Lionsgate", "Roy Warren Spencer", "Empire of Japan", "Whoopi Goldberg", "James Gunn", "Unbreakable", "Stephen Foster", "in vitro", "Yuzuru Hanyu", "Queen Victoria and Prince Albert", "Palatine", "Watford", "Brian Smith.", "Symbionese Liberation Army", "Ten South African ministers and the deputy president", "Miles Davis", "doges", "Zionism", "Cincinnati"], "metric_results": {"EM": 0.640625, "QA-F1": 0.738641826923077}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.923076923076923, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4040", "mrqa_hotpotqa-validation-4716", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-546", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-2935", "mrqa_naturalquestions-validation-7226"], "SR": 0.640625, "CSR": 0.49599358974358976, "EFR": 0.9130434782608695, "Overall": 0.6809480386008919}, {"timecode": 78, "before_eval_results": {"predictions": ["Professor Eobard Thawne", "2002 Mitsubishi Lancer OZ Rally", "Canterbury Tales", "Rigg", "Simon Callow", "early - to - mid fourth century", "2.5 %", "IBM", "Speaker of the House of Representatives", "a solitary figure who is not understood by others, but is actually wise", "Claudia Grace Wells", "Jessica Sanders", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Fix You", "Thomas Middleditch", "Barry Bonds", "Teri Garr", "2.45 billion years ago", "1980", "Francisco Pizarro", "Guy Mitchell", "1933", "the market", "the end", "1924", "Danny Veltri", "RMS Titanic", "Buddhism", "Americans who served in the armed forces and as civilians", "Eddie Van Halen", "foreign investors", "49 cents", "between the Eastern Ghats and the Bay of Bengal", "Vicente Fox", "Richard Crispin Armitage", "Edward IV", "December 19, 1971", "Thomas Edison", "four", "Eleanor Roosevelt", "inner edge of the constellation Arm", "13 May 1787", "2017 / 18 Divisional Round game", "Hathi Jr", "a young husband and wife", "Michael Crawford", "Thomas Jefferson", "Mount Sinai", "West Norse sailors", "Ali", "142,907 residents", "Ukraine", "Charlie Cairoli", "wish FM", "American pharmaceutical company", "seven members", "Robert Digges Wimberly Connor", "a U.S. helicopter crashed in northeastern Baghdad as", "dismissed all charges", "November 26,", "C.S. Lewis", "Monocerotis", "the Constitution", "core inflation"], "metric_results": {"EM": 0.65625, "QA-F1": 0.732057635347109}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8181818181818181, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-7217", "mrqa_triviaqa-validation-5097", "mrqa_hotpotqa-validation-4506", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-11100"], "SR": 0.65625, "CSR": 0.4980221518987342, "EFR": 0.9545454545454546, "Overall": 0.6896541462888377}, {"timecode": 79, "before_eval_results": {"predictions": ["a sixth season", "Kenny Chesney", "Buffalo Springfield", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "on the two tablets", "mid-March", "at specific locations, or origins of replication, in the genome", "David Ben - Gurion", "Billy Idol", "Peking", "2016", "pathology", "Niles", "France", "Americans acting under orders", "Gertrude Niesen", "U.S. service members who have died without their remains being identified", "1996", "between 1923 and 1925", "Bob Dylan", "Hans Christian Andersen", "at least two weeks of low mood that is present across most situations", "Nawab Sir Sahibzada Abdul Qayyum", "dress shop", "Jason Lee", "Massachusetts", "Warren Hastings", "Montgomery County", "September 30", "Bob Dylan", "A simple majority", "within a dorsal root ganglion", "December 12, 2017", "around 3.45 billion years ago ( 2.45 Ga ), during the Siderian period, at the beginning of the Proterozoic eon", "1913", "16", "the early 1980s", "across western North Carolina including Asheville, Cashiers and Saluda", "winter solstice", "Don Henley and Glenn Frey", "Cheryl Campbell", "7 correct numbers", "Road / Track ( no `` and '' )", "by capillary action", "Bonnie Aarons", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Bartolomeu Dias", "in the anterolateral corner of the spinal cord", "The outermost layer of human skin", "Paracelsus", "the season five episode `` Aqua ''", "table salt", "albatross", "\"bottom\" number", "Australian", "Danny Elfman", "SpongeBob SquarePants", "Dublin", "a student who admitted to hanging a noose in a campus library,", "colonel in the Rwandan army", "New Jersey", "the Time Warp Again", "The Three Stooges", "Mikado"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7324655583387201}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.8205128205128205, 0.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.35294117647058826, 0.8666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 0.9333333333333333, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.23529411764705885, 0.25000000000000006, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-3132", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-9047", "mrqa_naturalquestions-validation-7239", "mrqa_triviaqa-validation-918", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-393", "mrqa_searchqa-validation-2159", "mrqa_searchqa-validation-7947"], "SR": 0.5625, "CSR": 0.49882812499999996, "EFR": 0.9642857142857143, "Overall": 0.6917633928571428}, {"timecode": 80, "UKR": 0.65234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-368", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4205", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-774", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-603", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13391", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2908", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3201", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-3868", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3559", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-995"], "OKR": 0.828125, "KG": 0.49375, "before_eval_results": {"predictions": ["a full campaign story mode", "Gayla Peevey", "November 27, 2013", "any vessel approaching British waters", "22 November 1914", "students", "Jourdan Miller", "Pakhangba", "Bart Cummings", "David Motl", "Jodha Akbar", "Kristy Swanson", "Clarence L. Tinker", "the homicidal thoughts of a troubled youth", "Castleford", "1969", "the National Assembly", "dispense summary justice", "South Africa", "needle - like teeth", "regulatory site", "Ernest Rutherford", "Roger Federer", "Audrey II", "Walter Pauk", "1999", "Jason Gardiner", "111", "a transformative change of heart ; especially : a spiritual conversion", "Rachel Sarah Bilson", "Mark Jackson", "the Mayor's son", "Procol Harum", "1,149 feet ( 350 m )", "Thawne", "1912", "Bart Millard", "The User State Migration Tool ( USMT )", "3.5 pounds or 2.04 kg", "Bob Dylan", "Qutab Ud - Din - Aibak", "4 January 2011", "the digitization of social systems", "Dan Stevens", "a series of newsreel films depicting multiple alternative realities rather than a novel", "an Islamic shrine", "at Tandi, in Lahaul", "Arnold Schoenberg", "a centre for international trade", "in the 2006 film The Pursuit of Happyness", "79 official PGA Tour events", "table tennis", "Crystal Gayle", "horiz\u014dn kyklos", "Nick McCarthy", "Texas Tech University", "3", "Michelle Rounds", "The Bronx County District Attorneys Office", "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and says \"it should stay that way.\"", "Marat", "whiskey", "Paris", "3"], "metric_results": {"EM": 0.671875, "QA-F1": 0.733625228937729}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-7509", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-10706", "mrqa_naturalquestions-validation-2124", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-4832", "mrqa_newsqa-validation-2655", "mrqa_searchqa-validation-6250", "mrqa_searchqa-validation-15255"], "SR": 0.671875, "CSR": 0.5009645061728395, "EFR": 1.0, "Overall": 0.6950366512345678}, {"timecode": 81, "before_eval_results": {"predictions": ["Tessa Virtue", "eight", "December 1, 2017", "Baker, California, USA", "white rapper B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "Kareem Abdul - Jabbar", "Roxette", "all transmissions", "one of Jesus'disciples", "Hyderabad", "1924", "9.7 m ( 31.82 ft )", "Joanna Moskawa", "Tara / Ghost of Christmas Past", "runoff will usually occur unless there is some physical barrier", "Hellenism", "Zoe McLellan", "Portugal", "the American Civil War", "Jenny", "Alan Shearer", "Kate Walsh", "France's Legislative Assembly", "Sunday evenings", "the com TLD", "around the Brewster family, descended from the Mayflower, but now composed of insane homicidal psychotics", "April 12, 2017", "Hercules", "New Zealand", "the Internal Revenue Service", "Tracy McConnell", "the courts", "early 2014", "marks locations", "Janie Crawford, an African - American woman in her early forties", "a pop ballad", "T - Bone Walker", "in the thylakoid lumen", "Angus Young", "Mickey Mantle", "Norman Pritchard", "Conrad Lewis", "Exodus 20 : 7", "Startup neutron source", "3", "Warren Zevon", "Elvis Presley", "September", "nobiliary particle indicating a noble patrilineality", "about 1 nautical mile ( 2 km ) off - coast from Piraeus", "through the semilunar pulmonary valve ( one for each lung ), which branch into smaller pulmonary arteries that spread throughout the lungs", "Belgium", "Italy", "David Lodge", "The Nikki Giovanni Poetry Collection", "Indian film playback singer, director, writer and producer", "Baudot code", "crude oil", "President Robert Mugabe", "Three aid workers", "The Devil\\'s Dictionary", "Johann Wolfgang von Goethe", "Band of Brothers", "Rat"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7156021495865246}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true], "QA-F1": [0.4, 1.0, 1.0, 0.8, 0.1, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.625, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-3175", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-367", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3947", "mrqa_searchqa-validation-11427"], "SR": 0.609375, "CSR": 0.5022865853658536, "EFR": 0.92, "Overall": 0.6793010670731707}, {"timecode": 82, "before_eval_results": {"predictions": ["Sunni", "The New York Mets", "(Thomas) Jefferson", "sheep", "sweatshirt", "Cram", "Alice", "Basques", "Radames", "computer science", "The Three Musketeers", "The Real Thing", "Marsha Hunt", "(Charlotte) Renoir", "Zhou Enlai", "a fox", "Buddhism", "Domestic Boxer", "Qwerty", "the vest", "a heart", "Libya", "meter", "Psycho", "phloem", "Mabel's Lovers", "Carnation", "the Central Intelligence Agency", "a turquoise", "a ballistic missile submarine", "(Thomas) Watson", "1 million lbs.", "Vaslav Nijinsky", "(William) Wordsworth", "mercury", "( Sidney) Sheldon", "an observer", "Charley", "The Weekly World", "the Aleutian Islands", "the House of Representatives", "Roman Catholicism", "Jor-El", "Devo", "eponym", "M. R. Tolkien", "embalming", "China", "Let it Ride", "Schwarzenegger", "Rachel Carson", "1991", "1994", "Allison Janney", "Salyut 1", "Bobby Vinton", "fingers", "conservative", "Jennifer Aniston", "Otto Eduard Leopold", "Russia", "Newcastle", "misdemeanor assault", "Sim\u00f3n Jos\u00e9 Antonio de la Sant\u00edsima Trinidad de Bol\u00edvar y Palacios"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6312499999999999}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.13333333333333333]}}, "before_error_ids": ["mrqa_searchqa-validation-6805", "mrqa_searchqa-validation-14721", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-8636", "mrqa_searchqa-validation-12988", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-3155", "mrqa_searchqa-validation-10832", "mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-15550", "mrqa_searchqa-validation-13324", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-9281", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-4918", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-3253", "mrqa_hotpotqa-validation-2174"], "SR": 0.546875, "CSR": 0.5028237951807228, "EFR": 1.0, "Overall": 0.6954085090361446}, {"timecode": 83, "before_eval_results": {"predictions": ["he's been drawn to since he was a boy.", "the radical Islamist militia that controls the city", "1981,", "The presiding judge Shemsu Sirgaga", "Tillakaratne Dilshan scored his sixth Test century", "\"People have lost their homes,", "a skilled hacker", "near the George Washington Bridge,", "Greeley, Colorado,", "in the jungle", "Nirvana", "in Galveston, Texas,", "United York-based Human Rights Watch", "34", "The Tennis Channel", "Sunday,", "Peshawar", "Saturday", "four", "Islamabad", "$273 million", "\"Up,\"", "President Obama", "California, Texas and Florida,", "the wars in Iraq and Afghanistan", "Piers Morgan,", "Henrik Stenson", "March 24,", "evidence of collusion between the colossus of the North [the United States] and the col Colossus of the South [Brazil),\"", "shelling of the compound", "1918", "supply vessel Damon Bankston", "scored a hat-trick", "Matamoros, Mexico,", "environmental videos", "The Sopranos", "Both men were hospitalized and expected to survive,", "housing, business and infrastructure repairs,", "the grunge craze", "it -- you know -- black is beautiful,\"", "Tukel", "desperately wanted to make her mother proud.", "West Palm Beach, Florida,", "\"His treatment met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "Hanin Zoabi,", "maintain an \"aesthetic environment\" and ensure public safety,", "Kerstin Fritzl,", "to clean up Washington State's decommissioned Hanford nuclear site,", "in the nursing home she prefers.", "Robert Kimmitt", "21,", "10 June 1940", "2002", "c. 1000 AD", "Nanak", "Tacitus", "Dry Ice", "Arsenal", "2011", "German", "a bowl game", "Baikal", "Beethoven", "France"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6145119790340379}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-1188", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1957", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1234", "mrqa_hotpotqa-validation-3738", "mrqa_hotpotqa-validation-3873", "mrqa_searchqa-validation-9555", "mrqa_searchqa-validation-15010"], "SR": 0.53125, "CSR": 0.5031622023809523, "EFR": 1.0, "Overall": 0.6954761904761905}, {"timecode": 84, "before_eval_results": {"predictions": ["off Somalia's coast.", "30-minute", "Suzanne Hubbard, director of the Division of Adult Institutions,", "heroin and morphine were trafficked out of the country in 2007,", "Chris Robinson,", "Iran of trying to build nuclear bombs,", "the coalition", "the Revolutionary Armed Forces of Colombia, better known as FARC,", "Scarlett Keeling", "14", "\"extremely weak\" and said he weighs barely 100 pounds in a court document filed this week, but he walked on his own during the 45 minutes he was at the ceremony.", "Larry King?\"", "strife in Somalia,", "U.S. Army as a German citizen,", "the United Nations", "bullet-riddled body", "used-luxury market", "I was born in Nizhny Novgorod", "1616.", "the Bronx.", "Rolling Stone", "energy-efficient", "The pilot,", "Iggy Pop formed a blues band called the Prime Movers.", "natural disasters", "in the prestigious museum", "Pakistan from Afghanistan,", "Matthew Fisher,", "English", "carnival-like sounds of the steam-whistle calliope.", "1825", "one-shot", "36", "Tim Clark, Matt Kuchar and Bubba Watson", "the Sadr City and Adhamiya districts of Baghdad City,", "Bill,", "\"Empire of the Sun\"", "broken pelvis,", "Gaslight Theater.", "Thabo Mbeki, the mediator in the talks,", "gun charges,", "Abhisit Vejjajiva", "the security breach", "63", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "clogs", "AbdulMutallab,", "Dr. Maria Siemionow,", "his father", "its intention to set up headquarters in Dublin.", "India", "Atlanta, Georgia", "Matthew Gregory Wise", "the Pir Panjal Range in Jammu and Kashmir", "In God We Trust", "Little Dorrit", "Prussian Landsturm", "Australian", "iTunes", "Portal A Interactive", "Empire of the Sun", "the Pocono Mountains", "the mitral valve", "Wonderwall Music"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6125554078014184}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1702127659574468, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-3364", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-3393", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-501", "mrqa_triviaqa-validation-1613", "mrqa_searchqa-validation-5505"], "SR": 0.578125, "CSR": 0.5040441176470588, "EFR": 0.9629629629629629, "Overall": 0.6882451661220043}, {"timecode": 85, "before_eval_results": {"predictions": ["more than 2.5 million", "Mitt Romney", "Michael Schumacher", "25", "Almost all", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "$150 billion", "about 5 percent", "dismissed all charges Wednesday night and ordered the release of the four men", "religious", "in the killings this month of three people with ties to the U.S. Consulate in Ciudad Juarez, Mexico,", "Strategic Arms Reduction Treaty and nonproliferation.", "The pun-loving fashion designer", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film", "eight.", "Aniston, Demi Moore and Alicia Keys", "Karthik Rajaram,", "at the end of the season.", "Marxist guerrillas", "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "Sen. Joe Lieberman, I-Connecticut,", "death", "1995", "\"explosion of violence.\"", "The Rev. Alberto Cutie -- sometimes called \"Father Oprah\" because of the advice he gave on Spanish-language media", "D.J. Knight of Pearlman, Texas, decided to ride", "leaky valve", "11 to 12 year old", "a sixth person, described as an \"associate\" of the family,", "Six members of Zoe's Ark", "in a 1,700 year old Roman mosaic entitled Chamber of the Ten Maidens.", "into some of the most hostile war zones,", "More than 22 million people in sub-Saharan Africa are infected with HIV,", "AbdulMutallab", "U.S. 93 in White Hills, Arizona,", "four other people", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "Elena Kagan", "the Golden Gate Yacht Club of San Francisco", "\"one of the most magnificent expressions of freedom and free enterprise in history\" and", "Philippines", "Monday and Tuesday", "in a post-Hosni Mubarak era", "Manuel Mejia Munera was a drug lord with ties to paramilitary groups,", "bank robber John Dillinger,", "Steven Gerrard", "former Himalayan kingdom", "The pilot,", "Sunday", "Tuesday night", "Missouri River", "supervillains", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "Dengue fever", "jellyfish", "woman", "Michael Cimino", "ExCeL Exhibition Centre", "the sixth novel", "John Stuart Mill", "Smith", "grease", "Lorman, Mississippi"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5803520698051947}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false], "QA-F1": [0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5333333333333333, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.625, 0.0, 0.0, 1.0, 0.5, 0.5, 0.1, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-860", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-3804", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1267", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-4138", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-2040", "mrqa_naturalquestions-validation-8408", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5139", "mrqa_searchqa-validation-16089", "mrqa_hotpotqa-validation-5644"], "SR": 0.453125, "CSR": 0.5034520348837209, "EFR": 1.0, "Overall": 0.6955341569767441}, {"timecode": 86, "before_eval_results": {"predictions": ["Ginuwine", "The Stranger", "fluorescent lights", "Pat Sajak", "nuclear", "Hill Street Blues", "Jason", "nests", "cremation", "Mahre", "Mondrian", "The New York Times", "Elizabeth II", "The Granite State", "Montana", "the Amstel River", "The New Yorker", "home", "a crystal globe", "Medicaid", "a Scotch Eggs", "the Silk Road", "a resurfacer", "Russia", "(Vijay) Singh", "tentacles", "Lilo", "bones", "the Epstein-Barr virus", "a fish", "Clarksdale Mississippi", "words", "The Kiss", "Syria", "cheese", "Falcon Crest", "Espresso", "Graceland", "pearls", "Tulip mania", "Manhattan", "flagella", "Let's Make a Deal", "Roosevelt", "Garfield", "blackjack", "The Lorax", "Anthony Perkins", "Chile", "Tusk", "John Roberts", "Hathi Jr", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Harlem River", "Cornwall", "Lois", "Gianni Versace", "Lord's Resistance Army", "100 million copies", "\"The Simpsons\"' thirteenth season", "in the country's largest city of Karachi.", "Reid's dismissal.", "Donald Duck", "45 minutes,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7020630411255411}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.8, 1.0, 0.5454545454545454, 0.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-4105", "mrqa_searchqa-validation-15302", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-5232", "mrqa_searchqa-validation-4901", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-16817", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-10383", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-10057", "mrqa_searchqa-validation-12633", "mrqa_triviaqa-validation-5322", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-3820", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-4078"], "SR": 0.609375, "CSR": 0.5046695402298851, "EFR": 1.0, "Overall": 0.695777658045977}, {"timecode": 87, "before_eval_results": {"predictions": ["Marxist guerrillas", "Marie-Therese Walter.", "diagnosed with skin cancer.", "two", "the \" Michoacan Family,\"", "435", "Bill Stanton", "Don Draper", "Kim Il Sung", "an upper respiratory infection,\"", "the public endorsement", "two tickets to Italy", "the iReport form", "Expedia", "early 2008,", "Apple Inc.", "breast cancer.", "was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "sought Cain's help finding a job", "A member of the group dubbed the \"Jena 6\"", "Democratic", "spend $60 billion on America's infrastructure.", "1937,", "at least 12 months.", "David McKenzie", "105-year history.", "2.5 million copies,", "27-year-old", "Sen. Barack Obama", "braces.", "Sporting Lisbon", "10 percent", "304,000", "April.", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "a shortfall in their pension fund and disagreements on some work rule issues.", "Arsene Wenger", "a one-shot victory in the Bob Hope Classic on the final hole", "The oceans", "Police", "Virgin America", "\"disagreements\" with the Port Authority of New York and New Jersey,", "France's reputation as rugby's Jekyll and Hyde team", "Afghanistan's restive provinces", "mental health and recovery.", "President Obama", "Turkey,", "40-year-old", "he tried to throw a petrol bomb at the officers,", "Tehran,", "The father of Haleigh Cummings,", "Matthew Gregory Wise", "Asset = Liabilities + Equity", "during the American Civil War", "edward learners", "polly", "Alessandro Giuseppe Antonio Anastasio Volta", "2011", "baeocystin", "Sim Theme Park", "the pronghorn", "The Old Man and the Sea", "Paul Revere", "24"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6679726957070707}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9696969696969697, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-238", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-2741", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-3767", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-7957", "mrqa_triviaqa-validation-7246", "mrqa_triviaqa-validation-5210", "mrqa_hotpotqa-validation-1473", "mrqa_searchqa-validation-12578"], "SR": 0.578125, "CSR": 0.5055042613636364, "EFR": 0.9629629629629629, "Overall": 0.6885371948653198}, {"timecode": 88, "before_eval_results": {"predictions": ["an empty water bottle down the touchline", "Venezuela", "2008,", "12 hours", "Abbey Road", "Thailand", "17", "doesn't get along with her co-star Kristin Davis,", "Sen. Arlen Specter", "nuclear program.", "checkposts and military camps", "1-1 draw", "hiring of hundreds of foreign workers", "Museum-worthy pieces", "job training", "whether he should be charged with a crime,", "Shanghai", "Sen. Barack Obama", "dogs are \"active athletes,\"", "a point for Bayern Munich", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "five", "Jackson sitting in Renaissance-era clothes and holding a book.", "the governor", "1994,", "Bryant Purvis", "billions of dollars", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "The opposition group,", "A witness", "Burhanuddin Rabbani,", "4,000", "The UNHCR", "\"Three Little Beers,\"", "Ma Khin Khin Leh,", "$40 and a loaf of bread.", "Wednesday.", "Sunday", "Cologne's archive building", "15-year-old's", "10", "central business district of Bangkok", "the UK", "\"Empire of the Sun,\"", "21-year-old", "the two remaining crew members from the helicopter,", "President Robert Mugabe", "the murky waters of the Adelaide River,", "She leaves her husband of 20 years, Asif Ali Zardari, two daughters and a son.", "Pakistani officials,", "stepped into the museum with a rifle and began firing.", "Action Jackson", "Icarus", "Masha Skorobogatov", "the Atlantic", "Toyota", "peter nixon", "Bolshoi Theatre", "Bardot", "the Austin E. Knowlton School of Architecture", "the Enterprise", "soccer", "Christine", "The Twilight Zone"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6953484953484954}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.4, 0.8918918918918919, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-2145", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-66", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-2940", "mrqa_triviaqa-validation-396", "mrqa_hotpotqa-validation-2716"], "SR": 0.59375, "CSR": 0.506495786516854, "EFR": 0.9615384615384616, "Overall": 0.6884505996110631}, {"timecode": 89, "before_eval_results": {"predictions": ["Newcastle", "a crocodile", "Festival Foods", "Michael Jackson", "I, the chief executive officer, the one on the very top,", "Mombasa, Kenya,", "Osama bin Laden", "11 healthy eggs", "The Monroe County, Illinois, state's attorney's office", "3 thousand", "$1.5 million", "Basilan", "American Civil Liberties Union", "Boys And Girls Alone", "Sen. Barack Obama", "11", "three", "The remaining 240 patients will be taken to hospitals in other provinces", "iPod Touch", "August 19, 2007.", "a U.S. military helicopter", "Diego Milito's", "\"People have lost their homes, their jobs, their hope,\"", "19-year-old", "a satellite.", "CNN's \"Larry King Live.\"", "tells Larry King her son has strong values.", "United States", "Nafees Syed", "75 percent", "was found,", "Alwin Landry's supply vessel Damon Bankston", "Egypt.", "200", "later apologized,", "Little Rock", "Kim Clijsters", "Brett Cummins,", "11", "India", "6-2 6-1", "a bronze medal in the women's figure skating final,", "Nine out of 10 children", "general astonishment", "end of the season.", "social networking", "Samson D'Souza,", "Stoke City.", "KBR", "5,600", "led the weekend box office, grossing $55.7 million", "Eric Clapton", "the Coriolis force", "September 1972", "Fram Strait", "Allah", "five", "Kansas", "Salma Hayek", "Stalybridge Celtic", "The Borrowers", "crossword", "Thomas Merton", "John Denver"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6771205357142858}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.25, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.5, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-64", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-955", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-2978", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-3159", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-7242", "mrqa_triviaqa-validation-5995", "mrqa_triviaqa-validation-674", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-905", "mrqa_hotpotqa-validation-2156", "mrqa_searchqa-validation-16474"], "SR": 0.578125, "CSR": 0.5072916666666667, "EFR": 1.0, "Overall": 0.6963020833333333}, {"timecode": 90, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3026", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7065", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1821", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-204", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-12933", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14609", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-15669", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16185", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2909", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3002", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4105", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-9176", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-995"], "OKR": 0.794921875, "KG": 0.490625, "before_eval_results": {"predictions": ["southeast of the city", "Brazil", "restricted naturalization to `` free white persons '' of `` good moral character ''", "Palm Sunday celebrations", "A dragon he later names Saphira hatches from the stone, which was really an egg", "Part 2", "1924", "Great G minor symphony '',", "1979 -- 80 season", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "Japan", "MacFarlane", "John Cooper Clarke", "3,000", "United States,", "1992", "c. 3000 BC", "gas exchange between, ultimately, the interior of the cell ( s ) and the external environment", "August Darnell", "French Union", "Florida", "The sacroiliac joint or SI joint ( SIJ )", "by the early 1980s", "British", "H CO ( equivalently OC ( OH ) )", "the Pir Panjal Range", "Baker, California", "1983", "# 4 School of Public Health in the country", "Toronto, Ontario", "`` Heroes and Villains ''", "Eddie Murphy", "2003", "Bulgaria", "Francis Hutcheson", "the English", "to bring, and \u03bd\u03af\u03ba\u03b7, n\u00edk\u00ea, `` victory '', i.e. `` she who brings victory ''", "Dido", "the Indians", "October 27, 1904", "54 Mbit / s", "M\u00e1xima Zorreguieta Cerruti", "Richie Cunningham", "June 25, 1938", "Mike Czerwien", "his cousin D\u00e1in", "warplanes", "southeastern coast of the Commonwealth of Virginia", "2018", "Anthony Hopkins", "Peter Andrew Beardsley MBE", "john Ballard", "cheese", "cuticle", "Harlow Cuadra and Joseph Kerekes", "Radio Disney", "Seretse Khama", "U.S. Embassy", "United States, NATO member states, Russia and India", "off Somalia's coast.", "Borat", "Billy Bathgate", "Spider-Man", "1996 PGA Championship"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6753743131868131}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true], "QA-F1": [0.0, 0.5, 1.0, 0.4615384615384615, 0.15384615384615385, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.28571428571428575, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.25, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.07999999999999999, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-1784", "mrqa_triviaqa-validation-6482", "mrqa_hotpotqa-validation-5702", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2078", "mrqa_searchqa-validation-8850"], "SR": 0.546875, "CSR": 0.5077266483516483, "EFR": 0.896551724137931, "Overall": 0.6785900494979159}, {"timecode": 91, "before_eval_results": {"predictions": ["Tom Brady", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "Lori McKenna", "the location for the 1997 miniseries", "the order Octopoda", "passing of the year", "2007", "parthenogenic", "1908", "Sir Ronald Ross", "fascia surrounding skeletal muscle", "pigs", "Montreal", "Pyeongchang County, South Korea", "Set six months after Kratos killed his wife and child", "The Hudson River", "chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Colon Street", "Justin Timberlake", "New York City", "Rick", "Ariel Winter", "Shenzi", "euro", "September 1959", "1890s", "the Alamodome and city of San Antonio", "John Smith", "A high school player must wait at least a year to be eligible for selection", "Microfilaments", "the southeastern United States of unresolved taxonomic identity", "the'daily living'or metabolic phase of the cell, in which the cell obtains nutrients and metabolizes them, grows, reads its DNA, conducts other `` normal '' cell functions", "on the Isle of FERNANDO 'S!, a fictional location based in Puerto de la Cruz, Tenerife", "2000", "September 19 - 22, 2017", "the pilot", "as a fully centralized service with individual user accounts focused on one - on - one conversations", "alveolar bone", "lead", "1983 -- 84", "2016", "deceased - donor ( formerly known as cadaveric )", "red", "A turlough, or turlach", "lighter", "loosely on Eminem", "warm and is considered to be the most comfortable climatic conditions of the year", "Microsoft Windows", "B.R. Ambedkar", "1975", "Article 300 - A", "birmingham", "denmark strait", "Bangladesh", "AVN Adult Entertainment Expo (AEE)", "Henry Lau", "Mark Andrew Brayshaw", "Michael Jackson", "\"E! News\" on Tuesday.", "Susan Boyle", "the &quot", "King Amis", "the ukulele", "Emily Dickinson"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6021990938520543}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 0.09090909090909093, 1.0, 0.0, 0.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.33333333333333337, 0.2857142857142857, 1.0, 0.0, 1.0, 0.6, 0.0, 0.5263157894736842, 0.0, 1.0, 0.0, 0.10526315789473682, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-7107", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-7133", "mrqa_naturalquestions-validation-4818", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-10250", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2242", "mrqa_triviaqa-validation-1064", "mrqa_hotpotqa-validation-2046", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-4130", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-15474"], "SR": 0.4375, "CSR": 0.5069633152173914, "EFR": 0.9722222222222222, "Overall": 0.6935714824879227}, {"timecode": 92, "before_eval_results": {"predictions": ["Al Gore", "Edna St. Vincent Millay", "Augusta", "My Therapist", "\"The Tyger\"", "Elihu Root", "helix", "the Tower of London", "Ho Chi Minh", "Saudi Arabia", "fief", "Billy Joel", "Maurice Ravel", "Madagascar", "Cleveland", "the Amadeus Quartet", "Nesquik", "Jack Johnson", "an insurance company", "Augusta", "the longest encore", "King Henry VIII", "Ellen", "Athena", "the Little Mermaid", "an allergic situation eating malady liver-colored comatoseness and for... in CKD cats lasix plus in arlington virginia", "the New Deal", "gravity", "Mystery Science Theater 3000", "River Thames", "Washington", "(Harry) Houdini", "13", "the Ruby Gemstone", "fibreboard", "Heather Mills", "Robert Louis Stevenson", "St Crispin of Viterbo", "yellow", "Opal", "The Benchwarmers", "the Supreme Court", "Sherlock Holmes", "gallows", "John Edwards", "the King of Hearts", "shiatsu", "Noah\\'s Ark", "Nitrogen", "Porgy and Bess", "Mahatma Gandhi", "pigs", "President of India", "Asuka", "Thwaites", "denmark", "Alexei Kosygin", "Shohola Falls", "Francis Egerton, 3rd Duke of Bridgewater", "\"The Guest\"", "Samson D'Souza,", "ancient Greek site of Olympia", "murder in the beating death of", "Atlanta"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7353422619047618}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-15264", "mrqa_searchqa-validation-8474", "mrqa_searchqa-validation-1956", "mrqa_searchqa-validation-10412", "mrqa_searchqa-validation-11958", "mrqa_searchqa-validation-5876", "mrqa_searchqa-validation-3917", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-824", "mrqa_searchqa-validation-15364", "mrqa_searchqa-validation-6564", "mrqa_searchqa-validation-10029", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-1626", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-117", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-1239", "mrqa_newsqa-validation-1159"], "SR": 0.609375, "CSR": 0.5080645161290323, "EFR": 0.84, "Overall": 0.6673472782258065}, {"timecode": 93, "before_eval_results": {"predictions": ["Julie Stichbury", "the heart", "$72", "13,000 astronomical units ( 0.21 ly )", "Indirect rule", "Spencer Treat Clark", "Dan Stevens", "a Norwegian town", "Lager", "in September 1993", "1916", "Randy VanWarmer", "the sixth series", "Dr. Rajendra Prasad", "Gary Grimes", "all transmissions", "April 2018", "Sarah Silverman", "in vitro fertilization", "( 55 -- 69 % ) & 4 ( 40 -- 54 % )", "comic", "Ace", "1994", "the Juan de Fuca Plate", "1994", "U.S. Bank Stadium", "the economy", "her abusive husband", "introverted Thinking ( Te )", "San Francisco 49ers", "the world's second most populous country after the People's Republic of China", "capillary action", "the Charbagh structure", "Cee - Lo", "a premalignant flat ( or sessile ) lesion of the colon", "Mitch Murray", "northern Arizona", "Podujana Peramuna, led by former president Mahinda Rajapaksa", "native to Asia", "Teddy Randazzo", "in rocks and minerals", "NFL coaches, general managers, and scouts", "1961", "Egypt", "Angel Benitez", "seven", "London Symphony Orchestra and London Philharmonic", "Lykan Hypersport", "Turner Layton", "dromedary", "in the 18th century in the United Kingdom", "yachtsmen", "Tony Hart", "Velazquez", "Kim Jong-hyun", "Allies of World War I", "1998", "Jamaleldine", "Akio Toyoda", "curfew", "Liechtenstein", "Jordan", "an XML-based format for content distribution", "Cate Blanchett"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6083928571428572}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4799999999999999, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-684", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-5687", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-4569", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-4316", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-250", "mrqa_searchqa-validation-13551", "mrqa_searchqa-validation-14342", "mrqa_triviaqa-validation-5928"], "SR": 0.484375, "CSR": 0.5078125, "EFR": 0.9696969696969697, "Overall": 0.6932362689393939}, {"timecode": 94, "before_eval_results": {"predictions": ["L.K. Advani", "an adopted daughter of Thanos", "October 20, 1977", "the Pir Panjal Range in Jammu and Kashmir", "on the Spencer jet", "1038", "Jughead Jones", "the student's transition from the study of preclinical to clinical health sciences", "Justin Timberlake", "Mahatma Gandhi", "Bhupendranath Dutt", "Bruce Mackinnon as the mummy", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product", "1599", "Leo Arnaud", "Mangal Pandey of the 34th BNI", "an unmasked and redeemed Anakin Skywalker ( formerly Darth Vader )", "for the red - bed country of its watershed", "New England ( Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont )", "Nick Kroll", "Malina Weissman", "1980", "Gupta Empire", "al - Mamlakah al - \u02bbArab\u012byah", "Tuesday, 16 February", "Gary Speed", "September 2000", "Middle Eastern alchemy", "the inferior thoracic border", "a carnivore", "1960", "Thunder Road", "James Arthur", "save, rescue, savior", "Frankel", "Nigel Lythgoe, Mia Michaels, and Adam Shankman", "1,350 at the 2010 census", "1850", "1877", "as early as January 3, and as late as February 12", "Kida", "Nearer, My God, to Thee ''", "August 22, 1980", "Nikita Khrushchev", "at the center of the Northern Hemisphere", "listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "macOS High Sierra", "King Willem - Alexander", "Rococo - era France", "Atlanta", "to collect menstrual flow", "group of nerve cell bodies located in the autonomic nervous system", "the house sparrow", "Gary Barlow", "Cold Spring", "Sam Raimi", "lambics", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and", "1-1 draw", "Wednesday", "a gold rush", "a clarinet", "the Three Bs", "Taoism"], "metric_results": {"EM": 0.5, "QA-F1": 0.6357130138380138}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true], "QA-F1": [0.14814814814814814, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428572, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.7692307692307693, 0.4, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4444444444444445, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 0.5, 1.0, 1.0, 0.30769230769230765, 0.0, 1.0, 0.0, 0.5, 1.0, 0.923076923076923, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8950", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-8217", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-234", "mrqa_triviaqa-validation-5551", "mrqa_triviaqa-validation-2705", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3591", "mrqa_newsqa-validation-3200", "mrqa_searchqa-validation-7128", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-13034"], "SR": 0.5, "CSR": 0.5077302631578947, "EFR": 0.90625, "Overall": 0.6805304276315789}, {"timecode": 95, "before_eval_results": {"predictions": ["(Julius) Caesar", "the liver", "Kristi Yamaguchi", "the Phantom of the Opera", "Otis Elevator Company", "William Harvey", "seabirds", "chicken pot pie", "Bull", "the Dred Scott decision", "Birmingham", "trousse", "a constitution", "rice", "a cad", "Nixon", "Canada", "Ellis Island", "brilliant", "Marie Antoinette", "France", "Thomas Stearns Eliot", "the referee", "Dustin Hoffman", "(James) Cook", "Avengers", "Frank Sinatra", "Handel", "Agatha Christie", "Ponce de Len", "Mr. Rogers", "silver", "the Metropolitan Museum of Art", "Good Will Hunting", "Lebanon", "Puccini", "Arby\\'s", "a girl", "Wayne Gretzky", "the stock market crash", "the Festival of Weeks", "Herod the Great", "Athens", "a rattlesnake", "World War II", "World War I", "the Bridges of Madison County", "(John) Jay", "\"Bojangles\" Robinson", "Jerome Kern", "Phoenix", "behaves as an antagonist ( a substance that binds to a receptor but does not activate and can block the activity of other agonists )", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "April 26, 2005", "India", "Lady Susan, and the Watsons", "Poland", "1945", "Bishop's Stortford", "My Cat from Hell", "Ricardo Valles de la Rosa,", "President Sheikh Sharif Sheikh Ahmed", "Thursday.", "\"Thrilla in Manila\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.6762648809523809}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.9142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-13671", "mrqa_searchqa-validation-15121", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-13237", "mrqa_searchqa-validation-16186", "mrqa_searchqa-validation-13627", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-6320", "mrqa_searchqa-validation-11992", "mrqa_searchqa-validation-5579", "mrqa_searchqa-validation-11176", "mrqa_searchqa-validation-14498", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-15715", "mrqa_searchqa-validation-4538", "mrqa_searchqa-validation-7822", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-11504", "mrqa_searchqa-validation-15916", "mrqa_searchqa-validation-6172", "mrqa_naturalquestions-validation-9749", "mrqa_triviaqa-validation-2151", "mrqa_newsqa-validation-3181", "mrqa_triviaqa-validation-7401"], "SR": 0.5625, "CSR": 0.50830078125, "EFR": 1.0, "Overall": 0.6993945312500001}, {"timecode": 96, "before_eval_results": {"predictions": ["weather", "Ted Kennedy", "Elizabeth I", "jack russell", "ruda", "Fred Trueman", "Charlie Chan", "the Beatles", "Mazowieckie prov.", "Pygmalion", "The Green Mile", "Paul Anka", "a Brat Pack", "Artemis", "mase", "March", "Space Oddity", "Mikhail S. Gorbachev", "yellow", "manila", "dark", "rhododendron", "when we were kings", "rugby", "the Blind Beggar", "manila", "milk", "fish", "a Bachelor of Science (B.S. or B.Sc.) degree", "Help!", "antelope", "manila", "Little Dorrit", "Yeats", "Monty Python's Spamalot", "a wise black panther", "horseshoes", "horses", "kendo", "cabbage", "botulism", "Small Faces", "St. Louis", "a paddington", "Benfica", "30", "left-wing political movement", "The Iron Duke", "Virginia", "jocky Wilson", "Bubba Watson, Jr.", "the Ramones", "brothers Norris and Ross McWhirter", "its population, serving staggered terms of six years", "Aamir Khan", "1945", "Delacorte Press", "clogs", "Rany Freeman,", "peppermint oil, soluble fiber, and antispasmodic drugs", "Mercury", "the small intestine", "Islamic Republic", "Gene Wilder"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6480654761904763}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.14285714285714288, 0.5, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4461", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-5753", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-3030", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7416", "mrqa_triviaqa-validation-7093", "mrqa_triviaqa-validation-2018", "mrqa_triviaqa-validation-5547", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-3848", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-98", "mrqa_searchqa-validation-1848"], "SR": 0.578125, "CSR": 0.509020618556701, "EFR": 1.0, "Overall": 0.6995384987113402}, {"timecode": 97, "before_eval_results": {"predictions": ["julius caesar", "Mary Pickford", "the bark", "Roux", "the Netherlands", "pfeffernuesse", "Vienna", "Lake Champlain", "Eight", "an ex-1", "Slavic", "Deimos", "James Jeffords", "Space Cadet", "Fen-phen", "Candice Bergen", "Andrew Johnson", "Venice", "Hairspray", "Tina Brown", "the Lone Ranger", "a fuel cell", "OK Go", "Ruben Amaro", "Bay of Fundy", "Savannah", "Turlogh Dubh O'Brien", "Pearl Jam", "chisel", "Gaius", "Bob Dylan", "Montessori", "microwave", "Peter Shaffer", "a turquoise", "Casablanca", "White", "a potato", "Red Hot Chili Peppers", "Woody Guthrie", "Thyroid", "Plutarch", "Hephaestus", "Iraq", "Whatchamacallit", "Tuscaloosa", "degas", "Mountain Dew", "a blank slate", "pomegranate", "a mortar", "December 1, 2017", "Glenn Close", "diastema", "nairobi", "slow March", "Robinson Crusoe", "Galleria Vittorio Emanuele II", "51st", "Araminta Ross", "Songs", "football", "three", "Sunday,"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7260416666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-9329", "mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-12001", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-14503", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-9453", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-12929", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-11766", "mrqa_naturalquestions-validation-3553", "mrqa_triviaqa-validation-1434", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2386"], "SR": 0.671875, "CSR": 0.5106823979591837, "EFR": 1.0, "Overall": 0.6998708545918368}, {"timecode": 98, "before_eval_results": {"predictions": ["the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah", "In 1804", "1968", "Guwahati", "parthenogenesis", "2018", "July 8, 1997", "Cheap Trick", "Massachusetts", "the base of the right ventricle", "the lumbar cistern", "Kingsford, Michigan", "biological taxonomy, a domain ( Latin : regio ), also superkingdom or empire, is the highest taxonomic rank of organisms", "During Hanna's recovery masquerade celebration", "as - yet - unknown purpose", "the sixth season", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "the French ceded Canada in accordance with the Treaty of Paris ( 1763 )", "Matt Monro", "distant objects show a larger parallax than farther objects when observed from different positions, so Parallax can be used to determine distances", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "photoelectric", "Kyla Coleman", "Ben Faulks", "seven", "retina", "Roger Nichols and Paul Williams", "during the 1890s Klondike Gold Rush", "Ali", "71 -- 74 \u00b0 C ( 160 -- 165 \u00b0 F )", "US President John F. Kennedy", "1960 Summer Olympics in Rome", "John J. Flanagan", "France", "terrier", "Americans who served in the armed forces and as civilians during World War II", "Melissa Disney", "H CO ( equivalently OC ( OH ) )", "Tatsumi", "Pradyumna", "a lower index of refraction, typically a cladding of a different glass, or plastic", "Speaker of the House of Representatives", "the homicidal thoughts of a troubled youth", "Wales and Yorkshire", "the Han dynasty", "UNICEF's global programing", "The Maidstone Studios in Maidstone, Kent", "between 3.9 and 5.5 mL / L ( 70 to 100 mg / dL )", "adenosine diphosphate", "Graham McTavish", "Orangeville, Ontario", "France", "Edward lear", "photographer and filmmaker", "Attorney General and as Lord Chancellor of England", "A1 Recordings", "a churro", "at least 300", "2002", "U.S. Secretary of State Hillary Clinton,", "a topaz", "Wolfgang Amadeus Mozart", "cuddle", "New York City Mayor Michael Bloomberg"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7205188041125541}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false], "QA-F1": [0.14285714285714288, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.0, 0.0, 0.7272727272727272, 1.0, 0.8, 0.8, 0.18181818181818182, 1.0, 0.0, 0.9142857142857143, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.8571428571428572, 0.8, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-2960", "mrqa_naturalquestions-validation-2966", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-10260", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-3714", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-2380", "mrqa_triviaqa-validation-49", "mrqa_hotpotqa-validation-5770", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2212"], "SR": 0.546875, "CSR": 0.5110479797979798, "EFR": 0.8620689655172413, "Overall": 0.6723577640630443}, {"timecode": 99, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5270", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3026", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-540", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7065", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-9123", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1821", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-204", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10355", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-10947", "mrqa_searchqa-validation-11027", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14609", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-15291", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15669", "mrqa_searchqa-validation-15703", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15980", "mrqa_searchqa-validation-16185", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2466", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2909", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3002", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4105", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-5806", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6237", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-7899", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-976", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1414", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-460", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7069", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-995"], "OKR": 0.83203125, "KG": 0.53671875, "before_eval_results": {"predictions": ["its absolute temperature", "optical smoke detector", "May 5, 1904", "Western cultures", "March 31, 2013", "October 27, 1904", "the courts", "in April 2011", "Pakhangba", "4 January 2011", "pulmonary heart disease ( cor pulmonale ), which is usually caused by difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "Billie `` The Blue Bear '', a German ex-prostitute who has a reputation as a dirty fighter", "4 percent cumulative effect", "a solitary figure who is not understood by others, but is actually wise", "13", "Nueva Extremadura", "the Supreme Court of Canada", "in honey, tree and vine fruits, flowers, berries, and most root vegetables", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Bhupendranath Dutt", "October 2012", "McKim Marriott", "the Mishnah", "revolution or orbital revolution", "1979", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "atlantic ocean", "Billy Idol", "early 1974", "Patris et Filii et Spiritus Sancti", "Tanvi Shah", "administrative supervision over all courts and the personnel thereof", "diffuse interstellar medium ( ISM )", "Dr. Rajendra Prasad", "In 1889", "Kiss", "775", "season four", "Akshay Kumar", "Mahatma Gandhi", "across western North Carolina including Asheville, Cashiers and Saluda", "Michael Crawford", "12 '' x 12 '' attached giant - sized booklet with state - of - the - art photography of the band's performance and outdoor session pictures", "Michael Phelps", "Matt Monro", "8 January 1999", "Henry Selick", "1", "94 by 50 feet", "a subduction zone", "2009", "Rudolf Hess", "alzheimer", "Arafura Sea", "Little Big League", "40 Days and 40 Nights", "The Handmaid\\'s Tale", "crocodile eggs", "Thursday", "the two-state solution to the Mideast", "grave", "a hyperbola", "A Room with a View", "Walford East"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7318824404761904}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.09523809523809525, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.39999999999999997, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-10529", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-10182", "mrqa_triviaqa-validation-1857", "mrqa_hotpotqa-validation-5478", "mrqa_newsqa-validation-406", "mrqa_searchqa-validation-708", "mrqa_searchqa-validation-4996"], "SR": 0.6875, "CSR": 0.5128125, "EFR": 0.95, "Overall": 0.707328125}]}