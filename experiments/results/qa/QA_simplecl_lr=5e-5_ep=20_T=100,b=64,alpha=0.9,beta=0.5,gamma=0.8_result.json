{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=5e-5_ep=20_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=5e-5_ep=20_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 8860, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "red algal endosymbiont", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "The Love Boat", "Frank Marx", "the architect or engineer", "$2 million", "superintendent of New York City schools", "San Francisco Bay Area", "Kingdom of Prussia", "the country in the same league as the Asian Economic Tigers", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "China", "redness, swelling, heat, and pain", "Edgar Scherick", "the 14th to the 19th century", "Gibraltar and the \u00c5land islands", "the Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches", "it is impossible to determine what the acceleration of the rope will be", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "the Ikh Zasag", "Central Bridge", "Europe", "King James Bible", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "a pair of long, slender tentacles", "the mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Manakin Episcopal Church", "John Michael Rysbrack", "due to ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "the Capitol held its first session of the United States Congress", "It is the currency used by the institutions of the European Union", "Djokovic", "a generic cover and none of the Wyeth"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7332589285714286}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8661", "mrqa_squad-validation-7332", "mrqa_squad-validation-6031", "mrqa_squad-validation-27", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-8134", "mrqa_squad-validation-6517", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-2328", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-5588", "mrqa_squad-validation-9166", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187", "mrqa_searchqa-validation-2579"], "SR": 0.703125, "CSR": 0.765625, "EFR": 0.9473684210526315, "Overall": 0.8564967105263157}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "lower-paid", "Labor", "time and storage", "special efforts", "rhetoric", "the British occupation", "a year", "Genghis Khan", "a supervisory church body", "35", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "31 October", "Stanford University", "1991", "LOVE Radio", "ambiguity", "Khasar", "Sky Digital", "99.4", "about a third", "the issue of laity having a voice and vote", "1995", "the genes it donated to the former host's nucleus", "rocketry and manned spaceflight", "linebacker", "water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "human rights abuses and war crimes", "three", "Lowry Digital", "worst-case time complexity", "2010", "363 feet", "Buffalo Lookout", "Missouri", "The User State Migration Tool", "1773", "Cadmium", "October 6, 2017", "night", "Haliaeetus", "Sir Henry Bartle Frere", "James Zeebo", "through the weekend", "Ty Hanks"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7910389957264957}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-1672", "mrqa_squad-validation-335", "mrqa_squad-validation-10217", "mrqa_squad-validation-2538", "mrqa_squad-validation-7781", "mrqa_squad-validation-6171", "mrqa_squad-validation-9876", "mrqa_squad-validation-8754", "mrqa_squad-validation-3812", "mrqa_squad-validation-9717", "mrqa_squad-validation-3909", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-6211", "mrqa_newsqa-validation-174"], "SR": 0.734375, "CSR": 0.7552083333333334, "EFR": 1.0, "Overall": 0.8776041666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member", "James E. Webb", "Huguenot refugees", "phycoerytherin", "a losing proposition", "swimming-plates", "10 July 1856", "130 million cubic foot", "a \"teleforce\" weapon", "Heinrich Himmler", "0-4", "Baptism", "Decision problems", "customs of his tribe", "1953", "The Day of the Doctor", "Muhammad Khan", "New Orleans' Mercedes-Benz Superdome", "the Council", "February 9, 1953", "March", "sea gooseberry", "1961", "Trio Tribe", "Dai Setsen", "Late Medieval Catholic Church", "January 1979", "phagocytic", "Rankine cycle", "$2.2 billion", "Seine", "Newton's Law of Gravitation", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers series", "Kenyans for Kenya", "Fresno", "Saudi", "Presiding Officer", "an intuitive understanding", "default emission factors", "Inherited wealth", "Michael P. Millardi", "Goldman Sachs", "the BRAAVOO website", "aproveitando espaos, cama suspensa, armario", "South Dakota", "praying in Latin", "the children were nestled all snug in their beds", "the Great Temple at Abu Simbel", "the Leyden jar", "a list of the subjects that candidates", "a German opera composer of Jewish birth", "the borders of Germany", "70%", "the children of New York", "the risk of a fire or a flood", "the British", "early 1960s", "April 1917", "poor hygiene"], "metric_results": {"EM": 0.578125, "QA-F1": 0.631436011904762}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-3270", "mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-2595", "mrqa_squad-validation-494", "mrqa_squad-validation-6072", "mrqa_squad-validation-5860", "mrqa_squad-validation-120", "mrqa_squad-validation-5262", "mrqa_squad-validation-10369", "mrqa_squad-validation-7993", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156"], "SR": 0.578125, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "The Prince of P\u0142ock", "hormones", "1840", "occupational stress", "in the parts of the internal canal network", "in no way", "Tesla Electric Company", "African-American", "Thomson", "1905", "\"Nun komm, der Heiland\"", "John Fox", "in all health care settings", "cut in half", "the study of rocks", "colonies", "lower wages", "geophysical surveys", "Protestantism", "their actual social power and wealth", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "between 25-minute", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs into reality", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "a protective radiation shield", "Nederrijn", "a multi-cultural city", "pump water out of the mesoglea", "Zeebo", "Australia", "a judicial officer", "mathematical model", "Henry Purcell", "Ram Nath Krishind", "anembryonic gestation", "Todd Griffin", "Sandy Knox and Billy Stritch", "Hudson Bay", "Lee Freedman", "a bow bridge", "the Russian Soviet Federative Socialist Republic", "Nicole Gale Anderson", "1", "sedimentary", "Mrs. Wolowitz", "theory of plate tectonics", "Colombia", "Yolande of Brienne", "Kris Allen", "UNESCO"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7434027777777779}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4539", "mrqa_squad-validation-2456", "mrqa_squad-validation-6319", "mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-8093", "mrqa_squad-validation-7708", "mrqa_squad-validation-3497", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_hotpotqa-validation-4815", "mrqa_searchqa-validation-172"], "SR": 0.65625, "CSR": 0.7, "EFR": 1.0, "Overall": 0.85}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "at the beginning of the 20th century", "1974", "ABC", "dictatorial", "Ben Johnston", "the results of a measurement are now sometimes \"quantized\"", "Book of Exodus", "Synthetic aperture radar", "Mission Impossible", "patients' prescriptions and patient safety issues", "No, that's no good", "1697", "3 January 1521", "a new magma", "a \"principal hostile country\"", "a new Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "the machine gun", "Theatre Museum", "August 10, 1948", "if they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "The Melbourne Cricket Ground", "Wednesdays", "most common", "concentration gradient", "The flushing action of tears and urine", "six years", "plants and algae", "Republic Day", "1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "in 1942", "March 2016", "Texas, Oklahoma", "a balance sheet", "Irsay", "1963", "William the Conqueror", "1922", "no embryo", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "Joe Pizzulo and Leeza Miller", "the head of Lituya Bay in Alaska", "Sarah", "the topology of the network", "The euro", "Ultraviolet Ultraviolet", "2000", "KCNA", "all of our building products", "all these big numbers"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6839953864070711}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3076923076923077, 0.5714285714285715, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-3770", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-8904", "mrqa_squad-validation-6439", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_newsqa-validation-2404", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15169"], "SR": 0.59375, "CSR": 0.6822916666666667, "EFR": 1.0, "Overall": 0.8411458333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "Gender pay gap in favor of males in the labor market", "Pliocene period", "relationship between teachers and children", "LeGrande", "After the sixth sermon", "10 Cloverfield Lane", "11.1%", "nearly 60,000", "University of Chicago College Bowl Team", "the decline of organized labor in the United States", "Santa Clara Marriott", "oxygen chambers", "two", "two catechisms", "Cologne, Germany", "1991", "Silk Road", "Surveyor 3 unmanned lunar probe", "145 galleries", "growth and investment", "the centers were computer service bureaus", "Vampire bats", "antiforms", "U. S. flags left on the Moon during the Apollo missions were found to still be standing", "weight", "Mongolians to refer to their country as \"Genghis Khan's Mongolia", "American increased their reserves (by expanding their money supplies) in amounts far greater than before", "Beyonc\u00e9 and Bruno Mars", "a university or college", "More than 1 million people", "pseudorandom number generators", "Japan", "Coriolis effect", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains", "Panamanian government", "alpaca fiber and mohair from Angora goats", "the king of all political powers", "two", "Lewis Hamilton", "April 10, 2018", "Gorakhpur", "How I Met Your Mother", "elected", "December 15, 2016", "Abraham Gottlob Werner", "Jourdan Miller", "in the weeks before the release of Xscape", "Mandy '' Moore", "Denmark", "Broken Hill and Sydney", "159", "China", "Judith Cynthia Aline Keppel", "medellin", "Crown Holdings Incorporated", "Expedia", "Large Space Telescope", "Columbian mammoth", "a failure of leadership at a critical moment in the nation's history"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6500198046727894}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.30769230769230765, 0.9032258064516129, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.17391304347826084, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-606", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-7422", "mrqa_squad-validation-4000", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-516", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3442", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-429"], "SR": 0.546875, "CSR": 0.6629464285714286, "EFR": 0.9655172413793104, "Overall": 0.8142318349753694}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "McManus", "Two", "1066", "2008", "Mojave Desert", "Operating System Principles", "St. Lawrence and Mississippi watersheds", "27%", "4000", "Rhine Gorge", "stromal thylakoids", "638,000", "impact process effects", "generally by caning, remains commonplace in schools in some Asian, African and Caribbean countries", "Kings Row and Casablanca", "pharmacists", "high-voltage", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation in the western Rhine Delta", "The European Commission", "SAP Center", "lost one-fourteenth of the air's volume before extinguishing the subjects", "352", "the law \u2013 in any form \u2013 should not be preached to Christians anymore", "October 6, 2004", "The Day of the Doctor", "Pakistan", "November 1999", "September 6, 2019", "English", "During the fourth season", "three times", "Nick Kroll", "the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Billy Gibbons", "an apprentice of the fictional Wars Order", "in the brain", "31", "1970s", "U.S. State Department", "Art Carney", "accomplish the objectives of the organization", "generally spaced 12 to 36 days apart", "January 1923", "Category 4", "September 2017", "1 September 1939", "silk floss tree", "Terrell Owens", "3, 1", "five", "Dolph Lundgren", "Hampton Court Palace", "Sela Ann Ward", "her decades-long portrayal of Alice Horton", "Jeopardy", "James Gaffigan", "an isosceles triangle", "New York"], "metric_results": {"EM": 0.5625, "QA-F1": 0.640408940018315}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.9714285714285714, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.923076923076923, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-762", "mrqa_squad-validation-4629", "mrqa_squad-validation-8819", "mrqa_squad-validation-4348", "mrqa_squad-validation-1938", "mrqa_squad-validation-5781", "mrqa_squad-validation-6409", "mrqa_squad-validation-3416", "mrqa_squad-validation-2521", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6183", "mrqa_triviaqa-validation-6548"], "SR": 0.5625, "CSR": 0.650390625, "EFR": 0.9642857142857143, "Overall": 0.8073381696428572}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "p-adic norm", "Maududi", "Gottfried Fritschel", "the primary logo used on all media and merchandise relating to past Doctors", "ular plastoglobulus", "pound-force", "Ming dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36%", "Giambologna", "April 20", "biomass", "see no need to accept punishment for a violation of criminal law that does not infringe the rights of others", "K MJ-TV", "Foreign Protestants Naturalization Act", "southern and central parts of France", "10%", "not designed to fly through the Earth's atmosphere or return to Earth", "Metro Trains Melbourne", "BBC 1", "$2 million", "Lombardi Trophy", "Galileo", "in linked groups or chains, still always anchored to a thylakoid network", "a song recorded, written, and produced by American musician Lenny Kravitz for his second studio album", "river Aniene", "1885", "James Madison", "Ryan Pinkston", "a federal republic", "lacteal", "2007", "foreign investors", "N\u0289m\u0289n\u0289", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "1930", "Julie Adams", "Thomas Jefferson", "February 2017 in Japan and in March 2018 in North America and Europe", "October 29, 2015", "customary system", "Millerlite", "2004", "Billy Hill", "Mara", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "2005", "Lydia Pinkham", "Albert", "a coma in a grave condition", "Croatia", "Drew Kesse", "six", "a group of 20 similar cars making an annual road trip"], "metric_results": {"EM": 0.5, "QA-F1": 0.5881448412698412}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.2222222222222222, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2666666666666667, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-9615", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-5724", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-402", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-5999", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3043", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.5, "CSR": 0.6336805555555556, "EFR": 1.0, "Overall": 0.8168402777777778}, {"timecode": 9, "before_eval_results": {"predictions": ["an attempt to emphasize academics over athletics", "3,600", "nine", "individual states and territories", "30%", "one of his wife's ladies-in-waiting", "liquid phase at this point", "Dirichlet's theorem on arithmetic progressions", "Europe", "the cell membrane", "a renegade Time Lord who desires to rule the universe", "Laverne & Shirley", "carbohydrates", "his butchery is exaggerated", "Jean Ribault", "March 2011", "Continental Edison Company in France", "1985", "robustly associated with more equality in the income distribution", "X is no more difficult than Y", "age 38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half the carbon fixed by the Calvin cycle", "lowest", "the can with a thousand uses", "jennifer Wilson", "Georgie Porgie", "a fermented alcoholic drink", "the Shakespeare First Folio", "the Fray", "Venus", "Helen Hayes", "Canberra", "jennifer", "Alexander Graham Bell", "Anna Pavlova", "a person who computes premium rates, dividends, risks, etc.", "John Campanis", "a boy & Cecil", "the Billy Goat", "jennifer bistro.com", "a goat", "jennifer Collins", "jenniferore Wells", "jennifer Wagner", "the White Nile", "jennifer Irons", "the chimney", "Andrew Jackson", "a \"paranoid control Freak\"", "jennifer Matisse", "a sailfish", "jennifer ecco", "jennifer Myers", "Egypt", "James Hutton", "jennifer ecc.", "Shepherd Neame", "a professional wrestling tag team, composed of Doug Basham and Danny Basham", "jennifer Whitesides", "China and Japan", "Appathurai"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5098513465700966}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.0, 0.7692307692307693, 0.36363636363636365, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3687", "mrqa_squad-validation-3392", "mrqa_squad-validation-8969", "mrqa_squad-validation-7700", "mrqa_squad-validation-6229", "mrqa_squad-validation-1240", "mrqa_squad-validation-166", "mrqa_squad-validation-7492", "mrqa_squad-validation-1748", "mrqa_squad-validation-374", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-1507", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-7463", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-1007"], "SR": 0.421875, "CSR": 0.6125, "EFR": 1.0, "Overall": 0.80625}, {"timecode": 10, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.9140625, "KG": 0.396875, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "causing fish stocks to collapse", "Chris Keates", "its many castles and vineyards", "the Cinerama Productions/Palomar theatrical library", "Antigone", "3.5 million", "Carolina Panthers", "1997", "A \u2192 G deamination", "since 2001", "A", "1784", "Narrow alleys", "another problem", "economic growth", "John and Benjamin Green", "1530", "installed electrical arc light based illumination systems", "two", "lower wages", "Irish Hospitals' Sweepstakes", "Pearl Jam", "Grey's Anatomy", "silk", "Bruce Springsteen", "Wounded Knee", "Maria Callas", "Henry Moore", "(William) Albert", "Charlotte, North Carolina", "the 10th hole", "(Prince) Albert", "Fred Williamson", "South Africa's diamond industry", "needles", "the Holy Grail", "The Smashing Pumpkins", "his dwelling", "Ludwig Van Beethoven", "(Prince Albert) Albert", "tidal streams", "A&W Root Beer", "Fram", "Sarah Orne Jewett", "Velvet Revolver", "(William) Bacon", "a subtropical jet stream", "The Match Game", "China", "Nova Scotia", "(Prince) Albert", "Narnia", "Franklin Pierce", "Pearl Harbor", "Michael Schumacher", "a four - page pamphlet in 1876", "the cue ball", "(Prince) Albert", "Paul W. S. Anderson", "Hugh Caswall Tremenheere Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6059895833333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.26666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8990", "mrqa_squad-validation-5887", "mrqa_squad-validation-22", "mrqa_squad-validation-6655", "mrqa_squad-validation-7353", "mrqa_squad-validation-7333", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-1326"], "SR": 0.53125, "CSR": 0.6051136363636364, "EFR": 0.9666666666666667, "Overall": 0.7187310606060606}, {"timecode": 11, "before_eval_results": {"predictions": ["the Horn of Africa", "Grumman", "to civil disobedients", "1671", "St. Johns", "the AS-205 mission was canceled", "The President of the Council and a Commissioner can sit in on ECB meetings", "Ismailiyah, Egypt", "phycobilisomes on the thylakoid membranes", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "both PNU and ODM camps", "T(n) = O(n2)", "Bill Clinton", "the qu", "International Crops Research Institute", "a straight line", "Germany", "autoimmune", "David Leslie", "his advocacy of young earth creationism", "Seoul", "2005", "December 24, 1973", "May 21, 2000", "the 100 metres", "January 2016", "seven", "Samuel Beckett", "Eilean Donan", "Sonic Mania", "Homeland", "Carson City", "League of the Three Emperors", "Seth D. Harris", "Nickelodeon", "Washington, D.C.", "December 13, 2015", "Front Row", "(Willa) van Oldenbarnevelt", "Vixen", "The Benchwarmers", "Mach number", "1993", "Michael A. Cremo", "Gangsta's Paradise", "The A41", "Bette Davis", "five", "Indiana", "Esteban Ocon", "ABC", "Jean Baptiste Point DuSable", "the National Lottery", "2018", "Howard Ashman", "(Willa) Lucas", "Richard Avedon", "81st", "a full tropical garden", "the Sudan", "a pillar", "Yahya Khan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5229707792207792}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.4, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-6759", "mrqa_squad-validation-3113", "mrqa_squad-validation-3954", "mrqa_squad-validation-4150", "mrqa_squad-validation-8594", "mrqa_squad-validation-8321", "mrqa_squad-validation-4883", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-358", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1700", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-971", "mrqa_naturalquestions-validation-3485"], "SR": 0.453125, "CSR": 0.5924479166666667, "EFR": 0.9714285714285714, "Overall": 0.7171502976190476}, {"timecode": 12, "before_eval_results": {"predictions": ["a Wi-Fi or Power-line connection", "water pump", "Tesla coil", "1946", "21 to 11", "the Parliamentary Bureau", "Japan and Latin America", "send missionaries, backed by a fund to financially reward converts to Catholicism", "Arizona Cardinals", "382", "1540s", "John Fox", "American Indians in the colony of Georgia", "receive television pictures of the lunar surface on Christmas Eve", "poison", "quickly", "pathogens", "March 1896", "Percy Jackson series", "James `` Jamie '' Dornan", "W. Edwards Deming", "annual", "biochemistry", "$2.187 billion", "current day", "Accounting Standards Board ( ASB )", "Ole Einar Bj\u00f8rndalen", "General George Washington", "following graduation with a Bachelor of Medicine, Bachelor of surgeries degree and start the UK Foundation Programme", "Djokovic", "snoods ( or gangions )", "1961", "the dome of the U.S. Capitol building", "1997", "Procol Harum", "Sheev Palpatine", "Dan Rooney", "punk rock", "nasolacrimal", "the First Family", "vaskania ( \u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1 )", "the church at Philippi", "1940", "Brenda", "bohrium", "unfair", "nasal septum", "the Iraq War", "Spanish American wars of independence", "Tristan Rogers", "Owen Vaccaro", "Walter Brennan", "around 1872", "Mike Alstott", "1992 to 2013", "Richard II", "Principality of Liechtenstein", "Chuck vs. First Class", "Dana Scully", "the Kooyong Classic", "18", "Swing Low, Sweet Chariot", "Locked-in syndrome", "the Burrard Inlet"], "metric_results": {"EM": 0.484375, "QA-F1": 0.551333648989899}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0909090909090909, 1.0, 0.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-978", "mrqa_squad-validation-3130", "mrqa_squad-validation-9863", "mrqa_squad-validation-3994", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-10620", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-5292", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-765", "mrqa_searchqa-validation-9822", "mrqa_searchqa-validation-10098"], "SR": 0.484375, "CSR": 0.5841346153846154, "EFR": 0.9696969696969697, "Overall": 0.715141317016317}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "momentum-carrying gauge bosons", "Hamburg merchants and traders", "Department of Justice", "comb jellies", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "number of quality rental units", "Pittsburgh Steelers", "Edward Teller", "the geographical area it covers as well as the frequency of meeting", "to stay, so long as there was at least an \"indirect quid pro quo\" for the work he did", "Andrew Lortie", "invertebrates", "Thirty years after the Darth Civil War", "an extracellular domain, which is able to bind a specific ligand, a transmembrane domain, and an intracellular catalytic domain,", "eight years", "the Vulcans", "Longline fishing", "various locations in Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "1935", "Paradise, Nevada", "Herman Hollerith", "Dr. Sachchidananda Sinha", "Rick Marshall", "hairpin bend", "over two days", "IB Diploma Program", "During metaphase the X-shape structure", "it activates a relay which will handle the higher current load", "Donald Trump", "Liam Cunningham", "the spectroscopic notation for the associated atomic orbitals", "Veronica", "moral", "rotation axes", "Sauron", "Gustav Bauer", "2006", "Mohammad Reza Pahlavi", "the southeastern coast of the Commonwealth of Virginia", "sedimentary rocks", "Jourdan Miller", "10,605", "714", "1773", "Jesse McCartney", "73", "at West Quoddy Head in Lubec, Maine", "1978", "Catherine Zeta-Jones", "Michael Crawford", "264,152", "10,000", "missing", "Salt Lake Tabernacle Choir", "carbon dioxide", "The Roar of the Greasepaint", "Sunshine State"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5192434660645868}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333334, 0.1379310344827586, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.8, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.4, 0.0, 0.6666666666666666, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10475", "mrqa_squad-validation-4528", "mrqa_squad-validation-7576", "mrqa_squad-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4097", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-1166"], "SR": 0.40625, "CSR": 0.5714285714285714, "EFR": 1.0, "Overall": 0.7186607142857142}, {"timecode": 14, "before_eval_results": {"predictions": ["a Tulku", "Quaternary", "the Treaty of Aix-la-Chapelle", "Brad Nortman", "the Museum of the Moving Image", "Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius", "the depths of the oceans and seas", "118", "a mainline Protestant Methodist denomination", "Albert Einstein", "the Vince Lombardi Trophy", "death in body and soul, if only as highwaymen and murderers.", "Candice Susan Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "Indiana", "1949", "Red", "Australia", "citizen Khan", "jena Malone", "John M. Dowd", "twelfth", "Republican", "New York", "Southern Rock Allstars", "a novella of the same title by Prosper M\u00e9rim\u00e9e", "cricket fighting", "14th Street", "guitar", "Brad Wilk", "2012", "in New Orleans, Louisiana", "Robert &quot", "May 4, 1924", "Australian", "US tour", "2012", "1926", "27th congressional district", "ciaramello", "a mother of gods", "the EA-18G Growler carrier-based electronic warfare jet aircraft", "Georgia Tech", "Ludwig van Beethoven", "(a Viacom subsidiary)", "Manchester United", "Saudi Arabian", "1942", "October 6, 2017", "at a given temperature", "wolf", "Ganges", "January 24, 2006", "in some of the poorest parts of South Africa", "a radio frequency (RF) signal", "Baltimore, Maryland", "1917", "in the Blue Ridge Mountains of Virginia"], "metric_results": {"EM": 0.484375, "QA-F1": 0.566659279591023}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.10526315789473684]}}, "before_error_ids": ["mrqa_squad-validation-10168", "mrqa_squad-validation-7674", "mrqa_squad-validation-8229", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-11270", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-1813"], "SR": 0.484375, "CSR": 0.565625, "EFR": 1.0, "Overall": 0.7175}, {"timecode": 15, "before_eval_results": {"predictions": ["1915", "the Educational Institute of Scotland and the Scottish Secondary Teachers' Association", "June 4, 2014", "The Eleventh Hour", "a stronger, tech-oriented economy in the Bay Area and an emerging Greater Sacramento region.", "John Houghton", "heterokontophyte", "NP-complete", "Tenggis Khan", "128,843", "a simple majority vote of the Council to approve", "56.2%", "11 points", "The Tower of London", "KlingStubbins", "Edward Albert Heimberger", "the Mountbatten Internship Programme", "Alcorn", "You're Next", "The Today Show", "Philadelphia, Pennsylvania.", "12 members", "A41 road", "Eminem", "Pimp My Ride", "1998", "casting, job opportunities, and career advice", "Mary Harron", "Flashback: The Quest for Identity", "Eenasul Fateh", "Chicago", "Brisbane, Australia", "2014", "the Japanese Malayan Campaign", "Lismore", "rural areas", "teenage actor or teen actor", "Summerlin, Clark County, Nevada", "Jack Benny Binion", "YG Entertainment", "Rusalka", "Noel", "the \"Pour le M\u00e9rite\" 1", "Trey Parker and Matt Stone", "Riot Act", "Aqua", "various registries", "four operas", "Christy Walton", "Lt. Gen. Ulysses S. Grant", "Hechingen in Swabia", "Black Sabbath", "Luis Enrique", "8,211", "Tim Allen", "in the cell nucleus", "a Bristol Box Kite", "1961", "Diprivan", "Iraq", "Douglas Fir", "kiss a fool", "military commissions", "Philip Markoff"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5183579441391941}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.6153846153846153, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1453", "mrqa_squad-validation-2094", "mrqa_squad-validation-7651", "mrqa_squad-validation-2835", "mrqa_squad-validation-6279", "mrqa_squad-validation-4298", "mrqa_squad-validation-257", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-99", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-2178", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757", "mrqa_newsqa-validation-4202"], "SR": 0.390625, "CSR": 0.5546875, "EFR": 1.0, "Overall": 0.7153125}, {"timecode": 16, "before_eval_results": {"predictions": ["the Central Secretariat (Zhongshu Sheng)", "Puritan", "James Wolfe", "March 1974", "2003", "Frederick II", "Lower taxes", "Raimbaud", "redistributive taxation", "Seattle Seahawks", "physicians", "the reduction process takes polynomial time", "revelry", "Krishna Rajaram", "Cutie", "Mark Thompson", "5-0", "Kim Il Sung", "second-degree aggravated battery", "John McCain", "\"", "Charman Sinkfield, 30; Demario Ware, 20", "be silent", "200", "2,000", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "injuries", "Michael Jackson", "Caylee", "10 below", "women", "Manmohan Singh", "downtown", "1983", "cancer", "Al-Shabaab", "Casalesi Camorra", "\"One, you let them know what the case involves", "Appathurai", "Eintracht Frankfurt", "opium poppies", "The Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "Arturo Gonzalez", "Chinese", "Pakistan", "FBI", "Akio Toyoda", "boy", "\"propaganda.\"", "Sri Lanka", "India", "Miami Heat", "a combination of genetics", "Senegal", "Windermere", "Field of Dreams", "Kevin Peter Hall", "two more stars", "toke", "Sex Pistols", "Chingler"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5753720238095239}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8294", "mrqa_squad-validation-5513", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1843", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2899", "mrqa_naturalquestions-validation-7301", "mrqa_triviaqa-validation-4966", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-4356"], "SR": 0.515625, "CSR": 0.5523897058823529, "EFR": 1.0, "Overall": 0.7148529411764706}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "public schools", "tertiary education (universities and/or TAFE colleges)", "a glass case suspended from the lid", "phagocytic", "2000", "five", "weight", "Leukocytes (white blood cells)", "3D printing", "Ong Khan,", "colonel", "a \"stressed and tired force.\"", "a co-principal Commission.", "Wigan Athletic", "Vertikal-T,", "Graeme Smith", "228", "the Bush administration's controversial system of military trials", "a Florida girl", "Miami", "She is the Magneto to my Wolverine,", "the explosion of a train seconds after it leaves the Liverpool Street Station", "helicopters and unmanned aerial vehicles", "and rain melted snow south of Fargo and Moorhead, Minnesota,", "African National Congress", "Nivose,", "a 1,700 year old Roman mosaic", "1959,", "Adam Yahiye Gadahn,", "Mark Sanford", "150", "buses, subways and trolleys", "the equator,", "Chinese President Hu Jintao", "183", "warning", "too many glass shards left by beer drinkers in the city center,", "Larry King", "a tree-planting ceremony southern Israel", "11th year in a row", "and Cezanne's \"Boy in a Red Vest\"", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela.", "Austin Wuennenberg,", "a 10-person dance group", "injected with drugs by ICE agents against his will.", "Oaxaca", "buckling under pressure from the ruling party.", "a staircase.", "100", "lost bouts,", "Marianela Galli", "MacFarlane", "convert single - stranded genomic RNA into double - stranded cDNA", "Sharon Knolle", "Andes", "Peter Robert Auty", "2009", "The Jack Paar Show", "Marilyn Monroe", "Kings of Leon", "Jodi Benson's", "Tacos"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4679194050082208}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.7272727272727273, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.9473684210526316, 0.0, 1.0, 0.0, 0.5, 0.4, 0.0, 1.0, 0.4864864864864865, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7136", "mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-6898", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3494", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1542", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-7327", "mrqa_hotpotqa-validation-1968", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1649"], "SR": 0.359375, "CSR": 0.5416666666666667, "EFR": 1.0, "Overall": 0.7127083333333334}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "a pump is used to pressurize the working fluid which is received from the condenser as a liquid not as a gas", "1999,", "mesoglea", "a body of treaties and legislation,", "liquid", "socially owned", "Mark Twain's", "in amylopectin starch granules that are located in their cytoplasm,", "the Tower District", "\"a violent and brutal extremist group with a number of individuals affiliated with al Qaeda.", "two of the dead were teenagers and one was an Islamic scholar,", "tusks", "overthrow the socialist government of Salvador Allende in Chile,", "The HMS Beagle was a 70 ft, three-masted ship built from pine and oak,", "a rally at the State House", "2,000", "Michael Schumacher", "Ventures,", "seven", "hanging a noose", "lost its majority in the Chamber of Deputies after being defeated in 18 of 60 races,", "\"I don't quit.", "21,", "diplomatic relations", "harrison ford", "Daniel Radcliffe", "Muslim", "five", "mother", "$10 billion", "Zoe's Ark", "Galveston, Texas,", "9-week-old", "to stop selling unapproved pain-relief drugs.", "Reggae legend Lucky Dube,", "A Lion Among Men.", "James Newell Osterberg", "At least 40", "NATO", "Lindsey Vonn", "\"TSA has reviewed the procedures", "Mike Weland,", "International Polo Club Palm Beach in Florida.", "Nevaeh (heaven spelled backward)", "HPV (human papillomavirus)", "workers", "25", "an Islamic emirate", "the Islamic militant group Harkat-ul-Jihad al-Islami ( HuJi)", "Nicole", "Stoke City", "Jund Ansar Allah", "Downton Abbey", "January 2017", "30", "sweater", "Waylon Albright", "people working in film and the performing arts,", "The Hershey Company", "Your sticky room at home", "Marlborough", "1968", "The Krypto Report"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4880540306732741}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.12500000000000003, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.09090909090909091, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.06666666666666668, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3444", "mrqa_squad-validation-7516", "mrqa_squad-validation-1235", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2733", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-5209", "mrqa_hotpotqa-validation-2986", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-3428"], "SR": 0.40625, "CSR": 0.5345394736842105, "EFR": 1.0, "Overall": 0.7112828947368421}, {"timecode": 19, "before_eval_results": {"predictions": ["the blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "an Executive Committee,", "New Orleans", "the death of Elisabeth Sladen in early 2011.", "The annual NFL Experience", "English", "61%", "plastoglobulus, sometimes spelled plast\u00f6obule(s)", "three", "Turkey", "a hair-free muzzle", "Kenny Baker", "a bone", "Peyton Place", "a fancy dark blue diamond", "Gin Rummy", "Pilate", "enamel", "bone", "\" help yourself to happiness\"", "Battle of Hastings", "the Caspian Sea", "a work journal", "Hodgepodge", "The Washington Post", "a nickname", "Don Juan De Marco", "Fes", "the final", "Interlaken", "Mystic Pizza", "Princeton", "Mandy Well you came.", "the top 100 largest", "Malay Peninsulamakes", "Herman Wouk", "Frederick IV,", "the mouth of a fool", "The Courier Journal", "Napoleon", "Stock Dinosaurs", "unassisted", "thermodynamics", "Derek Smalls", "dalits", "Harry Houdini", "\"Randy\"", "double Vision", "dollop", "Lust for Life", "Revolvy", "James Ross Clemens", "a hole-in-one", "1991", "to universalize the topic of the song into something everyone could relate to and ascribe personal meaning to in their own way", "Cuban cigars", "\"Thrilla in Manila\"", "India", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "East", "layered systems of sovereignty", "near the city of Cairo, Illinois"], "metric_results": {"EM": 0.359375, "QA-F1": 0.49623799770965465}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.5, 1.0, 0.75, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.13953488372093023, 1.0, 0.5, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7872", "mrqa_squad-validation-8465", "mrqa_squad-validation-8786", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-3559"], "SR": 0.359375, "CSR": 0.52578125, "EFR": 0.975609756097561, "Overall": 0.7046532012195122}, {"timecode": 20, "UKR": 0.64453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.828125, "KG": 0.409375, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300", "an attack on New France's capital, Quebec", "two-thirds", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge in Edinburgh", "1959", "the Lincoln Laboratory", "a grizzly bear", "Dracula", "Sid Vicious", "nitrous oxide", "Overture", "Frederic Remington", "the south coast", "Arkansas", "an object oriented programming", "1998", "\"a discharge based on military\"", "the Whig journal, Log Cabin", "the emergency room", "a yellow lotus", "Curtis Stone", "The Princess Diaries", "Arkansas", "Mao Zedong", "a multilingual person", "a dialect or a bell clapper", "the Wells Fargo Wagon", "the Sundance Kid", "a house of prayer", "an amber", "Hepburn", "Umbria", "a Roth IRA", "Quentin Tarantino", "the Palatine Hill", "Kentucky", "a statement", "the second Sunday", "a skirt", "the airplane", "Libby", "a flood", "Reba", "the west coast of Africa", "a genie", "bowling", "Walter Reed", "Aerobic", "Anaheim", "Steve Hale", "the distribution and determinants of health and disease conditions in defined populations", "Belgium", "David Jason", "the 137th", "Merck", "semiconductors", "English and Russian", "Strasbourg", "Hagrid", "Phil Mickelson"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5727864583333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-4028", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-7628", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-12477", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-11227", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-9183", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-4992", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-436"], "SR": 0.484375, "CSR": 0.5238095238095238, "EFR": 1.0, "Overall": 0.6811681547619047}, {"timecode": 21, "before_eval_results": {"predictions": ["a lesson plan", "the laws of physics", "an ill-fated attempt at intercontinental wireless transmission,", "Welsh", "the people themselves", "criminal", "a monthly subscription", "10,000 BC", "novella", "the President of the United States", "above the light source and under the sample in an upright microscope, and above the stage and below the light sources in an inverted microscope", "November 3, 2007", "1939", "April 1917", "the `` Molly and Johnny Theme ''", "the outlaw couple Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "September 19 - 22, 2017", "tolled ( quota ) highways", "perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Linda Creed", "Nagar Haveli", "Dick Rutan and Jeana Yeager", "Paracelsus", "2010", "the gay ( LGBT ) community", "as early as January 3, and as late as February 12", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Rice Records", "push the food down the esophageal", "Splodgenessabounds", "to jump - shoot, to dribble ( drive ) past the defender or to pass it to a teammate", "Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown and alley Faldo", "a white one, the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "diastema", "Clarence, the owner of the barber shop", "video game", "the Secretaries of State and Defense and the National Security advisor", "flour and water", "the Super Bowl", "the 5th - century CE", "verification code ( V - code or V code ), or signature panel code ( SPC )", "T - Bone Walker", "Ray Charles", "Hutcheson", "1937", "at Cairo, Illinois", "Barbara Windsor", "British military resources in the colonies at a time when France was unwilling to risk large convoys to aid the limited forces that they had in New France", "Gladys Knight & the Pips", "in the Executive Residence of the White House Complex", "the eighth episodes of the third season of Arrow", "Canadian case law", "Kanawha River", "athletics", "an isosceles", "1898", "Queen Elizabeth", "WFTV", "tennis", "Las Vegas", "Austrian", "breast cancer", "Harry Nicolaides,", "the man to drop the bat and get down on his knees."], "metric_results": {"EM": 0.390625, "QA-F1": 0.5328989694730794}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9473684210526315, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.11111111111111112, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.7499999999999999, 1.0, 0.9428571428571428, 0.0, 0.75, 1.0, 0.0, 0.23529411764705882, 0.08, 0.5, 0.0, 0.0, 0.5, 0.35294117647058826, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 0.923076923076923, 0.5714285714285714, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_squad-validation-1194", "mrqa_squad-validation-2336", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-115", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-6087", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-11032", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.390625, "CSR": 0.5177556818181819, "EFR": 0.9230769230769231, "Overall": 0.664572770979021}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy", "the bark of mulberry trees", "drama", "1806", "\"distributive efficiency\"", "on issues related to the substance of the statement", "Athens", "Continental drift", "Frank Oz", "1975", "775", "Kimberlin Brown", "AD 95 -- 110", "the status line", "the disk", "repel bullets and fly at sub-sonic speeds, much like Ms. Marvel could", "handheld subscriber equipment", "a number of English country estates", "a hexamer in secretory vesicles", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "a lightning strike", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Coton in the Elms", "a stem", "Wakanda", "1992", "two theories ; active osmotic water absorption and Active non-osmotic", "digital transmission", "Mansa Musa came to the throne after a series of civil wars and ruled for thirty years", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Robert Hooke", "the study of the interstellar medium ( ISM ) and giant molecular clouds ( GMC )", "Alicia Vikander", "Jepsen", "Kylie Jenner", "5 liters", "somatic cell nuclear transfer ( SCNT )", "Betty", "usually related to ABO blood group incompatibility - the most severe of which often involves group A red cells being given to a patient with group O type blood", "June 8, 2009", "head - up display", "a presidential representative democratic republic, whereby the President of El Salvador is both head of state and head of government, and of an Executive power is exercised by the government", "Ferm\u00edn Francisco de Lasu\u00e9n", "a moral tale", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "2009", "Spanish / Basque", "Meredith Quill", "Atlanta", "2002", "nasal septum", "a coffee house", "Jail Service", "Cheshire", "Blue Valley Northwest", "24800 mi", "liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico", "the incident Sunday evening", "Mulder", "biometrics", "Forever"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5372901722120472}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.24000000000000002, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7407407407407407, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.19047619047619044, 0.0, 1.0, 0.4, 0.5714285714285715, 0.0, 0.4, 0.2857142857142857, 0.888888888888889, 0.14285714285714288, 0.22857142857142856, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.23076923076923078, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-5608", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-6936", "mrqa_triviaqa-validation-4496", "mrqa_hotpotqa-validation-2169", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-7662"], "SR": 0.421875, "CSR": 0.5135869565217391, "EFR": 0.9459459459459459, "Overall": 0.668312830493537}, {"timecode": 23, "before_eval_results": {"predictions": ["around 100,000", "Tyneside Classical", "algebraic", "his birthtown, Smiljan", "Persia", "\"ABC-DuMont\"", "an automobile's failure rate in its fifth year of service may be many times greater than its failure rate", "the First World War", "John Constable", "Charlie Harper", "the Nominative", "James I", "Everton", "September 17th", "\u201cI came, I saw, I conquered\u201d", "the Bull Moose Party", "The Masters", "Demi Moore", "the College of Cardinals", "Cornell University", "Robert Stroud", "Alice in Alice", "caffeine", "The Blind Side", "11", "17", "Yasser Arafat", "Quentin Tarantino", "Michael Miles", "Swansea", "Wyatt Earp", "Chuck Hagel", "Hispaniola", "Bangladesh", "argument form", "Sean Maddox", "one king, one knight, two bishops, and eight pawns", "Bristol", "tinctures", "Guy Pearce", "Action Movie Anatomy", "a freeze", "the \"Queen of the Hanse\"", "the Crusades", "Matilda", "ThunderCats", "Dandy", "the Council", "Volkswagen", "King George III", "the hamsa", "India", "Justice Lawrence John Wargrave", "Thomas Jefferson", "the central plains", "Matilda of Anjou", "Barbary pirates", "Sir William Collins", "his land", "held hostage by a still unidentified group of bandits", "most devices carry few security risks", "crop Histories", "the Lone Star", "Bahadur Shah Zafar II"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5497285549595332}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.08695652173913045, 0.09090909090909091, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6205", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-6584", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3413", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-2663", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2325", "mrqa_hotpotqa-validation-1558", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-13686"], "SR": 0.46875, "CSR": 0.51171875, "EFR": 0.9411764705882353, "Overall": 0.6669852941176471}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon,", "The Victorian Alps in the northeast", "skin damage", "three", "Regulations and Directives", "Steve McQueen", "Olivier Dahan", "guitar", "Midtown", "\"unidentified aircraft, presumably hostile,\"", "shoes", "tennis", "Geneva", "call for the Dead", "Woodrow Wilson", "Menorca", "Wales, England, Scotland, Ireland and France", "murraybank Stadium", "Bulldog Drummond", "a helpline", "Henry VIII and Edward VI", "willow", "claire", "saxophonist", "post-modernism", "jaws", "Iain Banks", "Andalusia", "gluten", "Jan van Eyck", "Brenda", "carans", "andy murray", "The Lady in Question", "i", "Spartan army", "george iv", "El Capitan", "andy murray", "king of Strathclyde", "about 3 minutes for Mercury, to about 5.3 hours for Pluto.", "radionuclides", "eagle", "\"Hello Boys\"", "West Point", "a nation", "variables", "Dr Ichak Adizes", "Whittle", "andy murray", "We Interrupt This Week", "Chester", "Spanish, Rio Paraguai in Portuguese, Ysyry Paragu\u00e1i in Guarani", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Jonathan Craven", "1698", "Bill Clinton", "\"AS IS/where IS\" at a salvage yard in Kearny, New Jersey.", "a fall at her home in suburban Los Angeles.", "246", "hurricane season", "andy murray", "smallpox"], "metric_results": {"EM": 0.359375, "QA-F1": 0.3982784277504105}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.06896551724137932, 0.48275862068965514, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_squad-validation-4046", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3769", "mrqa_triviaqa-validation-3085", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-142", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-6358", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_hotpotqa-validation-4580", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-1857"], "SR": 0.359375, "CSR": 0.505625, "EFR": 1.0, "Overall": 0.67753125}, {"timecode": 25, "before_eval_results": {"predictions": ["Thoreau", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "lau placid", "fame", "Laos", "bullseye", "bluebird", "carmen cans", "300", "1894", "manhattan", "jon pertor", "camelyn addams", "Billie Holiday", "Gingerbread", "Phil Mickelson", "jon pertina flessibile", "Len Deighton", "Highland Garb Act", "Alex Garland", "toxoids", "king minus", "prime minister Benjamin Disraeli", "Johannesburg", "Martin Luther King", "Bridgeport", "frighteners", "Bagel set", "dory", "Albert Reynolds", "Newfoundland and Labrador", "Eddie Cochran", "Alessandro Volta", "OutKast", "Wanderers", "sunsy After afternoon", "the Biafra secession", "Angela Bassett", "spain", "Cuba", "dove", "Heston Blumenthal", "Harold Godwinson", "Tommy Burns", "Ritchie Valens", "spain", "carwii", "vlotho", "Gargantua", "Krypton", "hair jelly", "if the concentration of a compound exceeds its solubility", "Morning Edition", "Nicholas John \" Nick\" McCarthy", "Paul Kushner", "underprivileged.", "Alicia Keys", "80,", "Maldives", "Matt Leinart", "physicist Stephen Hawking"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5852564102564102}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-6724", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-439", "mrqa_searchqa-validation-13257"], "SR": 0.515625, "CSR": 0.5060096153846154, "EFR": 1.0, "Overall": 0.677608173076923}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "the centre of Basel", "earl of birth", "Informal rule", "earl of animals", "raven", "helium", "John Logie Baird", "chicago", "pickwick", "Titanic", "earl of chicago", "taekwondo", "chicago", "earl of Rome", "earl of fesche", "bone", "earl of chicago", "earl of Derby", "oxygen", "earl of Hesse", "earl of chicago", "Venus", "earl of wigan", "French", "god Zeus", "earl of the world", "earl of chicago", "earl of chicago", "earl of fire", "Australia", "earl of the brain", "earl of MGM", "chicago", "earl of chicago", "chicago", "earl of chicago", "beetles", "chicago", "chicago", "the Gulf of Suez", "beard", "earl of chicago", "lithium", "earl of the Universe", "earl of chicago", "earl of chicago", "earl of fc", "Rio de Janeiro", "peacock", "chicago", "chicago", "6ft 1in", "Judiththia Aline Keppel", "94 by 50", "earl of Mississippi", "photographs, film and television", "March 17, 2015", "AbdulMutallab", "earl of an eye", "two", "earl of earl", "Dennis Miller", "earl"], "metric_results": {"EM": 0.3125, "QA-F1": 0.37648809523809523}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-2368", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-5478", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-4517", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-5278", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-1409", "mrqa_hotpotqa-validation-3563", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1535", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-1898"], "SR": 0.3125, "CSR": 0.49884259259259256, "EFR": 1.0, "Overall": 0.6761747685185184}, {"timecode": 27, "before_eval_results": {"predictions": ["CBS", "16th", "60%", "smartphones", "Three", "not", "Herman Cain", "to \"wipe out\" the United States", "The man ran out of bullets and blew himself up", "Pittsburgh", "wildland", "paintings", "that they are angry and scared,", "Elena Kagan", "humbert humbert", "an estimated 750", "the North Korean regime intends to fire a missile toward Hawaii", "prc", "the front door", "prc", "allergens", "$2 billion", "prc", "Rawalpindi", "Santaquin City, Utah,", "Sunday", "four", "Indonesian Hercules", "Johannesburg", "nearly $2 billion", "Six members of Zoe's Ark", "2002", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"It has never been the policy of this president or this administration,\"", "Herman Thomas", "prc", "Lee Myung-Bak", "kerstin", "that they don't feelMisty Cummings and Crystal Sheffield,", "was made out of either heavy flannel or wool -- fabrics that would not be transparent when wet,\"", "Melbourne", "into the Southeast,", "Sunday,", "$273 million", "Salt Lake City, Utah,", "millionaire's surtax", "an animal tranquilizer,", "Arlington National Cemetery", "prc", "Jaime Andrade", "1994,", "dance Your Ass Off", "Santiago Ram\u00f3n y Cajal", "Pittsburgh", "a fortified complex at the heart of Moscow", "Magic Circle", "prc", "prc", "Lin-Manuel Miranda", "15,024", "novelist", "mantle", "prc", "marshmallows"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5079534181096681}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.7272727272727272, 0.5, 1.0, 0.0, 0.6666666666666666, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.9166666666666666, 0.8, 0.0, 0.0, 0.6666666666666666, 0.2222222222222222, 0.38095238095238093, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_squad-validation-5644", "mrqa_squad-validation-526", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-1534", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-282", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-4411", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-13251"], "SR": 0.359375, "CSR": 0.4938616071428571, "EFR": 0.975609756097561, "Overall": 0.6703005226480836}, {"timecode": 28, "before_eval_results": {"predictions": ["Denver's Executive Vice President of Football Operations and General Manager", "illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins,", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Jamie Elman", "the highway between the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Joseph Nye Welch", "200 lakh rupees ''", "members of the gay ( LGBT ) community", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "Twickenham Stadium", "the largest wildlife conservation charity in Europe", "1783", "Continental drift", "Julie Adams", "a combination of genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Wayne Warren as Jarius `` G", "Thirty years after the Galactic Civil War,", "''", "Padm\u00e9 Amidala", "October 22, 2017", "April 17, 1982", "the Speaker of the House of Representatives", "London, United Kingdom", "its judgment", "tourneys or slow wheels", "the population", "Club Bijou on Chapel Street", "pre-Columbian times", "the central plains", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "guests dress up in costumes", "the gastrointestinal tract through a series of ducts", "Kenny Anderson", "beneath the liver", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "Nathan Hale", "Jesse Frederick James Conaway", "naos", "near temples", "the port of Nueva Espa\u00f1a to the Spanish coast", "September 19, 2017", "West Egg on prosperous Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "winter", "Charles Bird", "Laura Williams and Sally Dworsky", "Flanagan", "Florida.", "Mediterranean", "Manor of More", "Jeffrey Perry", "the Cash for Ash scandal", "Michael Partain,", "1,700 year old Roman mosaic entitled Chamber of the Ten Maidens.", "The Orchid Thief", "\"Saw the nakedness of his father\"", "Balfour Declaration", "Narrative of Arthur Gordon"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5995828649442233}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631577, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.16666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-7330", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-2150", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-12829"], "SR": 0.515625, "CSR": 0.49461206896551724, "EFR": 0.9354838709677419, "Overall": 0.6624254379866518}, {"timecode": 29, "before_eval_results": {"predictions": ["Sophocles' play Antigone", "Meuse, through the Hollands Diep and Haringvliet estuaries, into the North Sea", "In 1806 during the Napoleonic Wars", "New Delhi", "MacFarlane", "the final two games", "Hon July Moyo", "many forested parts of the world", "Narendra Modi", "Fossil fuels such as coal and petroleum", "Aaron Harrison", "White House Executive Chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Emily Blunt", "Panthalassa", "Jonathan Breck", "the epidermis", "Joe Pizzulo and Leeza Miller", "Ming", "201", "Chuck Noland", "St. Louis Blues", "Britney Spears", "Waylon Jennings", "Nancy Jean Cartwright", "Beyonc\u00e9", "Ephesus", "Boston Red Sox", "1996", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "United States customary units", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "February 2002", "September 1959", "Louis Hynes", "Bonnie Lipton", "\" Tip and Ty ''", "the alpha level", "the fourth quatrain", "Elijah", "the central plate", "Tagalog or English", "Ernest Rutherford", "Napoleon Bonaparte", "the 12th century", "Yosemite National Park", "Norman Pritchard", "April 2011", "Mustelidae", "the Big Bang", "King Henry VI", "July 1, 2003", "motorsport world championship", "Kauai", "the Defense of Marriage Act", "Bronx.", "almost 9 million", "\"Tennessee Waltz\"", "abacus", "Cyrus"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6051156085178262}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true], "QA-F1": [0.5, 0.18181818181818182, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6638", "mrqa_squad-validation-9225", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4865", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-2195", "mrqa_newsqa-validation-1426"], "SR": 0.46875, "CSR": 0.49375, "EFR": 0.9705882352941176, "Overall": 0.6692738970588235}, {"timecode": 30, "UKR": 0.599609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.806640625, "KG": 0.42734375, "before_eval_results": {"predictions": ["space suit materials", "1992", "four", "Genesis", "real estate", "Louisiana's Bayou", "the carat", "Mission: Impossible", "di- dia- + -konos", "Edinburgh", "Merahi metua no Tehamana", "Galpagos", "Bill Murray", "Battle of Chancellorsville", "boxing", "Xbox One", "Suez Canal", "Dave Matthews Band", "henry hODGEPODGE", "dentures", "Friday the 13th", "Kinko's", "a platypus", "photon", "the Skull Beneath the skin of the Mango", "Cherokee Nation of Oklahoma", "necropolis", "Eleanor Roosevelt", "Oyster", "henry vi", "henry Gill", "Bamboos", "Sir Isaac Newton", "Unabomber", "Narnia", "Viktor Frankl (1905 1997)", "Ruby", "librettos", "Gulliver", "Elizabeth Taylor", "Alexis Arquette", "bison", "henry Hendrix", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "splinting", "milk", "pannonian Plain and the central Balkans", "paralysis", "henry vi", "angular cheilitis", "Castleford", "annually ( usually in May ) at the Palais des Festivals et des Congr\u00e8s", "henry vi", "Sahara desert", "henry vi", "American action horror-thriller film directed and edited by Adam Wingard and written by Simon Barrett", "Headless Body in Topless Bar", "political correctness", "Zoabi", "Dodi Fayed, and their driver, Henri Paul.", "homicide by undetermined means, said Jan Garavaglia, medical examiner for Orange County, Florida."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5306547619047619}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.14285714285714288]}}, "before_error_ids": ["mrqa_squad-validation-3969", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-15473", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-4851", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2855", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-6237", "mrqa_hotpotqa-validation-1239", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3728"], "SR": 0.453125, "CSR": 0.49243951612903225, "EFR": 1.0, "Overall": 0.6652066532258064}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Khasar", "2007", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor )", "the University of Oxford", "Before 1923", "slowly adding excess bromine to a hot solution of phenolsulfonphthalein in glacial acetic acid", "winter", "the Khoisan language of the \u01c0Xam people", "the Khasi and Jaintia Hills", "either in front or on top of the brainstem", "Janie Crawford", "Jim Capaldi, Paul Carrack, and Peter Vale", "A to B", "6 January 793", "the Immigration and Naturalization Service's Fore forensic Document Laboratory", "Blind carbon copy to tertiary recipients", "the `` round '', the rear leg of the cow", "1957", "Martin Lawrence", "An error does not count as a hit but still counts as an at bat for the batter unless, in the scorer's judgment, the batter would have reached first base safely", "Andreas Vesalius", "Moscazzano", "Kristy Swanson", "sedimentation", "Shinsuke Nakamura", "Jay Baruchel", "the island of Tasmania", "revolution or orbital revolution", "the Houston Astros", "Six Degrees of Separation", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "the retina", "the fascia surrounding skeletal muscle", "Pangaea", "2017", "near the inner rim of the Orion Arm", "Ricky Nelson", "the player shouts in order to attract the listener's attention", "Debbie Gibson", "Mad - Eye Moody and Hedwig", "the Mishnah", "The Shiva Linga", "mid-August", "Angola", "the King James Bible", "Harlem River", "1998", "R.E.M.", "332", "above the light source and under the sample in an upright microscope", "Auburn Tigers football team", "Illinois", "Northumberland", "Northern Ireland", "Travis", "the Boston Bruins", "Adam Dawes", "Democrats", "Zed", "the Mumbai suburb of Chembur,", "a dummy", "Aristotle", "the Ventured"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6382867918261871}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.6666666666666666, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.6666666666666666, 0.888888888888889, 1.0, 1.0, 0.5862068965517241, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6428571428571429, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 0.5, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6169", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-5599", "mrqa_triviaqa-validation-3940", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-5831", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3518", "mrqa_searchqa-validation-6752"], "SR": 0.46875, "CSR": 0.49169921875, "EFR": 0.8823529411764706, "Overall": 0.6415291819852941}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "1985", "1982", "Albert Lee \"Al\" Ueltschi", "Giotto di Bondone", "1985", "more than 26,000", "Lakshmibai", "the Premier League", "French", "2009", "a fictional world", "himpanzee", "Robby Benson", "Greg Gorman and Helmut Newton", "Shameless", "stolperstein", "1901", "Ernst Abbe", "Arsenal FanTV", "Bambi, a Life in the Woods", "Germaine", "2004", "Dario Franchitti", "one season", "\"Twice in a Lifetime\"", "the Sun", "Greg Hertz", "1984 in Kolkata", "\"The Walking Dead\"", "Ted Nugent", "an American jewelry designer", "Charlie Wilson", "Maleficent", "Coll\u00e8ge de France", "Miami-Dade County", "Marty Ingels", "1945", "one of WSU's most famous alumni, Edward R. Murrow", "Conservatorio Verdi in Milan", "Mindy Kaling", "June 10, 1982", "beer", "Liga MX", "Donald Duck", "Falz", "Lord Chancellor of England", "a Teachta D\u00e1la (TD)", "The English Electric Canberra", "Richa Sharma", "48,982", "The Sound of Music", "83", "Michigan State Spartans", "Frank Langella", "an elephant", "cuckoo", "red bull acid", "the British capital's other two airports, Stansted and Gatwick,", "homicide", "maintain an \"aesthetic environment\" and ensure public safety", "Charles de Gaulle", "the M1 Abrams", "Hannah Montana"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5872996794871794}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.2, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-444", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-4361", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-4655", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4641", "mrqa_searchqa-validation-4628"], "SR": 0.484375, "CSR": 0.4914772727272727, "EFR": 1.0, "Overall": 0.6650142045454545}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "special university classes, called Lehramtstudien (Teaching Education Studies)", "CTV", "13\u20133,", "American", "July 25 to August 4", "1958", "Norway", "twenty-three", "Crips", "The Crowned Prince of the Philadelphia Mob", "Kentucky Derby", "Charles Edward Stuart", "historic buildings, arts, and published works", "November 6, 2018", "Batman", "eastern Tennessee", "G\u00e9rard Depardieu, Daniel Auteuil", "many books, films and other media", "King Duncan", "Europop", "1835", "Mayor Ed Lee", "Ghana", "Norwegian", "Dutch", "1976", "January 23, 1898", "light quadricycles (L6e)", "Israel", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "political geographer Karl Haushofer", "United States of America", "Ryukyuan", "coaxial cable", "November 15, 1903", "Ant Timpson", "1961", "1952", "Indian", "one child, Lisa Brennan-Jobs", "Pablo Escobar", "ZZ Top", "Steve and Rudy", "Russian Empire", "bi-fuel", "National Park", "King of Cool", "The border between the Cocos Plate and North American Plate", "Barry Bonds", "Owen Vaccaro", "Vitcos", "Exile", "a downtown caf\u00e9", "\"It has not intercepted any Haitians attempting illegal crossings into U.S. waters.", "U Win Tin,", "$1.45 billion", "onomatopoeia", "Southeast Asia", "a bone"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6916508838383839}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.9090909090909091, 1.0, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-4473", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_naturalquestions-validation-1519", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-742", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-15477"], "SR": 0.546875, "CSR": 0.4931066176470589, "EFR": 0.9655172413793104, "Overall": 0.6584435218052739}, {"timecode": 34, "before_eval_results": {"predictions": ["Peyton Manning", "the applied force", "over 20 million records", "a simple iron boar crest", "Vienna", "the greater risk-adjusted return of value stocks over growth stocks", "the Harpe brothers", "Bill Clinton", "Dirk Nowitzki", "Detroit, Michigan", "Bury St Edmunds, Suffolk, England", "novelty songs, comedy, and strange or unusual recordings", "Mahoning County", "[6 January 1915 \u2013 16 November 1973", "There Is Only the Fight", "Bohemia", "Dobbs Ferry, New York", "The Washington Post", "400 MW", "Ghanaian", "Household Words", "Northolt Aerodrome", "Kagoshima Airport", "Minette Walters", "CTV", "comic books", "2013", "Les Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Louis King", "gull-wing doors", "devious authority figures", "\"codenamed Operation Neptune\"", "Attack the Block", "House of Commons", "Hessian colonel", "Battle of Chester", "Wayne County, Michigan", "Samoa", "mistress of the Robes", "Duchess Eleanor", "macho", "November 13, 2015", "Guardians of the Galaxy Vol.", "President of the United States", "1963", "Bologna Process", "Paris", "Nebraska Cornhuskers", "Salman Rushdie", "FUTA, I.R.C. ch. 23", "the Hongwu Emperor", "commemorating fealty and filial piety", "Chihuahua", "Arkansas", "septic throat", "1979", "his father", "$55.7 million", "Red Heat", "Miriam Makeba", "a mesio-occlusal cavity"], "metric_results": {"EM": 0.5, "QA-F1": 0.6375868055555556}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4, 1.0, 0.6, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-384", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-2136", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-712", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-1420", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-4143", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-3158", "mrqa_searchqa-validation-9394"], "SR": 0.5, "CSR": 0.4933035714285714, "EFR": 1.0, "Overall": 0.6653794642857143}, {"timecode": 35, "before_eval_results": {"predictions": ["the DuMont Television Network", "Mount Kenya", "Albany", "1908", "3 May 1958", "1986", "Ronald Wilson Reagan", "Chiltern Hills", "Ted 2", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "country music", "The Hawai\u02bbi State Senate", "Operation Watchtower", "Paul W. S. Anderson", "15 February 1970", "Talib Kweli", "Shooter Jennings", "Cincinnati", "\"Bad Moon Rising\"", "Kris Kristofferson", "\"Spanky\" Fincke", "violet", "Trey Parker and Matt Stone", "Matt Gonzalez", "Helensvale", "1979", "\u00c6lfgifu of York", "PlayStation 4", "Malta", "1966", "Key West", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fabio Cannavaro", "\"Chappelle's Show\"", "Prince George's County", "EQT Plaza", "1891", "Aerol\u00edneas A\u00e9reas", "Gainsborough Trinity Football Club", "Los Angeles", "October 13, 1980", "a water sprite", "Afghanistan", "Syracuse University", "FIFA Women's World Cup", "Orange County", "76,416", "various submucosal membrane sites", "mathematical modeling and statistical estimation", "Brian Steele", "a supercomputer", "Albert Reynolds", "Grover Cleveland", "U.S. senators", "Current TV", "two", "one bath", "The Lost Boys", "Succotash"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6618063071188072}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.29629629629629634, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6592", "mrqa_triviaqa-validation-2712", "mrqa_searchqa-validation-16021"], "SR": 0.59375, "CSR": 0.49609375, "EFR": 0.9615384615384616, "Overall": 0.6582451923076923}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "whether to close some entrances, bring in additional officers, and make security more visible to the public.", "a music video on his land.", "a bank", "in July for A Country Christmas", "Giovanni Falcone and Paolo Borsellino", "Tulsa, Oklahoma.", "381,000", "Old Trafford", "\"release\" civilians", "Number Ones", "Zac Efron", "the Indian embassy in Kabul", "a Los Angeles grand jury room", "jinx", "Annie Duke", "an arms embargo on Israel,", "that the legislation will foster racial profiling,", "producing rock music with a country influence.", "the Kirchners", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "to root out terrorists within its borders.", "a violent separatist campaign", "the ancient Greek site of Olympia", "3,000", "closing these racial gaps", "Detroit, Michigan,", "22", "3-0", "150", "two remaining crew members from the helicopter,", "U.N. agencies", "more than 30", "the man was dead,", "one", "taller than Burj Dubai tower", "to discuss the fight against the Taliban and al Qaeda in Afghanistan and Pakistan,", "Virgin America", "fuel economy and safety while boosting", "Raymond Soeoth", "summer", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "2nd District of Colorado", "mental health and recovery", "56", "\"Gruesome photos from the scene", "Frank Ricci", "the men in white hoods", "90", "Cash for Clunkers program", "Argentina", "in 1998", "103", "Carolyn Sue Jones", "vanilla", "Hercules", "to divide the world into six major climate regions", "Gian Carlo Menotti", "Basketball Hall of Fame", "Semites", "brother", "brother William", "Nokia Company Detaiils", "Apollo"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5576115366366725}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.5714285714285715, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.923076923076923, 0.8, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 0.4, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.9565217391304348, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-4038", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-2623"], "SR": 0.4375, "CSR": 0.4945101351351351, "EFR": 1.0, "Overall": 0.665620777027027}, {"timecode": 37, "before_eval_results": {"predictions": ["over 760 mm (2 ft 6 in) narrow gauge lines", "the state's attorney", "Abdullah Gul,", "Ed McMahon", "success in response to the crisis in Darfur.", "off Somalia's coast.", "hand-painted Swedish wooden clogs", "an upper respiratory infection", "Of the seven killed, two were in 1986, three in 1995, one in 1997 and one in 2007.", "tells stories of different women coping with breast cancer in five vignettes.", "Gov. Bobby Jindal", "Dr. Jennifer Arnold and husband Bill Klein,", "Facebook and Google,", "Damon Bankston", "Justine Henin", "Defense Minister Kim Kwan Jim", "Robert Mugabe", "a female soldier", "an occupation", "J. Crew", "$1.4 million", "the Democratic VP candidate", "al-Aqsa mosque", "momentous discovery", "a three-story residential building in downtown Nairobi.", "Robert Barnett", "South Korea", "Matthew Fisher", "Zimbabwean", "Ben Roethlisberger", "three", "43 percent", "Sgt. Jason Bendett", "Brazil", "Austin, Texas,", "$14.1 million", "a jury", "Salt Lake City, Utah,", "Sunday", "Robert Mugabe", "13", "One of Osama bin Laden's sons", "for security reasons and not because of their faith.", "We tortured (Mohammed al ) Qahtani and five other men in connection with the 9/11 attacks.", "autonomy", "the Arctic north of Murmansk down to the southern climes of Sochi,", "Long Island convenience store", "Ma Khin Khin Leh,", "400 years", "Kerstin", "Washington State's decommissioned Hanford nuclear site,", "the breast or lower chest of beef or veal", "winter", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Georgia", "bullfight", "Viennese", "1887", "Atlantic Coast Conference", "uncle Juan Nepomuceno Guerra", "Pennsylvania", "Rabbit", "Brunswick County", "Labrador"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5548586923804609}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.1111111111111111, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.5263157894736842, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8235294117647058, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.4, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2949", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9856", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-12609"], "SR": 0.453125, "CSR": 0.493421052631579, "EFR": 1.0, "Overall": 0.6654029605263158}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven", "The Mexican military", "Pakistani officials,", "$7.8 million", "Stratfor's", "Madeleine K. Albright", "Roland Martin", "ties,", "the German Foreign Ministry,", "10,000 refugees,", "vitamin \"drips\"", "Red Lines,", "in body bags on the roadway near the bus,", "40", "georgia peach,", "Islamic", "in the heart of Los Angeles.", "October 29", "Sunni Arab and Shiite tribal leaders", "credit card information", "stings", "North Korea", "Hong Kong", "from the families of three missing military men,", "recanted her allegations,", "ties,", "Ahmed,", "Saturday's Hungarian Grand Prix.", "from the Zimbabwean people,\"", "from her small home in gritty Soweto township.", "in an artificial coma", "7-1", "Africa", "because they suffer from depression,", "Zimbabwe's", "Richard Phillips", "first grand Slam,", "\"it should stay that way.\"", "CNN", "Joe Pantoliano,", "the strength of its brand name and the diversity of its product portfolio,", "U.S. State Department and British Foreign Office", "Monday's suicide blast", "Alexandra Mac Keown", "sculptures", "Pakistan's High Commission in India", "Bryant Purvis,", "pain-relief", "In 1871", "Frederick Chiluba, Levy Mwanawasa, Rupiah Banda, Michael Sata, and current President Edgar Lungu", "philosophy of mind", "animals", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "greed", "twice", "Sesame Street"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5548014919108668}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.30769230769230765, 0.0, 1.0, 0.0, 0.0, 0.125, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.2666666666666667, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-13907"], "SR": 0.46875, "CSR": 0.49278846153846156, "EFR": 1.0, "Overall": 0.6652764423076923}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "better", "ovuliferous", "high sewing", "Silver Hatch", "feet, legs, and hands", "Ethiopia", "red Admiral", "warring factions", "Harrier", "astronaut", "special administrative zones", "Alastair Cook", "Enterprise", "Three Little Pigs", "Aceania", "hip joint", "Frank Sinatra", "meninges", "English and French", "Guildford Dudley", "Munich, Germany's", "Henry Mancini", "Fred Astaire", "seven", "woe", "Sudan", "French", "drama", "Proverbs", "stand-up comedian", "Jamaica", "Tornado,", "drama", "netherland", "pancreas", "peter-paul-and-mary/puff-the-magic-dragon", "football", "Antoine Lavoisier", "Trotsky", "Neuna", "laws", "Pet Shop Boys", "Matthew Boulton", "Algiers", "Marks & Co,", "Aabaptists", "utists", "Hebrew alphabet", "Jim Davidson", "virus", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors,", "Garfield Sobers", "in the mountains outside City 17,", "Daniel Louis Castellaneta", "Johannes Vermeer", "O.T. Genasis", "Climatecare,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Schalke", "Lost in America", "Autumn", "Soviet Union", "Republicans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5210152511961723}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.2631578947368421, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3132"], "SR": 0.453125, "CSR": 0.491796875, "EFR": 1.0, "Overall": 0.665078125}, {"timecode": 40, "UKR": 0.662109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.841796875, "KG": 0.41328125, "before_eval_results": {"predictions": ["a \"chameleon circuit\"", "smith", "belgian", "hebrew", "pangram", "Live and Let Die", "smith", "smith", "brazil", "Robert Hooke", "Hadrian", "smith", "Sony", "smith", "green", "15, 1215", "delphinium", "Robinson Crusoe", "Charles Dickens", "belgian", "Egypt", "neutron star", "grommets", "New York Yankees", "Four Tops", "hudd", "July 20,", "9", "bali", "lilac", "Hilary Swank", "a scarlet", "a dove", "smith", "smith", "smith", "two", "smith", "Who's the cat", "the legs", "daily Mirror", "smith", "horse", "indus", "ceylon", "machu", "Carl Davis", "Madness", "smith", "Kansas", "australian", "A marriage officiant", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "Port Moresby, Papua New Guinea", "bass", "Security Management", "eight", "Russia", "her family is \"not defined by religion,\" Fakih", "smith", "Hank Aaron", "Gina", "Octopus"], "metric_results": {"EM": 0.40625, "QA-F1": 0.44895833333333335}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4551", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-1908", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-1746", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-3771", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6885", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-5984", "mrqa_naturalquestions-validation-6903"], "SR": 0.40625, "CSR": 0.48971036585365857, "EFR": 1.0, "Overall": 0.6813795731707317}, {"timecode": 41, "before_eval_results": {"predictions": ["Edward Teller", "food, music, culture and language of Latin America", "Los Angeles.", "\"still trying to absorb the impact of this week's stunning events.\"", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Tim Clark, Matt Kuchar and Bubba Watson", "Philip Markoff,", "Haeftling", "forgery and flying without a valid license,", "Sea World in San Antonio,", "camorra clan", "Nat King Cole", "the 1800s", "convicts caught with phones", "16", "cancer", "$40", "\"fragrances of fries drifting under Mona Lisa's nose\"", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "France", "President Obama and Britain's Prince Charles", "more than 100", "South Africa's", "William S. Cohen", "air support.", "back at work,\"", "get out of the game,", "two people were found killed and a third person is still believed missing in a morning explosion,", "five", "the hiring of hundreds of foreign workers", "The father of Haleigh Cummings,", "Elisabeth's father,", "his club", "they", "$60 billion", "$50", "J.G. Ballard,", "Republicans", "\"appeared in the pages of a local newspaper apparently wiping away tears from a handkerchief as he apologized and begged for forgiveness.", "the Airbus A330-200", "United States, NATO member states, Russia and India", "fatally shooting a limo driver on February 14, 2002.", "ties", "dogs who walk on ice in Alaska.", "pleaded not guilty in an appearance last week in Broward County Circuit Court.", "China", "Steve Jobs", "\"Rin Tin Tin: The Life and the Legend\"", "Sri Lanka's", "The Arkansas weatherman", "the state's attorney", "John Brown", "Wilt Chamberlain", "Brevet Colonel Robert E. Lee", "Telegraph Media Group Limited 2017", "Rabin", "japan", "Kristy Lee Cook", "John Samuel Waters Jr.", "Norman Mark Reedus", "\"Del- phine\"", "MacArthur", "rice", "must sign a waivers prior to shooting"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5161055307539683}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.26666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0625, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.25, 0.25, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3949", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-5825", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2819", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2138", "mrqa_searchqa-validation-1621", "mrqa_naturalquestions-validation-8617"], "SR": 0.421875, "CSR": 0.48809523809523814, "EFR": 1.0, "Overall": 0.6810565476190475}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "246", "an obscure story of flowers and turned it into the masterful 1998 best-seller \"The Orchid Thief\"", "The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Obama", "20", "Oxygen Channel's", "some of the most gigantic pumpkins in the world,", "15", "AbdulMutallab, Tukel, chief of the National Security Unit", "British", "Democratic National Convention", "his native Philippines", "Kitty Kelley, biographer of the rich and famous,", "new Zealand", "Mike Meehan", "Amanda Knox's aunt", "Michael Krane", "15,000", "12", "the Gulf", "May 4", "3-0", "Zimbabwean President Robert Mugabe", "Five houses", "kill then-Sen. Obama", "10", "165", "last month,", "Ignazio La Russa", "Amir Zaki", "$40 and a loaf of bread.", "rural Tennessee", "Louisiana", "2-1", "nearly $2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "\"They are, of course, shattered.", "London's", "Prague", "more than 100", "Microsoft.", "Michael Partain,", "Mitt Romney", "Islamic militants", "prisoners", "part of the proceeds from sales", "does not grant full health-care coverage,", "in the season four episode `` Run ''", "886 AD", "1940", "Afghanistan", "sheriff Martin Howe", "tributary", "\"Shake It Off\"", "Cheshire", "Brookhaven", "Transamerica", "\"Goodbye, Columbus\"", "The Moonstone", "16"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5100439133986928}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.47058823529411764, 1.0, 0.26666666666666666, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.5, 0.5, 0.25, 0.0, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7499999999999999, 0.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-143", "mrqa_triviaqa-validation-1380", "mrqa_triviaqa-validation-6580", "mrqa_hotpotqa-validation-1900", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709", "mrqa_naturalquestions-validation-1640"], "SR": 0.40625, "CSR": 0.4861918604651163, "EFR": 1.0, "Overall": 0.6806758720930233}, {"timecode": 43, "before_eval_results": {"predictions": ["400", "14", "3-2", "the issue to a crowd at the White House,", "\"procedure on her heart,\"", "the Oaxacan countryside of southern", "the punishment for the player", "The wings,", "Vernon Forrest,", "Mandi Hamlin", "U.S. State Department and British Foreign Office", "five female pastors", "police", "\"We tortured (Mohammed al-) Qahtani,\"", "\"It's not where nature intended the animals to be,", "Six", "at 9:20 p.m. ET", "Washington.", "The Tupolev Tu-160,", "Dr. Conrad Murray", "Sheikh Abu al-Nour al-Maqdessi,", "the driver", "his health", "1,500", "from 7 p.m.", "three", "\"The e-mails]", "Aniston, Demi Moore and Alicia Keys", "January", "to hold onto his land", "Miguel Cotto", "\"It wasn't kissing and hugging --", "\"Bergdahl, 23,", "New Jersey", "thousands of anti-government protesters", "all buses, subways and trolleys that carry almost a million people daily.", "five", "Long troop deployments", "St. Louis, Missouri.", "\"It's not your car.", "a number of calls,", "Clifford Harris,", "Sweden in 1968, Nigeria in 1972 and Ghana in 1974.", "Republican", "almost 9 million", "Asashoryu,", "an upper respiratory infection,\"", "Adriano", "\"Zed,\"", "prime minister's handling of the L'Aquila earthquake,", "death squad killings", "1973", "Roanoke", "Kelley", "the skull", "the Red Sea", "Pacific", "Scotty Grainger", "Robert A. Iger", "Salgaocar", "Jane Austen", "Abercrombie & Fitch", "a soap opera", "Bobby Beathard"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6200047348484848}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.75, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5333333333333333, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-1086", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-6678", "mrqa_triviaqa-validation-3275", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-802"], "SR": 0.5625, "CSR": 0.48792613636363635, "EFR": 1.0, "Overall": 0.6810227272727272}, {"timecode": 44, "before_eval_results": {"predictions": ["help transfer and dissipate excess energy", "on Chesapeake Bay, south of Annapolis in Maryland", "peninsular mainland jutting out into the Mediterranean Sea at the southernmost tip of the Balkans", "Alex Ryan", "Wimpy's attempts to con other patrons", "2001", "2015", "Betty", "Leonard Nimoy,", "Rodney Crowell", "Jason Momoa", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "a donor molecule", "Jamie Foxx", "Iowa ( 36.6 % )", "1996", "The uvea", "The Director of the Federal Bureau of Investigation", "biological taxonomy", "Lula", "1977", "Department of Health and Human Services", "in France, shipped overseas in crate,", "development of electronic computers", "contemporary drama in a rural setting", "1938", "Kristy Swanson", "Jyotirindra Basu", "The nominating committee", "1998", "Congress in 1790", "the Colony of Virginia", "Arkansas", "1843", "at slightly different times when viewed from different points on Earth", "during the American Civil War", "the Executive Residence of the White House Complex", "420 mg", "Timothy B. Schmit", "March 2, 2016", "Thirty years after the Galactic Civil War,", "Woody Paige", "Christy Plunkett", "$75,000", "four", "in `` Blood is the New Black ''", "the part of the gastrointestinal tract between the stomach and the large intestine", "The Capture of USS Chesapeake", "from 2001 to 2017", "37", "President Lyndon Johnson", "Sarah Palin's", "Pacino", "Passion", "Kinnairdy Castle", "Teenage Mutant Ninja Turtles", "Kona", "his business dealings for possible securities violations", "nearly 40", "16", "atlantic", "Snowboarding", "dollop", "Algiers"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5991006069953438}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.9333333333333333, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.060606060606060615, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.14285714285714285, 0.6666666666666666, 0.4, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9445", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-9361", "mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-5874", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_searchqa-validation-7650", "mrqa_searchqa-validation-2656"], "SR": 0.484375, "CSR": 0.4878472222222222, "EFR": 0.9696969696969697, "Overall": 0.6749463383838383}, {"timecode": 45, "before_eval_results": {"predictions": ["run the length of their bodies and bear comb-like bands of cilia,", "an international collaborative effort led by the Hereditary Disease Foundation", "Cheryl Campbell", "The Satavahanas", "Michael Moriarty", "Canada", "90", "Virginia", "on a computer", "1940", "Doug Diemoz", "Charlene Holt", "Amanda Leighton", "Thomas Edison", "Mad - Eye Moody", "O'Meara", "Howard", "Missi Hale", "2003", "Eddie Murphy", "Asa Taccone", "Theodore Roosevelt", "1940", "parthenogenic", "Lynda Carter", "in Paradise", "Coconut Cove", "the season finale", "the biblical name of a Canaanite god associated with child sacrifice", "Mercedes -Benz", "10 June 1940", "Bill Pullman", "the `` Great G minor symphony ''", "to gain information, and allowing Dean to become a Dracula", "April 2010", "786 -- 802", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Franklin and Wake counties in the U.S. state of North Carolina", "Justin Timberlake", "generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Potter", "Ella Mitchell", "Effy", "to symbolize connection between Vesta's fire and the sun as sources of life", "Jurchen Aisin Gioro clan", "Muhammad", "Americans", "August 22, 1980", "Professor Kantorek", "guardians of the galaxy blue guy", "the next episode, `` Seeing Red ''", "(UHG)", "bushfires", "vyvyan,", "(the six others were Mickey Mouse, Donald Duck, Goofy, Pluto, Chip 'n' Dale (counting as one), and Figaro", "Big 12 Conference", "Debbie Reynolds", "Dubai", "the legitimacy of that race.", "in Salt Lake City,", "Java", "OPEC", "jazz saxophonist and composer", "U.S. President Bill Clinton"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5604387515583605}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, false], "QA-F1": [0.25000000000000006, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.9600000000000001, 0.4347826086956522, 1.0, 0.967741935483871, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4437", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3339", "mrqa_hotpotqa-validation-1172", "mrqa_newsqa-validation-637", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4457", "mrqa_newsqa-validation-2590"], "SR": 0.453125, "CSR": 0.4870923913043478, "EFR": 0.9714285714285714, "Overall": 0.6751416925465838}, {"timecode": 46, "before_eval_results": {"predictions": ["the main porch", "Pastoral farming", "The Nitty Gritty Dirt Band", "the ACU", "Luther Ingram", "Taron Egerton", "Lucius Verus", "Ishaan Anirudh Sinha", "Ray Harroun", "scrolls", "Frank Morris", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane", "the Heliocentric model of Copernicus", "a record or tuple", "the President pro tempore", "the fuel tank ( fount ), with the lamp burner attached", "electron donors to electron acceptors via redox", "T.J. Miller", "Ren\u00e9 Descartes", "the 2006 -- 06 season", "face - down on the table closer to the dealer", "1955", "the town of Acolman, just north of Mexico City", "23 September 1889", "indigenous to many forested parts of the world", "January 2018", "Cebu City", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "1923", "Hugh S. Johnson", "axons", "Lord Banquo", "joy", "lithium", "al - khimar", "276", "Anthony Hopkins", "the middle of the 15th century", "Joan Alison", "a political ideology", "c. 1000 AD", "Evaluation of alternative plans / policies", "Missouri River", "Venezuela and the remainder in Colombia", "Anakin Skywalker", "privatized", "the chief lawyer of the United States government", "2018", "C\u03bc and C\u03b4", "August 29, 2017", "wine", "Edward Woodward", "Black Swan", "Lake Wallace", "Sam Neill", "1866", "9 percent", "the Saudi town of Jeddah", "system of military trials", "letter of the alphabet", "Hinduism", "Gertrude Stein", "Ilkley"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5418456627377999}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.4, 0.30769230769230765, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.3870967741935484, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-4197", "mrqa_triviaqa-validation-2399", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1605", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-744", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-1063"], "SR": 0.46875, "CSR": 0.48670212765957444, "EFR": 0.9411764705882353, "Overall": 0.669013219649562}, {"timecode": 47, "before_eval_results": {"predictions": ["Germany", "Tim Russert", "Sebastian Lund ( Rob Kerkovich )", "Celtic", "1978", "two", "September 6, 2019", "Earle Hyman", "September 19, 2017", "the Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "Speaker of the House of Representatives", "May 17, 2010", "the frontal lobe", "the 1940s", "telecommunications, pharmaceuticals, aircraft, heavy machinery and other industries", "2 %", "approximately 5", "Games ( AKA `` appearances '' )", "the 17th episode in the third season of the television series How I Met Your Mother and 61st overall", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "the New York Yankees", "94 by 50 feet", "Sam Waterston", "Valinor ( Land of the Valar )", "an enumeration of 7 spiritual gifts originating from patristic authors", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak )", "bohrium", "Ravi Shastri", "the main road through the gated community of Pebble Beach", "a place of trade, entertainment, and education", "electron", "November 2014", "Alice's Adventures in wonderland", "Janis Joplin", "the South Pacific Ocean", "T'Pau", "Ethiopia ( Abyssinia )", "Blue laws", "in South America", "Edgar Lungu", "Donna Reed", "A status line", "House", "the Robots", "the centre of Munich", "Hermie", "2017", "Thomas Chisholm", "Tiffany", "nausea", "the sub prime mortgage", "Comcast", "Chattahoochee", "Human Sexuality in Four Perspectives", "\"Futurama\"", "Argentina", "Bismarck,", "Facebook and Google,", "decaffeinated coffee", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "the New Jersey Economic Development Authority's 20%"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5575475004313319}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.2857142857142857, 0.5, 0.47619047619047616, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.125, 1.0, 0.0, 0.4, 0.0, 0.33333333333333337, 0.0, 0.4, 0.17391304347826084, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-485", "mrqa_triviaqa-validation-4834", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-3606", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-336", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.46875, "CSR": 0.486328125, "EFR": 1.0, "Overall": 0.680703125}, {"timecode": 48, "before_eval_results": {"predictions": ["Sen. Barack Obama", "Stuttgart", "three", "Long troop deployments", "over 1,000 pounds", "looks at how children as young as eight would cope without their parents for two weeks.", "a key witness", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "people", "a man who said he had found it in the desert five months before.", "Swedish Prime Minister Fredrik Reinfeldt", "Microsoft.", "Ferraris", "2000 and 2004", "1620", "frees up a place", "Michael Krane,", "Russian bombers", "three gunmen outside the facility where aid distribution is coordinated.", "\"The First 48\" marathon", "2-1", "more than 200.", "fashion", "a one-shot victory in the Bob Hope Classic", "parents", "the chemical at the Qarmat Ali water pumping plant in southern Iraq", "\"It was perfect work, ready to go for the stimulus package,\"", "a bird strike disabled its engines", "Sicily", "racial intolerance.", "Friday", "\"Twilight\"", "Robert Kimmitt", "22-year-old college student in Boston, Massachusetts,", "both surrogacy and adoption", "in Nuevo Leon, one of two states in northeastern Mexico where drug cartel members blocked roads with hijacked vehicles", "10", "The Home Depot.", "Anil Kapoor", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli", "her mom,", "love and loss", "it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "1998", "more active role in countering the spread of the drug trade,", "London's O2 arena", "The EU naval force", "Cyprus", "a national policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies.", "$10,000 and $30,000", "1602", "Number 4, Privet Drive, Little Whinging in Surrey, England", "sugarcane", "$10 $20 $50 $100", "cap", "five", "E Street Band", "north London, England", "Chippewa", "heavy blood pressure", "best and the band wants to get back to where they belong", "the foreign exchange market ( currency )"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5049148101136087}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.08695652173913043, 0.0, 0.5263157894736842, 0.0, 0.0, 0.20000000000000004, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.15384615384615383, 0.09523809523809525, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.18181818181818182, 0.07692307692307693, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 0.0, 0.08695652173913043, 1.0, 0.7000000000000001, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 0.75]}}, "before_error_ids": ["mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-4169", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-3970", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-2918", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12129", "mrqa_naturalquestions-validation-3236"], "SR": 0.390625, "CSR": 0.484375, "EFR": 1.0, "Overall": 0.6803125}, {"timecode": 49, "before_eval_results": {"predictions": ["Pierre Laval", "Beckett", "\ufffd\u00bf\u00bd", "Michaela Tabb", "Orion", "Edward VIII", "Helen Shapiro", "falcon", "Stephen Fry", "Libya", "Pour Moi", "Robinson", "\"monks\" in English,", "Daily Mail", "William Shakespeare", "handley Page", "death and/or scientific study of dying, death and how human beings respond to the inevitability of their mortality", "Rod Laver", "Texas", "tempo marking molto allegro", "the largest and one of the most densely populated in the Mediterranean Sea", "hoy", "the need for human rights to be integral to all that the United Nations does.", "Brian Deane", "Volkswagen", "Emilia Fox", "October", "jumpin jack flash", "Catherine Zeta-Jones", "South Africa", "American Jim Braddock", "Mediterranean Sea", "1819", "the common torpedo", "Gibeon", "Hawaii - The Aloha StateHaw Hawaii", "vomiting in their dog at home.", "Richard Strauss", "sperm", "Syria", "seabird", "golf", "purpurea", "Amnesty International", "Spearchucker", "her skills", "the Kingdom of Lesotho", "Elgar/Enigma Variations", "Mauricio Pochettino", "Duke and Duchess of York", "RHDV", "the season - five premiere episode `` Second Opinion ''", "significant production of peaches as early as 1571, with exports to other states occurring around 1858", "Parker's pregnancy at the time of filming", "Dame Eileen June Atkins, DBE", "Battle of Chester", "James Gandolfini", "Bill Stanton", "Los Angeles Angels", "nine months in 1995.", "23", "Bertha", "eccentricity", "Samwise Gamgee"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5963045634920634}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2666666666666667, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.6666666666666667, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-7471", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-831", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-10445"], "SR": 0.46875, "CSR": 0.48406249999999995, "EFR": 0.9705882352941176, "Overall": 0.6743676470588235}, {"timecode": 50, "UKR": 0.603515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.83203125, "KG": 0.43203125, "before_eval_results": {"predictions": ["Christmas Carol", "Robert De Niro", "Bangladesh", "Dan Dare", "Stephen Ward", "Danish", "Berlin", "Rocky Horror Picture Show", "1925", "Prince Albert", "bill", "Pakistan International Airlines", "pig", "Popeye", "james boswell", "bull moose", "Genoa", "soprano voice", "Spectator", "Jamaica", "Jessica Simpson", "stooge", "earthquake", "Capua", "charlie Chan", "hawaii", "Audrey Hepburn", "king Louis XVIII", "Anne Boleyn", "basketball", "Thailand", "James Franco", "Cannes Film Festival", "fred burns", "London", "sarah armstrong", "hawaii", "Merry widow", "virtual images", "Wolfgang Amadeus Mozart", "Anne-Marie Duff", "Joan Rivers", "salt or sugar", "fifth oldest champion in history", "phobias", "soap", "guitar", "Toby", "Buenos Aires", "Kenny Everett", "fenn Street School", "1804", "spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "November 3, 2007", "jockey William Shand Kydd", "Marktown", "Bit Instant", "well over 1,000 pounds).", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "TV news coverage,", "james boswell", "Lamb of God", "legs", "1978"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5761295995670996}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-4901", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-5624", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5281", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-9447", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10670"], "SR": 0.515625, "CSR": 0.48468137254901966, "EFR": 1.0, "Overall": 0.670451899509804}, {"timecode": 51, "before_eval_results": {"predictions": ["cannes", "blue", "Robin Ellis", "mortadella", "bacall", "hydrogen", "the ankle and foot", "the natural world and mysticism", "bacall", "South Pacific", "bacall", "Bosnia and Herzegovina", "France", "Sparta", "Morningtown Ride", "squash", "new york", "Turkey", "bacall", "China", "diffusion", "David Bowie", "Robben Island", "bigfoot", "myrrh frankincense", "a zoom", "cannes", "Rocky Marciano", "zsa zsa Gabor", "bacall", "bacall", "can be called an Egyptian Christian or a Christian in the Middle East or Afro-Asiatic region", "a sparrow", "Eton College", "margot betti", "can be found in many shades from a domestic tabby cat", "argentina", "a highly dangerous Boojum", "can be grouped together to express any nuance of an idea,", "Valentine Dyall", "bacall", "pakistan", "The Prelature of the Holy Cross", "The Flying Pickets", "can only be solid when temperatures are below -78 oC.", "Kenya", "british festivals", "bakers van", "bacall", "stanley laurel", "blood left at crime scenes", "ummat al - Islamiyah", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf", "Daniel Radcliffe", "Chris Pine", "President Obama", "Croatia", "Tom Hanks", "bacall", "candy bar", "Rosa Parks", "can be mistaken for a bed bug"], "metric_results": {"EM": 0.40625, "QA-F1": 0.475}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-4257", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-4976", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-7596", "mrqa_naturalquestions-validation-1455", "mrqa_hotpotqa-validation-2075", "mrqa_searchqa-validation-7204", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-11960"], "SR": 0.40625, "CSR": 0.48317307692307687, "EFR": 1.0, "Overall": 0.6701502403846153}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "Alleyne v. United States", "Minnesota's 8th congressional district", "Erreway", "north Queensland", "George Clooney", "pamelyn Ferdin", "Christian Kern", "se7en", "$10.5 million (USD 8 million)", "2017", "Dutch", "2014", "rapper", "Missouri", "Rochdale, North West England", "50 best cities to live in.", "Virginia", "The Godfather Part II", "two", "bass", "Scunthorpe", "yasiin Bey", "motor vehicles", "Switzerland", "March, 1904", "Eugene O'Neill", "Revolutionary Chairman of the Libyan Arab Republic", "Scottish singer", "wooden roller coaster", "Sofia the First", "Islamic Studies", "animal foods & feed and veterinary products", "Roy Spencer", "Magnate", "The Saturdays", "Battle of White Plains", "North Carolina", "John Joseph Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "jazz", "a Hindustani classical musician of the Maihar gharana", "2009", "Northern Ireland", "1977", "Russian Ark", "Delacorte Press", "the voice of The Beast", "1624", "April 4, 2017", "Princess Leia Organa of Alder christie", "an ancient optical illusion toy", "nickel", "vickers-Armstrong", "Miami Beach", "Amy Bishop", "school,", "the lips", "Maine", "(Henry) Clay", "Richard Crispin Armitage"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5605875894938395}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.1818181818181818, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-1044", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4939", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-474", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-468", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-16103"], "SR": 0.484375, "CSR": 0.4831957547169812, "EFR": 1.0, "Overall": 0.6701547759433962}, {"timecode": 53, "before_eval_results": {"predictions": ["Michael Stipe", "1910", "30.9%", "Kittie", "American", "The Zombies", "a second platform", "The Vanguard Group", "American", "Ready to Die", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "Danish", "York County", "Seventeen", "Wake Island", "Australian Defence Force", "June 11, 1959", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Jennifer Stefanik", "Jennifer Taylor", "Estadio Victoria", "9Lives brand cat food", "Electronic Attack Squadron 135", "October 10, 1994", "Las Vegas", "42,972", "over 9,000", "Ashley Leggat", "Drunken Master II", "more than 100", "bassline", "E22", "The Allies of World War I, or Entente Powers", "Geraldine Sue Page", "Kristina Ceyton and Kristian Moliere", "music industry magazines", "Topeka, Kansas", "1964 to 1974 model years", "Big Fucking German", "law firm", "Hamlet", "Bow River and the Elbow River", "Gillian Anderson", "segues", "united Ireland", "\"Queen In-hyun's Man\"", "American musical group", "Virgil Ogletree", "# 4", "Topiary", "Quentin Tarantino", "1929", "Luca di Montezemolo", "over the", "blind Majid Movahedi,", "deck boss", "Hummer", "Marky Mark", "cheese"], "metric_results": {"EM": 0.4375, "QA-F1": 0.564288576007326}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.3333333333333333, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 0.0, 0.7692307692307693, 0.8, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-401", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_triviaqa-validation-6121", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-16209"], "SR": 0.4375, "CSR": 0.4823495370370371, "EFR": 1.0, "Overall": 0.6699855324074074}, {"timecode": 54, "before_eval_results": {"predictions": ["her brother", "in publishing", "callable", "When the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight episode", "the courts", "the boy or `` man - cub '' Mowgli", "in a official document permitting a specific individual to operate one or more types of motorized vehicles", "18", "Jewel Akens", "New England", "Irsay", "Abid Ali Neemuchwala", "summer", "Roxette", "the singer and a co-worker", "Peggy Lipton", "The Union", "the back of the head", "Authority", "drizzle", "1967", "Virginia", "due to Parker's pregnancy at the time of filming", "lakes or reservoirs at high altitudes", "the most popular forms of music in the country", "Times Square in New York City west", "the 1960s.", "IBM", "American singer Elvis Presley", "American author Elizabeth George Speare", "1998", "Karen Gillan", "the present Indian constitutive state of Meghalaya", "A rear - view mirror", "March 23, 2013", "flawed democracy", "September 23, 2017", "William Chatterton Dix", "Kristina Wagner", "Selena Gomez", "Steve Russell", "1881", "the president's power to commit the United States to an armed conflict without the consent of the U.S. Congress", "Timothy B. Schmit", "Games", "Cetshwayo", "Games", "Cambridge", "Oklahoma City", "Clear liquid", "1982", "2015", "Indian", "22", "two", "to pump money into small cities along the heartland's", "Jason Leigh", "the Ukrainian National Republic", "Napoleon's Life in His Own Words", "Arthur Schnitzler's 1926 novella"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6230931108333801}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.8125000000000001, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5294117647058824, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.823529411764706, 0.42857142857142855, 0.0, 0.923076923076923, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-716", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3483", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-2246", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-56", "mrqa_hotpotqa-validation-5124"], "SR": 0.484375, "CSR": 0.48238636363636367, "EFR": 0.8787878787878788, "Overall": 0.6457504734848485}, {"timecode": 55, "before_eval_results": {"predictions": ["the 1960s.", "Charlton Heston", "without deviating from basic strategy", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "the titular `` fool '', a solitary figure who is not understood by others, but is actually wise", "when the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "1922", "2017", "Adam Sandler", "2017", "Ethiopia", "Northern Ireland", "Abid Ali Neemuchwala", "Hodel", "real - time chat", "James Fleet", "one", "the medial epicondyle of the humerus", "Demon", "Massachusetts", "the citizens", "The at sign, @, also commonly called the at symbol or commercial at", "star", "60", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum'( compounded annually )", "9.0 -- 9.1", "the Deathly Hallows", "1966", "201", "2026", "1926", "October 20, 1977", "Cetshwayo", "50", "al - Mamlakah al - \u02bbArab\u012byah", "Garbi\u00f1e Muguruza", "17 % of the GDP", "the Detroit Tigers", "Rockwell", "in outer space", "Charlotte Hornets", "the lumbar cistern, a subarachnoid space inferior to the conus medullaris", "February 7, 2018", "God is great", "Elizabeth Taylor", "Sweden", "fourth", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "a delegation of American Muslim and Christian leaders", "iTunes,", "drummer", "the Qing", "Black Sox Scandal", "seven"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6135550213675214}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.923076923076923, 1.0, 0.6, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.5714285714285715, 1.0, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-1144", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-10163", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_triviaqa-validation-6549", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-9490", "mrqa_newsqa-validation-1458"], "SR": 0.484375, "CSR": 0.482421875, "EFR": 1.0, "Overall": 0.67}, {"timecode": 56, "before_eval_results": {"predictions": ["lady Chatterley et l'homme des bois", "Swedish", "The West Wing", "Adam Smith", "Luxembourg", "la Palma", "Salvador Domingo Felipe Jacinto Dal\u00ef\u00bf\u00bdnech", "Semiquaver", "tyne", "motorized", "The Blues Brothers", "onion", "1984", "Frottage", "Geraldine Somerville", "Kevin Painter", "trade in Betsy", "Messenger", "goose bump", "duck-billed platypus", "Montr\u00e9al", "Jeffrey Archer", "Four Tops", "Velazquez", "WED", "Aviva plc", "Charlie Chan", "Hearts of Darkness", "taekwondo", "Ishmael", "jubilee line", "the Comte de la F\u00e8re", "\"Elijah's Chariot,\"", "the head", "Passepartout", "Chuck Hagel", "haute", "zephyr", "300", "speedway", "France", "James Garner", "marinated dried fruits", "Jay-Z", "bird", "sexual arousal to pubescent children", "George lV", "Margaret Beckett", "Bozeman Daily Chronicle", "New Zealand national women\u2019s team", "United States", "the 18th century", "Austria - Hungary", "Sean O' Neal", "Donna Summer", "The New Yorker", "In Pursuit", "Aung San Suu Kyi", "his brother to surrender.", "\"Walk -- Don't Run\"", "Gene Wilder", "\"South Park\"", "Territorial Capital", "The Simpsons Movie"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6383184523809523}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true], "QA-F1": [0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-6719", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7433", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-13467"], "SR": 0.59375, "CSR": 0.484375, "EFR": 1.0, "Overall": 0.670390625}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "'Nelson'", "ellis", "Utah", "black lights", "lacrosse", "Packers", "utrecht", "Operation Overlord", "eldorado", "Virginia", "fern coltrane", "yachts", "potato", "1215", "pullover", "Diffusion", "the Wye", "jack London", "South Carolina", "ellesmere port", "Parsley", "lingua", "Santiago", "cambridge", "Lynda Baron", "Robert Guerrero", "Juan Manuel de Ayala", "90%", "Sven Goran Eriksson", "jane coltrane", "Monaco Grand Prix", "star", "Jordan", "to write things down", "Motown", "Sudan", "oliver mellors", "robbie coltrane", "collective noun for frogs", "ireland", "Anschluss", "silk", "oliver mellors", "medical", "Leo Tolstoy", "Austria", "oasis", "killer energy drink", "salvador", "stamps", "the university's science club", "2003", "Magnavox Odyssey", "Clark County", "to receive the benefits of the Morrill Acts of 1862 and 1890", "fifth level", "Veracruz", "Japan", "Dr. Maria Siemionow,", "vertex", "harry", "oliver mellors", "the Red Sea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.582014933166249}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.4444444444444444, 1.0, 1.0, 1.0, 0.9473684210526316, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-6233", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-6653", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2792", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-5198", "mrqa_searchqa-validation-8276"], "SR": 0.515625, "CSR": 0.4849137931034483, "EFR": 0.967741935483871, "Overall": 0.6640467707174639}, {"timecode": 58, "before_eval_results": {"predictions": ["oscar", "apples", "Galapagos Islands", "\u201cFor Gallantry;\u201d", "us", "onions", "us", "Mariah Carey", "blancmange", "times", "four inches", "Ishmael", "dicks Francis", "larkin", "dictes", "the opossum", "the Soviets", "european", "mceau", "charlie", "m*A*S*H", "helene hanff", "capua mortua", "condor", "molybdenum", "France", "Laos", "sports agent", "Puerto Rico", "John Huston", "posh", "domestic cat", "bajan", "aurochs", "alphonse diaghilev", "mike ather", "capone", "mercury", "capone", "michael andrew ather", "oscar", "Mary Poppins", "us", "Queensland", "oscar Lazenby", "George Eastman", "united states", "Kenya", "george iv", "tuscany", "Nissan", "Ptolemy", "Toto", "commemorating fealty and filial piety", "Heather Langenkamp", "Operation Iceberg", "in simple language", "mesac Damas", "Revolutionary Armed Forces of Colombia", "the Itawamba County School District and itawamba Agricultural High School.", "Dr Jekyll", "pole vaulting", "USS Maine", "Coleridge"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5442708333333333}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-2127", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-1089", "mrqa_triviaqa-validation-1383", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-3453", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-1832", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-2487", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-3341", "mrqa_hotpotqa-validation-1545", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-10238", "mrqa_searchqa-validation-444", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-5746"], "SR": 0.453125, "CSR": 0.484375, "EFR": 0.9714285714285714, "Overall": 0.6646763392857143}, {"timecode": 59, "before_eval_results": {"predictions": ["Jesus", "Aristotle", "Eliot Cutler", "a goalkeeper for Manchester City", "David Weissman", "london tipton", "comedy", "November 29, 1895", "the Goddess of Pop", "Sir Philip Anthony Hopkins", "near Philip Billard Municipal Airport", "the Big 12 Conference", "usually last two years", "Walt Disney and Ub Iwerks at the Walt Disney Studios", "Martin \"Marty\" McCann", "big four", "gainsborough Trinity Football Club is a football club based in Gainsborough, Lincolnshire, England", "turns out to be a terrible date", "$7.3 billion", "best known for his ten seasons with the Charlotte Hornets", "george i", "sixteen", "Rural Electrification Act", "2015", "Nicholas \" Nick\" Offerman", "Golden Globe Award", "XXXTentacion", "Dire Straits", "the professional and personal lives of several WAGs", "MGM Grand Garden Special Events Center", "Best Rock Song", "Pieter van Musschenbroek", "1979", "the 70 m and 90 m", "captain of the Yeomen of the Guard", "film", "Bulgarian", "KXII", "\"Dr No\" Bond", "Eastern College Athletic Conference", "the Indian state of Gujarat", "William Corcoran Eustis", "World Outgames", "Norwood", "Saturday", "Shooter Jennings", "Can't be Tamed", "Bolton, England", "Stephen Hawking", "John Duigan", "Saoirse Ronan", "Gene Kelly and Donald O'Connor", "a region in Greek mythology", "Todd Bridges", "lemon", "janeans", "Jaipur", "in the southern Gaza city of Rafah", "to the U.S. Consulate in Rio de Janeiro", "CNN", "ende gut, alles gut", "charles Ranhofer", "Rock Island", "Captain James Cook"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6618751040626041}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-679", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1491", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-561", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-2429", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-3147", "mrqa_newsqa-validation-4188", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-15613"], "SR": 0.515625, "CSR": 0.4848958333333333, "EFR": 1.0, "Overall": 0.6704947916666667}, {"timecode": 60, "UKR": 0.615234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.8203125, "KG": 0.43203125, "before_eval_results": {"predictions": ["Jacob Packer", "the Constitution of India", "Padawan Ezra Bridger", "786 -- 802", "onoma tou Patros", "19 July 1990", "Chuck Connors", "Nat Finkelstein", "September 1972", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "the British Isles of French and Latin origin", "BC Jean", "the BBC", "19 days in 1998", "961", "Jay Baruchel", "December 1886", "the U.S. now had the initiative on the military, diplomatic and public relations fronts", "at the state and national governmental level", "The courts", "Holly Marie Combs", "Nick Faldo", "2018", "Coroebus of Elis", "Solar System", "Crepuscular animals", "Hank Williams", "due to not being profitable", "in a nearby river bottom", "the nature of Abraham Lincoln's war goals", "around 10 : 30am", "David Ben - Gurion", "RMS Titanic", "Grey Wardens", "in San Francisco Bay", "Eight full seasons", "brass band parades", "Vasoepididymostomy", "the fourth quarter of the preceding year", "Rosalind Bailey", "a feminine form of the Hebrew Yohannan", "Broken Hill and Sydney", "the Reverse - Flash", "save, rescue, savior", "sedimentary rock", "Amartya Sen", "the scouting combine", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "UTC \u2212 08 : 00", "near Camarillo, California", "Cordelia", "tomato", "Guru Nanak", "professional footballer", "mixed martial arts", "James Tinling", "Rima Fakih", "165-room", "David Bowie", "Peach Melba", "\"When to go\"", "Godiva", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6366826749639249}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.5714285714285715, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-437", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-7489", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527"], "SR": 0.546875, "CSR": 0.4859118852459017, "EFR": 1.0, "Overall": 0.6706980020491804}, {"timecode": 61, "before_eval_results": {"predictions": ["the 1994 season", "Tenochtitlan", "Conrad Lewis", "lead vocalist Bart Millard", "Pangaea", "111", "Kiss", "Justice Harlan", "full '' sexual intercourse", "Valene Kane", "Georgia Groome", "autompne", "1973", "T.J. Miller", "The Deserted Village", "Malina Weissman", "Pasek & Paul", "state", "741 weeks from 1973 to 1988", "the chief Senate spokespeople for the political parties respectively holding the majority and the minority in the United States Senate", "neutral", "31 December 1960", "Space is the Place", "1999", "the beginning of the American colonies", "the concentration of a compound exceeds its solubility", "February 16, 2018", "lion", "Andrew Lloyd Webber", "Dollree Mapp", "the 15th century", "electron donors", "Nehemiah", "In a cell", "Detroit", "Mickey Mantle", "Shawn", "Sarah Simone Vangsness", "dress shop", "the 9th century", "1603", "September 25, 1987", "the 1970s and'80s", "1939", "Randy Newman", "1956", "Ravi River", "prokaryotic", "the # 4 School of Public Health in the country", "an active supporter of the League of Nations", "New York City", "shoes and boots", "(D) Goran Eriksson", "horses", "Vanarama", "Odysseus", "40 million", "Dr. Frank A. Papay,", "Brian Smith.", "gun charges", "Ronald Reagan", "titanium", "(Harold) Godwinson", "(Urien) Urien"], "metric_results": {"EM": 0.53125, "QA-F1": 0.642380275974026}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false], "QA-F1": [0.8, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.2424242424242424, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.1818181818181818, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-7356", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2177", "mrqa_newsqa-validation-1679", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.53125, "CSR": 0.48664314516129037, "EFR": 1.0, "Overall": 0.6708442540322581}, {"timecode": 62, "before_eval_results": {"predictions": ["\"Gansbaai\"", "horse", "Italy", "bees", "blarney", "a linesider", "63 to 144 inches", "denmark", "squash", "Britannica.com", "AFC Wimbledon", "Scotland", "Edward VIII", "Bugs rabbit", "Swaziland", "ambidevous", "arthurian legend", "Japan", "wake", "mercury", "Yahoo!", "Klaus Barbie,", "honey", "Joanne Harris", "The Forbidden Club", "Kunigunde Mackamotski", "Moldovan", "Chatsworth House", "India", "Bull Moose Party", "Mar Pac\u00edfico", "eagle", "stanley", "Laurent Planchon", "Hercules", "Real Madrid", "whiskey", "Sir Matthew Pinsent", "Iran", "salsa", "Cuba", "Pete Sampras", "Kia", "Robert Stroud", "Cat Stevens", "cuticle", "tyne", "oxygen", "mulberry", "trumpet", "Cockermouth", "Roman Empire", "October 1941", "ancient scholar Bharata Muni", "McG", "near Philip Billard Municipal Airport", "gull-wing doors", "1st March 2010, 12th July", "sexual harassment", "5,600", "a cup", "a vision of ourselves", "A Moon for the Misbegotten", "If These Dolls Could Talk ''"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6185267857142858}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-945", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-6613", "mrqa_triviaqa-validation-4965", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6819", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-5993", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-3652", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-15132", "mrqa_searchqa-validation-9158"], "SR": 0.578125, "CSR": 0.48809523809523814, "EFR": 0.9629629629629629, "Overall": 0.6637272652116402}, {"timecode": 63, "before_eval_results": {"predictions": ["mom", "southern city of Naples", "\"CNN Heroes: An All-Star Tribute\"", "for the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle,", "\"We say to the people of Gaza,", "his business dealings", "Saturday", "People Against Switching Sides ( climbs)", "Haiti,", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.", "Darrin Tuck,", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "a review of state government practices completed in 100 days.", "prostate cancer,", "Thirteen", "a birdie four at the last hole", "Rima Fakih", "JBS Swift Beef Company,", "37", "slayings of actress Sharon Tate and four others.", "33-year-old", "Christopher Savoie", "12-hour", "Judge Herman Thomas", "Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \" with no Negro dialect,", "oldest documented bikinis", "free laundry service.", "killed one soldier and wounded another at a Little Rock military recruiting center", "5-1", "learn in safer surroundings.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "Friday,", "Gavin de Becker", "400 years ago", "U.S. Consulate in Rio de Janeiro,", "Apple employees", "heavy turbulence", "$3 billion,", "Nkepile M abuse", "resources", "\"A Child\\'s Garden of Verses,\"", "Spc. Megan Lynn Touma, 23,", "Barack Obama", "Technological Institute of Higher Learning of Monterrey,", "Holley Wimunc, 24.", "\"It was incredible. We've had had so much rain, and yet today it was beautiful.", "David Russ,", "Gary Coleman", "Chinese", "a young husband and wife", "stability, security, and predictability of British law and government", "six", "india", "algebra", "john bach", "Detroit", "on the north bank of the North Esk", "singer", "dinosaurs", "the troposphere", "Thief knot", "Fort Kent, Maine"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6491830782883259}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.06666666666666667, 1.0, 1.0, 0.8000000000000002, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.7924528301886793, 1.0, 0.4, 0.13333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3319", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-3655", "mrqa_triviaqa-validation-130", "mrqa_hotpotqa-validation-2249", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-6571", "mrqa_searchqa-validation-9458", "mrqa_naturalquestions-validation-6670"], "SR": 0.515625, "CSR": 0.488525390625, "EFR": 1.0, "Overall": 0.671220703125}, {"timecode": 64, "before_eval_results": {"predictions": ["the Mongols", "beer", "sweepstakes", "General McClellan", "(John) Keats", "wine", "Dorothy", "talc", "Bologna", "potatoes", "Princeton", "in Chinese", "the Knight", "Evian", "the Ark", "in the market-place of Jerusalem", "the Andes", "Jim Jarmusch", "Martin Luther", "1949-50", "Tennessee", "Audrey Hepburn", "Falafel", "Aladdin", "The Prairie Wolf", "CraigIDWELLS", "Arthur C. Clarke", "the Washington Redskins", "the Vietnam War", "the Accused", "at the Gold Coast, Lismore and Coffs Harbour", "Christian louboutin", "monk seal", "beer", "communication", "less than 1% fat.", "Ginger Rogers", "Beijing", "Plumeria", "Lafayette", "Marie Osmond", "Martin Chuzzlewit", "buffalo", "a comet", "Chuck Yeager", "in the opposite direction (away from the wall),", "sheep", "the Inheritance Cycle", "Georgia", "French toast", "the Fifth Amendment", "Elvis Presley", "permanent display at the Louvre Museum in Paris", "a English parson may'have his nose up in the air ', upturned like the chicken's rear end", "bacall", "jaws", "denarius", "18 November [O.S. 6 November] 1860", "White Horse", "the 1824 Constitution of Mexico", "Kurt Cobain", "\"Nothing But Love\" comeback tour,", "Christopher Savoie", "London"], "metric_results": {"EM": 0.5625, "QA-F1": 0.624630376344086}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9333333333333333, 0.7096774193548387, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-13851", "mrqa_searchqa-validation-214", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-2395", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-16755", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-3875", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_hotpotqa-validation-4263", "mrqa_newsqa-validation-2011"], "SR": 0.5625, "CSR": 0.4896634615384615, "EFR": 0.9642857142857143, "Overall": 0.6643054601648352}, {"timecode": 65, "before_eval_results": {"predictions": ["Montana", "Ridder", "Casino Royale", "Albrecht G Kessler", "the Apprentice", "Aeschylus", "College of William and Mary", "Intelligence Quotient", "the Stranger in a Strange Land", "a beehive- or barrel-shaped container of baked clay", "the Mets", "cowboys", "Monty Python and the Holy Grail", "Ludwig van Beethoven", "the Stalin", "In God We Trust", "Portland", "China", "Absalom", "Castle Rock", "Bollywood", "Marcia Brady", "House of Habsburg", "Eudaimonia", "a Twinkie", "the altitude", "The Unbearable Lightness of Being", "Richard", "the Anne of Cleves", "SUFFIXES", "Fred Rogers", "Princess Liliuokalani", "the Pituitary", "the Boer War", "the pulp", "Michelle Pfeiffer", "Aswan", "Billy Ray Cyrus", "the shaftching", "The Body", "Impostor syndrome", "to displace", "Davy Crockett", "Scorpius", "the volcanoes at Okataina Volcanic Centre", "copper", "Dubliners", "Jules Verne", "Cuba", "the Taliban", "the Kennedy", "Tyler, Ali, and Lydia", "Otis Timson", "during prenatal development in the central part of each developing bone", "J.P. Pennington", "Carmen Miranda", "Joan Crawford", "the superhero Birdman", "Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Erich Maria Remarque", "the nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "ensure that all prescription drugs on the market are FDA approved,", "15-year-old's", "Firoz Shah Tughlaq"], "metric_results": {"EM": 0.5, "QA-F1": 0.617735843139938}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.4, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.25, 0.4, 0.5, 0.9655172413793104, 0.14814814814814817, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5082", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-15917", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-5095", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-11135", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-8703", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-2440", "mrqa_triviaqa-validation-2", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1062", "mrqa_naturalquestions-validation-10509"], "SR": 0.5, "CSR": 0.4898200757575758, "EFR": 0.96875, "Overall": 0.6652296401515152}, {"timecode": 66, "before_eval_results": {"predictions": ["eight", "fermenting dietary fiber into short - chain fatty acids ( SCFAs ), such as acetic acid and butyric acid", "December 20, 1951", "La\u02bbila\u02bbi", "Terry Kath", "inability to comprehend and formulate language", "in the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "1546", "Banquo", "January 1923", "CeCe Drake", "a habitat", "minced meat", "Geophysicists who specialize in paleomagnetism", "free floating", "Imperial Japan", "fifth studio album", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "from 13 to 22 June 2012", "T - Bone Walker", "Professor Kantorek", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "the Executive Residence of the White House Complex", "Article Two of the United States Constitution", "April 13, 2018", "Bush", "Yuzuru Hanyu", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "from the southernmost tip of the South American mainland, across the Strait of Magellan", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "in Rome", "January 1, 2016", "Leonardo da Vinci", "absolute temperature", "Thawne", "Philippe Petit", "Proposition 103", "2008", "within eukaryotic cells", "Julie Adams", "775", "Pakistan", "Xiu Li Dai and Yongge Dai", "Norman Whitfield", "Americans who served in the armed forces and as civilians", "Uplokayukta", "James Fleet", "the year 1 BC", "David Davis", "Dirty Dancing", "mumps", "Delphi Lawrence", "2 May 2015", "International Boxing Federation (IBHOF)", "J.Crew,", "Pakistani officials,", "British", "Jamaica", "copra", "Delphi", "not guilty of affray"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6518281234405183}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.8205128205128205, 1.0, 0.0, 1.0, 0.8, 1.0, 0.8095238095238095, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.9387755102040816, 1.0, 0.5, 1.0, 1.0, 1.0, 0.47058823529411764, 0.7058823529411764, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8181818181818181, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-4419", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-47", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1259", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-5072", "mrqa_newsqa-validation-37"], "SR": 0.515625, "CSR": 0.49020522388059706, "EFR": 0.967741935483871, "Overall": 0.6651050568728937}, {"timecode": 67, "before_eval_results": {"predictions": ["in Pyeongchang County, Gangwon Province, South Korea", "Padawan", "in a liquid solution", "April 1917", "London", "Utah", "1969", "by October 1986", "the referee blows the whistle", "Scots law", "parthenogenesis", "Ted Duchovny", "reproductive system", "Australia", "the Justices of the Peace", "part - Samoyed terrier", "Gibraltar", "September 1947", "7 July", "in the bone marrow", "Sophia Akuffo", "to distinguish them from the generally more popular ( and better compensated ) heavyweight champions", "each team has either selected a player or traded its draft position", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "in place", "the Tanakh", "Daren Maxwell Kagasoff", "at Steveston Outdoor pool in Richmond, BC", "Thomas Lennon", "in North America", "people who jointly oversee the activities of an organization", "Friedman Billings Ramsey", "Miami Heat of the National Basketball Association ( NBA )", "vasoconstriction", "Toronto Islands in Toronto, Ontario", "disposable lighter", "the final episode of the series", "Richard Carpenter", "Konakuppakatil Gopinathan Balakrishnan", "Ryan Evancic", "Border Collie", "Vesta", "1665 to 1666", "sugars and amino acids", "Aegisthus", "on the continent of Antarctica", "California", "the late 1970s", "the first half of 1349", "Ipswich Town", "Groot-Zundert", "British Airways", "Genderqueer", "14,597", "YouTube", "Chinese tourists", "Joel \"Taz\" DiGregorio,", "system of military trials", "the Great Basin and the Mojave Desert", "1972", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.5, "QA-F1": 0.6129052830339595}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true], "QA-F1": [0.923076923076923, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.12121212121212123, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.1, 0.8235294117647058, 1.0, 0.4444444444444445, 0.0, 0.4615384615384615, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9760", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1269", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-5634", "mrqa_triviaqa-validation-263", "mrqa_hotpotqa-validation-621", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207", "mrqa_searchqa-validation-2955"], "SR": 0.5, "CSR": 0.49034926470588236, "EFR": 0.96875, "Overall": 0.6653354779411764}, {"timecode": 68, "before_eval_results": {"predictions": ["Malay Peninsula", "nomadic people", "5.4%", "Parkinson\\'s Disease", "(John) Jay", "Warsaw", "Prague", "Murfreesboro", "South Africa", "(Clay) Aiken", "jedoublen/jeopardy", "Australia", "Mike Ditka", "high and dry", "a Doll", "the inquisition", "Cleopatra", "the International Space Station", "Iran", "Gaius Cassius Longinus", "the Charleston", "South Africa", "(John) Deere", "River Thames", "Oxford", "William Wordsworth", "Elphaba", "Tuscaloosa", "Cyprus", "Sabino Canyon", "Frasier Crane", "David Bowie", "Noto", "Herbert Hoover", "Zhou Enlai", "pep", "the Rhone", "lenny kravitz", "The Mole", "the HIV/AIDS epidemic", "Today", "Golden", "dutch", "Bern", "hollandaise sauce", "Jackie Robinson", "YouTube", "Diane Arbus", "Willa Cather", "\"sustained\"", "marathon", "Masha Skorobogatov", "Kyla Pratt", "Dumont d'Urville Station", "Union Gap", "Charlotte's Web", "the Republic of Cameroon", "Ding Sheng", "May 5, 2015", "Massapequa", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "December 7, 1941", "transit bombings", "acid phosphate"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5879407051282051}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3218", "mrqa_searchqa-validation-2956", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-13698", "mrqa_searchqa-validation-11405", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-2330", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-5828", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-12198", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-14324", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-1182", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_triviaqa-validation-3820"], "SR": 0.515625, "CSR": 0.4907155797101449, "EFR": 1.0, "Overall": 0.671658740942029}, {"timecode": 69, "before_eval_results": {"predictions": ["aqueduct", "a quark", "Christopher Reeve", "Slovak Republic", "Macbeth", "Jacob Astor", "a skunk", "Penn Station", "The Sun Also Rises", "Apache", "Ferrari", "banquet", "the High Plains", "Joe Hill", "Job", "Kentucky", "Supernatural", "Jean Foucault", "Montana", "deep brain stimulation", "at symbol", "the Amazon", "Tahlequah", "Anne Hathaway", "a Model B", "Iraq", "Vietnam", "Tintern", "Canuck", "Blake Day-Lewis", "Isaac Newton", "Blue Ridge Mountain", "Chopin", "Susan B. Anthony", "Dexter Morgan", "a quokka", "Washington", "Starsky and Hutch", "some of what Cuarn brought to Harry Potter and the Prisoner of Azkaban", "Knocked Up", "Space Chimps", "Christopher Wren", "jazz", "Boston", "Han Solo", "Hans Christian Andersen", "a triumphal arch", "Zapata", "lace", "Goldwater", "Guinness", "Portugal. The Man.", "1983", "Laura Williams", "pumas", "dwarf", "Greek", "Best Animated Feature", "1937", "Stephen Ireland", "a group of college students of Pakistani background", "Coptic Church", "an eye for an eye,\"", "Retina"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6125}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.8, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-16658", "mrqa_searchqa-validation-14223", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-5269", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-14157", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-6228", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-1816", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-14736", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-2333", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-2934", "mrqa_searchqa-validation-5427", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-5477", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-2435", "mrqa_hotpotqa-validation-2157"], "SR": 0.453125, "CSR": 0.49017857142857146, "EFR": 1.0, "Overall": 0.6715513392857143}, {"timecode": 70, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.82421875, "KG": 0.47734375, "before_eval_results": {"predictions": ["Kathy Najimy", "2006 -- 07", "the first Divergent Series : Insurgent", "Mel Tillis", "2026", "Pink Floyd", "Andrew Lloyd Webber", "at the TV studio in the Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Health or vitality", "Dave Kelly", "one", "1955", "Alex Karev", "parthenogenesis", "fertilization", "Perchik", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "the stems and roots of certain vascular plants", "a Czech word, robota", "skeletal muscle", "Nazi Germany and Fascist Italy", "Gunpei Yokoi", "David Motl", "60", "February 16, 2010", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "scrolls dating back to the 12th century", "Buddhism", "Kiss", "Syco Music", "Trace Adkins", "the optic canal", "to manage the characteristics of the beer's head", "United States, the United Kingdom, and their respective allies", "a large, high - performance luxury coupe", "James Intveld", "15 February 1998", "Christopher Allen Lloyd", "100,000", "January 2004", "Bartolomeu Dias", "Laura Haddock", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "potential of hydrogen", "Fall 1998", "the Jurchen Aisin Gioro clan in Manchuria", "Guwahati", "74", "South Africa", "Rufus and Chaka Khan", "eight", "Venado Tuerto", "Jamaica", "mead", "Tomorrowland", "Tallahassee City Commission", "John Kavanagh", "pesos", "Maj. Nidal Malik Hasan,", "123 pounds of cocaine and 4.5 pounds of heroin, Tempe, Arizona,", "In Memoriam", "the Mercury", "Law & Order: Special Victims Unit", "UNICEF"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6248177516927517}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.22222222222222224, 0.18181818181818182, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.9, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.07999999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-527", "mrqa_triviaqa-validation-4646", "mrqa_hotpotqa-validation-3032", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-236", "mrqa_searchqa-validation-9696"], "SR": 0.515625, "CSR": 0.4905369718309859, "EFR": 0.967741935483871, "Overall": 0.6867339064629714}, {"timecode": 71, "before_eval_results": {"predictions": ["the Easter Island", "George Balanchine", "David Bowie", "the Permian", "Austen", "Leonardo DiCaprio", "the Basque", "Cherry Jones", "Happy feet", "a guardian angel", "the Army", "the Stoke", "The Miz", "the Black and Caspian", "June Carter Cash", "Cape Town", "hermatypic", "(David) Glasgow", "Mar 17, 2008", "salaried", "platinum", "Marie Osmond", "Scrabble", "suckers", "Catholicism", "London", "Burgenland", "Halliburton", "the laser", "Boston", "Anamosa", "spelling bee", "poetry", "the Battle of Fort Donelson", "the USS Nautilus", "Rich and Famous", "sucrose", "Cheshire", "Cuba", "The Prince and the Pauper", "Thomas Paine", "Abraham Lincoln", "Lord North", "Charles I", "the Jemima", "Diane Arbus", "Bhavnagar", "George Bernard Shaw", "Utah", "the d(AACCCC)", "Kublai Khan", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "Kimberlin Brown", "Henry Selick", "Caviar", "( Neil) Armstrong", "argentina", "the vicar of Wantage", "Marc Bolan", "Jewish descent", "apartment building in Cologne, Germany,", "Kurdistan Gas City", "$40 and a loaf of bread.", "Nunavut"], "metric_results": {"EM": 0.5, "QA-F1": 0.5456225198412699}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-13029", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-2210", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-12776", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-12497", "mrqa_searchqa-validation-6350", "mrqa_searchqa-validation-15590", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3746", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3002", "mrqa_triviaqa-validation-1782"], "SR": 0.5, "CSR": 0.4906684027777778, "EFR": 1.0, "Overall": 0.6932118055555556}, {"timecode": 72, "before_eval_results": {"predictions": ["Austria", "a peninsula", "\"I am woman, hear me roar\"", "Brasilia", "Applebee\\'s", "America", "Backgammon", "Steely Dan", "Artemis", "Hobart", "Colorado Springs", "Cheap trick", "pizza", "Islam", "the Cerberus", "Robert E. Lee", "Tobago", "A Hymn To Him", "Columbus", "Elijah Muhammad", "Spain", "Federico Fellini", "Fenway Park", "C.T. Eisler", "The Princess Diaries", "water fluoridation", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Barbara Boxer", "Chicago", "Wallace & Gromit", "sesame oil", "Adidas", "Jack Nicholson", "nitrogen", "the Omaha", "dogs", "Paul Gauguin", "Francis Scott Key", "Mexico", "the Peashooter", "Joe Pozzuoli", "$200m", "Massachusetts", "ACTIVE", "a box office", "Alfred Hitchcock", "the Basques", "Ambrose Bierce", "The president", "around 2.45 billion years ago", "Ginnifer Goodwin", "Sharon Osbourne", "asteroids", "argentina", "Edinburgh", "Campbellsville", "a French mathematician and physicist", "127 acres.", "Wednesday,", "Brian Smith.", "Ballon d'Or"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6713541666666667}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-1805", "mrqa_searchqa-validation-11732", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-1578", "mrqa_searchqa-validation-10948", "mrqa_searchqa-validation-9502", "mrqa_searchqa-validation-14818", "mrqa_searchqa-validation-775", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-10907", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-1962"], "SR": 0.578125, "CSR": 0.4918664383561644, "EFR": 0.9629629629629629, "Overall": 0.6860440052638255}, {"timecode": 73, "before_eval_results": {"predictions": ["gondola", "Sinclair Lewis", "Hilary Swank", "Penthouse", "Sacred Wonders", "Israel", "Lundy", "Van Morrison", "nickel", "Stuart Bingham", "Frank Darabont", "Neutrality", "Napoleon I", "espresso", "organizational theory and behavior", "Volkswagen", "Bedser", "Oldham", "Netherland", "Jabba the Hutt", "Wimbledon", "Crystal Gayle", "Taylor", "Baku", "Chechnya", "John Buchan", "red", "Chester", "Hippety Hopper", "a palla", "Mt Kenya", "a pumpkin", "Latvia", "Sicily", "Switzerland", "William Oliver Wallace", "Julie Andrews Edwards", "Pancho Villa", "Nigeria", "Leeds", "Passover", "Cologne", "Oliver!", "Ryukyuan", "Ra\u00fal Castro", "Egypt", "Renzo Piano", "a Necker cube", "Mexico", "Tennessee", "The Rembrandts", "wall mounted faucet and the sink rim", "Tom Brady", "January to May 2014", "Forrest Gump", "Julianne Moore", "Leon Schlesinger Productions", "Arnold Drummond", "dining scene", "Thaksin", "Jackie Moon", "Maria Callas", "Edie", "intelligent design"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2845", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5684", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-5756", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-1816", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-4028", "mrqa_hotpotqa-validation-2694", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-2669", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16531"], "SR": 0.5625, "CSR": 0.49282094594594594, "EFR": 1.0, "Overall": 0.6936423141891892}, {"timecode": 74, "before_eval_results": {"predictions": ["james bardon", "smith", "Navy", "high-speed car crash", "apples", "orange", "Dreamgirls", "s\u00e8vres", "Florida", "viola duke", "allergic rhinitis", "gum syrup", "British Columbia", "it means that the rent doesn't include additional costs such as insurance or business rates", "whooping cough", "new amsterdam", "apple", "India", "a Labyrinth", "Chiricahua", "a pacemaker", "blucher", "Pius XII", "smell", "cubes", "george i", "Grantham", "z Zimbabwe", "coastline", "orange", "Anwar Sadat", "David Bowie", "Silent Spring", "smith", "Bob Balaban", "Frank Langella", "The Archers", "Tottenham Court Road", "Montmorency", "a condor", "twelve", "paul\\'s Cathedral", "Pinocchio", "cenozoic", "Jimmy Carter", "Jamie Oliver", "a \" Mechanical\"", "willy Russell", "Petula Clark", "new Democracy", "The Blue Boy", "Border Collie", "Kristy Swanson", "fourth season", "John and Charles Wesley", "Hermione Baddeley", "Floyd Casey Stadium", "David Bowie", "the Iraqi economy.", "Robert Kimmitt", "the Mammoth Cave", "recessive", "hearth", "Jermaine Lamarr Cole"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5447916666666666}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-2589", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-6107", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-6722", "mrqa_triviaqa-validation-3503", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-169", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-825", "mrqa_triviaqa-validation-4407", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-7353", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-4435", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-1344", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-8404", "mrqa_hotpotqa-validation-2801", "mrqa_hotpotqa-validation-3787"], "SR": 0.453125, "CSR": 0.4922916666666667, "EFR": 0.9714285714285714, "Overall": 0.6878221726190477}, {"timecode": 75, "before_eval_results": {"predictions": ["new Zealand", "1961", "tomatoes", "Moriarty", "viola", "arthurian", "michael de cervantes", "japan", "Gillette", "spain", "mediterranean", "Bash Street", "Tahrir Square", "the innermost digit of the forelimb", "Swallow Sidecar Company", "Chicago", "mavrakis", "netherlands", "peeves", "gold hallmark", "john Buchan", "Pyrenees", "17", "joshua", "canada", "Taenarum", "algebra", "Eddie Murphy", "Crete", "canada", "den lille Havfrue", "atria", "jane galsworthy", "orca", "Christopher Nolan", "purple rain", "chess", "ireland", "Diana vickers", "February", "Damian Green", "argon", "Bagel", "france", "South Dakota", "Alexander Dubcek", "Denver", "Chicago Cubs", "st. Louis", "Iberia", "Rosetta", "in the basic curriculum -- the enkuklios paideia or `` education in a circle '' -- of late Classical and Hellenistic Greece", "the Saudi Arab kingdom", "the nucleus", "from August 14, 1848", "1892", "outside the United States and Canada", "1,500", "Shenzhen in southern China.", "Ahmad Vahidi", "cola", "Washington, D.C.", "sedimentary rock", "golf"], "metric_results": {"EM": 0.5, "QA-F1": 0.5539434523809523}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-4799", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-3005", "mrqa_triviaqa-validation-5472", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5531", "mrqa_triviaqa-validation-6463", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-3745", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2663", "mrqa_searchqa-validation-10770", "mrqa_searchqa-validation-2183"], "SR": 0.5, "CSR": 0.49239309210526316, "EFR": 1.0, "Overall": 0.6935567434210527}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Rachel Griffiths", "California, Utah and Arizona", "disease development", "William Chatterton Dix", "1924", "September 27, 2017", "Alabama", "Scheria", "Sanchez Navarro", "Thomas Jefferson", "August 2, 1990", "Joe Pizzulo and Leeza Miller", "Julie Adams", "Richard Bremmer", "from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear )", "a Native American nation from the Great Plains", "in contact with the basal lamina ( one of the two layers of the basement membrane ) of the epithelium", "the ARPANET", "April 1979", "Tbilisi", "a security feature for `` card not present '' payment card transactions instituted to reduce the incidence of credit card fraud", "Tom Robinson", "Virginia", "Liam Cunningham", "2013", "ummat al - Islamiyah", "# 4", "August 1, 2016", "2017 season", "W. Edwards Deming", "Saphira", "passwords", "Galveston hurricane", "1981 -- 86", "Ajay Tyagi", "gavall\u00e9e", "sexton Robert Newman", "Augustus", "Thespis", "1990", "Zeus", "two", "April 10, 2018", "Lee County, Florida, United States", "annually", "Kevin Spacey", "Fa Ze Rug", "Lightning Thief", "the French", "hyperinflation", "Lingerie", "Dorrit", "Octavian", "Karolina Dean,", "around four hundred", "Caesars Entertainment Corporation", "the North Korean regime intends to fire a missile toward Hawaii on July 4.", "the sins of the members of the church,", "Marcus Schrenker,", "gen\u00e8ve", "Quartet", "ask for help", "Charice"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6514317279942279}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.11111111111111112, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-1907", "mrqa_hotpotqa-validation-4503", "mrqa_newsqa-validation-212", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1590"], "SR": 0.59375, "CSR": 0.4937094155844156, "EFR": 1.0, "Overall": 0.6938200081168832}, {"timecode": 77, "before_eval_results": {"predictions": ["Lyndon Johnson", "Istanbul", "Cana", "any object pointed out to him", "Figaro", "Valerie Bertolini", "glitter", "Bayer", "illustrator", "Karl Rove", "Russians", "Ireland", "Beeshsinil", "Portland", "florida", "Doctor Dolittle", "fish", "transmission", "hot air balloons", "vacuum tubes", "The Bridges of Madison County", "Italy", "iron", "LOUIS XIV", "ice cream", "king Louis XIV", "catfish", "Alien", "President John F. Kennedy", "Indira Gandhi", "pacarana", "Stephen Decatur", "Patti LaBelle", "Chaucer", "Molly Brown", "sister cities", "hurricanes", "The Wall Street Journal", "frag", "Tinactin", "Virgin Atlantic", "Perrier", "Eastwick", "king henry i", "trout", "pakistan", "Minnesota", "San Francisco", "rabbit", "latte", "92", "Brazil, Turkey and Uzbekistan", "Nicole DuPort", "species", "deep purple", "Caterina", "Surrealist", "July 25 to August 4", "1755", "Trey Parker and Matt Stone", "1.2 million", "president Luca di Montezemolo", "Roger Federer", "allori"], "metric_results": {"EM": 0.546875, "QA-F1": 0.628125}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-692", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-1044", "mrqa_searchqa-validation-8250", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-13399", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-2685", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_naturalquestions-validation-9830", "mrqa_triviaqa-validation-3041", "mrqa_triviaqa-validation-3098", "mrqa_newsqa-validation-1364", "mrqa_triviaqa-validation-5253"], "SR": 0.546875, "CSR": 0.49439102564102566, "EFR": 1.0, "Overall": 0.6939563301282051}, {"timecode": 78, "before_eval_results": {"predictions": ["Tycho Brahe", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Baghdad", "Jonny Quest", "Rwanda", "South Carolina", "never having to say you're sorry", "Captains Courageous", "Bryan Adams", "Moses", "Mechanical", "Chaucer", "the Toronto Blue Jays", "a second lieutenant", "(robot)", "Sayonara", "Orient Express", "Inferno", "Sir Walter Scott", "a cord", "Louisiana", "\"The Maltese Falcon (1941)", "Japan", "stain", "always to be blessed", "PG-13", "Occipital", "a spoonful", "Little Red Riding Hood", "the Jonas Brothers", "New Zealand", "the Popsicle", "Los Angeles", "a paladin", "\"Chelsea Morning\"", "a comb", "Venice", "Paraguay", "E. T. A. Hoffmann", "debts", "the Cowardly Lion", "El Supremo", "Foot Locker", "Princess Leia", "artichoke", "some concerts in Berlin with the RIAS Orchestra", "Hammurabi", "Alkalinity", "the ninth w\u0101,", "Matt Monro", "on the two tablets", "mount kenya", "\"Colonel Tom\" Parker", "Boston Legal", "Whitney Houston", "Channel 4", "Mark Donohue", "a dual Italian- Iraqii national,", "a share in the royalties for the tune.", "Arizona", "American 3D computer-animated comedy"], "metric_results": {"EM": 0.625, "QA-F1": 0.7240699404761906}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.7499999999999999, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-12459", "mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-11041", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11646", "mrqa_searchqa-validation-10152", "mrqa_searchqa-validation-12278", "mrqa_searchqa-validation-8259", "mrqa_searchqa-validation-11821", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-12891", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-4688", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-2489", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-2151", "mrqa_hotpotqa-validation-2673"], "SR": 0.625, "CSR": 0.49604430379746833, "EFR": 1.0, "Overall": 0.6942869857594938}, {"timecode": 79, "before_eval_results": {"predictions": ["the bicep", "Banquo", "Detroit", "flower", "\"w\"", "Ford", "Joseph Campbell", "curmudgeon", "Faith Hill", "Novel", "a broom", "Edinburgh", "engineering", "Cyprus", "savanna", "a tandoor", "floatplane", "piano", "Sure Deodorant", "oyster", "Gilbert Grape", "Barbara Bush", "Rene Zellweger", "Louis Prima", "eggshells", "the Atsugi military base on Honshu", "the Sadler\\'s Wells Ballet", "Urdu", "The 1st Special Forces Operational Detachment- Delta", "Aaron Burr", "Johns Hopkins University", "Jason", "the Mississippi River", "Damascus", "Oahu", "Devo", "biology", "stuffing", "the Reading Railroad", "George Eliot", "the Cotton Bowl", "the Battle of Shiloh", "ventriloquist", "Takana", "apples", "Aromatic Cedar", "Almond Joy", "The Children", "Sam Houston", "Caesar salads", "cable cars", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "1923", "colonelings castle", "Coronation Street", "The Boar", "Waylon Jennings, Kris Kristofferson", "Sarajevo", "Annie Ida Jenny No\u00eb Haesendonck", "Mother\\'s Day poems", "a fifth successive season", "The son of Gabon's former president", "wildcats"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5684895833333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-15134", "mrqa_searchqa-validation-8338", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-6381", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_searchqa-validation-787", "mrqa_searchqa-validation-1853", "mrqa_searchqa-validation-9372", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-3923"], "SR": 0.46875, "CSR": 0.495703125, "EFR": 1.0, "Overall": 0.69421875}, {"timecode": 80, "UKR": 0.646484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.83203125, "KG": 0.475, "before_eval_results": {"predictions": ["India", "a partridge", "codeine", "Austria", "George IV", "azerbaijan", "alphabetic", "Sisyphus", "Italy", "a coffee house", "Cambodia", "moldova", "The Taking of Pelham", "Ethiopia", "Frank McCourt", "gremlins (Dante)", "Arkansas", "Texas", "Norway", "archer", "William Blake", "cape horn", "Roger Federer", "Charlie Chan", "Galileo", "Great British Bake Off", "World War I", "shekel", "george sand", "Michael Caine", "Professor Brian Cox", "brabham", "lord emsworth", "casualty", "McDonnell Douglas", "tyne", "Missouri", "Emma Chambers", "Buckinghamshire", "Turkey", "domestic cat", "David Lynch", "eight", "One Direction", "Groucho Marx", "Brazil", "Maggie Smith", "Pakistan", "August 1925", "Sweeney Todd", "Rio Grande", "After a player has been designated for assignment, the other 30 NHL teams can put in a claim or waive their claim for that player", "12 February 1999", "David Joseph Madden", "The Braes of Balquhither", "Mary Astor", "Battle of Qala-i-Jangi", "natural gas", "CNN's best ten golf movies ever made", "Madonna", "Hawaii", "Monaco", "P. D. James", "MacFarlane"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7331845238095238}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-2921", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-2733", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-9986", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-3690", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-12999"], "SR": 0.6875, "CSR": 0.498070987654321, "EFR": 1.0, "Overall": 0.6903173225308643}, {"timecode": 81, "before_eval_results": {"predictions": ["California State Route 1", "the first letter of an interrogative sentence or clause to indicate that a question follows", "Minneapolis Lakers", "1800s", "the Alamodome and city of San Antonio", "rear - view mirror ( or rearview mirror )", "Golden Gate Bridge", "Hitler", "BC Jean and Toby Gad", "1994", "September 2017", "Universal Pictures and Focus Features", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "2017", "five", "Tbilisi, Georgia", "April 1917", "1900", "Bryan Cranston", "Geothermal gradient", "around 10 : 30am", "posterior parietal cortex", "to Napoleon's planned invasion of the United Kingdom", "potential of hydrogen", "volcanic activity", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Egypt", "As of January 17, 2018, 201 episodes", "the pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "1986", "2018", "rapid destruction of the donor red blood cells by host antibodies", "1623", "English author Rudyard Kipling", "March 16, 2018", "Fusajiro Yamauchi", "the rez", "2013", "the breast or lower chest", "Nick Wilton", "Flex Builder", "2018", "Saint Peter", "1960", "August 19, 2016", "Madison", "Washington, Jay and Franklin", "ABC", "Brevet Colonel Robert E. Lee", "glockenspiel", "alaskan", "john antrobus", "Elbow", "Dundalk", "Division II", "the Airbus A330-200", "50", "he never swore in front of women.", "Portugal", "gravity", "Hercule Poirot", "Yemen"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6333525788295525}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true], "QA-F1": [0.6666666666666666, 0.5555555555555556, 0.0, 0.6666666666666666, 0.2857142857142857, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.7407407407407407, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4210526315789474, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-9521", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-2319", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4135", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-5829", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-2328"], "SR": 0.515625, "CSR": 0.49828506097560976, "EFR": 0.9354838709677419, "Overall": 0.6774569113886704}, {"timecode": 82, "before_eval_results": {"predictions": ["jon stanley hercules", "king henry I", "Ross Kemp", "jumanji", "kirk douGLAS", "michael wood", "Christmas", "African violet", "hercules berlin mit grellbunten Turnschuhen gerockt", "Gerald Ford", "fagott", "pembrokeshire coast park", "amola", "South Africa", "sows", "The Persistence of Memory", "orangutan", "The Time Machine", "Uranus", "godsaidmansaid", "lady gaga", "Mecca", "cirrus uncinus", "krubera", "myxomatosis", "joseph fforde", "Philippines", "xerophyte", "blur", "The King and I", "The Last King of Scotland", "a hard-shelled eggs", "peter", "John Steinbeck", "The Bulletin", "guitar and bass", "Ross Bagdasarian", "Mark Hamill", "pierce Brosnan", "Burmah", "a rural producers who often don't own land and justice in 1996", "cryonics", "j\u00f8rn Utzon", "Another Day in Paradise", "decorate", "gl\u00fcnderzeit", "ATR", "australia", "rapid eye movement", "j. S. Bach", "a prince of Greece and Denmark", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "the Lost Battalion", "January 11, 2016", "White Knights of the Ku Klux Klan", "6,000", "fill a million sandbags and place 700,000 around our city,\"", "Herman Thomas", "a clock cycle", "a Mazur tempo", "@font-face", "1945 to 1951"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5295386904761905}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3307", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-7545", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-6450", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-2965", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4711", "mrqa_newsqa-validation-3596", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-2044"], "SR": 0.453125, "CSR": 0.49774096385542166, "EFR": 1.0, "Overall": 0.6902513177710843}, {"timecode": 83, "before_eval_results": {"predictions": ["Libya", "Syriza", "underwater", "wrigley", "PJ Harvey", "kerkyra", "3-1", "Charles Taylor", "palm sunday", "thailand", "The Wicker Man", "a pulmonary ligament", "chess", "a serpention", "peter Nichols", "bear grylls", "Count Basie", "john glenn", "Plato", "amundsen", "jazz", "Pensacola, Florida", "whitsunday", "Michael Hordern", "Gerald Durrell", "Ishmael", "france", "climate", "tank", "madonna", "etruscan army", "vivistic", "owls", "Bulls Eye", "south africa", "a pair of welding boots", "Helen Gurley Brown", "abes", "The Jungle Book", "jayannam", "Massachusetts", "douglas", "a Hamlet", "Team GB", "pakistan", "a Zeppelin", "planchette", "rock follies", "australia", "ann darrow", "shakespeare", "1996", "a few central Mexican recipes", "16 June", "Squam Lake", "3D computer-animated comedy", "1902", "due to a shortage of landing fields available for practice, an offer to land near the Middleton house on April 3 was readily accepted.", "The Impeccable", "The ruling Justicialist Party, or PJ by its Spanish acronym,", "Ming dynasty", "Prince Albert", "Pavlov", "al Qaeda."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6069353070175438}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-352", "mrqa_triviaqa-validation-2857", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-5596", "mrqa_newsqa-validation-1285", "mrqa_searchqa-validation-581", "mrqa_searchqa-validation-6285"], "SR": 0.53125, "CSR": 0.49813988095238093, "EFR": 0.9666666666666667, "Overall": 0.6836644345238095}, {"timecode": 84, "before_eval_results": {"predictions": ["ganges", "David Hilbert", "Halifax", "caf\u00e9s", "Q", "Franklin Delano Roosevelt", "Buncefield Depot", "carbon", "cappuccino", "aughra", "new Brunswick", "Oklahoma", "Brad Pitt", "city of florida", "william Lamb", "jinni", "angi Jo omdahl", "nouakchott", "helen", "casa di Giulietta", "once every two weeks", "Crystal Gayle", "noah", "Naboth", "don budge", "queen of Saxe-Coburg-Gotha", "Quentin Tarantino", "d Dick Whittington", "city of russia", "rowing", "ouwerks", "gin", "supertramp", "ireland", "halogens", "Jackie Kennedy", "blue and blue", "calcium carbonate", "tinware", "cuba", "lorraine", "Nicola Adams", "cologne", "Andes", "Essex Eagles", "carry on Cleo", "American History X", "dysmenorrhea", "south west", "helen and r Ralph Richardson", "argentina", "approximately 26,000 years", "Travis Tritt and Marty Stuart", "Norway", "Perdita", "BPI gold-selling \" Shut Up\"", "Tottenham Hotspur", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "Ma Khin Khin Leh,", "the Wii", "Germaine Greer", "Isaac, Esau", "Surrey"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5263020833333334}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.1111111111111111, 0.2222222222222222, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7646", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-7251", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-4672", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-2381", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-4587", "mrqa_hotpotqa-validation-3085", "mrqa_hotpotqa-validation-875", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-2718", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-14852"], "SR": 0.4375, "CSR": 0.4974264705882353, "EFR": 0.9444444444444444, "Overall": 0.679077308006536}, {"timecode": 85, "before_eval_results": {"predictions": ["eagle", "teacher", "Shaft", "semicubical parabola", "jets", "cradle of civilization", "alahuatl", "Fast Forward", "Rudyard Kipling", "rolleriot", "The Life and Opinions of Tristram Shandy", "vincenzo Nibali", "160", "scorpion", "pram", "lexis", "c Cyprus", "sheep", "Laos", "Joseph Gayetty", "Andes Mountains of Chile and Argentina", "georgia clare\u017ft", "10", "minder", "christian neame", "the shoulder", "severn", "legs", "leighton park", "Saturday Night", "afterlife", "around May 1", "1982", "bea", "Danish", "priests", "paul Escobar", "South Africa", "Microsoft", "Bolivia", "ussia and Prussians", "secretary or scribe", "apocalypse now", "judy gumm", "Amnesty International", "roll", "Treaty of Waitangi", "southern", "Renzo Piano", "50", "florence", "before the first year begins", "100 m ( 328 ft )", "Hold On", "1919", "patosaurus", "Teatro Metastasio", "australian", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "police", "Daredevil", "Dr. George Washington Carver", "giant panda", "California, Texas and Florida,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5570436507936508}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2922", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-4290", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-6065", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-3672", "mrqa_triviaqa-validation-3123", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1317", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-3561", "mrqa_hotpotqa-validation-372", "mrqa_hotpotqa-validation-4899", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-15855", "mrqa_newsqa-validation-2338"], "SR": 0.46875, "CSR": 0.49709302325581395, "EFR": 0.9411764705882353, "Overall": 0.6783570237688099}, {"timecode": 86, "before_eval_results": {"predictions": ["shoppers would be considering whether to close some entrances, bring in additional officers, and make security more visible.", "money or other discreet aid for the effort if it could be made available,", "41,", "adidas", "to promote the attempts", "suicide bombing", "iTunes service", "school in the Oaxacan countryside of southern", "South Africa's", "shot in the head", "in the head with a.40-caliber pistol,", "Kenneth Cole", "$17,000", "183", "dental work", "urged NATO to take a more active role in countering the spread of the", "Shiza Shahid,", "Theoneste Bagosora,", "\"The Lost Symbol\"", "Haiti\\'s", "about 2,000", "German authorities", "his brother to surrender.", "Roy Foster", "in the Somali capital of Mogadishu", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "16 times.", "Jackson sitting in Renaissance-era clothes and holding a book.", "fighting charges of Nazi war crimes", "Big Brother.", "argina has always claimed sovereignty over the islands and invaded them in 1982,", "ALS6,", "revolution of values", "Number Ones", "joesworld.", "parents", "an empty water bottle down the touchline", "Samuel Herr,", "forged credit cards and identity theft", "they \"have a very hard decision ahead of them\" in considering whether to appeal or ask for a pardon.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "beach volleyball", "celebrities and ministers,", "Facebook and Google,", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "NATO's International Security Assistance Force", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England this weekend.", "a U.S. military helicopter", "mental health", "plastination.", "anti-trust laws.", "Thomas Edison", "Donna", "Ricardo Chavira", "1947", "borough of budapest", "mariette", "Boston Celtics", "Australian", "Oahu", "florida", "bagels", "the Seine", "Mary Tyler Moore"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5380314193766937}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false], "QA-F1": [0.0, 0.5555555555555556, 1.0, 1.0, 0.4, 0.0, 0.5, 0.2, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.19512195121951217, 0.0, 0.04761904761904762, 0.0, 0.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-2239", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-3162", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-3913", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625", "mrqa_searchqa-validation-14248", "mrqa_triviaqa-validation-2788"], "SR": 0.4375, "CSR": 0.4964080459770115, "EFR": 1.0, "Overall": 0.6899847341954023}, {"timecode": 87, "before_eval_results": {"predictions": ["four", "yellow", "whooping cough", "Vincent Motorcycle Company", "harrison fc", "Reservoir", "\u221223.5\u00b0", "webcam", "\"Sugar Baby Love\"", "1981", "Bernardo Bertolucci", "The Seven Year Itch", "Dieppe", "the Nile River", "spinach", "la Boh\u00e8me", "Nicky Morgan", "midsomer Murders", "mrs. Bing", "arthur", "archenemy Black jack", "American Civil War", "yves Saint Laurent", "St. Pauls", "domestic chicken", "blossom", "shingles rash", "king george VI", "rupiah", "bonita Melody Lysette", "charles II", "Illinois", "danish", "The Landlord\\'s Game", "spores", "Valerie Hobson", "Silver Hatch", "watchmaking", "wales", "clogs", "Hannibal Heyes and Thaddeus Jones", "lolita", "edwina currie", "Baton Rouge", "walesaw", "2010", "Carole King", "drizzle", "casualty", "Trimdon", "sleepless in seattle", "Telma Hopkins, Joyce Vincent Wilson and her sister Pam Vincent on backing vocals", "1624", "Milira", "Shaftesbury, Dorset", "Austrian Volksbanks", "1848 to 1852", "his dad's son.", "Turkish President Abdullah Gul,", "President Obama", "the abacus", "the Maine", "the Marquis de Lafayette", "The Osmonds"], "metric_results": {"EM": 0.421875, "QA-F1": 0.46705729166666665}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2178", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-1047", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-2127", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-1688", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-7150", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_naturalquestions-validation-2862", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-5487", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-720"], "SR": 0.421875, "CSR": 0.4955610795454546, "EFR": 0.9459459459459459, "Overall": 0.6790045300982801}, {"timecode": 88, "before_eval_results": {"predictions": ["a chicken", "a luau", "Pat Paulsen", "paddington bear", "Antarctica", "charles fred", "Mensheviks", "prada", "Euphoria", "a hatchet", "The Aston Martin", "baboon", "Chicken Little", "Bach", "Bangkok", "Eli Whitney", "(John) Smith", "James Buchanan Eads", "\"A Bug\\'s Life\"", "racing", "a quiver", "the joker", "Richard Nixon", "Mussolini", "a bull", "Robert Burns", "Ebony", "Jack Nicklaus", "pen", "Las Vegas", "fiber", "poppy", "a portrait", "Lord of the Flies", "The Wedding Date", "Nickelback", "succotash", "jack london", "Falkland", "acetone", "jell-O", "murder", "frankfurter", "roanoke", "Blackbeard", "Tiger Woods", "(Elmer) Bull", "volcanic", "Amish", "dachshunds", "Robert Frost", "Virginia Dare", "Germany", "the first quarter of the 19th century", "George Washington", "Puerto Rico", "David Graham", "1987", "Jacobite rebellion", "Leinster", "a tenement in the Mumbai suburb of Chembur,", "Monday's", "Illinois Reform Commission", "edward p hinchliffe"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6375135281385281}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-13827", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-6279", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_naturalquestions-validation-1538", "mrqa_triviaqa-validation-3013", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.609375, "CSR": 0.4968398876404494, "EFR": 0.96, "Overall": 0.6820711025280899}, {"timecode": 89, "before_eval_results": {"predictions": ["Bill Bryson", "Pink Panther", "jodan, officially known as the Hashemite Kingdom of Jordan", "sweden", "Jean-Paul Sartre", "Motown", "Carl Johan (26 January 1763)", "Venus", "riyadh", "Margot fonteyn", "Marlon Brando", "plutocracy", "dominoes Portal", "Manchester Airport", "Radio 4 Extra", "handbags", "violin", "U2", "Barcelona", "australian", "auk", "weir", "Southampton", "soybean", "George Best", "time bandits", "Antoine de Caunes", "red rock west", "the xy plane", "Zagreb", "handley Page", "Marine One", "zachary Taylor", "Hitler", "All Holies Day", "Shaft", "brazil", "Louis Le Vau", "Scotland", "Tripoli's", "jubilee line", "Abbey Theatre", "Maine", "clefts", "cire retinater", "Denver", "June 14th", "Mel Blanc", "Lily Allen", "terrorist", "oats", "Wisconsin", "The Crossing", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler", "more than 230", "Serie B league", "Mark O'Connor", "Colombia.", "a federal judge in Mississippi", "he said the murder of the boss could never be justified.", "the Hoboken Five", "the Untouchables", "vitiate", "Amy Brenneman"], "metric_results": {"EM": 0.59375, "QA-F1": 0.679110863095238}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-6744", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-2307", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-1687", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-9329", "mrqa_searchqa-validation-15793"], "SR": 0.59375, "CSR": 0.4979166666666667, "EFR": 0.9615384615384616, "Overall": 0.6825941506410257}, {"timecode": 90, "UKR": 0.625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.826171875, "KG": 0.45390625, "before_eval_results": {"predictions": ["shooting star", "colleen bricassart", "joshua", "cyprus", "f. Lee Bailey and Alan Dershowitz", "jingle", "japan", "a dove", "giraffe", "meadow", "Conservative MP for Stockton-on-Trent", "Venus", "david harrison", "wroxham", "colleen McCullough", "Egypt", "\"Nabucodonosor\"", "drew carey", "three Mile Island", "s Sicily", "sunset boulevard", "enigma", "brussels", "arrows", "june keane", "pasta joke", "Frogmore estate or Gardens", "TV Week", "caucausus", "88", "cold Comfort Farm", "new year\u2019s Eve", "iceland", "david hilbert", "mediterranean", "Declaration of Independence", "jean bry synger", "fish", "jean antony", "go back to the times and work of Bert Jansch and John Renbourn", "joshua joachim", "whisky galore", "freda bach", "michael caine", "fonds de la recherche Scientifique", "jean watson", "1929", "The X-Files and I Want To Believe", "sue", "daily herald", "joshua b", "Brian Steele", "a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "Philadelphia, which is Greek for brotherly love", "the Cherokee River", "Benedict of Nursia", "Wichita", "ancient Jewish tradition", "Friday,", "take care of the reproducing dinosaur kin", "a dove", "method acting", "Annie Proulx", "\"Nude, Green Leaves and Bust\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5384114583333333}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.125, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5797", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-6049", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-3038", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-7098", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4932", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-4925", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508"], "SR": 0.5, "CSR": 0.49793956043956045, "EFR": 0.96875, "Overall": 0.6743535370879121}, {"timecode": 91, "before_eval_results": {"predictions": ["Evangelical magazine", "2011", "John McClane", "Emmanuelle", "Princeton University", "conservative", "Lombardy", "writer", "Lee Hall", "Newcastle upon Tyne", "Ivanovich yashin", "Blackwood Partners Management Corporation", "1958", "2007", "robot overlords", "1776", "geographical center of the state", "public house", "1944", "Austria", "Ron Cowen and Daniel Lipman", "The Soloist", "indoor", "north town line and the northeast corner of the town of North Greenbush", "January 30, 1930", "Dr. Alberto Taquini", "John Gotti", "Anderson Silva", "Santiago del Estero Province", "Harrison Ford", "C. H. Greenblatt", "Taoiseach of Ireland", "Dissection", "The Killer", "seacoast region", "Javed Miandad", "Dorothy", "early 2017", "people working in film and the performing arts", "June 2, 2008", "The 8th Habit", "one", "London", "australian", "Ready Player One", "World Rowing Championships", "1989", "15,023", "North Atlantic Conference", "the highland regions of Scotland", "Jeanne Tripplehorn", "March 2018", "Rotorua", "62", "flybe", "perry michael", "oscar goldsmith", "Afghan lawmakers", "director of adult prisons", "al Qaeda,", "Ford", "Matt Damon", "Aktion", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6160849567099567}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false], "QA-F1": [0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-5419", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-4040", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-4619", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-2628", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-3250", "mrqa_naturalquestions-validation-1636", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-4341", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-14674", "mrqa_searchqa-validation-9994", "mrqa_newsqa-validation-719"], "SR": 0.515625, "CSR": 0.49813179347826086, "EFR": 1.0, "Overall": 0.6806419836956522}, {"timecode": 92, "before_eval_results": {"predictions": ["Margery Williams", "a region of Vietnam north of Hanoi", "Terry Richardson", "youngest publicly documented people to be identified as transgender", "Australia women's national soccer team", "Odense Boldklub", "SpongeBob SquarePants 4-D", "Oldham County", "Grammar, logic, and rhetoric", "Wright brothers", "a research university with high research activity", "O.T. Genasis", "science fiction drama", "Speedway World Championship", "Citric acid", "About 200", "a deciduous tree in the genus \" Liquidambarisella\"", "Gerald Hatten Buss", "Delacorte Press", "close range combat", "twice", "Eli Raphael Roth", "Adelaide", "Lincoln Riley", "December 13, 1920", "Richard B. Riddick", "John McClane", "rural", "Orchard Central", "Art of Dying", "Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "on the Bahamian island of Great Exuma", "John Ford", "classical", "Marvel Comics", "between 7,500 and 40,000", "crafting and voting on legislation", "Magic Johnson", "Clark County, Nevada", "Yasir Hussain", "Victoria", "Jennifer Joanna Aniston", "Long Island", "Volcano Bay", "25 December 2009", "the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "his son", "\"Der Rosenkavalier\", \"Elektra\", \"Die Frau ohne Schatten\" and \"Four Last Songs\"", "Minnesota's 8th congressional district", "NBA 2K16", "Little Mo", "Professor Eobard Thawne", "India", "14", "beetle", "australian", "Vera Zvonareva of Russia and Austria's Daniel Koellerer", "Kearny, New Jersey.", "computer problems", "shoes", "Tennessee Williams", "Illinois", "bindweed"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7256291885198135}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.375, 0.8000000000000002, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 0.9090909090909091, 0.6666666666666666, 0.8, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4211", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-41", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-5501", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-4735", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.578125, "CSR": 0.498991935483871, "EFR": 1.0, "Overall": 0.6808140120967743}, {"timecode": 93, "before_eval_results": {"predictions": ["YIVO", "Archbishop of Canterbury", "Samuel Beckett", "March 17, 1941", "The Primate of All Ireland", "close range combat", "Iran", "Katherine Murray Millett", "Timothy Matthew Howard", "\"Lucky\"", "Do Kyung-soo", "John Hunt", "Kongo", "William Finn", "Sam Raimi", "Khalifa International Stadium", "Klemzig is a suburb of Adelaide in the City of Port Adelaide Enfield", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "Marine Corps Air Station Kaneohe Bay", "August 17, 2017", "Sergei Prokofiev", "Walter R\u00f6hrl", "his left hand", "Vladimir Menshov", "his biggest commercial success is the erotic thriller \"Ch Chloe\" (2009)", "Denmark and Norway", "Love and Theft", "C. W. Grafton", "Valerie Stowe", "\"My Love from the Star\"", "143,372", "Jack Kilby", "Cold Spring, New York", "Afghanistan", "Operation Aqueduct", "guitar feedback", "\"Curse of the Were-Rabbit\"", "George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Tampa", "1st Special Forces Operational Detachment-Delta", "140 million", "SpongeBob SquarePants 4-D", "StubHub Center", "Argentinian", "Neotropical realm", "a large portion of rural Maine, published six days per week in Bangor, Maine", "1998", "The More", "John F. Kennedy and First Lady Jacqueline Kennedy", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "The Avengers", "Rick Wakeman", "mental health", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "hardship", "berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7079310140431464}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 0.2, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-386", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-2085", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4776", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3259", "mrqa_hotpotqa-validation-4585", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-1063"], "SR": 0.65625, "CSR": 0.5006648936170213, "EFR": 1.0, "Overall": 0.6811486037234042}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush", "kilo Lima", "6", "golf", "Einstein", "michael", "Southampton", "Bleak House", "Vienna", "Harry S Truman", "new york", "to make wrinkles", "Amy Tan", "The Green Light in The Great Gatsby", "charles Chan", "1664", "Good Will Hunting", "viscount", "Iain Duncan Smith", "engraver", "george orwell", "Jim Peters", "oxygen", "pipe(s)", "oliver Brighenti", "heiresses", "a peacock", "PPTH", "The Wicker Man", "green", "Avonlea", "emmybeer", "guardian", "John Huston", "Passenger Pigeon", "Anne Frank", "manchega ewes", "mexico", "pi\u00f1a colada", "fauntleroy", "kachhi biryani", "Petula Clark", "Dr Tamseel", "Flo Rida", "Comedy of Errors", "mead", "chemical origins of life", "Finland", "fructose", "stuffed grape/vine leaves", "Kempton Park", "Cress", "Vesta", "The Michael Scott Paper Co.", "Tiffany & Company", "2010 to 2012", "Nathan Bedford Forrest", "by the week's end.", "Six", "that things are going well for them personally.", "preservation", "Tulane University", "Kimber Montag", "Bactrian camels"], "metric_results": {"EM": 0.5, "QA-F1": 0.5514632936507936}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.22222222222222224]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-1264", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-1767", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-7529", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-1393", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-2671", "mrqa_naturalquestions-validation-9903", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-6403", "mrqa_searchqa-validation-8477", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.5, "CSR": 0.5006578947368421, "EFR": 0.96875, "Overall": 0.6748972039473685}, {"timecode": 95, "before_eval_results": {"predictions": ["the Korean War", "$1.5 million", "Fernando Caceres", "receptionist with a gunshot wound in her stomach", "opposition parties", "warrior", "\"the United States is not and will never be at war with Islam.\"", "top-ranking U.S. official regarding Pakistan's connection to the November 26-29 attack on Mumbai,", "my wife's first name", "mammogram", "U.S. senators", "to put a lid on the marking of Ashura", "Alexey Pajitnov", "Spc. Megan Lynn Touma,", "inspectors in the agency's Colorado office", "the west African nation", "Leo Frank,", "Sri Lanka", "Johannesburg", "last year's", "devastating impact on the city's population causing enormous suffering and massive displacement,\"", "heavy flannel or wool", "an unjust war for an America", "Quetta, the capital of Balochistan province,", "walk", "an independent homeland", "bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "The torch will be carried on an eight-day trip through Greece,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "Adidas", "walk", "\"E! News\"", "Cologne, Germany", "she was lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "70,000 or so", "\"The station", "northwestern province of Antioquia,", "the most important attacks on the church don't come from the outside,", "large accumulations of ice in places such as the north Georgia mountains,", "East Java", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between June 20 and July 20,\"", "Rod Blagojevich,", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Majid Movahedi,", "anti-trust laws.", "Jonas", "billions of dollars in Chinese products each year,", "Windows Media Video ( WMV )", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Joe Pizzulo and Leeza Miller", "Alan Turing", "paisley", "thumbelina", "117", "iPod Classic", "Lithuanian", "Wayne's World", "Krakauer", "jake Barnes", "Rob Reiner"], "metric_results": {"EM": 0.5, "QA-F1": 0.6216185877031466}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.06060606060606061, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.9047619047619047, 0.13333333333333333, 1.0, 0.4, 0.9411764705882353, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.5, 1.0, 0.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1018", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-3916", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-10142"], "SR": 0.5, "CSR": 0.5006510416666667, "EFR": 0.96875, "Overall": 0.6748958333333335}, {"timecode": 96, "before_eval_results": {"predictions": ["Secretary of State Hillary Clinton", "poppy production", "security breach", "urged NATO to take a more active role in countering the spread of the", "public opinion in Turkey is a long-term affair,\"", "Christopher Savoie", "Brad Blauser,", "Tuesday afternoon.", "Iowa,", "Dr. Jennifer Arnold and husband Bill Klein,", "Arizona", "Michael Bloomberg", "1.2 million", "the estate with its 18th-century sights, sounds, and scents.", "Keating Holland.", "\"They're both tough, they're both smart and they both hate each other,\"", "in time", "FARC rebels.", "Mexico", "Bobby Darin,", "Alberto Espinoza Barron,", "spiral into economic disaster.", "Four", "Math teacher Mawise Gumba", "burned over 65 percent of his body", "Brian Smith", "2-1", "motor scooter", "Chuck Bass", "April 2010.", "\"Nothing But Love\" comeback tour,", "Mandi Hamlin", "people look at the content of the speech, not just the delivery.", "Yemen,", "Dodi Fayed,", "Kim Jong Il.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "The vessel boasts over 1000 square meters in forward deck space, allowing for such features as a full garden and pool, a tennis court, or several heli-pads.", "Manny Pacquiao", "\"I didn't think I was going to learn so much about myself through the process,\"", "\"The Angels family has suffered a tremendous loss today,\"", "President Thabo Mbeki", "Bright Automotive, a small carmaker from Anderson, Indiana,", "Jeffrey Jamaleldine", "Negotiators for Zelaya and Roberto Micheletti,", "U.S.", "Salt Lake City,", "his business dealings", "hardship for terminally ill patients and their caregivers,", "nearly 28 years of rule.", "Hollywood", "V \u00d7 2", "ThonMaker", "parashah ( or parshah / p\u0251\u02d0r\u0283\u0259 / or parsha )", "Syria", "Benedictine", "Cecil's Storehouse of Human Knowledge", "sexy Star", "March 31, 1944", "Dutch", "a varvoloka", "Marcus Garvey", "a zodiac", "obsessive-compulsive"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5551406926406927}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.9166666666666666, 0.0, 1.0, 0.0, 0.0, 0.2, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3578", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-1138", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-6184", "mrqa_hotpotqa-validation-5312", "mrqa_searchqa-validation-14538", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.4375, "CSR": 0.5, "EFR": 0.9722222222222222, "Overall": 0.6754600694444445}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "2018", "Exodus and Deuteronomy", "John Adams and Benjamin Franklin", "at the level of L1 or L2 ( closer to the head )", "his father King Dasharatha, on request of his second wife Kaikeyi", "India", "for the red - bed country of its watershed", "the Soviet Union and its satellite states", "Rashida Jones", "cut off close by the hip, and under the left shoulder", "warm and is considered to be the most comfortable climatic conditions of the year", "Peter Andrew Beardsley MBE", "Season two", "Terry Reid", "1273.6 cm", "May 3, 2005", "639", "Rashidun Caliphs", "British Columbia, Canada", "a central place in Christian eschatology", "Pyeongchang County, Gangwon Province, South Korea", "diffuse nebulae", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "1943", "Tokyo for the 2020 Summer Olympics", "Lituya Bay in Alaska", "their present home, the SAP Center at San Jose", "Panzerwagen VIII Maus", "July 2, 1776", "accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida", "Laura Jane Haddock", "In May 2016 Canada officially removed its objector status to UNDRIP, almost a decade after it was adopted by the General Assembly", "Bacon", "1994", "Matthew Broderick", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "April 2016", "Michael Schumacher", "Massachusetts", "the altitude changes it dramatically, particularly the temperature, reaching values very different according to the presence of different thermal floors", "2008", "post translational modification", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "writ of certiorari", "Jules Shear", "Senegal", "island", "head and neck", "Romeo Montague (Italian: \"Romeo and Juliet\" )", "De La Soul", "Delilah Rene", "July as part of the State Department's Foreign Relations of the United States series.\"", "warning to those who deny human rights", "$81,8709", "the Missouri", "jade", "Frank Sinatra", "Long troop deployments"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6702220271504811}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.3076923076923077, 0.375, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.14285714285714285, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-2425", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1987", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2892"], "SR": 0.578125, "CSR": 0.5007971938775511, "EFR": 0.8888888888888888, "Overall": 0.658952841553288}, {"timecode": 98, "before_eval_results": {"predictions": ["purple", "Phobos", "Lana Turner", "a Polaroid picture", "manchester", "June Carter Cash", "owling", "Colleen", "fat", "poison ivy", "Denny McLain", "road", "Edith Wharton", "Liberia", "rockabilly", "Buckingham Palace", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "funk rock band best known for their song \"Play That Funky Music\"", "Dr. Pepper", "misery", "the python", "coal mining", "Iowa", "detention", "(John) Paul II", "a dictopier", "Syria", "Margaretta D'Arcy", "the grain", "the Bean Sidhe", "Japan", "Zephyr", "ballistic missile submarine", "Ambrose Bierce", "Walt Whitman", "frequency", "Macbeth", "the Colorado", "vice presidential", "Tommy Franks", "Botswana", "Mousehunt", "the Dow Jones", "(Sir Winston) Churchill", "Vietnam", "a horn", "a Croque Madam", "Kyla Pratt", "Wisconsin", "March 11, 2018", "top cat", "goose bumps", "Adrian Cronauer", "1 August 1971", "Australia", "Bronwyn Bishop", "Jacob,", "Madhav Kumar Nepal", "Joe Jackson", "About 200"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6925080128205128}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-14933", "mrqa_searchqa-validation-1461", "mrqa_searchqa-validation-11742", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-9104", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-6160", "mrqa_searchqa-validation-2884", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1955"], "SR": 0.65625, "CSR": 0.5023674242424243, "EFR": 1.0, "Overall": 0.6814891098484849}, {"timecode": 99, "UKR": 0.619140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.828125, "KG": 0.503125, "before_eval_results": {"predictions": ["Biloxi", "Gulliver's Travels", "Minnesota", "the Rosetta Stone", "Japanese", "a crumpets", "Lord Bill Astor", "peripheral", "(Henry) Wadsworth", "Canton", "Hormel Foods", "syllable", "Theodore", "South Carolina", "Roger Williams", "Niels Bohr", "sun", "Moby Dick", "horror", "Surf\\'s Up", "Scorpio", "a Maine Coon cat", "Finding Nemo", "the International Space Station", "Shakira", "Candice Bergen", "a shark", "Ireland", "(George) Mitchell", "(Samuel, John)", "Gauguin", "Mary Stuart", "bamboo", "Barbie", "Crete", "Frank Sinatra", "(George) Custer", "barney stinson", "March 18", "Marlee Matlin", "Ben-Hur: A Tale of the Christ", "Hoad Nomo", "Dan Rather", "KLM", "food combining", "a tutor", "elephants", "Arkansas", "a Bank of America", "a piccolo", "a tuba", "Jason Marsden", "1998", "Garfield Sobers", "Germany", "Jimmy Carter", "mexico", "Detroit, Michigan", "ethno-nationalist", "ARY Digital Network", "Sri Lanka", "Omar Bongo,", "commission, led by former U.S. Attorney Patrick Collins,", "the Islamic prophet Muhammad"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7463541666666667}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-3515", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-2630", "mrqa_searchqa-validation-7727", "mrqa_triviaqa-validation-3439", "mrqa_hotpotqa-validation-4325", "mrqa_hotpotqa-validation-4869", "mrqa_naturalquestions-validation-6637"], "SR": 0.671875, "CSR": 0.5040625000000001, "EFR": 1.0, "Overall": 0.690890625}]}