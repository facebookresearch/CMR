{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.1,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5610, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["polynomial algebra", "Sydney", "Von Miller", "Kalenjin", "thymus and bone marrow", "Black's Law Dictionary", "building construction, heavy and civil engineering construction, and specialty trade contractors", "3.5 billion", "John Elway", "1798", "Albert Einstein", "monophyletic", "Xingu", "the New York Times", "white flight", "males", "uncivilized", "Greenland", "the equality of forces between two objects", "to encourage investment", "Grissom, White, and Chaffee", "N\u2013S", "Tolui", "18 million volumes", "since the Sui and Tang dynasties", "Ps. 31:5", "a new magma", "in an adult plant's apical meristems", "the Lisbon Treaty", "cysteine and methionine", "12", "17", "Honorary freemen", "Seine", "1072", "Academy of the Pavilion of the Star of Literature", "1969", "cilia", "main porch", "Denver Broncos", "electric eels", "San Jose Marriott", "Half of Paris's population of 100,000 people died. In Italy, the population of Florence was reduced from 110\u2013120 thousand inhabitants in 1338 down to 50 thousand in 1351", "Richard Leakey", "\u00dcberseering BV v Nordic Construction GmbH", "regeneration", "Wisconsin v. Yoder", "MODES", "1943", "Thomson", "1950s", "2010", "two", "Genghis Khan", "Serbian", "Warsaw", "Skylab", "The United Methodist Church", "Jerome Schurf", "a computer network funded by the U.S. National Science Foundation (NSF)", "14th to 17th centuries", "through homologous recombination", "University of Washington", "Falls"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9189338235294118}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4117647058823529, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8544", "mrqa_squad-validation-10333", "mrqa_squad-validation-6099", "mrqa_squad-validation-5065", "mrqa_squad-validation-7666", "mrqa_squad-validation-7049"], "SR": 0.90625, "CSR": 0.90625, "EFR": 1.0, "Overall": 0.953125}, {"timecode": 1, "before_eval_results": {"predictions": ["the poor", "higher", "9", "Canterbury", "Mediterranean", "Brown v. Board of Education of Topeka", "A turlough", "Australian Peter Siddle is the only bowler to take a hat - trick on his birthday", "the first recording of the song to be released was produced by Whitfield for Gladys Knight & the Pips", "In Time", "energy moves from producers ( plants ) to primary consumers", "The main television coverage of Celebrity Big Brother was screened on CBS during the winter of the 2017 -- 18 network television season", "Dawn ( Lizzy Greene ) is the oldest of the quadruplets", "W. Edwards Deming", "Langdon played a scatter - brained defendant on trial", "lithium - ion batteries", "first message was sent over the ARPANET in 1969", "The Methodist revival began with a group of men, including John Wesley ( 1703 -- 1791 ) and his younger brother Charles ( 1707 -- 1788 )", "the show has run on BBC One since 15 May 2004, primarily on Saturday evenings with a following Sunday night results show", "Henry Gibson as Wilbur, a pig who was almost killed due to being a runt", "the Indian Hockey Federation ( IHF ) had also been established and it sent a hockey team to the Summer Olympics", "iron ore production in Western Australia, and Australia as a whole, was negligible", "multinational retail corporation", "Pakhangba", "the ancestral virus, of avian origin, crossed the species boundaries and infected humans as human H1N1", "Bieber singing in a snappy and spiteful tone", "Speaker of the House of Representatives shall, upon his resignation as Speaker and as Representative in Congress, act as President", "It now plays a central role in the management of balance of payments difficulties and international financial crises", "by December 1349 conditions were returning to relative normalcy", "Sakshi Malik", "2017 Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "the Baltimore teenagers Ivan Ashford, Markel Steele, Cameron Brown, Tariq Al - Sabir and Avery Bargasse", "Jacob Packer is the Ripper", "Jerry Leiber and Mike Stoller", "TLC", "muscle contraction", "It is the heaviest fully enclosed armoured fighting vehicle ever built", "1700 Cascadia earthquake", "their son Jack ( short for Jack - o - Lantern ) is born on Halloween 2023", "the eighteenth season of Law & Order : Special Victims Unit debuted on Wednesday, September 21, 2016", "with an initial worldwide gross of over $1.84 billion, Titanic was the first film to reach the billion - dollar mark", "the algebraic rules for the elementary operations of arithmetic with such numbers", "Gustav Bauer, the head of the new government, sent a telegram stating his intention to sign the treaty if certain articles were withdrawn", "No. 23 retired by Chicago Bulls", "May 3, 2005", "Bieber", "Tami Lynn", "the north pole", "A substitute good is one good that can be used instead of another", "1985", "the chant was first adopted by the university's science club in 1886", "The papillary dermis is the uppermost layer of the dermis", "J. S. Bach", "the English dance thus apparently arose as part of a wider 15th-century European fashion for supposedly \"Moorish\" spectacle", "Westminster Abbey", "Liverpool", "Gracie Mansion", "2004 Paris Motor Show", "the Haitian National Police", "a rally", "dual nationality", "Burt's Bees is an \"Earth Friendly, Natural Personal Care Company.", "the Indiana Republican Party", "Beautiful Swimmers: Watermen, Crabs"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4627478308907295}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.1, 0.4444444444444445, 1.0, 0.25, 0.5, 0.2222222222222222, 0.12121212121212122, 0.18181818181818182, 0.0, 0.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 0.10526315789473684, 0.3636363636363636, 1.0, 0.25, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.35294117647058826, 0.10526315789473684, 0.0, 0.1904761904761905, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.5, 0.7272727272727273, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.3636363636363636, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-3971", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6130", "mrqa_hotpotqa-validation-2137", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-4059", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-748"], "SR": 0.34375, "CSR": 0.625, "EFR": 0.9047619047619048, "Overall": 0.7648809523809523}, {"timecode": 2, "before_eval_results": {"predictions": ["Oxygen therapy", "Lou Diamond Phillips", "Fran", "North Holland", "Westgate Las Vegas Resort & Casino", "21 July 2015", "Porto of Portugal", "University of Texas at Austin", "Blake Shelton", "Coalwood, West Virginia", "G\u00e9rard Depardieu", "tailless", "Francophone and French", "Polish", "Jim Davis", "Lockhart", "1946", "34.9 kilometres", "South African", "Christopher Tin", "Los Angeles Dance Theater", "Anglo-Frisian", "Durban", "clapping", "Macau", "Premier League", "Rule of three", "\"media for the 65.8 million,\"", "2009", "Bhaktivedanta Manor", "Vyd\u016bnas", "47,818", "The Catcher in the Rye", "Fyvie Castle", "Margaret Pellegrini", "Oracle Corporation", "Dennis Kux", "The Spiderwick Chronicles", "Maseru", "La Familia Michoacana", "Nikolai Morozov", "Rural Electrification Act", "Golden Gate National Recreation Area", "July 22, 1946", "Edmund Ironside", "John of Gaunt", "Treaty of Trianon", "Sturt", "Kind Hearts and Coronets", "Conservative Party", "2015 Monaco GP2 Series round", "Ravi Shastri", "four", "Highlands County, Florida", "the tallest building in the world", "Wilma Flintstone", "Downton Abbey", "MS Columbus", "the skull", "Swedish Prime Minister Fredrik Reinfeldt", "Scribd", "neutrino", "an 11 to 1 oz shot glass", "\"Merchant's Harbor\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5648809523809524}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.25, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-2307", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1462", "mrqa_naturalquestions-validation-10098", "mrqa_triviaqa-validation-7335", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-2045", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-11990"], "SR": 0.5, "CSR": 0.5833333333333333, "EFR": 1.0, "Overall": 0.7916666666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["Frankfort", "Ada Monroe", "Priscilla Presley", "is shortening his billing", "Australia", "Blair House", "trespasser", "King Menkaure (Mycerinus)", "the case of Von Bischoff", "Anja Prson", "Thomas Lanier", "is a German theoretical physicist", "a sub-adult raccoon", "are a genus of antelopes, Connochaetes", "a man named Alexander Selkirk", "a sixteen-year-old boy", "\"The Man Show\"", "St. Petersburg", "Poseidon", "politician", "Geraldine A. Ferraro", "throat", "Maryland", "games named for the skull", "a little tiger", "is a serious threat to Mexico", "copies of stolen State Department documents", "is the top 15 nonfiction book", "is a rock-forming mineral", "\"Tattoo\"", "In 2000 the pres. of this country", "Minnesota", "admiral", "isabella for Castile", "is", "Anyone who Hate children and dogs", "a point", "Fleetwood Mac", "cells of the immune", "\"Temper, Temper\"", "Boston", "is the bulging (or prolapse) of one or both of the mitral", "Marlon Brando", "Sierra Leone", "Wilbur Wright", "the Pelican", "aujourd'hui", "is a classic of... than the college curriculum", "the duck (bill and webbed feet), beaver (tail), and otter (body and fur)", "Cordelia", "a young Shakespeare", "Dougie MacLean", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Geophysicists", "Columbus", "Omid Djalili", "red", "concentration camp", "79 AD", "1998", "2-0", "\"He is more American than German.\"", "the Kurdish militant group in Turkey", "Sakua Hanai"], "metric_results": {"EM": 0.265625, "QA-F1": 0.29558566433566436}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.4, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10393", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-7799", "mrqa_searchqa-validation-9678", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-13362", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-3001", "mrqa_searchqa-validation-5662", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-8974", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-5332", "mrqa_searchqa-validation-16871", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-1327", "mrqa_searchqa-validation-10805", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-12812", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-9842", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-2811", "mrqa_searchqa-validation-3798", "mrqa_searchqa-validation-12303", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-6857", "mrqa_triviaqa-validation-96", "mrqa_hotpotqa-validation-3406", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1508", "mrqa_hotpotqa-validation-5606"], "SR": 0.265625, "CSR": 0.50390625, "retrieved_ids": ["mrqa_squad-train-13716", "mrqa_squad-train-67276", "mrqa_squad-train-38926", "mrqa_squad-train-29908", "mrqa_squad-train-45864", "mrqa_squad-train-38125", "mrqa_squad-train-29925", "mrqa_squad-train-27576", "mrqa_squad-train-11759", "mrqa_squad-train-44441", "mrqa_squad-train-20562", "mrqa_squad-train-80549", "mrqa_squad-train-8739", "mrqa_squad-train-78732", "mrqa_squad-train-42745", "mrqa_squad-train-68369", "mrqa_hotpotqa-validation-738", "mrqa_naturalquestions-validation-4751", "mrqa_squad-validation-8544", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-10557", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-2137", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-3618", "mrqa_newsqa-validation-1512", "mrqa_squad-validation-5065", "mrqa_hotpotqa-validation-2492"], "EFR": 1.0, "Overall": 0.751953125}, {"timecode": 4, "before_eval_results": {"predictions": ["Woodstock", "the World Bank", "1789", "Rio de Janeiro", "a brain injury", "the Plough", "Siamese", "Ken Burns", "Two", "Lady Gaga", "George I", "Milwaukee", "an arrycteropus", "Copenhagen", "George W. Bush", "George IV", "Aidan Crawley", "Angostura", "an acute triangle", "Luke", "Tears for Fears", "Polish", "bauxite", "(Bach)", "the Hooded Claw", "New Years Day", "1984", "gin", "El Aneto", "Otto von Bismarck", "Mathematics", "Edwina Currie", "Wellington", "Pisces", "Montezuma", "the NATO (North Atlantic Treaty Organisation) phonetic alphabet", "French Guiana", "Prime Minister of the Commonwealth of Australia", "French Polynesia", "East Berlin and West Berlin", "Burkina Faso", "Kiki", "Daedalus", "Illinois", "Coretta Scott King", "San Francisco", "Electra Aeschylus", "\"Bellyhands\"", "\"Aquarius\"", "Harry Potter", "a company that provides Internet services", "China", "qualitative data, quantitative data", "Franklin Roosevelt", "Sharmi Albrechtsen", "1992", "ExCeL Exhibition Centre", "the Dalai Lama", "Chicago, Illlinois", "Larry King", "a dream", "Brian Austin Green", "Leontyne Price", "bass"], "metric_results": {"EM": 0.515625, "QA-F1": 0.569171626984127}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-4899", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-415", "mrqa_triviaqa-validation-4566", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-2055", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-5215", "mrqa_hotpotqa-validation-1777", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-2133", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-3699"], "SR": 0.515625, "CSR": 0.50625, "EFR": 0.9354838709677419, "Overall": 0.7208669354838709}, {"timecode": 5, "before_eval_results": {"predictions": ["revenge and karma", "functions", "starch", "Cyanea capillata", "Roger Dean Stadium", "1936", "help bring creative projects to life", "Billy Idol", "Cairo, Illinois", "Idaho", "October 12, 1979", "Speaker of the House of Representatives", "1600 BC", "What Is To Be Done", "the person compelled to pay for reformist programs", "April 6, 1917", "Andy Serkis", "Narendra Modi", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "manta rays and Scorpion fish", "Abid Ali Neemuchwala", "Steve Hale", "Manhattan Island", "2017 season", "nucleotides", "Ethiopia and Liberia", "is the ultimate exercise for the bored and lazy", "2018", "Vice President", "Killer Within", "branch roots", "Javier Fern\u00e1ndez", "1959", "down to the ground", "2013", "games", "Major Molineux", "Mustafa Ali", "1998", "May 31, 2012", "Gareth Barry", "the Pir Panjal Range", "Lieutenant Templeton `` Faceman '' Peck", "an unknown recipient", "10 logarithm", "Joseph Heller", "After Margaret Thatcher became Prime Minister in May 1979", "in the 1820s", "Javier Fern\u00e1ndez", "the septum", "1799", "Iwo Jima", "gums", "east Ethiopia", "the Harpe brothers", "\"There Is Only the Fight... : An Analysis of the Alinsky Model", "the Three Caesars' Alliance", "4 meters (13 feet) high", "England Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League", "\"stateless actors\"", "tango", "j Jacob Marley", "scariest", "tennis"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5686212678541966}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.1818181818181818, 1.0, 1.0, 0.5, 0.0, 0.0689655172413793, 0.5, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.25, 0.10526315789473684, 0.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-7047", "mrqa_triviaqa-validation-4912", "mrqa_hotpotqa-validation-1704", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1173", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-12461"], "SR": 0.46875, "CSR": 0.5, "EFR": 1.0, "Overall": 0.75}, {"timecode": 6, "before_eval_results": {"predictions": ["Birmingham", "round ball", "Spain", "gooseberry", "American racing", "the Vietnam war", "Australia", "zoology", "1982", "Calvors Brewery", "a hole-in-one", "white", "Edwardlear", "the \u201cBloody Assizes\u201d of 1685", "Chief Inspector of Prisons", "a pervasive pattern of excessive emotionality and attention seeking", "Peter Paul Rubens", "Gerber Technology", "tomato", "Cheshire", "Snow White", "aurochs", "American shipping heir Aristotle Onassis", "Wannabe", "Robert F. Kennedy", "American Gaga", "American", "Leeds, West Yorkshire", "American guitar", "wool", "Stockton-on-Trent", "winter", "a whiteberry", "Michael Faraday", "SuperiorPics.com", "a American greyhound", "American", "American Ivy", "John Nash", "Miranda v. Arizona", "Judas Iscariot", "The Big Bopper", "a downtown restaurant late at night", "South Korea", "tamale", "o.B.E.", "1994", "American writer Jules Verne", "Strabo", "Newcastle Falcons", "The Odd Couple", "74", "in the northwest, it connects with the Arafura Sea through the Torres Strait", "Vienna", "American actress and comedian", "1967", "Marco Fu", "$250,000", "Zac Efron", "Suba Kampong township on the Philippine island of Basilan", "a desktop microcomputer", "copper", "an earthquake", "12\u201318"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4248263888888889}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2893", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-7569", "mrqa_triviaqa-validation-4942", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7525", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3853", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-28", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4367", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-4384", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-3406", "mrqa_searchqa-validation-8172", "mrqa_hotpotqa-validation-244"], "SR": 0.34375, "CSR": 0.4776785714285714, "retrieved_ids": ["mrqa_squad-train-11030", "mrqa_squad-train-75", "mrqa_squad-train-69824", "mrqa_squad-train-69517", "mrqa_squad-train-67315", "mrqa_squad-train-71380", "mrqa_squad-train-31884", "mrqa_squad-train-50458", "mrqa_squad-train-56162", "mrqa_squad-train-19258", "mrqa_squad-train-59427", "mrqa_squad-train-13668", "mrqa_squad-train-38509", "mrqa_squad-train-49282", "mrqa_squad-train-25666", "mrqa_squad-train-10702", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-5494", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5396", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-12647", "mrqa_naturalquestions-validation-672", "mrqa_searchqa-validation-13362", "mrqa_newsqa-validation-2045", "mrqa_naturalquestions-validation-5599", "mrqa_triviaqa-validation-412", "mrqa_searchqa-validation-10805", "mrqa_hotpotqa-validation-510", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-6856"], "EFR": 1.0, "Overall": 0.7388392857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["sour", "edward lear", "NUT", "Churchill Downs", "Fotheringhay Castle in England", "gregor", "mary Pickford", "Gal\u00e1pagos Islands", "bactrian", "Narendra Modi", "FBI", "Z", "Maine", "ascorbic acid", "Vincent Craddock", "Dilbert", "Lorenzo da Ponte", "hair", "Celtic", "Taggart", "Mexico", "Benazir Bhutto", "China", "Brian Close", "jumanji", "Rocky Horror Show", "Thwaites", "(Second Reich)", "Magnolia", "krak\u00f3w", "Venice", "Sarah Sawyer", "Robert Devereux", "British South Africa", "Fermanagh", "candelabrum", "Oklahoma City", "Fancy Dress Shop", "berlin", "iron", "Blackburn", "Antonia Fraser", "1937", "french connection", "Moon River", "lead singer", "murderer", "prime minister Yitzhak Rabin", "Princeton-Plainsboro Teaching Hospital", "kentist", "orange juice", "Border Collie", "December 1, 2009", "a simple majority vote", "gwyneth Paltrow", "2006", "Ang Lee", "fighting charges of Nazi war crimes", "Michael Jackson", "Nineteen", "baroque", "gregor", "Hugo Chvez", "Tempo"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5979166666666667}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-2006", "mrqa_triviaqa-validation-1185", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-1941", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-4132", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5412", "mrqa_newsqa-validation-2052", "mrqa_searchqa-validation-12069", "mrqa_searchqa-validation-11191"], "SR": 0.53125, "CSR": 0.484375, "EFR": 1.0, "Overall": 0.7421875}, {"timecode": 8, "before_eval_results": {"predictions": ["Douglas MacArthur", "w.C. Handy", "Don garlits", "crimson tide", "belle", "holly", "conan conan", "belle", "turbojet", "Weimar", "horse", "smith", "godot", "charlie leepson", "Sikhism", "eyebrows", "George Frideric Handel", "Mendenhall Glacier", "smackdown", "badger", "army of the Potomac", "salmon", "Steve Carell", "phlegethon", "Alexander Graham Bell", "belle", "belle", "crimson tide", "crimson tide", "belle", "Mars", "smith", "a jigger", "crimson tide", "corvidae", "roman doyle", "belle", "Riding in Cars with Boys", "Rachel Carson", "stroke", "chow chow", "belle", "Yuri Gagarin", "Mary (Wollstonecraft) Shelley", "wheat bran", "Bastille", "Sam Houston", "crimson tide", "roman", "drone", "Ethiopia", "Pradyumna", "Bonhomme Carnaval", "April 29, 2009", "Robert Stroud", "moem Hunter", "moz conan eriksson", "aresomeness", "1956", "German", "Tennessee", "crashing his private plane into a Florida swamp", "English", "\"Watchmen\""], "metric_results": {"EM": 0.40625, "QA-F1": 0.43338815789473684}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7368421052631579, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-5680", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-501", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-3365", "mrqa_searchqa-validation-4714", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-14732", "mrqa_searchqa-validation-7439", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-8794", "mrqa_searchqa-validation-4823", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-11757", "mrqa_searchqa-validation-8853", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-6607", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-12805", "mrqa_naturalquestions-validation-5611", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-6521", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-4388", "mrqa_newsqa-validation-2689"], "SR": 0.40625, "CSR": 0.4756944444444444, "EFR": 1.0, "Overall": 0.7378472222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["were anti- Strike by a ratio of 9 to 1 on Tuesday", "37", "oil may be present in thin intervals but that reservoir quality is poor", "appealed against the punishment", "peanuts", "President Obama", "27", "Alexey Pajitnov", "$15 billion", "2001", "the war of words", "Egypt", "Casalesi Camorra clan", "Lashkar-e-Jhangvi", "Iraq", "30", "Kim Jong Un", "the U.S. Holocaust Memorial Museum", "Brian David Mitchell", "105", "the strength of its brand name and the diversity of its product portfolio", "80", "Robert Wagner", "123 pounds of cocaine and 4.5 pounds of heroin", "club managers", "15,000", "sexual assault on a child", "Turkish President Abdullah Gul", "a radical Muslim sheikh called Friday for the creation of an Islamic emirate in Gaza", "propofol", "Hillary Clinton", "a monthly allowance", "40", "Aryan Airlines Flight 1625", "Swansea Crown Court", "Spaniard", "Roma", "Tulsa, Oklahoma", "Sporting Lisbon", "Miss USA", "A Lion Among Men", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States", "Somalia", "one", "July 18, 1994", "Immigration Minister Eric Besson", "Susan Atkins", "Mad Men", "July", "Arthur E. Morgan III", "Draquila -- Italy Trembles", "Adam Mitchell", "grace Zabriskie", "counter clockwise", "a Cockney", "Marni Nixon", "Harnoncourt", "Five Summer Stories", "9 February 1971", "Fatih Ozmen", "Kenny G", "Jerry Maguire", "jade", "Wanda"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5924045138888889}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-627", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-1766", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-5354", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-6241"], "SR": 0.546875, "CSR": 0.4828125, "retrieved_ids": ["mrqa_squad-train-38465", "mrqa_squad-train-58138", "mrqa_squad-train-26217", "mrqa_squad-train-51283", "mrqa_squad-train-37878", "mrqa_squad-train-71004", "mrqa_squad-train-46608", "mrqa_squad-train-27765", "mrqa_squad-train-40946", "mrqa_squad-train-75687", "mrqa_squad-train-27559", "mrqa_squad-train-31321", "mrqa_squad-train-4007", "mrqa_squad-train-47909", "mrqa_squad-train-52428", "mrqa_squad-train-61835", "mrqa_triviaqa-validation-5860", "mrqa_searchqa-validation-8588", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-4751", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-7309", "mrqa_triviaqa-validation-1657", "mrqa_searchqa-validation-3320", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-412", "mrqa_hotpotqa-validation-4740", "mrqa_triviaqa-validation-7525", "mrqa_triviaqa-validation-7097", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-6326", "mrqa_searchqa-validation-12378"], "EFR": 1.0, "Overall": 0.74140625}, {"timecode": 10, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-103", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1170", "mrqa_hotpotqa-validation-1354", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1462", "mrqa_hotpotqa-validation-1623", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1970", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2292", "mrqa_hotpotqa-validation-2307", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3083", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3406", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-949", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-115", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3525", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-171", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1954", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2064", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3057", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-862", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10805", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11331", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-11908", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13362", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14219", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14732", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-16135", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-16871", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2590", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2811", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-3001", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3428", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-3759", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4714", "mrqa_searchqa-validation-4823", "mrqa_searchqa-validation-501", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5332", "mrqa_searchqa-validation-5435", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-5680", "mrqa_searchqa-validation-6059", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6603", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7439", "mrqa_searchqa-validation-7445", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7799", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8853", "mrqa_searchqa-validation-8974", "mrqa_searchqa-validation-9842", "mrqa_squad-validation-10333", "mrqa_squad-validation-10378", "mrqa_squad-validation-1115", "mrqa_squad-validation-1155", "mrqa_squad-validation-1158", "mrqa_squad-validation-1975", "mrqa_squad-validation-2240", "mrqa_squad-validation-2550", "mrqa_squad-validation-2655", "mrqa_squad-validation-2832", "mrqa_squad-validation-317", "mrqa_squad-validation-3705", "mrqa_squad-validation-384", "mrqa_squad-validation-3913", "mrqa_squad-validation-3986", "mrqa_squad-validation-4140", "mrqa_squad-validation-4183", "mrqa_squad-validation-4239", "mrqa_squad-validation-4436", "mrqa_squad-validation-4484", "mrqa_squad-validation-4711", "mrqa_squad-validation-4953", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5095", "mrqa_squad-validation-5174", "mrqa_squad-validation-5311", "mrqa_squad-validation-5315", "mrqa_squad-validation-5503", "mrqa_squad-validation-6099", "mrqa_squad-validation-6763", "mrqa_squad-validation-679", "mrqa_squad-validation-6792", "mrqa_squad-validation-7021", "mrqa_squad-validation-7049", "mrqa_squad-validation-7060", "mrqa_squad-validation-7080", "mrqa_squad-validation-7257", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-7455", "mrqa_squad-validation-7552", "mrqa_squad-validation-7666", "mrqa_squad-validation-7903", "mrqa_squad-validation-7917", "mrqa_squad-validation-8064", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-8313", "mrqa_squad-validation-8544", "mrqa_squad-validation-8846", "mrqa_squad-validation-8954", "mrqa_squad-validation-9149", "mrqa_squad-validation-9782", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1122", "mrqa_triviaqa-validation-1185", "mrqa_triviaqa-validation-1323", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-1724", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2055", "mrqa_triviaqa-validation-2078", "mrqa_triviaqa-validation-211", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2893", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3349", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-3523", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3638", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3900", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-415", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4215", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-4487", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4560", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-4899", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4942", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-5224", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-5303", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-5712", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-633", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6815", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6852", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7084", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7525", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-905", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-995"], "OKR": 0.8828125, "KG": 0.49296875, "before_eval_results": {"predictions": ["24", "$60 billion", "Ike", "Stella McCartney", "Newark's Liberty International Airport,", "Dick Cheney,", "seven", "At least 40", "pilot", "leftist Workers' Party", "Zuma", "\"One man refuses to leave his home", "Schalke", "the same drama that pulls in the crowds", "Russia", "Johnny Depp", "two months ago", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Elena Kagan", "intricate Flemish tapestries", "naming physicist Steven Chu as secretary of energy", "Two pages", "Tsvangirai's", "Joel \"Taz\" Di Gregorio", "Nico Rosberg", "Mandi Hamlin", "Veracruz", "the insurgency", "United States", "enjoys a cold shower in his home in New Zealand.", "Josef Fritzl,", "a bag", "17", "a", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "bankruptcies", "40", "served in the military", "military veterans", "\"Twilight", "Obama's early words and actions send a powerful, positive signal to Muslims.", "the fact that the teens were charged as adults.", "Mary Phagan Kean", "the Ku Klux Klan", "China", "Sarah", "\"peregruzka\"", "the legitimacy of that race", "A British man who strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "voice-assistant", "Apple employees", "Elizabeth Dean Lail", "providing telecommunication services to enterprises and offices", "HTTP / 1.1", "Eddie Murphy", "Isles of the Blessed", "Belgium", "Bourbon", "Pinellas", "Winnie the Pooh", "Fox's", "Torah", "Hartford", "tissues"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4912660256410257}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.06666666666666667, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.10000000000000002, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-674", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-887", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-1702", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-2818", "mrqa_hotpotqa-validation-2600", "mrqa_searchqa-validation-10607", "mrqa_searchqa-validation-4162", "mrqa_naturalquestions-validation-8386"], "SR": 0.40625, "CSR": 0.4758522727272727, "EFR": 1.0, "Overall": 0.7281392045454546}, {"timecode": 11, "before_eval_results": {"predictions": ["Florida", "Kim Smith", "Mafia", "Patrick Henry", "gondola song", "Louis XV", "In God We Trust", "U.S.", "\"Church of Christ,\"", "CouscousCouscous", "plants", "Handel", "Stephenie Meyer", "Ralph Vaughan Williams", "Willie Nelson", "Ridley Scott", "The National Council for the Unmarried Mother", "Tom Hanks", "Sir Walter Scott.", "a symbol of position and title.", "Dame Maggie Smith", "United Republic of Tanzania", "blue ivy", "6", "the solar system", "Jupiter", "Genesis", "The Quatermass Experiment", "gold", "giorgio Armani,", "Jack Ruby", "Rugby", "Model A", "Il Divo", "narwhal", "Champion New Zealand-bred Nightmarch", "a machine that cuts the bread finely", "Real Madrid CF", "\"Duke\"", "\"Penry\"", "Fernando Lamas", "Aintree", "McDonalds", "The PRIX DE L'ARC DE TRIOMPHE", "Kansas City", "Oswald Cobblepot", "French", "Venus", "Salt Lake City, Utah", "Jeroboam", "Henkel", "Giancarlo Stanton", "After being absent for a time,", "Houston Dynamo", "Beatles", "mid-engine sports car", "Chelsea", "2011", "The Livesay's reports came in the form of tweets that alternated between raw descriptions and expressions of hope", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "Great Paris", "The Book of Ruth", "The clone Wars", "business"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5728760822510823}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.12121212121212123, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-3355", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-4205", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-1608", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-7520", "mrqa_triviaqa-validation-1229", "mrqa_triviaqa-validation-2558", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-6916", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-2620", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-3376", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-178"], "SR": 0.515625, "CSR": 0.47916666666666663, "EFR": 1.0, "Overall": 0.7288020833333333}, {"timecode": 12, "before_eval_results": {"predictions": ["Lady Gaga", "stella", "sugarloaf", "mccini\u2019s", "Anita Roddick", "Gulf of Aden", "vibraslap percussion", "Four Tops", "billy crystal", "Mark Rothko", "george Garden", "Byker Grove", "Hampton Court", "holly", "sound and light", "loadsamoney", "billy crystal", "phillymawks", "billy crystal", "heart", "hongai", "beer", "billy crystal", "The Meadows", "Kenya", "power station", "phoenician", "Vince Cable", "Uncle Tom\u2019s Cabin", "Thebes", "Love Never Dies", "Adam Faith", "france", "blood", "partridge", "Wolfgang Amadeus Mozart", "diffusion", "3.762", "cutter\u2019s", "Beaujolais", "france", "george Fox", "denarius", "Crosspool", "Miss Dent", "Devonport", "1978", "baulieu", "end", "Novak Djokovic", "omerta", "Orange Juice", "The Annunciation", "Super Bowl XXXIX", "Oklahoma Sooners", "Capture of the Five Boroughs", "the world", "air support", "Robert Barnett", "\"I have never thought about taking children away from their father, never,\"", "blood type", "knock on wood", "Calamity Jane", "Real Madrid"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5006810897435897}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307691, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6733", "mrqa_triviaqa-validation-4128", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-222", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-666", "mrqa_triviaqa-validation-5773", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-2321", "mrqa_naturalquestions-validation-1946", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-1899", "mrqa_searchqa-validation-4281", "mrqa_hotpotqa-validation-1987"], "SR": 0.46875, "CSR": 0.4783653846153846, "retrieved_ids": ["mrqa_squad-train-72089", "mrqa_squad-train-46994", "mrqa_squad-train-64701", "mrqa_squad-train-60260", "mrqa_squad-train-28468", "mrqa_squad-train-16313", "mrqa_squad-train-67067", "mrqa_squad-train-15", "mrqa_squad-train-61504", "mrqa_squad-train-4297", "mrqa_squad-train-21153", "mrqa_squad-train-29555", "mrqa_squad-train-69206", "mrqa_squad-train-82327", "mrqa_squad-train-47398", "mrqa_squad-train-28597", "mrqa_hotpotqa-validation-2796", "mrqa_searchqa-validation-12647", "mrqa_squad-validation-6099", "mrqa_triviaqa-validation-4942", "mrqa_naturalquestions-validation-2690", "mrqa_searchqa-validation-12717", "mrqa_triviaqa-validation-1963", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2228", "mrqa_searchqa-validation-6241", "mrqa_triviaqa-validation-2635", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-6943", "mrqa_triviaqa-validation-7151", "mrqa_newsqa-validation-2689", "mrqa_searchqa-validation-12134"], "EFR": 1.0, "Overall": 0.728641826923077}, {"timecode": 13, "before_eval_results": {"predictions": ["Duck", "Pink Floyd", "in the Chicago metropolitan area in 1982", "Doug Pruzan", "Nodar Kumaritashvili", "1881", "Bob Dylan, George Harrison, Jeff Lynne", "an unknown recipient", "- ase", "Prince - Electors", "trade", "Ben Willis", "Haytham Kenway", "6 January 793", "Hellenism", "Gorakhpur Junction", "John Rhoades", "Ptolemy", "transmissions", "Poems : Series 1", "1931", "Michael Clarke Duncan", "East India Company", "InterContinental Hotels Group", "the monsoons", "2018", "gravitation", "IIII", "Charles Perrault", "1926", "at birth", "Branford College", "Tim McGraw", "D\u00e1in", "A complex sentence", "Marty Robbins", "two", "Mexico", "season seven", "nominally a civil service post", "ninth", "2 %", "maquiladora", "1912", "$2 million", "state legislators of Assam", "Old Trafford", "Thomas Jefferson", "regulatory", "Fall 1998", "USS Chesapeake", "ozone", "beta", "McDonnell Douglas", "Hermione Baddeley", "John Andr\u00e9", "Guardians of the Galaxy", "roald", "$89", "a passenger's name", "better safe than sorry", "k Kyrgyzstan", "pilot", "the insurgency"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5704910714285714}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true], "QA-F1": [0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.24000000000000002, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-8460", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-2506", "mrqa_triviaqa-validation-1651", "mrqa_hotpotqa-validation-4345", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-826", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-13535"], "SR": 0.4375, "CSR": 0.4754464285714286, "EFR": 0.9444444444444444, "Overall": 0.7169469246031747}, {"timecode": 14, "before_eval_results": {"predictions": ["the efferent nerves that directly innervate muscles", "Marcus Atilius Regulus", "Omar Khayyam", "they qualify as a medical practitioner following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "The Han", "an iron -- nickel alloy", "Giancarlo Stanton", "a presidential veto", "late January or early February", "the state in which both reactants and products are present in concentrations which have no further tendency to change with time", "Andy Serkis", "8ft", "Masha Skorobogatov", "the source of the donor organ", "Human fertilization", "Alastair Cook", "the Rashidun Caliphs", "a transformation change of heart", "members of the gay ( LGBT ) community", "18", "Robber Barons", "up to 100,000 write / erase cycles", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Los Angeles", "13 February", "J. Presper Eckert", "an Easter egg", "if the concentration of a compound exceeds its solubility", "land, fresh water, air, rare earth metals and heavy metals", "Arunachal Pradesh", "961", "Antarctica", "January 2004", "Currington", "J.P. Zenger High", "Warren Zevon", "in the books of Exodus and Deuteronomy", "Union forces", "Xiu Li Dai", "W. Edwards Deming", "referee", "Strabo", "Meri", "Joel", "thick skin", "The federal government", "Jason Flemyng", "Muhammad Yunus", "the variety b --", "a habitat", "longer from shore to shore", "Evening Prayer", "Richard Wagner,", "the rent doesn't include additional costs such as insurance or business rates", "the nuclear fission products", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "the Mayor of the City of New York", "1918-1919", "three", "Steven Gerrard", "the Rock and Roll Hall of Fame", "Marie Osmond", "Keats", "Sheikh Abu al-Nour al-Maqdessi,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5820231851481852}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, false], "QA-F1": [0.5, 0.0, 1.0, 0.09523809523809525, 0.0, 1.0, 1.0, 0.0, 0.5000000000000001, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.923076923076923, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-7440", "mrqa_triviaqa-validation-2506", "mrqa_hotpotqa-validation-3972", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-2732"], "SR": 0.515625, "CSR": 0.478125, "EFR": 1.0, "Overall": 0.72859375}, {"timecode": 15, "before_eval_results": {"predictions": ["Eleanor Roosevelt", "a sidereal day", "Baltimore Stallions", "the ship's five anchors", "Italy", "Gang of Four", "the Capulets & the Montagues", "Viggo Mortensen", "peron", "Green Party", "Exodus 32", "the Song Dynasty", "meatballs", "\"Little Red Riding Hood\"", "grover clever", "the marine iguanas", "gershwin", "Molly Ringwald", "bebop", "Nikolai Gogol", "In Country", "spontaneous", "Nashville", "Abraham Lincoln", "STUPID ANSWERS:", "Canada", "savanna mosaic", "Tuscany", "the Vulgar Tongue", "Ray Kroc", "vivicia", "a song from the musical wicked, composed by Stephen Schwartz, originally recorded by Idina Menzel and Kristin Chenoweth", "D.C.", "Dairy Queen", "New Jersey", "West Virginia", "Lewis and Clark", "John Grunsfeld,", "Transformers:", "beans", "Norway", "Hawaii", "pound sterling", "Mario Puzo", "Wilkie Collins", "George C. Marshall", "3", "Nile", "Kermit Roosevelt", "Richie Rich", "yellow fever", "necessary, but not sufficient", "2013", "1995", "gregor", "Thailand", "matricide", "\"Can't Be Tamed\"", "a schoolmaster", "Singapore", "Nechirvan Barzani,", "allegedly involved in forged credit cards and identity theft", "At least 15", "Lorazepam"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5883506811815635}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.22727272727272727, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-9127", "mrqa_searchqa-validation-92", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-8642", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-7306", "mrqa_searchqa-validation-1336", "mrqa_searchqa-validation-15570", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-4546", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-2805", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-14765", "mrqa_searchqa-validation-7874", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7554", "mrqa_triviaqa-validation-1555", "mrqa_hotpotqa-validation-1539", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3200", "mrqa_naturalquestions-validation-4463"], "SR": 0.484375, "CSR": 0.478515625, "retrieved_ids": ["mrqa_squad-train-31756", "mrqa_squad-train-17850", "mrqa_squad-train-39262", "mrqa_squad-train-8350", "mrqa_squad-train-38", "mrqa_squad-train-21816", "mrqa_squad-train-42252", "mrqa_squad-train-64642", "mrqa_squad-train-43550", "mrqa_squad-train-32809", "mrqa_squad-train-46380", "mrqa_squad-train-43118", "mrqa_squad-train-79616", "mrqa_squad-train-84440", "mrqa_squad-train-14089", "mrqa_squad-train-36264", "mrqa_triviaqa-validation-412", "mrqa_searchqa-validation-1951", "mrqa_hotpotqa-validation-1987", "mrqa_squad-validation-8544", "mrqa_hotpotqa-validation-1777", "mrqa_newsqa-validation-832", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-4912", "mrqa_newsqa-validation-270", "mrqa_triviaqa-validation-2192", "mrqa_naturalquestions-validation-9235", "mrqa_newsqa-validation-2514", "mrqa_searchqa-validation-11281", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-3206"], "EFR": 1.0, "Overall": 0.7286718750000001}, {"timecode": 16, "before_eval_results": {"predictions": ["watermark", "the word at the top of... 11a was interesting", "the Monitor", "Chomolungma", "J.K. Rowling", "Errol Morris", "Smith & Wesson", "the hussar busby", "Kindergarten Cop", "white chocolate n Breakfast Chicken Fried Chicken", "masks", "Jean-Paul Marat", "Sergeant John Ryan", "Anne Frank", "Endymion", "the masses", "a sandstorm", "Ben & Jerry", "1,000,000", "Billy Joel", "Abigail Breslin", "Avril Lavigne", "Robin Williams", "Hephaestus", "an arroyo", "goober", "Hallmark Cards", "the cardinal", "San Francisco", "Bloomingdale's", "gravitational field", "Lady M", "Harrison Ford", "Martinique", "a colonel", "Frank Lloyd Wright", "Josephine", "Jack Johnson", "cytokinesis", "Beau Bridges", "Tudor", "James Cook", "Crayola", "Neil Simon", "a 919mm Parabellum pistol", "San Francisco", "Princess Leia", "the lithosphere", "Gibraltar", "Lebanon", "ballpoint pen", "2007 World Series", "100", "Burbank, California", "Rugby School", "an acid phosphate", "pityriasis", "1989 until 1994", "Golden Gate", "Future", "\"CNN Heroes: An All-Star Tribute\"", "Washington State's decommissioned Hanford nuclear site,", "Les Bleus", "Annales de chimie"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5708791208791208}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3076923076923077, 0.6666666666666666, 0.0, 0.0, 0.8571428571428571, 0.5714285714285715, 0.0, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-14068", "mrqa_searchqa-validation-14048", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-13258", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-14646", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-4679", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-1464", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-8645", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-12678", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-2160", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-956", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1229", "mrqa_hotpotqa-validation-4813"], "SR": 0.453125, "CSR": 0.47702205882352944, "EFR": 1.0, "Overall": 0.7283731617647059}, {"timecode": 17, "before_eval_results": {"predictions": ["James Brown", "October 29, 2015", "Red Red", "230 million kilometres ( 143,000,000 mi )", "sperm and ova", "Spain", "December 2, 2013, and the third season concluded on October 1, 2017", "Lionel Hardcastle", "at least 28", "Rockwell", "Lady Gaga", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law", "February 27, 2007", "Confederate States Army", "V\u1e5bksayurveda", "Etienne de Mestre", "1992", "John Young", "Brazil", "January 15, 2007", "Bay of Montevideo", "2012", "Australia", "the only way to go forward is to just keep living her life", "December 25", "Georgia Groome", "Games", "22 November 1970", "the external genitalia", "X&Y ( 2005 )", "a contemporary drama in a rural setting", "The border between the Cocos Plate and North American Plate", "management team", "between the stomach and the large intestine", "Director of National Intelligence", "Duisburg", "in an explosion", "House of Representatives", "October 1941", "111", "Paracelsus", "Mohammad Reza Pahlavi", "Zedekiah", "Peggy Lipton", "Afghanistan", "775 rooms", "Carol Worthington", "Greek mythology", "Matt Monro", "1890s", "in ancient Mesopotamia", "Edinburgh City F.C.", "T.S. Eliot", "Tartar", "its eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music.", "Tel Aviv University", "Kentucky", "Monday night", "the North Korean regime intends to fire a missile toward Hawaii", "Missouri", "hydrogen", "Mount Kilimanjaro", "the Alpide belt", "February 26, 1948"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4909448764917515}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.9090909090909091, 0.5, 1.0, 0.0, 0.0, 0.26666666666666666, 1.0, 1.0, 0.7837837837837839, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.20000000000000004, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.4, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-7589", "mrqa_triviaqa-validation-5576", "mrqa_triviaqa-validation-4099", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-4624", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3300", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-16690", "mrqa_searchqa-validation-8548"], "SR": 0.390625, "CSR": 0.4722222222222222, "EFR": 0.9230769230769231, "Overall": 0.7120285790598291}, {"timecode": 18, "before_eval_results": {"predictions": ["somatic cell nuclear transfer ( SCNT )", "the toe", "Tara", "the Munchkin maiden", "a federal republic", "3 lines of reflection", "Kyla Coleman", "1939", "Millennium Tower", "1876", "summer", "The Fellowship of the Ring ( 2001 ), The Two Towers ( 2002 ) and The Return of the King ( 2003 )", "September 27, 2017", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "Zeebo", "Beyonc\u00e9", "Warren Hastings", "April 26, 2005", "12 November 2010", "Babe Ruth", "the season seven finale", "Italy", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates", "to collect menstrual flow", "1980s", "cartilage", "Spektor", "East River", "a federal republic", "Bosnia and Herzegovina, Croatia, Macedonia, Montenegro, Serbia and Slovenia", "the president", "Los Angeles Dodgers", "cells", "As of October 2017, 54 episodes, which are referred to as `` Chapters '', have been released", "2010", "961", "Hyderabad", "the early 19th century", "September 9, 2010", "30 October 1918", "2018", "in the napkin ), napkin, and flatware ( knives and spoons to the right of the central plate, and forks to the left )", "the spoiled, bedridden daughter of wealthy businessman James Cotterell ( Ed Begley )", "In 1970, Congress took their anti-smoking initiative one step further and passed the Public Health Cigarette Smoking Act, banning the advertising of cigarettes on television and radio starting on January 2, 1971", "when each of the variables is a perfect monotone function of the other", "Alaska, Arizona, California, Colorado, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington and Wyoming", "a stepping stone in the development of the modern state system", "bicameral", "Lewis Carroll", "2018", "the government - owned Panama Canal Authority", "Microsoft", "Charlie Chaplin", "Michigan", "Wonder Woman", "Chancellor of Austria", "Schutzstaffel", "Joe Pantoliano", "Opryland", "a German citizen", "James Watt", "(Casey) Stengel", "sake", "B\u00e9la Bart\u00f3k"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5285375931435714}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [0.888888888888889, 0.0, 0.33333333333333337, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.8, 0.8571428571428571, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.6666666666666666, 0.08695652173913042, 1.0, 0.3846153846153846, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-3884", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-3320", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-150", "mrqa_triviaqa-validation-5761"], "SR": 0.40625, "CSR": 0.46875, "retrieved_ids": ["mrqa_squad-train-83863", "mrqa_squad-train-71081", "mrqa_squad-train-11052", "mrqa_squad-train-86228", "mrqa_squad-train-62722", "mrqa_squad-train-63171", "mrqa_squad-train-50811", "mrqa_squad-train-35102", "mrqa_squad-train-52469", "mrqa_squad-train-9301", "mrqa_squad-train-72692", "mrqa_squad-train-36353", "mrqa_squad-train-10511", "mrqa_squad-train-56694", "mrqa_squad-train-5478", "mrqa_squad-train-27730", "mrqa_newsqa-validation-3219", "mrqa_hotpotqa-validation-2492", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-14646", "mrqa_searchqa-validation-8548", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-4367", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-4501", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-7858", "mrqa_newsqa-validation-2719", "mrqa_searchqa-validation-1498"], "EFR": 0.9210526315789473, "Overall": 0.7109292763157895}, {"timecode": 19, "before_eval_results": {"predictions": ["January 15, 2007", "up to 100,000", "Nancy Jean Cartwright", "60 by West All - Stars", "the Chinese Exclusion Act in 1882", "birth", "July 25, 2017", "786 -- 802", "7", "A simple majority", "United Nations Peacekeeping Operations", "signaling neurotrophins", "the optic chiasma", "in each state's DMV, which is required to drive", "Confederate victory", "Yosemite National Park", "the inverted - drop - shaped icon that marks locations in Google Maps", "Rock Island, Illinois", "Quantitative psychological research", "March 2016", "Dan Bern and Mike Viola", "pneumonoultramicroscopicsilicovolcanoconiosis", "Laura Jane Haddock", "an advantage without deviating from basic strategy", "December 1, 2009", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013", "1957", "Mirzapur", "A diastema", "January 2018", "growing faster than the rate of economic growth", "London", "1917", "Anglican", "Malina Weissman", "23 % of GDP", "the trunk", "Kaley Christine Cuoco", "1986", "Prafulla Chandra Ghosh", "Zhu Yuanzhang", "Chernobyl Nuclear Power Plant", "Sedimentary rock", "Woodrow Wilson", "white blood cell", "Johannes Gutenberg", "Humphrey Bogart, Ingrid Bergman, and Paul Henreid", "Muhammad", "King Dasharatha", "the chryselephantine statue of Athena Parthenos", "2004", "karkaroff", "Sparks", "George Eliot", "Coronation Street", "Tom Jones", "David Kossoff", "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown", "13", "a home in an upscale San Fernando Valley neighborhood,", "Lumiere", "Cars Toys and Goodies", "a conjunction Junction", "Thabo Mbeki"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6415054563492064}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.25, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-4930", "mrqa_triviaqa-validation-436", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-361", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-4349"], "SR": 0.546875, "CSR": 0.47265625, "EFR": 0.9655172413793104, "Overall": 0.7206034482758621}, {"timecode": 20, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1539", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2098", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2292", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-617", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-949", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-22", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2703", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-4966", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-5026", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-5634", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6292", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-730", "mrqa_naturalquestions-validation-7374", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-8147", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-846", "mrqa_naturalquestions-validation-858", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9297", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-171", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4140", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-674", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-887", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10600", "mrqa_searchqa-validation-10811", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-11752", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-12812", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13101", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-1327", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14732", "mrqa_searchqa-validation-14765", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2071", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-3699", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3759", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-4187", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-46", "mrqa_searchqa-validation-4823", "mrqa_searchqa-validation-493", "mrqa_searchqa-validation-5197", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7198", "mrqa_searchqa-validation-7306", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-7631", "mrqa_searchqa-validation-8015", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-8642", "mrqa_searchqa-validation-8696", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9447", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-9842", "mrqa_searchqa-validation-996", "mrqa_squad-validation-2240", "mrqa_squad-validation-2550", "mrqa_squad-validation-2832", "mrqa_squad-validation-3", "mrqa_squad-validation-30", "mrqa_squad-validation-317", "mrqa_squad-validation-3781", "mrqa_squad-validation-3913", "mrqa_squad-validation-4059", "mrqa_squad-validation-4484", "mrqa_squad-validation-4752", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5174", "mrqa_squad-validation-5315", "mrqa_squad-validation-6792", "mrqa_squad-validation-7006", "mrqa_squad-validation-7257", "mrqa_squad-validation-7333", "mrqa_squad-validation-7903", "mrqa_squad-validation-7917", "mrqa_squad-validation-8157", "mrqa_squad-validation-8187", "mrqa_squad-validation-8224", "mrqa_squad-validation-8733", "mrqa_squad-validation-8846", "mrqa_squad-validation-9149", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1570", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1640", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-211", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2164", "mrqa_triviaqa-validation-2192", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3355", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-3884", "mrqa_triviaqa-validation-3900", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4739", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-5242", "mrqa_triviaqa-validation-5245", "mrqa_triviaqa-validation-5303", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5490", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6248", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6386", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6390", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-6902", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-953"], "OKR": 0.83984375, "KG": 0.48125, "before_eval_results": {"predictions": ["Haggith", "Queen", "Joe Jackson", "\"Goodbye My lover\"", "Lady Chatterley's", "a shark", "king Claudius", "Queen Anne", "Louis C. Tiffany", "Dwight D. Eisenhower", "caramels", "a German valley", "Harvard", "Max Planck", "The Grand Ole Opry", "The Walt Disney Company", "electors", "Poseidon", "a blood clot in a brain blood vessel", "men and women", "The Snickers Candy Bar", "a fish", "War and Peace", "John Grisham", "The Abstract Expressionists of the 1940's and '50's", "New Years Day", "inka", "USA", "the manatee", "Alfred Hitchcock", "Daisy", "Catherine the Great", "A Midsummer Night's Dream Police", "the Yalta Conference", "The Novels of Jane Austen", "24", "Maravich", "Christopher Wren", "insulin", "Isaac Asimov", "George Mason", "a miracle chemical of some sort", "the Nile Perch", "a lady-led indie rock band from the East Coast of Canada", "Jason Voorhees", "a belief can only be discovered by assessing its consequences for action", "ovulation", "Hugh Laurie", "Helen of Troy", "The Bidwell-Bartleson", "The plentiful praises and flatteries", "In Time", "Jackie Robinson", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak )", "soybeans", "Morgan Spurlock", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions", "1986", "\"50 best cities to live in.\"", "Melbourne Storm", "\"Three Little Beers,\"", "an Internet broadband deal with a Chinese firm", "her decades-long portrayal of Alice Horton", "Sammi Smith"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5383814102564102}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.8, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12958", "mrqa_searchqa-validation-939", "mrqa_searchqa-validation-16708", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-7413", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-3576", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-9046", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-11357", "mrqa_searchqa-validation-6300", "mrqa_searchqa-validation-16741", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-12403", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-10522", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-15081", "mrqa_searchqa-validation-14278", "mrqa_searchqa-validation-7001", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-10762", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-7707", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-2112", "mrqa_naturalquestions-validation-7651"], "SR": 0.390625, "CSR": 0.46875, "EFR": 1.0, "Overall": 0.706796875}, {"timecode": 21, "before_eval_results": {"predictions": ["\"Shifted Balance of Power\" Jeffords", "Germany", "the Mississippi River", "A Clockwork Orange", "jockey", "a waltz", "Giuseppe Garibaldi", "South Korea", "the Amazon", "a pearl", "John Donne", "laryngitis", "Synchronicity", "Emma Stone", "The Two Gentlemen", "Fairbanks", "Adam", "Nancy Sinatra", "(AGTR 2)", "Tupelo", "the French Legion of Honour", "a tumbler", "Aesop", "a nymph", "Planet of the Apes", "pajamas", "Kermit", "Wales", "polio", "Sweden", "Edgar Rice Burroughs", "Madagascar", "albacore", "a cleaver", "white", "The Big Easy", "Wobblier", "a magnetic compass", "Slovakia", "peanuts", "Orton", "a sharpener", "Corcoran", "Vega$", "Midas", "at the top", "Lake Coeur d'Alene", "Council of Better Business Bureaus", "a possum", "a short circuit", "totalitarianism", "Brobee", "early 1988", "tomato pur\u00e9e generally lacks the additives common to a complete tomato sauce and does not have the thickness", "Sam Cooke", "a \"major science finding from the agency's ongoing exploration of Mars.\"", "Ankh-Morpork", "a suburb", "Marilyn Martin", "\"The Process\"", "Picasso's muse and mistress, Marie-Therese Walter", "Kevin Kuranyi", "Hapag-Lloyd Cruises", "Belleville, Illinois"], "metric_results": {"EM": 0.578125, "QA-F1": 0.634276593701997}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false], "QA-F1": [0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9032258064516129, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-5321", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-8461", "mrqa_searchqa-validation-8527", "mrqa_searchqa-validation-9649", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-14957", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-4549", "mrqa_searchqa-validation-16591", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-11511", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-2945", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-6048", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-4021", "mrqa_newsqa-validation-637", "mrqa_hotpotqa-validation-2637"], "SR": 0.578125, "CSR": 0.47372159090909094, "retrieved_ids": ["mrqa_squad-train-13704", "mrqa_squad-train-76753", "mrqa_squad-train-34629", "mrqa_squad-train-18831", "mrqa_squad-train-11366", "mrqa_squad-train-40767", "mrqa_squad-train-45538", "mrqa_squad-train-43067", "mrqa_squad-train-11120", "mrqa_squad-train-51938", "mrqa_squad-train-1418", "mrqa_squad-train-82242", "mrqa_squad-train-31804", "mrqa_squad-train-9258", "mrqa_squad-train-55563", "mrqa_squad-train-46067", "mrqa_searchqa-validation-6896", "mrqa_triviaqa-validation-2801", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-11990", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-9235", "mrqa_hotpotqa-validation-2526", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-2420", "mrqa_newsqa-validation-1512", "mrqa_naturalquestions-validation-9753", "mrqa_hotpotqa-validation-3872"], "EFR": 1.0, "Overall": 0.7077911931818182}, {"timecode": 22, "before_eval_results": {"predictions": ["Marie Van Brittan Brown", "saecula saeculorum in Ephesians 3 : 21", "around 1872", "George Halas", "the focal point", "1997", "Identification of alternative plans / policies", "`` Heroes and Villains ''", "1963", "Kaley Christine Cuoco", "a divergent tectonic plate boundary", "Spanish missionaries", "pit road", "Toto", "The Lightning thief", "1977", "the coffee shop Monk's", "Johnny Logan", "Carroll O'Connor", "Missouri", "span", "Christianity", "Billy Idol", "in a thousand years", "sexual bandmates ( Tavish Crowe )", "Veronica Lodge", "247.3 million", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Emmanuelle Chriqui", "Telltale Games", "Lana Del Rey", "over", "1992", "Lauren Tom", "the mid-1980s", "1871 A.D. Pt. Buddhiballav Pant", "Paradise", "Partial", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen ) is a numeric scale used to specify the acidity or basicity of an aqueous solution", "the ACU", "Charles Sherrington", "October 30, 2017", "US $24,250", "Gunpei Yokoi", "the New York Yankees", "the inner core", "medieval", "4th century", "Lake Wales, Florida", "before the first year begins", "Thorleif Haug", "Billie (Piper)", "cr\u00ef\u00bf\u00bdme de cassis", "ghee", "Polish-Jewish", "Cleopatra", "EQT Plaza in Pittsburgh, Pennsylvania", "269,000", "January 24, 2006", "March 4", "pew", "the Bean Sidhe", "our country", "ice cream"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6538978542124996}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.631578947368421, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.9189189189189189, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.5, 0.5, 0.2608695652173913, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-7477", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-2509", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4210", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-9572"], "SR": 0.515625, "CSR": 0.4755434782608695, "EFR": 0.9032258064516129, "Overall": 0.6888007319424965}, {"timecode": 23, "before_eval_results": {"predictions": ["Boyd Gaming", "local South Australian and Australian produced content", "1919", "Ted Nugent", "1998", "Everbank Field", "Alexandre Dimitri Song Billong", "Chrysler", "Carlos Santana", "Jack Posobiec", "7 October 1978", "a basilica", "westberforce, Ohio, United States", "1988", "2017", "Enkare Nairobi", "the Durban International Convention Centre (ICC Arena)", "between 11 or 13 and 18", "\"Vera Cruz\"", "Elijah Wood", "a British astronomer and composer of German and Czech-Jewish origin, and brother of fellow astronomer Caroline Herschel, with whom he worked", "C. J. Cherryh", "Natalie Chandler", "The Golden Egg", "Spanish", "Sir Charles Benedict Ainslie", "Dutch", "a royal concubine against her will", "Bank of China Tower", "the 28th season", "Barcelona", "Albert Park", "sim", "Pablo Escobar", "1941", "Hazel Keech", "DJ Scotch Egg", "June 24, 1935", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Chad", "Derry City F.C.", "Paris", "the \"Anglo-Saxon Chronicle\"", "Saturday", "boar-crested helm", "Bury St Edmunds, Suffolk, England", "The Lancia Rally", "Michael Jordan", "Harold Lipshitz", "nine", "C. J. Cherryh", "Virginia Beach is an independent city located on the southeastern coast of the Commonwealth of Virginia in the United States", "the defendant's negligence was gross, that is, it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "IIII", "Downton Abbey", "Westminster Abbey", "Eminem", "28", "Saturday", "Rod Blagojevich", "aroucho", "a centiliter", "a car", "arranged for the bodyguard to pick up $12,000 in cash from a bank to buy the guns"], "metric_results": {"EM": 0.375, "QA-F1": 0.47769206240188383}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.08333333333333334, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.6666666666666666, 0.5, 0.33333333333333337, 0.7692307692307693, 0.9387755102040817, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38095238095238093]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-1699", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1214", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-1226", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2955", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1195", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3629", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-5409", "mrqa_newsqa-validation-1242"], "SR": 0.375, "CSR": 0.47135416666666663, "EFR": 0.975, "Overall": 0.7023177083333334}, {"timecode": 24, "before_eval_results": {"predictions": ["the fourth season", "Morgan Freeman", "The Bangladesh -- India border", "Richard Carpenter", "Colman", "A-Lot of Mess", "A Turtle's Tale : Sammy's Adventures", "Nathan Hale", "Emma Watson", "The Chesapeake", "1983", "Yugoslavia", "George Harrison", "New England", "from 1922 to 1991", "total cost", "the nasal septum", "The player named on 75 % or more of all ballots", "the show's producers, questioning whether it was necessary to go further with the transformations.", "Brandon Scott", "Mitch Murray", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "Paul", "Erica Rivera", "Labour Party", "a Norwegian town", "Thomas Edison", "1959", "The crossing of Highway 68 ( Holman Highway / Sunset Drive )", "a pole", "Hendersonville, North Carolina", "aorta", "Atlanta, Georgia", "Sonu Nigam", "a flood defense system", "Donna Reed", "Total Drama World Tour", "Staci Keanan", "18th century", "Debbie Gibson", "Jodie Foster", "U.S. Electoral College", "The reservation nourishes the historically disadvantaged castes and tribes", "Richard Stallman", "Pasek and Paul", "qualitative", "1603", "September 27, 2017", "Nepal", "BC Jean and Toby Gad", "six", "Sir Robert Walpole", "Lee Harvey Oswald", "\"Mauritania\"", "guitar feedback", "John Robert Cocker", "Isla de Xativa", "January 24, 2006", "March 8", "terminal brain cancer.", "the Canal", "mezcal", "Solidarity", "heads"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6532919898728722}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.19999999999999998, 0.0, 0.5, 1.0, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-6592", "mrqa_triviaqa-validation-6158", "mrqa_hotpotqa-validation-4926", "mrqa_searchqa-validation-1583", "mrqa_searchqa-validation-11851", "mrqa_triviaqa-validation-4719"], "SR": 0.5625, "CSR": 0.475, "retrieved_ids": ["mrqa_squad-train-28683", "mrqa_squad-train-7382", "mrqa_squad-train-71813", "mrqa_squad-train-77357", "mrqa_squad-train-28402", "mrqa_squad-train-4160", "mrqa_squad-train-68544", "mrqa_squad-train-31443", "mrqa_squad-train-18787", "mrqa_squad-train-72610", "mrqa_squad-train-33192", "mrqa_squad-train-30530", "mrqa_squad-train-45916", "mrqa_squad-train-17210", "mrqa_squad-train-70547", "mrqa_squad-train-15104", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2006", "mrqa_searchqa-validation-14646", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-1016", "mrqa_naturalquestions-validation-7047", "mrqa_searchqa-validation-8645", "mrqa_searchqa-validation-14278", "mrqa_naturalquestions-validation-886", "mrqa_searchqa-validation-2805", "mrqa_triviaqa-validation-7329", "mrqa_newsqa-validation-2052", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-4549", "mrqa_searchqa-validation-4349"], "EFR": 0.8928571428571429, "Overall": 0.6866183035714286}, {"timecode": 25, "before_eval_results": {"predictions": ["one", "\"Rudolpho Valentino\"", "Crete", "Churchill Downs", "\"Un Giorno di Regno\"", "Robert Hooke", "1963", "\"Corse of Scotland\"", "sea shells", "The Daily Herald", "Eddie Shoestring", "A medium shot", "curb-roof", "armada", "Moldova", "Florida", "the showplace of the Nation", "Switzerland", "The Hague Conventions", "Frederick Delius (1862-1934)", "Utrecht", "Washington", "zesty orange peel", "Rijksmunt", "Bunratty", "American rock 'n' roll pioneer", "piano", "The Merchant of Venice", "\"Rivers of Blood\"", "Temple of Artemis", "Lindisfarne", "hollandaise sauce", "The Undertones", "George Santayana", "Philistine", "Doctor Who", "Yocheved", "Andrew Jackson", "Italy", "bhojpuri", "nose", "Sandi Toksvig", "Guinea", "Los Angeles", "carbon", "Margaret Thatcher (Meryl Streep)", "Reform Club", "The Three Spires", "pyrotechnics", "Guy the gorilla", "polyhedron", "741 weeks", "Ed Sheeran", "over 800", "over 281", "James David Graham Niven", "23 July 1989", "Ford", "Roger Federer", "those traveling near the Somali coast", "Diners' Club Card", "Lost in Space", "Calcium", "punish participants"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5845486111111111}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-2986", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-4868", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-6100", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-7284", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-3881", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-3174", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-5049", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-5405", "mrqa_newsqa-validation-2457", "mrqa_searchqa-validation-16751", "mrqa_newsqa-validation-1619"], "SR": 0.515625, "CSR": 0.4765625, "EFR": 1.0, "Overall": 0.708359375}, {"timecode": 26, "before_eval_results": {"predictions": ["2,579", "Bemis Heights", "H CO", "in Paradise, Nevada", "The Continental Congress", "2017 season", "Charles Darwin", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "Dr. Rajendra Prasad", "at the 1964 Republican National Convention in San Francisco, California", "Steve Nash", "Annette", "Total Drama World Tour", "more than 80", "Brenda", "Tony Orlando and Dawn", "Marie Fredriksson", "1830", "687 keV", "45 %", "786 -- 802", "milling process", "Andrew Gold", "Henry Purcell", "either two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "General George Washington", "2017", "Have I Told You Lately ''", "in Middlesex County, Province of Massachusetts Bay", "April 4, 2017", "either the Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee,", "The Ministry of Corporate Affairs", "The House of Representatives", "Toledo", "Abid Ali Neemuchwala", "seven", "2", "digitization of social systems", "1975", "James Brown", "Kristy Swanson", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "around 10 : 30am", "1966", "Glen W. Dickson", "Rent", "Manhattan", "various newspaper reporters, including Sylvia F. Porter", "John J. Flanagan", "Kimberlin Brown", "1955", "benjamin Disraeli", "hierarchical web portal", "holly", "villanelle", "Bayern Munich", "Atlanta, Georgia", "Barbara Streisand", "Lindsey Vonn", "Jennifer Arnold and husband Bill Klein,", "cracklings", "Tom Sennett", "Philippines", "a basilica"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6775787260984629}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.3076923076923077, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.21052631578947367, 1.0, 0.5, 1.0, 0.5384615384615384, 0.6666666666666666, 0.8571428571428572, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-8582", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-1212", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-1384", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-16051"], "SR": 0.5625, "CSR": 0.47974537037037035, "EFR": 0.9285714285714286, "Overall": 0.6947102347883598}, {"timecode": 27, "before_eval_results": {"predictions": ["the Great Lakessome", "scotland", "the Ronald Reagan Presidential Library", "scotland", "Treasure Island", "Gloria Steinem", "Luxembourg", "curly", "The Troggs", "the Boston Tea Party", "Pandora", "volcanic cones", "Louisiana", "East of Eden", "the kitchen sink", "Hadrurus arizonensis", "Charles W. Fairbanks", "dressage", "lipoprotein", "The Austere Academy", "a streetcar", "Sarah, Plain and Tall", "Misery", "The Rolling Stones", "malaria", "Hamlet", "a Race-based Dessert", "a mitre", "caribou", "Jabberwocky", "Making the Band", "Virgin", "The Evergreen State", "Federalist Papers: Primary Documents of American History", "the Lone Ranger", "The Boeing Everett Factory", "The Ladies' Singles Trophy", "the Luna Awards", "nautilus", "Richelieu", "Gwalior", "Rapa Nui National Park", "NFL Head Coach", "Carrie Underwood", "the Aegean Sea", "eustachian tube", "Florida State", "hiccups", "Epidural Steroid Injections", "Brigham Young", "Department of Transportation", "President Theodore Roosevelt", "just after the Super Bowl", "16,801", "cami de Repos", "\"The Naked and The Dead\"", "Basel, Switzerland", "the 2007 Formula One season", "San Francisco, California with offices in New York City and Atlanta", "The Hard Way", "The Glasgow, Scotland concert", "gang rape of a 15-year-old girl", "Toffelmakaren", "protons"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4751736111111111}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.42857142857142855, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-8261", "mrqa_searchqa-validation-1867", "mrqa_searchqa-validation-2008", "mrqa_searchqa-validation-11353", "mrqa_searchqa-validation-13722", "mrqa_searchqa-validation-2490", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3723", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-13064", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-7995", "mrqa_searchqa-validation-6022", "mrqa_searchqa-validation-6617", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-14352", "mrqa_searchqa-validation-16897", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-4526", "mrqa_searchqa-validation-11202", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-692", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-3547", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-1918", "mrqa_triviaqa-validation-554"], "SR": 0.390625, "CSR": 0.4765625, "retrieved_ids": ["mrqa_squad-train-64223", "mrqa_squad-train-2383", "mrqa_squad-train-35473", "mrqa_squad-train-40217", "mrqa_squad-train-62989", "mrqa_squad-train-61676", "mrqa_squad-train-47615", "mrqa_squad-train-44946", "mrqa_squad-train-51369", "mrqa_squad-train-28103", "mrqa_squad-train-43765", "mrqa_squad-train-77842", "mrqa_squad-train-15849", "mrqa_squad-train-36601", "mrqa_squad-train-43787", "mrqa_squad-train-43250", "mrqa_newsqa-validation-1508", "mrqa_triviaqa-validation-2321", "mrqa_newsqa-validation-3789", "mrqa_triviaqa-validation-1651", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9921", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-15525", "mrqa_newsqa-validation-2045", "mrqa_triviaqa-validation-4128", "mrqa_searchqa-validation-1583", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-674", "mrqa_naturalquestions-validation-6237", "mrqa_searchqa-validation-4348", "mrqa_naturalquestions-validation-10057"], "EFR": 1.0, "Overall": 0.708359375}, {"timecode": 28, "before_eval_results": {"predictions": ["CAIR", "Venus flytrap", "humbert", "Fl orence's", "1876", "Ty Hardin", "gizzard", "denier", "March", "Republic of Biafra", "gasoline", "Colombia", "Lewis Carroll", "a robe", "Jason Voorhees", "the moon", "the Crystal Palace", "Scotland", "Milton Keynes", "leadbetter", "a lie detector", "Italy", "blue ivy", "vincent donofrio", "Dundee", "The Colossus of Rhodes", "60 or more", "John, Jr.", "Hyperbole", "and", "Manchester", "Delilah", "Bridge", "the Black Sea", "the Reform Club", "U.K.", "West Point", "the Heavyweight title", "phosphorus", "work", "an\u00famero", "Ross Bagdasarian", "switzerland", "Mercedes-Benz", "Alexandria", "Wordsworth", "a bronze medal", "Joshua Tree National Park", "Trainspotting", "mammals", "the Black Death", "1967", "the Atlantic Ocean", "Dan Stevens", "The Royal Albert Hall", "Philip K. Dick", "2006", "menstruation", "the American Civil Liberties Union", "Kerstin", "Fat Man", "Thurgood Marshall", "Gainsborough", "four"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6208333333333333}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-510", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-978", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2123", "mrqa_triviaqa-validation-949", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5893", "mrqa_triviaqa-validation-901", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-5184", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-6976", "mrqa_newsqa-validation-359", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-9687"], "SR": 0.59375, "CSR": 0.4806034482758621, "EFR": 0.9230769230769231, "Overall": 0.6937829492705572}, {"timecode": 29, "before_eval_results": {"predictions": ["Florida Everglades", "lula da Silva", "Mohamed Mohamud Qeyre", "UNICEF", "in Hong Kong's Victoria Harbor", "ethnic", "used car", "70,000", "Asashoryu", "President Bush", "last week", "Rawalpindi", "22", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan", "Ashley \"A.J.\" Jewell", "Mikkel Kessler", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "at about 6:30 p.m.", "acute stress disorder", "in the Gulf of Aden", "Manuel Mejia Munera", "Jewish", "Africa", "Omar Bongo", "Swedish Transport Agency", "Rwanda declared a cease-fire", "job", "Dr. Jennifer Arnold and husband Bill Klein,", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "gasoline", "president Idriss Deby", "At least 14", "Anjuna beach", "Larry Ellison", "murder", "\"they take a piece of gauze, and they wipe off all the dead skin,", "Barack Obama:", "garth Brooks", "gang rape", "\"Empire of the Sun,\"", "Kgalema Motlanthe", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "Genocide Prevention Task Force", "Stratfor", "Ricardo Valles de la Rosa", "colonel", "\" Invadersers of the Lost Ark.\"", "bipartisan rhetoric", "Two", "one", "kerstin Fritzl", "A footling breech", "During Hanna's recovery masquerade celebration", "cat", "vincent van Gogh", "three", "\"In God We Trust\"", "Katherine Harris", "Sandusky", "Lambic", "manhattan", "give love a bad name", "ivory", "Psy"], "metric_results": {"EM": 0.453125, "QA-F1": 0.592145920794087}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.7659574468085107, 1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 0.33333333333333337, 0.10526315789473685, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.7272727272727272, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-2105", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-925", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-2946", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-5631", "mrqa_triviaqa-validation-524", "mrqa_hotpotqa-validation-3991", "mrqa_searchqa-validation-16692"], "SR": 0.453125, "CSR": 0.47968750000000004, "EFR": 0.9714285714285714, "Overall": 0.7032700892857143}, {"timecode": 30, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1631", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3498", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-949", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-115", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2138", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-22", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5242", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7074", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-730", "mrqa_naturalquestions-validation-7374", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7802", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9297", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-674", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-925", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10522", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10805", "mrqa_searchqa-validation-10834", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-1099", "mrqa_searchqa-validation-1100", "mrqa_searchqa-validation-11189", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11476", "mrqa_searchqa-validation-11486", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12301", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13535", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-13666", "mrqa_searchqa-validation-13722", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14499", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14755", "mrqa_searchqa-validation-14957", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15051", "mrqa_searchqa-validation-1509", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-15260", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16690", "mrqa_searchqa-validation-16692", "mrqa_searchqa-validation-1671", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-1875", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2274", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2605", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4349", "mrqa_searchqa-validation-493", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5218", "mrqa_searchqa-validation-5248", "mrqa_searchqa-validation-5409", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6022", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-6300", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7001", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8400", "mrqa_searchqa-validation-8483", "mrqa_searchqa-validation-8527", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8707", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-8794", "mrqa_searchqa-validation-8974", "mrqa_searchqa-validation-9046", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-996", "mrqa_searchqa-validation-9992", "mrqa_squad-validation-2550", "mrqa_squad-validation-2832", "mrqa_squad-validation-317", "mrqa_squad-validation-3781", "mrqa_squad-validation-4059", "mrqa_squad-validation-4752", "mrqa_squad-validation-4986", "mrqa_squad-validation-5065", "mrqa_squad-validation-5174", "mrqa_squad-validation-6537", "mrqa_squad-validation-679", "mrqa_squad-validation-6792", "mrqa_squad-validation-7257", "mrqa_squad-validation-7903", "mrqa_squad-validation-8157", "mrqa_squad-validation-8187", "mrqa_squad-validation-8224", "mrqa_squad-validation-8733", "mrqa_squad-validation-8954", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-1277", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1640", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1685", "mrqa_triviaqa-validation-1798", "mrqa_triviaqa-validation-2123", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-2863", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-3068", "mrqa_triviaqa-validation-3138", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-3206", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3355", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3546", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-4241", "mrqa_triviaqa-validation-4253", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-436", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-4579", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-463", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4684", "mrqa_triviaqa-validation-4719", "mrqa_triviaqa-validation-4739", "mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-5242", "mrqa_triviaqa-validation-5245", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5712", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5765", "mrqa_triviaqa-validation-5770", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6390", "mrqa_triviaqa-validation-642", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-6902", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7061", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-953"], "OKR": 0.796875, "KG": 0.4390625, "before_eval_results": {"predictions": ["No. 4", "The workers should be dealt (with) with compassion and should not be pushed so hard that they resort to whatever that had happened in Nodia\"", "The Casalesi Camorra clan", "\"Dancing With the Stars.\"", "paid tribute to pop legend Michael Jackson,", "pregnant soldier", "the World Food Program", "an angel,", "two Metro transit trains that crashed the day before, killing nine", "Stop the War Coalition", "boyhood experience in a World War II internment camp", "Trevor Rees-Jones,", "Naples", "keyboardist and original member of The Charlie Daniels Band,", "Iran's parliament speaker", "14", "Gary Player,", "the Dutch patent office", "The man ran out of bullets and blew himself up.", "Sodra nongovernmental organization,", "246", "3,000 kilometers (1,900 miles)", "five", "Chile", "\"scared I won't be able to go home.", "outside influences in next month's run-off election,", "1981", "Susan Boyle", "forgery and flying without a valid license", "the way their business books were being handled.", "jazz", "Janet and La Toya", "100 percent", "Ashley \"A.J.\" Jewell", "well over 1,000 pounds", "\"iKini\"", "Little Rock Central High School", "help me stay on track and get me through prison,\"", "a birdie four at the last hole", "12-1", "Dean Martin, Katharine Hepburn and Spencer Tracy", "South Korean President Lee Myung-bak,", "the defendants, who were charged with stealing the personal credit information of thousands of American and European consumers, are allegedly members of five organized crime rings with ties to Europe, Asia, Africa and the Middle East.", "four", "the legislation will foster racial profiling,", "Roger Federer", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials,", "Six members of Zoe's Ark were arrested last week as they tried to put the children on a plane to France,", "\"wider relationship\" between the two countries", "the amount of fuel used at high altitude.", "the procedures should be changed.", "Hold On", "Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida", "special economic zones", "Benny Hill", "riyadh", "benedictus", "River Clyde", "on the north bank of the North Esk,", "Debbie Harry", "the Indians", "Sam Houston", "4-line verse", "an open field northwest of Bemis Heights belonging to Loyalist John Freeman"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5722952476445125}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.8, 1.0, 0.14285714285714285, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.5, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.888888888888889, 0.0, 1.0, 1.0, 0.22222222222222224, 0.24242424242424243, 1.0, 0.0, 1.0, 0.0606060606060606, 0.0, 0.0, 0.14285714285714285, 0.4, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3188", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3706", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-921", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-392", "mrqa_naturalquestions-validation-6670", "mrqa_triviaqa-validation-2324", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-1592"], "SR": 0.453125, "CSR": 0.47883064516129037, "retrieved_ids": ["mrqa_squad-train-13096", "mrqa_squad-train-66104", "mrqa_squad-train-38336", "mrqa_squad-train-19648", "mrqa_squad-train-81844", "mrqa_squad-train-75001", "mrqa_squad-train-68818", "mrqa_squad-train-36627", "mrqa_squad-train-8612", "mrqa_squad-train-80707", "mrqa_squad-train-8200", "mrqa_squad-train-22376", "mrqa_squad-train-24930", "mrqa_squad-train-29616", "mrqa_squad-train-71045", "mrqa_squad-train-50621", "mrqa_naturalquestions-validation-5638", "mrqa_searchqa-validation-7706", "mrqa_triviaqa-validation-6130", "mrqa_naturalquestions-validation-9220", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-11259", "mrqa_naturalquestions-validation-8478", "mrqa_triviaqa-validation-436", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-5934", "mrqa_searchqa-validation-3323", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-556", "mrqa_naturalquestions-validation-5912", "mrqa_hotpotqa-validation-1987", "mrqa_triviaqa-validation-4227"], "EFR": 0.9428571428571428, "Overall": 0.6795719326036866}, {"timecode": 31, "before_eval_results": {"predictions": ["Lonnie", "Ronnie White,", "\"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "District of Columbia National Guard", "a folding table", "Gavin de Becker", "Brian Smith", "over 1,000 pounds", "31 meters (102 feet)", "about 3,000 kilometers (1,900 miles)", "on China, Taiwan, Hong Kong and Mongolia,", "monarchy", "Gary Player,", "Three aid workers", "Kurdistan Freedom Falcons,", "more than 30", "underlining Turkey's European identity as a secular democracy.", "All three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "state's first lady,", "UNICEF", "Little Rock military recruiting center", "the Dalai Lama", "Susan Atkins,", "The entertainer, whose real name is Clifford Harris, was arrested Saturday just hours before he was scheduled to perform at the BET Hip Hop Awards.", "a lightning strike", "North Korea intends to launch a long-range missile in the near future,", "CEO of an engineering and construction company with a vast personal fortune of more than 30 billion won ($30.2 million) to the poor.", "'overcharged.'\"", "\"Five of us for the United States and two against us because they were stranded in Japan\" when the war came.", "she had 30 days to vacate.", "of murder in the beating death", "at least 25", "misdemeanor assault", "19-year-old", "Michoacan Family,\"", "roughly $5.5 billion to build.", "near the Somali coast", "Russia and China", "his comments", "Nearly eight in 10", "jobs up and down the auto supply chain: from dealers to assembly workers and parts markers.", "she's in love,", "Halloween is no exception.", "Philippines", "Italy", "refusal or inability to \"turn it off\"", "Dan Brown's", "mental health and recovery.", "Caster Semenya", "\"Dancing With the Stars.\"", "1940's", "in 1972", "Acid rain", "Dunedin, Port Chalmers and on the Otago Peninsula, Saint Bathans in Central Otago and at the Cape Campbell Lighthouse in Marlborough", "Pandora", "Paul C\u00e9zanne", "gregoralf", "three", "Mazda Capella", "Mountain West Conference", "American Revolution", "Arthropoda", "Paul Gauguin", "St. George"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5691773504273505}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 0.888888888888889, 0.26666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.09523809523809523, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 1.0, 0.07692307692307691, 1.0, 1.0, 0.4615384615384615, 0.0, 0.1, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.5714285714285715, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-3401", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2738", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-1304", "mrqa_triviaqa-validation-6547", "mrqa_hotpotqa-validation-4710", "mrqa_searchqa-validation-10219", "mrqa_searchqa-validation-5260", "mrqa_searchqa-validation-5038"], "SR": 0.4375, "CSR": 0.4775390625, "EFR": 1.0, "Overall": 0.6907421874999999}, {"timecode": 32, "before_eval_results": {"predictions": ["of parents", "a place for another non-European Union player in Frank Rijkaard's squad.", "in central Cairo,", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "the club's board has yet to make a decision of how it will proceed.", "1973's", "Kim Il Sung", "at \"CNN Heroes: An All-Star Tribute\" as a", "will not support the Stop Online Piracy Act,", "Franklin, Tennessee,", "\"Swingin' Down the Lane.\"", "four", "consumer confidence", "once on New Year's Day and once in June,", "United States, NATO member states, Russia and India", "for an independent homeland since 1983.", "is not a project for commercial gain.", "the insurgency,", "United States", "two-state solution", "Stanford University", "The Departed.", "Dennis Davern,", "breast cancer", "the U.N. Security Council resolution in 2006 banned North Korea from conducting ballistic missile activity.", "\"We have duty to keep cases under continuous review, and following expert evidence from a psychiatrist it was suggested no useful purpose would be served by Mr Thomas being detained and treated in a psychiatric hospital,\"", "More than 150,000", "finance", "of 23 politicians originally in the vote to support the opposition,", "the guerrillas were not targeting indigenous populations but took the action \"against people who independent of their race, religion, ethnicity, social condition etc. accepted money and put themselves at the service of the army in", "Washington State's", "11", "anaphylactic shock.", "June 6, 1944,", "Iran's nuclear program.", "\"tuatara\" is derived from a Maori word meaning \"spiny back.\"", "NATO's Membership Action Plan, or MAP,", "a certain carrier based in Texas.", "for strategy, plans and policy on the Army staff.", "Indian Army", "Obama", "supplies power to almost 9 million Americans,", "1913", "Turkey", "the 2009 Swamp Soccer World Championship held in Scotland", "Tuesday's display attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "Myanmar,", "motor bike accident.", "small latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "U.S. President-elect Barack Obama", "4, the highest ever position", "The stability, security, and predictability of British law and government", "Thomas Middleditch", "the Philippines", "Albert Finney", "Kosovo", "violin", "Cyclic Defrost", "Peter Seamus O'Toole", "Illinois", "Dostoevsky", "Blackbird", "General Hospital", "the final episode of the series"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5746209902230123}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [0.4, 0.15384615384615383, 0.8, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6153846153846153, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.5, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 0.4, 0.375, 1.0, 0.0, 0.4166666666666667, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-2655", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-372", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2167", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-2740", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-5856", "mrqa_searchqa-validation-4039"], "SR": 0.421875, "CSR": 0.4758522727272727, "EFR": 1.0, "Overall": 0.6904048295454545}, {"timecode": 33, "before_eval_results": {"predictions": ["Iran", "Garth Brooks", "off the coast of Dubai", "how it will proceed.", "a long-range missile", "a \"new chapter\" of improved governance", "fears a desperate country with a potential power vacuum that could lash out.", "Japan and Singapore,", "U.N. drug chief.", "133 people", "the Southern Baptist Convention,", "kept the details on both the timing and selection of the running mate under wraps.", "lightning strikes", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "August 4, 2000", "Wednesday.", "Marc Jacobs", "funded by a German company and affiliated with the group Bread for the World.", "Bangladesh", "Haleigh Cummings,", "calls \"Azzam the American,", "Sabina Guzzanti", "the Southeast,", "137", "demonstrators", "the insurgency,", "start a dialogue of peace", "Minerals Management Service Director Elizabeth Birnbaum", "heavy flannel or wool", "school,", "1913,", "Portuguese water dog", "U.N. General Assembly", "2011.", "Misty Cummings,", "\"Rin Tin Tin: The Life and the Legend\"", "Thailand", "minimalism", "September,", "Majid Movahedi,", "\"People have lost their homes, their jobs, their hope,\"", "public-television", "the oceans", "August 19, 2007.", "Haiti", "March 24,", "in his 60s,", "1616.", "the children of street cleaners and firefighters.", "\"The ties were handmade, by the way,\"", "about 5:20 p.m.", "~ 55 - 75", "The prequel film Revenge of the Rebel ( 2005 )", "Wales", "benjamin franklin,", "Liechtenstein", "New Democracy", "Broad Top Township, Bedford County", "Giacomo Puccini", "whaling and seal hunting", "Denmark", "Newfoundland", "Blue Hawaii", "Kirk Douglas"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5989474067599068}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.8, 0.1818181818181818, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3936", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3480", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-6881", "mrqa_triviaqa-validation-5738", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-1332", "mrqa_searchqa-validation-5234"], "SR": 0.546875, "CSR": 0.4779411764705882, "retrieved_ids": ["mrqa_squad-train-77227", "mrqa_squad-train-73153", "mrqa_squad-train-77837", "mrqa_squad-train-76297", "mrqa_squad-train-4866", "mrqa_squad-train-82524", "mrqa_squad-train-1999", "mrqa_squad-train-4542", "mrqa_squad-train-70743", "mrqa_squad-train-71949", "mrqa_squad-train-2632", "mrqa_squad-train-27728", "mrqa_squad-train-71782", "mrqa_squad-train-49136", "mrqa_squad-train-19045", "mrqa_squad-train-35851", "mrqa_triviaqa-validation-3174", "mrqa_naturalquestions-validation-2169", "mrqa_triviaqa-validation-3546", "mrqa_hotpotqa-validation-3376", "mrqa_naturalquestions-validation-8287", "mrqa_triviaqa-validation-3881", "mrqa_newsqa-validation-1660", "mrqa_naturalquestions-validation-3971", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-5576", "mrqa_naturalquestions-validation-2387", "mrqa_searchqa-validation-11476", "mrqa_naturalquestions-validation-321", "mrqa_triviaqa-validation-4244", "mrqa_naturalquestions-validation-6429", "mrqa_triviaqa-validation-4872"], "EFR": 0.9655172413793104, "Overall": 0.6839260585699797}, {"timecode": 34, "before_eval_results": {"predictions": ["$22 million", "Philippines", "comfort those in mourning,", "phone calls or by text messaging", "the punishment for the player", "the L'Aquila earthquake,", "a man", "two and a half hours.", "A Brazilian supreme court judge", "Wednesday", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential", "Fullerton, California,", "Sarah,", "one American diplomat to a \"prostitute\"", "Hillary Clinton", "\"They were nothing,\"", "the United States", "the Rockies", "the leader of a drug cartel", "Dennis Ray Gerwing", "state senators", "4.6 million", "the New York Philharmonic Orchestra in North Korea to Dharamsala,", "Haiti", "Austin Wuennenberg,", "10-day retreat,", "\"seems to be more often a reward car.", "$14.1 million", "Christmas", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Nineteen", "Ross Perot", "the Beatles", "Iran's", "President Paul Biya,", "a 12-year veteran of the Utah state police,", "after Michael Jackson's death", "revenge for Israel's air and ground assault on Gaza", "Six", "the first five Potter films", "Sunday's", "\"The Kirchners have been weakened by this latest economic crisis,\"", "Haiti", "80", "her home", "can play an important role in Afghanistan as a reliable NATO ally.", "Authorities in Fayetteville, North Carolina,", "\"We Found Love\"", "Marcell Jansen", "Gordon Brown's", "ending aggressive militarism", "1979 / 80", "between $10,000 and $30,000", "germany", "small passenger cars", "Leslieley Lawson (n\u00e9e Hornby;", "Oklahoma", "Aloha \u02bbOe", "Boyd Gaming", "a firmament", "Anna Karenina", "the Pacific Ocean", "Mississippi"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6075601408825093}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.4, 0.5, 0.10526315789473684, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.09523809523809525, 0.6666666666666666, 1.0, 0.8, 0.1818181818181818, 0.888888888888889, 1.0, 1.0, 0.5714285714285715, 0.0, 0.8, 0.32, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3779", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4768", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-3347", "mrqa_searchqa-validation-6092", "mrqa_searchqa-validation-14603"], "SR": 0.453125, "CSR": 0.4772321428571429, "EFR": 0.9714285714285714, "Overall": 0.6849665178571429}, {"timecode": 35, "before_eval_results": {"predictions": ["Partial weight - bearing", "in the season - five premiere episode `` Second Opinion ''", "Database - Protocol driver", "the governor of West Virginia", "Charles Lebrun", "cella", "John Bull", "Charlotte Thornton", "January 1923", "Noel Kahn", "7000301604928199000", "Eddie Murphy", "a religious covenant", "Thomas Lennon", "Robber Barons", "in people and animals", "10.5 %", "in 1993", "Mike Leeson and Peter Vale", "collect menstrual flow", "Leo Arnaud", "Speaker of the House of Representatives", "September 2017", "since the early 20th century", "Latin liberalia studia", "Charles Path\u00e9", "Mel Tillis", "Florida", "four", "O'Meara", "demonstrations", "November 17, 2017", "Bobby Darin", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "Ben Findon, Mike Myers and Bob Puzey", "Donny Osmond", "system of state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning", "Napoleon Bonaparte", "statistical analysis of data", "the nerves and ganglia outside the brain and spinal cord", "`` Tip and Ty ''", "24 November 1949", "Martin Lawrence", "In 1984", "September 29, 2017", "In 1998", "@", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "Ali Daei", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "Colon Street", "ed. o'Brien", "a sliding outer or inner door", "pickled peppers", "Sunday, November 2, 2003,", "Citizens for a Sound Economy", "Harry Robbins \"Bob\" Haldeman", "in a crime-stricken neighborhood of the Bronx.", "in exchange for two Israeli soldiers,", "off east  Africa", "Hawaii", "Derek Jeter", "sheep", "Sailors"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6027219002402826}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.923076923076923, 0.6666666666666666, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 0.5714285714285715, 0.8571428571428571, 1.0, 1.0, 0.4444444444444445, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.7843137254901961, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2818", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-4064", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-1144", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-243", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-934", "mrqa_hotpotqa-validation-3489", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-16578", "mrqa_newsqa-validation-3307"], "SR": 0.453125, "CSR": 0.4765625, "EFR": 1.0, "Overall": 0.690546875}, {"timecode": 36, "before_eval_results": {"predictions": ["dogs", "The striker Tevez remained on the bench despite a rousing reception when he went on a touchline warm-up during the game.", "the deployment of 30,000 additional U.S. troops to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government.", "a Yemeni cleric", "Amir Zaki", "central Cairo,", "at least 300 were injured", "the club's board has yet to make a decision of how it will proceed.", "The Red River", "Bialek", "Switzerland", "in Haiti", "visitors used to spend only an hour or so at his house, then leave still thinking of George Washington as that grim, old man on the dollar bill.", "Kris Allen,", "Akshay Kumar", "her more recent recordings including \"Summer Nights\" and \"You're The One That I Want\"", "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.", "Joan Rivers", "Goa", "Robert Kimmitt.", "David Beckham", "September 6, 1918,", "The defense attorney appointed to represent an Alabama professor accused of shooting her colleagues", "at a hotel near Amstetten,", "the Internet", "137", "between 1917 and 1924", "John Lennon and George Harrison,", "stylish clothes", "Christiane Amanpour", "Muslim festival of Eid al-Adha.", "Alfredo Astiz,", "\"Allahu akbar,\"", "beetle", "upper respiratory infection", "Bob Bogle,", "broadband television network.", "rally at the State House", "dead, naked body", "Kenyan forces", "Dr. Maria Siemionow,", "Pope Benedict XVI", "Opry Mills,", "ALS6", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "that Birnbaum had resigned \"on her own terms and own volition.\"", "\"She was focused so much on learning that she didn't notice,\"", "Her husband and attorney, James Whitehouse,", "heavy brush,", "frees up a place", "India", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "Linda Davis", "1961", "Leeds", "Backgammon", "Chongqing", "Paige O'Hara", "Champion Jockey", "The Crucible", "Sinclair Lewis", "fish", "sand", "mortars"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6578456220314772}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.7499999999999999, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.09090909090909093, 0.3157894736842105, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1263", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-3828", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2215", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2293", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-3625", "mrqa_naturalquestions-validation-4891", "mrqa_triviaqa-validation-206", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1740", "mrqa_searchqa-validation-7508"], "SR": 0.5625, "CSR": 0.4788851351351351, "retrieved_ids": ["mrqa_squad-train-62772", "mrqa_squad-train-572", "mrqa_squad-train-22332", "mrqa_squad-train-45613", "mrqa_squad-train-19239", "mrqa_squad-train-85184", "mrqa_squad-train-50046", "mrqa_squad-train-22773", "mrqa_squad-train-51092", "mrqa_squad-train-4665", "mrqa_squad-train-28053", "mrqa_squad-train-51864", "mrqa_squad-train-32125", "mrqa_squad-train-73793", "mrqa_squad-train-6681", "mrqa_squad-train-16796", "mrqa_newsqa-validation-2052", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-1608", "mrqa_naturalquestions-validation-5758", "mrqa_searchqa-validation-3706", "mrqa_naturalquestions-validation-1975", "mrqa_searchqa-validation-14626", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2236", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-3109", "mrqa_searchqa-validation-2950", "mrqa_newsqa-validation-1537", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4913", "mrqa_naturalquestions-validation-9715"], "EFR": 0.9642857142857143, "Overall": 0.6838685448841699}, {"timecode": 37, "before_eval_results": {"predictions": ["space shuttle Discovery", "Martin \"Al\" Culhane,", "five female pastors", "longo-Ciprelli: Fourth time lucky in Atlanta in 1996.", "the river will crest Saturday about 20 feet above flood stage.", "from Geraldine Ferraro to Bill Clinton.", "\"The Hutus were considered inferior,", "2,000 euros ($2,963)", "Wednesday", "India", "Bob Dole", "terrorism", "identity documents belonging to Miguel Mejia Munera.", "since 1983.", "Zuma", "to hold onto his land", "how great it must be to live in a rich country like America,", "Sunday", "the Somali coast", "Arizona", "southern city of Naples", "38 people", "meeting with the president to discuss her son.", "Marc Jacobs", "civilians", "backbreaking labor", "Gen. Stanley McChrystal,", "grossing $55.7 million during its first weekend.", "Nineteen", "jazz", "racial intolerance.", "\"underwear bomber\" Umar Farouk AbdulMutallab", "Felipe Calderon", "Wednesday", "a female cadaver", "collaborating with the Colombian government,", "Three", "6-2 6-1", "Scarlett Keeling", "\u00a320 million ($41.1 million) fortune", "Israel", "the man facing up, with his arms out to the side.", "George Washington", "be silent.", "Nigeria, Africa's largest producer.", "whether he should be charged with a crime,", "Jewish civil rights activists", "Manmohan Singh's Congress party", "Peter Maiyoh,", "last week", "Dilshan scored his sixth Test century of a remarkable year to give Sri Lanka a fine start to the third match of their series against India in Mumbai on Wednesday.", "Julia Ormond", "the Baltic Fleet", "all transmissions", "Chicago", "Stephen King", "vivian", "Big 12", "Frog Stone", "vivian", "Salinity", "Billy Pilgrim", "the Boer War", "I Love You"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6288588352007469}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-2898", "mrqa_naturalquestions-validation-3373", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4382", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-2921"], "SR": 0.53125, "CSR": 0.48026315789473684, "EFR": 1.0, "Overall": 0.6912870065789474}, {"timecode": 38, "before_eval_results": {"predictions": ["acid attack", "More than 22 million people", "Bryant Purvis", "Kurdistan Freedom Falcons, known as TAK,", "The portrait of William Shakespeare", "Halloween", "700", "troops to \"conduct an analysis\" of whether it is militarily essential to conduct a raid at night or whether it can be put off until daylight,", "criminals", "Jenny Sanford,", "Fayetteville, North Carolina,", "Manchester United", "former U.S. secretary of state.", "helped nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "$3 billion,", "Larry Ellison,", "the National September 11 Memorial Museum", "Kim Jong Il's", "Brazil jolted the global health community", "Gary Brooker", "baseball bat", "closing these racial gaps.", "Bollywood", "up to $50,000 for her", "bankruptcies", "A witness", "Arnold Drummond", "a good opening bit and a couple surprise things throughout.", "forgery", "fake his own death by crashing his private plane into a Florida swamp.", "to secure more funds from the region.", "Fukuoka,", "1994", "Rima Fakih", "a pair of hot-looking, two-seater sports cars.", "Roy Foster's", "Ryan Adams.", "41,", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "the lead plaintiff in perhaps the most controversial case involving Judge Sonia Sotomayor,", "Zetas", "gun", "President Obama and Britain's Prince Charles", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "3,000 kilometers (1,900 miles),", "Oxbow,", "pirates", "\"But they are realists as well -- they recognize that bad times somewhere else in the U.S. may eventually come to affect them.", "Caster Semenya", "reducing \"greenhouse emissions intensity,\"", "A Whiter Shade of Pale", "north", "Randy", "Jonathan Breck", "Achille Lauro", "Rio de Janeiro", "Jane Austen", "River Shiel", "Kennedy John Victor", "Kentucky, Virginia, and Tennessee", "foxes", "Battle of San Juan Hill", "a Senator", "Joe DiMaggio"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6852955514102193}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.8235294117647058, 1.0, 1.0, 1.0, 0.3333333333333333, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3, 1.0, 1.0, 1.0, 0.27586206896551724, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-1595", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-2748", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-10724", "mrqa_triviaqa-validation-5307", "mrqa_hotpotqa-validation-2833", "mrqa_searchqa-validation-2738"], "SR": 0.59375, "CSR": 0.48317307692307687, "EFR": 1.0, "Overall": 0.6918689903846154}, {"timecode": 39, "before_eval_results": {"predictions": ["Sunday", "the \" Michoacan Family,\"", "Steven Green", "More than 150,000", "revelry", "April 24 through May 2.", "12-hour-plus", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "removal of his diamond-studded braces.", "House", "Robert Park", "30", "calls for him to step down as majority leader.", "Meredith Kercher", "10 percent", "being evicted", "Siemionow", "highest ranking former member of Saddam Hussein's regime", "\"undulating beauty of the Canadian landscape,\"", "Muslim festival", "China", "Laura Ling and Euna Lee,", "from Amsterdam, in the Netherlands,", "Morgan Tsvangirai.", "\"With the economy, I don't have enough cash to do cable, so I'm just really debating whether or not I really want to make that switch to digital,\"", "Muslim", "Thomas", "\"drove aimlessly around the northern part of the state\"", "consumer confidence", "South Africa", "Sunday,", "Iowa's critical presidential caucuses", "a bank", "Long Island", "\"Don't Ask, Don't Tell\"", "in-cabin lighting system", "vitamin injections", "\"fusion teams,\"", "maintain an \"aesthetic environment\" and ensure public safety", "the Russian air force", "CNN/Opinion Research Corporation", "U.S. Army scout", "between June 20 and July 20", "opium", "to take the Rio Group to a new level by creating the organization.", "\"I want to express my deepest sympathy to Mikey and his family,\"", "81st minute", "Gloria Allred,", "Afghanistan", "Abhisit Vejjajiva", "a man's lifeless, naked body", "Lula", "April 10, 2018", "in the central plains", "Casino Square", "Anglesey", "Fancy Dress Shop", "1992", "American", "Ellie Kemper", "Old Maid, Go Fish", "owl", "New Orleans", "\"Little\" Harpe"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7204800407925408}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.6666666666666666, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3875", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-687", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2844", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-7661", "mrqa_triviaqa-validation-2959", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-16859", "mrqa_hotpotqa-validation-3810"], "SR": 0.65625, "CSR": 0.48750000000000004, "retrieved_ids": ["mrqa_squad-train-51533", "mrqa_squad-train-18974", "mrqa_squad-train-75914", "mrqa_squad-train-75937", "mrqa_squad-train-40507", "mrqa_squad-train-77108", "mrqa_squad-train-66734", "mrqa_squad-train-70914", "mrqa_squad-train-40991", "mrqa_squad-train-57219", "mrqa_squad-train-78254", "mrqa_squad-train-1898", "mrqa_squad-train-41483", "mrqa_squad-train-42281", "mrqa_squad-train-47408", "mrqa_squad-train-31493", "mrqa_triviaqa-validation-3592", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-2690", "mrqa_searchqa-validation-7856", "mrqa_triviaqa-validation-1016", "mrqa_searchqa-validation-899", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-556", "mrqa_searchqa-validation-14765", "mrqa_searchqa-validation-7462", "mrqa_triviaqa-validation-1144", "mrqa_searchqa-validation-8645", "mrqa_naturalquestions-validation-6429", "mrqa_newsqa-validation-1400", "mrqa_naturalquestions-validation-6877"], "EFR": 1.0, "Overall": 0.6927343749999999}, {"timecode": 40, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1450", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1723", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2637", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-845", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1539", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2167", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-2224", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2501", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3579", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-572", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-687", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-90", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-951", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10811", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11486", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12745", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14404", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-1474", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16708", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-16906", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4069", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6300", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6873", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7033", "mrqa_searchqa-validation-723", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7731", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10344", "mrqa_squad-validation-1158", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4140", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-6763", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-2766", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4487", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-466", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7078", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-978"], "OKR": 0.814453125, "KG": 0.4828125, "before_eval_results": {"predictions": ["jest", "2 million", "sanctuary", "the Lincoln National Monument Association", "stanzas", "Thomas Edison", "on or directly in front of the pitching rubber,", "Cy Young", "Bethulia", "forwards", "Frederic", "Frasier", "nightshirts", "Las Vegas", "Australian", "the Department of Justice", "James Cook", "Utah", "the BBC", "Greek Meatballs", "Pope John Paul II", "Dehlia Draycott", "\"Big, fluffy bun,\"", "Google", "the stethoscope", "Israeli", "Harry S. Truman Library", "crystals", "a charging cable", "nailsula", "a lighthouse", "Edinburgh", "colored sands", "October 1, 1903", "wren", "Cubism", "shoes", "the Erie Canal", "All the President's Men", "Simon & Garfunkel", "The Silence of the Lambs", "the banjo", "The Exorcist", "Bednye liudi", "poetry", "INXS", "the Tigris", "Leyden", "Bobby Jones", "Halley's", "March", "orbit", "Congress in 1790", "25 September 2007", "Polynesian", "Peter Pan", "Lake Placid,", "John W. Henry", "B-17 Flying Fortress", "Eugene O'Neill", "Tuesday.", "Nineteen political prisoners", "4.6 million", "the Yuan palaces be razed"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5617931547619047}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5714285714285715, 0.7499999999999999, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7421", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-220", "mrqa_searchqa-validation-8762", "mrqa_searchqa-validation-5185", "mrqa_searchqa-validation-1020", "mrqa_searchqa-validation-1404", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-11866", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-15206", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-7436", "mrqa_searchqa-validation-7203", "mrqa_searchqa-validation-156", "mrqa_searchqa-validation-5932", "mrqa_searchqa-validation-2705", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-4531", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-15712", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-13632", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-6972", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-7687", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-2044", "mrqa_newsqa-validation-732", "mrqa_naturalquestions-validation-8907"], "SR": 0.453125, "CSR": 0.4866615853658537, "EFR": 1.0, "Overall": 0.7095198170731708}, {"timecode": 41, "before_eval_results": {"predictions": ["male attire", "kinetic", "Bob Dylan", "Oz", "Lanai", "Dorothy", "American Bicentennial", "ringing the bell", "Wordsworth", "Mount Rushmore", "dressage", "Geena Davis", "Marie Osmond", "a cross", "Christmas", "John Foster Dulles", "a therapist", "Goodyear", "carbon fiber", "enormous", "a truck", "the Bill of Rights", "Manila", "Dublin", "Tainted Love", "Doom", "the Frog", "3Com", "Ned", "sound", "Hungary", "Washington, DC", "Goldenrod", "head", "a hummingbird", "manager", "Swithin", "Nathalie", "a candy cane", "Somerset Maugham", "Glacier National Park", "Lincoln Park", "Rene Auberjonois", "Irving G. Thalberg Memorial Award", "the Colosseum", "The Simple Life", "guitar", "a", "pharaoh", "Aesop", "Philip Marlowe", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "2010", "All living former presidents and their spouses are now entitled to receive lifetime Secret Service protection", "Tasmania", "glycerol", "Alex Kramer", "Theo James Walcott", "the British Army", "John John Florence", "Caylee Anthony", "Steven Gerrard was found not guilty of affray by a court in his home city on Friday.", "Camp Lejeune, North Carolina", "seven"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6876021241830065}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.2222222222222222, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12050", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-7308", "mrqa_searchqa-validation-5905", "mrqa_searchqa-validation-15056", "mrqa_searchqa-validation-2002", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-9410", "mrqa_searchqa-validation-6892", "mrqa_searchqa-validation-7887", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-8700", "mrqa_searchqa-validation-6743", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-1731", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1857"], "SR": 0.59375, "CSR": 0.48921130952380953, "EFR": 1.0, "Overall": 0.7100297619047619}, {"timecode": 42, "before_eval_results": {"predictions": ["University of Vienna", "music teacher, and conductor of the late Russian Romantic period", "Speedway World", "June 11, 1986", "Blackheart Records", "526 people per square mile", "Count Schlieffen", "German Shepherd", "the Moselle", "United States", "Hawaiian", "The Weeknd", "Friedrich Nietzsche", "Sex Drive", "Tel Aviv", "Oregon", "Attorney General and as Lord Chancellor of England", "Martha Wainwright", "David Irving", "2 November 1902 \u2013 27 August 1944", "Harvard University", "Russian Ark", "\"The Land of Enchantment\"", "hunt", "Seoul", "Eve Hewson", "Liguria", "Virgin", "Kevin Peter Hall", "Black Panthers", "Newfoundland and Labrador", "Operation Gladio", "Charles Russell", "\"She of Little Faith\"", "Final Fantasy XII", "Yemoja", "Alfred Preis", "Bayern Munich", "World Outgames", "Hanoi", "Philip Quast", "841", "Gabrielle-Suzanne Barbot de Villeneuve", "Father Dougal McGuire", "Munyiri", "2017", "Lommel", "Sky News", "Gweilo", "Canada Goose", "the Mediterranean", "Bart Millard", "1931", "a cake", "the armpit", "rabies", "astronomer", "\"Nude, Green Leaves and Bust\"", "March 22,", "Darrin Tuck,", "Grambling State University", "Russia", "\"Kabuki\"", "Steffy Forrester"], "metric_results": {"EM": 0.59375, "QA-F1": 0.658609068627451}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.33333333333333337, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-984", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-2131", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-2005", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-4414", "mrqa_triviaqa-validation-5439", "mrqa_searchqa-validation-4267", "mrqa_searchqa-validation-2446", "mrqa_searchqa-validation-1023", "mrqa_naturalquestions-validation-8695"], "SR": 0.59375, "CSR": 0.49164244186046513, "retrieved_ids": ["mrqa_squad-train-36471", "mrqa_squad-train-61281", "mrqa_squad-train-33011", "mrqa_squad-train-2698", "mrqa_squad-train-36527", "mrqa_squad-train-21727", "mrqa_squad-train-44452", "mrqa_squad-train-84784", "mrqa_squad-train-59406", "mrqa_squad-train-53358", "mrqa_squad-train-65872", "mrqa_squad-train-75108", "mrqa_squad-train-64272", "mrqa_squad-train-6270", "mrqa_squad-train-36534", "mrqa_squad-train-84697", "mrqa_triviaqa-validation-3929", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-8588", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-9235", "mrqa_searchqa-validation-13258", "mrqa_newsqa-validation-3035", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-3844", "mrqa_triviaqa-validation-6130", "mrqa_hotpotqa-validation-4483", "mrqa_naturalquestions-validation-7246", "mrqa_triviaqa-validation-3498", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3384"], "EFR": 1.0, "Overall": 0.7105159883720931}, {"timecode": 43, "before_eval_results": {"predictions": ["Pat Houston", "Bromley-By- Bow", "\"The Brontosaurus\"", "red hair", "dodo", "Prince Eddy", "Poirot", "George Sand", "Janis Joplin", "July 23", "Neuna", "rivers", "M69. Coventry to Leicester Motorway", "Eat porridge", "Steptoe and Son", "1921", "Louis Le Vau", "Novak Djokovic", "London King's Cross", "Centaurs", "Emily Dickinson", "vitamin D", "Usain Bolt", "Jimmy Perry and David Croft", "French", "yin", "Allardyce", "1951", "Washington", "d\u0292\u0259 v\u026an", "Trainspotting", "Francisco de Goya", "absinthe", "Ganges", "origami", "Buxton", "Phaethon", "laryngeal prominence", "twenty", "South Carolina", "Granada", "1969", "Paul Maskey", "French", "Fiat", "parochial undertaker", "Charlie Brown", "mercury", "Tasmanian", "five", "Tony Cozier", "Rose Stagg ( Valene Kane )", "Isaiah Amir Mustafa", "Bonanza Creek Ranch", "Conservative", "1898", "\"The Dragon\"", "the media", "public opinion", "poems telling of the pain and suffering of children just like her", "The Color Purple", "All Children Need Children's Hospitals", "polar", "anaphylaxis"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5984375}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-5893", "mrqa_triviaqa-validation-7685", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-3712", "mrqa_triviaqa-validation-6929", "mrqa_triviaqa-validation-7493", "mrqa_triviaqa-validation-6032", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-3189", "mrqa_triviaqa-validation-6111", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-4746", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-3073", "mrqa_searchqa-validation-2908"], "SR": 0.546875, "CSR": 0.4928977272727273, "EFR": 0.9655172413793104, "Overall": 0.7038704937304076}, {"timecode": 44, "before_eval_results": {"predictions": ["henry george seinfeld", "kaleidoscopes", "socialism", "henry bery, Jr.", "Charlotte Elizabeth Diana", "nimrod", "aijalon", "Robin Hood", "The Blue Boy", "leavenworth", "Ascot", "Only Fools and horses", "Japan", "Mars", "co- operation", "Spanish", "Tim Roth", "king Ferdinand", "Joanne Harris", "ancient optical illusion toy", "mungo Park", "vitamin c", "Irish Setter", "purple", "whey", "\"The Meadows\"", "secretary", "x-Men Origins: Wolverine", "boxelder bug", "big brother", "vito corleone", "Amnesty International", "Washington", "papelino", "1833", "John McCarthy", "Jennifer Lopez", "muffin man", "yichang", "Wat Tyler", "severn", "basingstoke, Hampshire", "paddy dooley", "vincent van Gogh", "france", "Elizabeth I", "Robert Hooke", "Iain Banks", "pullman brown", "brazil", "checkers", "Jason Flemyng", "mind your manners '', `` mind your language ''", "multiplication, and division are represented by the +, -, *, and / keys, respectively", "Welterweight", "Bonkyll Castle", "21", "as soon as 2050,", "three", "Gary Coleman", "claymore", "Neil Simon", "North Hero", "Nan Britton"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5864309210526315}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-5091", "mrqa_triviaqa-validation-2397", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-796", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-1705", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-3559", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4876", "mrqa_triviaqa-validation-1472", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-10364", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2935", "mrqa_searchqa-validation-13913"], "SR": 0.5625, "CSR": 0.49444444444444446, "EFR": 0.9285714285714286, "Overall": 0.6967906746031747}, {"timecode": 45, "before_eval_results": {"predictions": ["arthur", "windmills", "spain", "arthur", "james Saunders", "scurvy", "Tony Blair", "Iran", "cogs", "tonsure", "horses", "denburn", "sargento", "Taipei", "William Hherty", "cast", "pulsar", "cabot", "t.S. Eliot", "River Fleet", "adolphe Adam", "\"A Metro\u2013Goldwyn Mayer Picture\u201d", "spain", "james mccartney", "Mickey Mouse", "Peter Townsend", "gillingham", "arthur", "green", "joshua", "spain", "anastasio Volta", "spain", "Reservoir", "james v", "stilts", "Gary Oldman", "13", "five", "disraeli", "gooseberry", "Lisieux", "Vimto", "arthur", "Tripoli", "Andrew Jackson", "Salvador Dal\u00ed", "Bibendum", "Iona", "turnip", "samovar", "Latitude", "2017", "Elizabeth Dean Lail", "December 1974", "Amundsen Sea", "\"Polovetskie plyaski\"", "56", "Barack Obama", "Djibouti,", "Obama", "sandman", "Fred Rogers", "Republic of Ireland"], "metric_results": {"EM": 0.40625, "QA-F1": 0.48913690476190474}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-4795", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-1160", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-3226", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-1588", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-1153", "mrqa_triviaqa-validation-5214", "mrqa_triviaqa-validation-569", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-4732", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-413", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-4284", "mrqa_newsqa-validation-1977", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-8212", "mrqa_searchqa-validation-11910", "mrqa_hotpotqa-validation-988"], "SR": 0.40625, "CSR": 0.49252717391304346, "retrieved_ids": ["mrqa_squad-train-17744", "mrqa_squad-train-14910", "mrqa_squad-train-39172", "mrqa_squad-train-57923", "mrqa_squad-train-25355", "mrqa_squad-train-46469", "mrqa_squad-train-37580", "mrqa_squad-train-48538", "mrqa_squad-train-16887", "mrqa_squad-train-78507", "mrqa_squad-train-75884", "mrqa_squad-train-6183", "mrqa_squad-train-29260", "mrqa_squad-train-65390", "mrqa_squad-train-85534", "mrqa_squad-train-79692", "mrqa_searchqa-validation-9649", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-2844", "mrqa_searchqa-validation-2908", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3563", "mrqa_triviaqa-validation-5883", "mrqa_newsqa-validation-3824", "mrqa_searchqa-validation-3320", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-9723", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-1242", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-2833"], "EFR": 1.0, "Overall": 0.7106929347826088}, {"timecode": 46, "before_eval_results": {"predictions": ["honey", "h Hesiod", "terPodions", "republica", "zero", "Halloween", "entropy", "hickory", "Drew Carey", "the Kentucky Derby", "silvergrass", "willy Russell", "ThunderCats", "27", "h Herbert Lom", "the Gambia", "table salt", "jonesia", "le Havre", "phobias", "Annie Lennox", "henna", "Hungary", "graphite", "david Jason", "flybe", "mancunian", "they had no problems.", "james Chadwick", "Old Sparky", "aracens", "bankside power station", "indonesia", "oivo", "1969", "atrium", "Billy Fury", "hugh Laurie", "zanzibar", "palladium", "cyclops", "jian quidor", "denarius", "p Pablo Picasso", "j Narendra Modi", "rabin", "Tasmania", "\"I feel a deep sense of responsibility for not being able to produce results for our year-end business,\"", "ryan o'Brien", "centaur", "jose caiaphas", "June 12, 2018", "Thomas Middleditch", "Mike Mushok", "Hayley Catherine Rose Vivien Mills", "White Horse", "October 3, 2017", "five", "Vertikal-T,", "poor families", "a troll", "The Twelve Days of Christmas", "bockwurst", "Drew Barrymore"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5515625}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-5422", "mrqa_triviaqa-validation-7313", "mrqa_triviaqa-validation-6405", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-5224", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-6497", "mrqa_triviaqa-validation-6202", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6836", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-5712", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2078", "mrqa_triviaqa-validation-5764", "mrqa_naturalquestions-validation-1089", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-3446", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9466", "mrqa_naturalquestions-validation-1618"], "SR": 0.46875, "CSR": 0.4920212765957447, "EFR": 0.9705882352941176, "Overall": 0.7047094023779725}, {"timecode": 47, "before_eval_results": {"predictions": ["burt Reynolds", "an endocrine gland", "Pluto", "364", "Rome", "Herald of Free Enterprise", "james blunt", "bright light", "j Jim hacker", "Wikipedia", "Vietnam", "australia", "bertrand Russell", "human rights lawyer", "australia", "salyut 1", "do I have to use earplugs", "don and Phil Everly", "radius", "whist", "n Nova Scotia", "Cannes Film Festival", "november", "niger", "Tombstone, Arizona", "woodstock", "Parkinson's", "washington", "translations", "Arctic Monkeys", "significant achievement", "en\u00b7tro\u2032pi\u00b7cal\u00b7ly adv.", "Tangier", "disc-shaped and balanced", "us friends in the North", "Iceland", "eggs benedict", "mercia", "today", "newcastle brown ale", "Jean-Paul Gaultier", "Cockermouth", "28", "james chipperfield Architects", "dry rot", "Cleckheaton", "charles haughey", "indonesia", "b Brian Clough", "charles charles cai november", "no-talent gay director", "Shareef Abdur - Rahim", "`` skin - changer ''", "Space is the Place", "Rose Theatre", "Bob Gibson", "the Corps of Discovery", "different women coping with breast cancer", "\"wacko.\"", "Transportation Security Administration", "an AK47", "airbags", "akhmim", "coercivity"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5505208333333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5190", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7632", "mrqa_triviaqa-validation-4918", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-5710", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-3656", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-673", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-2717", "mrqa_triviaqa-validation-1729", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-5639", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-5235", "mrqa_triviaqa-validation-6764", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-1552", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-8972", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-4751", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-417", "mrqa_naturalquestions-validation-5927"], "SR": 0.46875, "CSR": 0.49153645833333337, "EFR": 1.0, "Overall": 0.7104947916666667}, {"timecode": 48, "before_eval_results": {"predictions": ["space", "Akon", "buzzards", "cycling", "wrigley", "0-6-0T", "priam of troy", "wind turbines", "doe", "bakerloo", "army", "greece", "babbage", "14", "7", "madonna", "William Randolph Hearst", "yaroshinskaya 1990", "samovar", "brest", "Pete Best", "ligons", "space shuttle Challenger", "sedimentary", "george hitler", "the eye", "hans lippershey", "yisrael", "yisrael", "canmore", "st Moritz", "Christine Keeler", "j\u00f8rn Utzon", "barry Levinson", "rodents", "james kennedy", "japan", "narcolepsy", "james Mason", "Dirty Dancing", "Saga Noren", "pain", "Cardigan", "james adams", "scotia", "zimbabwe", "troy mix", "yisrael", "stoned to death", "Syriza", "nova", "Missi Hale", "Salman Khan", "Battle of Antietam", "CBS", "Lewis Carroll's", "Arsenal Football Club", "of the Movement for Democratic Change", "\"The Rosie Show,\"", "Eleven", "goal", "a Purple Heart", "a palace", "vicuna"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5026041666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-5779", "mrqa_triviaqa-validation-7161", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-7494", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-974", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-65", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-7287", "mrqa_naturalquestions-validation-6806", "mrqa_hotpotqa-validation-1677", "mrqa_hotpotqa-validation-2335", "mrqa_newsqa-validation-661", "mrqa_searchqa-validation-8159", "mrqa_searchqa-validation-15427"], "SR": 0.453125, "CSR": 0.49075255102040816, "retrieved_ids": ["mrqa_squad-train-28074", "mrqa_squad-train-73195", "mrqa_squad-train-53455", "mrqa_squad-train-7035", "mrqa_squad-train-64989", "mrqa_squad-train-56036", "mrqa_squad-train-27119", "mrqa_squad-train-1099", "mrqa_squad-train-38966", "mrqa_squad-train-6795", "mrqa_squad-train-25858", "mrqa_squad-train-5323", "mrqa_squad-train-20607", "mrqa_squad-train-26635", "mrqa_squad-train-158", "mrqa_squad-train-29449", "mrqa_searchqa-validation-10558", "mrqa_naturalquestions-validation-4870", "mrqa_hotpotqa-validation-665", "mrqa_searchqa-validation-6743", "mrqa_newsqa-validation-1044", "mrqa_triviaqa-validation-5712", "mrqa_naturalquestions-validation-7376", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-2129", "mrqa_newsqa-validation-3401", "mrqa_triviaqa-validation-978", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-11353", "mrqa_newsqa-validation-1386"], "EFR": 1.0, "Overall": 0.7103380102040816}, {"timecode": 49, "before_eval_results": {"predictions": ["Lake Mead", "Jesus", "Jamie Lee Curtis", "a porter", "The Bridge on the River Kwai", "egg", "Swaziland", "Donkey", "Niccol Machiavelli", "the Shuttle", "Sarah Bernhardt", "fermentation", "Simon Legree", "Cicero", "\"Tragedy of Coriolanus\"", "Pinocchio", "the Battle of San Juan Hill", "Drag and Drop", "Henry Hudson", "a British-American epic war film", "William Shakespeare", "Gemini", "a crossword", "The Untouchables", "Boris Godunov", "Sam Malone", "a principality located along the Mediterranean Sea", "a soda fountain", "Muhammad Ali", "a howitzer", "a Kodak slide projector", "a mu shu", "paleoconservatism", "Richard Branson", "Arlington", "Andorra", "student loan", "the moon", "a wave", "a centigrade", "Jimmy", "a steak tartare", "Captain Nemo", "aarto", "hydroelectric", "Charlie Bartlett", "analog", "\"Huckleberry Finn\"", "peppers", "a coral snake", "H. L. Hunley", "spinal cord", "electron donors", "muscle cells", "microwave oven", "james Foster", "birds", "August 10, 1933", "India", "boundary river", "in the Oaxacan countryside of southern Mexico", "Wally", "Wigan", "Francis Hutcheson"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6223958333333334}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-968", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-4021", "mrqa_searchqa-validation-9025", "mrqa_searchqa-validation-6407", "mrqa_searchqa-validation-3766", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-6276", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-15767", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-3705", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-8703", "mrqa_searchqa-validation-16361", "mrqa_searchqa-validation-9008", "mrqa_searchqa-validation-6115", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-6380", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-511", "mrqa_naturalquestions-validation-578", "mrqa_triviaqa-validation-6070", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-9"], "SR": 0.546875, "CSR": 0.49187499999999995, "EFR": 1.0, "Overall": 0.7105625}, {"timecode": 50, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-1450", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4261", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4417", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-845", "mrqa_hotpotqa-validation-848", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-22", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-518", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10617", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11259", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12081", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12399", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13413", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14034", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14404", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-1474", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16906", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1901", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2786", "mrqa_searchqa-validation-3077", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4021", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5469", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7033", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-8548", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-8700", "mrqa_searchqa-validation-8703", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-9459", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-9951", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10344", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1162", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2397", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2520", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-2953", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3343", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3738", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4335", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4935", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5190", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5344", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5473", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6836", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7108", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7313", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-889", "mrqa_triviaqa-validation-978"], "OKR": 0.818359375, "KG": 0.48984375, "before_eval_results": {"predictions": ["Armageddon", "a Lackawanna Six", "Mark Darcy", "red", "Sparks", "Anne Frank", "Wembley", "Uganda", "r\u00edo de la Plata River", "Lady Gaga", "u", "electric locomotives", "wan lake", "Paraguay", "greece", "nigeria", "Bashir", "Steve Davis", "the Crusades", "john Buchan", "Rick James", "Hebrew", "Barbadian", "steel", "david Bowie", "chief Inspector Barnaby", "Some Like It Hot", "red", "francia", "duke", "vicky letch", "golf", "ram", "john ford", "republic of republic of Moldova", "alaska", "le Havre", "elephants", "Ace of Spades", "Big Ben", "george", "Carmen", "red", "Venus", "Whitney Houston", "igneous rocks", "sweden", "john ford", "Venus", "Tamar", "battle of Thermopylae", "David Yow", "Saphira", "Darlene Cates", "Archbishop of Canterbury", "James J. Hill's Great Northern Railway transcontinental railway line", "Sevens", "about the shootings,", "Anil Kapoor", "nuclear weapon", "Hypothermia", "cleveland", "Heroes", "local authorities, specifically London boroughs, Metropolitan boroughs, unitary authorities, and district councils"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6183035714285714}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-4685", "mrqa_triviaqa-validation-4787", "mrqa_triviaqa-validation-2430", "mrqa_triviaqa-validation-214", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-3308", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-3158", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-248", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-830", "mrqa_naturalquestions-validation-9322", "mrqa_hotpotqa-validation-3417", "mrqa_hotpotqa-validation-2974", "mrqa_searchqa-validation-1408", "mrqa_naturalquestions-validation-6194"], "SR": 0.59375, "CSR": 0.49387254901960786, "EFR": 1.0, "Overall": 0.7092432598039216}, {"timecode": 51, "before_eval_results": {"predictions": ["Australia", "g\u00e9rard depardieu", "Guanabara bay", "khaki uniforms", "Arthur Hailey", "coffee", "european languages", "john ford", "dennis caffari", "philadelphia", "Julie Andrews", "Anita roddick", "wiltshire", "john keats", "john Mellencamp", "philadelphia", "dennis gisbert", "european parliament", "titanium", "jean Fellowes", "Margaret Beckett", "Wanderers", "astronauts", "guatemala", "beaver", "dennis taylor", "dennis hockney", "chloride", "bodhisattva", "crackerjack", "Buckinghamshire", "bertrand Russell", "d\u0113mokritos", "caffeine", "Gianni Versace", "george best", "\"One Night / I Got Stung\"", "phil Roberts", "mandible", "america", "birmingham", "ankle fracture", "adjectives", "goat Island", "placebo", "queen", "ecclesiastical communities", "fluorine", "european", "melanocytes", "queen", "Jackie Robinson", "Augustus Waters", "2010", "Eliot Cutler", "Lerotholi Polytechnic Football Club", "Wu-Tang Clan", "Fort Bragg in North Carolina.", "Mitt Romney", "suicides", "woodcarver", "Delaware", "Mikhail Baryshnikov", "Fa Ze YouTubers"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5785984848484849}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-579", "mrqa_triviaqa-validation-7764", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-3320", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-2850", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2042", "mrqa_triviaqa-validation-3930", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-6935", "mrqa_searchqa-validation-14492", "mrqa_searchqa-validation-10843", "mrqa_naturalquestions-validation-3297"], "SR": 0.515625, "CSR": 0.4942908653846154, "retrieved_ids": ["mrqa_squad-train-9559", "mrqa_squad-train-40646", "mrqa_squad-train-24989", "mrqa_squad-train-6753", "mrqa_squad-train-32973", "mrqa_squad-train-63864", "mrqa_squad-train-71115", "mrqa_squad-train-26543", "mrqa_squad-train-20152", "mrqa_squad-train-30188", "mrqa_squad-train-86253", "mrqa_squad-train-56822", "mrqa_squad-train-72772", "mrqa_squad-train-37619", "mrqa_squad-train-4342", "mrqa_squad-train-72594", "mrqa_newsqa-validation-2873", "mrqa_searchqa-validation-5905", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-5638", "mrqa_triviaqa-validation-524", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-7421", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-4714", "mrqa_newsqa-validation-2544", "mrqa_searchqa-validation-8775", "mrqa_squad-validation-10333", "mrqa_hotpotqa-validation-4345", "mrqa_searchqa-validation-6156", "mrqa_newsqa-validation-3356", "mrqa_triviaqa-validation-7244"], "EFR": 1.0, "Overall": 0.709326923076923}, {"timecode": 52, "before_eval_results": {"predictions": ["John Mills", "Robert Marvin \"Bobby\" Hull,", "Vancouver", "Pyrrha Baddeley", "critical quotations", "40 million", "Kolkata", "Winecoff", "Warrington", "Windermere", "Joseph \" Joe\" Estevez", "\"Nina\"", "Luke Bryan", "Ghana Technology University College", "Yunnan-Fu (\u4e91\u5357\u5e9c, \"Y\u00fann\u00e1nf\u01d4\")", "Bigfoot", "Chick", "Monty Python's Flying Circus", "Gainsborough Trinity", "Benny Andersson", "Dana Andrews", "Las Vegas", "George Adamski", "12 April 1961", "Backstreet Boys", "Formula E", "House of Fraser", "KlingStubbins", "Christopher Tin", "Jack Kilby", "skerries", "Humberside", "Polonius", "My Beautiful Dark Twisted Fantasy", "June 11, 1986", "Kathleen O'Brien", "13", "adult entertainment", "Long Island", "1,691", "Michelle Anne Sinclair", "Alexander Lippisch", "Linda Ronstadt", "Richa Sharma", "Jack Murphy Stadium", "cricket", "U.S.", "Louis King", "a cockpit", "from 1973 to 1996", "commercial", "an American actor, known for his recurring role as Noel Kahn on the teen drama television series,Pretty Little Liars", "RAF Bovingdon", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "condor", "eight", "sask", "the lump in Henry's nether regions was a cancerous tumor.", "Sunday", "150", "red", "a foot", "hair brush", "Florence"], "metric_results": {"EM": 0.375, "QA-F1": 0.5248854558270677}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.16666666666666666, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.21052631578947367, 1.0, 0.125, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-2040", "mrqa_hotpotqa-validation-4131", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-427", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-5479", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-2046", "mrqa_hotpotqa-validation-5653", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-1581", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-1359", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-7513", "mrqa_newsqa-validation-4028", "mrqa_searchqa-validation-4862", "mrqa_searchqa-validation-14559"], "SR": 0.375, "CSR": 0.4920400943396226, "EFR": 0.975, "Overall": 0.7038767688679245}, {"timecode": 53, "before_eval_results": {"predictions": ["James Augustine Aloysius Joyce", "five aerial victories", "the first Circle-Vision show that was arranged and filmed with an actual plot and not just visions of landscapes, and the first to utilize Audio- Animatronics", "Argentine", "SM Lifestyle Cities", "2001 NBA All-Star Game", "Ringo Starr", "United States", "Sparky", "James David Lofton", "Samantha Spiro", "New Orleans Saints", "The Walt Disney Company", "Scottish", "Joaqu\u00edn Rodrigo", "a United States Army lieutenant general", "Battle of Prome", "Fat Man", "Clovis I", "Sir Ahmed Salman Rushdie, FRSL", "Mark Alan Dacascos", "45th Infantry Division", "a minor basilica", "Pim Fortuyn List", "Bardot", "Sutton Hoo", "American", "Gust Avrakotos", "President Bill Clinton", "Canadian", "holy servant of Christ", "Australian", "Guthred", "Afghanistan", "1867", "a fictional sailor and the hero of a story-cycle of Middle Eastern origin", "State House in Augusta", "Love the Way You Lie", "Sam Bettley", "Basileia t\u014dn Rh\u014dmai\u014dn", "England national team", "David Abelevich Kaufman", "eight", "Colonel Patrick John Mercer, OBE", "more than 230", "27th congressional district", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "Pennacook", "Valeri Vladimirovich \"Val\" Bure", "New York Giants", "Old World fossil representatives", "Hundreds or even thousands", "1987", "1939 -- 1940", "1941", "washed or repaired wagons", "Genghis Khan.", "personal information.", "David Beckham", "sportswear", "T rex", "General Hospital", "Joseph Lieberman", "San Francisco"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5243336218520043}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.22222222222222218, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.8571428571428571, 0.11764705882352941, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-752", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5079", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-2962", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3003", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-486", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-8028", "mrqa_triviaqa-validation-7348", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-1911", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-5642"], "SR": 0.390625, "CSR": 0.4901620370370371, "EFR": 1.0, "Overall": 0.7085011574074074}, {"timecode": 54, "before_eval_results": {"predictions": ["Lawrence", "1942", "100 million", "AT&T", "May 23, 1898", "Cincinnati", "The Maze Runner", "Columbus Crew Soccer Club", "Ashanti", "Wayne Conley", "County Executive", "dance music", "Virginia", "1730", "7 October 1978", "University of Missouri", "Stratfor", "129,007", "the Seasiders", "Symphony No. 7", "Buck Owens and the Buckaroos", "boxer", "Liverpool Bay", "1991", "the Crips", "Edward James Olmos", "848", "16\u201321", "Vilnius Old Town", "the Saint Petersburg Conservatory", "James Fell", "Newcastle upon Tyne, England", "Big Kenny", "Ben Stokes", "5,000,000", "Los Angeles", "10 June 1921", "John D Rockefeller", "Germany", "a heliocentric orbit", "Black Sabbath", "Sada Carolyn Thompson", "the Austro-Hungarian Army", "21 August 1986", "David Irving", "Neneh Mariann Karlsson", "Norsemen", "Afro-Russians", "Western District of Victoria, Australia", "Captain Marvel", "Hong Kong", "a Celtic people living in northern Asia Minor", "to capitalize on her publicity", "capillaries, alveoli, glomeruli", "Edward Lear", "China", "10", "Osan Air Base.", "30", "going through a metamorphosis from blobs of orange to art as night falls.", "5200", "a dynamite", "the Puget Sound", "is not immediately returned."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5748511904761905}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.4, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444444, 0.0, 0.0, 0.0, 0.8, 1.0, 0.8571428571428571, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.2]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-574", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2178", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-3641", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-5881", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_triviaqa-validation-4314", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-1763", "mrqa_searchqa-validation-14988", "mrqa_newsqa-validation-1829"], "SR": 0.453125, "CSR": 0.4894886363636364, "retrieved_ids": ["mrqa_squad-train-53703", "mrqa_squad-train-67097", "mrqa_squad-train-21060", "mrqa_squad-train-69234", "mrqa_squad-train-27654", "mrqa_squad-train-11788", "mrqa_squad-train-83695", "mrqa_squad-train-39267", "mrqa_squad-train-37225", "mrqa_squad-train-57714", "mrqa_squad-train-37284", "mrqa_squad-train-29312", "mrqa_squad-train-61739", "mrqa_squad-train-377", "mrqa_squad-train-12697", "mrqa_squad-train-1753", "mrqa_newsqa-validation-1303", "mrqa_naturalquestions-validation-9271", "mrqa_triviaqa-validation-4942", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-6130", "mrqa_searchqa-validation-7995", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-3611", "mrqa_searchqa-validation-1400", "mrqa_newsqa-validation-2167", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-3348", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-35", "mrqa_searchqa-validation-1592"], "EFR": 1.0, "Overall": 0.7083664772727272}, {"timecode": 55, "before_eval_results": {"predictions": ["UTC \u2212 09 : 00", "Milira", "Tsetse can be distinguished from other large flies by two easily observed features", "Norman given name Robert", "April 12, 2017", "the nasal septum", "Victory gardens", "Texas - style chili con carne", "development of electronic computers", "Australia's Sir Donald Bradman", "the first time a universal payment, paid for each child", "digital transmission modes", "Eurasian Plate", "Jenny", "Charlton Heston", "March 2, 2016", "Glenn Close", "August 18, 1998", "the Russian army", "skeletal muscle and the brain", "The UN General Assembly", "July 2017", "Mike Czerwien", "to ensure party discipline in a legislature", "cells", "24", "New Zealand", "May 2010", "It is slated to premiere on November 5, 2017", "February 7, 2018", "the Jurchen Aisin Gioro clan in Manchuria", "around the Brewster family, descended from the Mayflower", "Rick Rude", "in the pouring rain at a rest stop", "number of games where the player played, in whole or in part", "Paul Revere", "the American Revolutionary War", "2015", "Ian Hart", "the first area of a bone", "Ricky Nelson", "citizens of other Commonwealth countries who were resident in Scotland", "1898", "Heather Stebbins", "horizontal tricolor of red, white, and blue", "When the others arrive", "the Tin Woodman", "William Strauss and Neil Howe", "to accomplish the objectives of the organization", "The first feature film originally presented as a talkie was The Jazz Singer", "six", "Rambo", "earth", "Mr. Bennet", "119", "Christopher Michael \"Chris\" DeStefano", "his confirmation as OMB Director in 2017.", "September 23,", "44th", "their \"Freshman Year\" experience", "vowels", "Jupiter's fifth moon, Io", "Pharmacy", "(Jack) Johnson"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5266401350385725}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0625, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.7272727272727273, 0.0, 1.0, 0.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-973", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-8346", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-4988", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-3011", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-3176", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-7391"], "SR": 0.421875, "CSR": 0.48828125, "EFR": 1.0, "Overall": 0.708125}, {"timecode": 56, "before_eval_results": {"predictions": ["Saturday,", "rural Tennessee.", "hank Moody", "he was diagnosed with skin cancer.", "her fianc\u00e9,", "fitzasca hydro-electric dam in Switzerland", "Cyprus", "Stratfor", "it would be torture to give a powerful anti-psychotic drug to somebody who isn't even mentally ill.", "May 4", "that both countries should not be able to take part in NATO's Membership Action Plan, or MAP, which is designed to help aspiring countries meet the requirements of joining the alliance.", "it could contact the insured drivers who have failed to comply,\"", "racially-tinged remark made by his former caddy,", "The worst snowstorm to hit Britain in 18 years", "one of its diplomats in northwest Pakistan", "three people", "three", "gains access to a reported \u00a320 million ($41.1 million) fortune as he turns 18 on Monday,", "give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well, because the main assailant on the nation and on Palestine is the American imperialism.", "Columbian mammoth", "J. Crew", "the other women who couldn't or wouldn't.\"\"I really hope that what I did will enable other women to come forward in similar situations,\"", "\"Watchmen\"", "over the", "Ben Roethlisberger", "$40 and a loaf of bread.", "Bill Stanton", "a monthly allowance,", "the cancer that strikes fewer than 2,000 men a year, compared with about 200,000 women.", "his comments to Rolling Stone magazine that he can \"t Totally understand\" O.J. Simpson", "Omar Bongo,", "Elizabeth Birnbaum", "Tim Masters,", "we Found Love", "they did not know how many people were onboard.", "aesthetic environment", "it was like going somewhere very special, far away, because under the Communist regime you didn't travel that much and Prague was \"wow.\"", "42 years old", "refusal or inability to \"turn it off\"", "the Transportation Security Administration", "not guilty", "in a public housing project,", "250,000 unprotected civilians are trapped in the area where the fighting is taking place,", "Washington", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.iReport.com:", "up to $5,600", "Nigeria,", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "fighting charges of Nazi war crimes", "5:20 p.m.", "it clearly warrants a complete re-examination of all the evidence related to the murder of Peggy Hettrick,\"", "10th century", "ta\u026a\u02c8t\u00e6n\u026ak /", "During his epic battle with Frieza", "Imagine Dragons", "passevaux", "Afghanistan", "Singapore", "consulting", "2006", "Castle Rock Entertainment", "a leeches", "crimes motivated by prejudice for someone's race", "Sheev Palpatine"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5244279266584683}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0975609756097561, 0.75, 0.0, 0.2222222222222222, 0.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.1, 1.0, 0.0, 0.35714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.06666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.11764705882352941, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.22222222222222224, 0.4444444444444445]}}, "before_error_ids": ["mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-762", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-3925", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-56", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-7115", "mrqa_triviaqa-validation-7106", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-16115", "mrqa_searchqa-validation-6610", "mrqa_naturalquestions-validation-5986"], "SR": 0.40625, "CSR": 0.48684210526315785, "EFR": 1.0, "Overall": 0.7078371710526316}, {"timecode": 57, "before_eval_results": {"predictions": ["allegedly involved in forged credit cards and identity theft", "misdemeanor assault charges", "Manchester United", "a skilled hacker", "We Found Love", "glass shards", "at least 27", "anyone wanting to harm them.", "citizenship", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "September 21.", "Two U.S. helicopter", "Saturday,", "11", "Lebanese", "\"Zed,\" a Columbian mammoth", "Larry King", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Symbionese Liberation Army", "e-mail to Jezebel.com's Crap E-mail From A Dude", "Kurt Cobain's dead body was discovered by an electrician.", "$83,03013.", "two weeks after Black History Month", "Arizona", "Ryan Adams.", "Al Gore.", "auction off one of the earliest versions of the Magna Carta later this year,", "18", "Coptic Church spokesman Father Abdelmaseeh Baseet", "Port-au-Prince, Haiti", "Harry Potter star Daniel Radcliffe", "the final resting place for many casualties of the wars in Iraq and Afghanistan.", "on Lifeway's 100-plus stores nationwide", "for a full facial transplant since 2004.", "the man facing up, with his arms out to the side.", "Al-Aqsa mosque", "between Denver and Winter Park.", "cortisone.", "July 1999,", "mated", "in 1980.", "Arsene Wenger", "Ava Zinna ate an allergen-free meal at the Worry Free Dinners event on Sunday.", "on Saturday.", "At least 14", "American designer.", "JBS Swift Beef Company,", "from Amsterdam, in the Netherlands, to Ankara, Turkey,", "Toffelmakaren.", "Jason Chaffetz", "U.N. nuclear watchdog agency's strongest warning yet that Iran could be aiming to build a nuclear bomb.", "Spektor", "Atlanta Hawks", "the Atlantic Ocean", "los Angeles", "Las Vegas", "March 10, 1997,", "Summer Olympic Games", "people working in film and the performing arts", "The Royal Navy", "a comb", "Korean War", "Philadelphia", "United Arab Emirates"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6034023268398269}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true], "QA-F1": [0.19047619047619047, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.8, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.4, 0.7272727272727273, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-2269", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-726", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-5567"], "SR": 0.46875, "CSR": 0.48653017241379315, "retrieved_ids": ["mrqa_squad-train-21577", "mrqa_squad-train-45504", "mrqa_squad-train-19083", "mrqa_squad-train-66291", "mrqa_squad-train-4049", "mrqa_squad-train-4272", "mrqa_squad-train-37874", "mrqa_squad-train-77224", "mrqa_squad-train-74160", "mrqa_squad-train-45897", "mrqa_squad-train-2944", "mrqa_squad-train-54832", "mrqa_squad-train-3264", "mrqa_squad-train-58503", "mrqa_squad-train-27126", "mrqa_squad-train-59980", "mrqa_newsqa-validation-3435", "mrqa_triviaqa-validation-6749", "mrqa_searchqa-validation-8172", "mrqa_triviaqa-validation-1236", "mrqa_naturalquestions-validation-2182", "mrqa_triviaqa-validation-1925", "mrqa_newsqa-validation-3854", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-3853", "mrqa_triviaqa-validation-2532", "mrqa_naturalquestions-validation-4874", "mrqa_newsqa-validation-2076", "mrqa_naturalquestions-validation-2618", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-5841", "mrqa_triviaqa-validation-510"], "EFR": 1.0, "Overall": 0.7077747844827587}, {"timecode": 58, "before_eval_results": {"predictions": ["IV cafe.", "club managers,", "city of romance, of incredible architecture and history.", "Olivia Newton-John", "There's no chance", "5 1/2-year-old", "Ricardo Valles de la Rosa,", "Los Angeles", "40 lashes", "\"I want to express my deepest sympathy to Mikey and his family,\"", "reached an agreement late Thursday", "E. coli", "the United States", "more than 100.", "may", "Museum-worthy pieces", "Susan Atkins,", "Bright Automotive,", "Too many glass shards", "Brazil", "a delegation of American Muslim and Christian leaders", "\"release\" civilians,", "Dr. Jennifer Arnold and husband Bill Klein,", "acid attack", "one", "to host the Olympic Games in Rio de Janeiro.", "sailor", "help rebuild the nation's highways, bridges and other public-use facilities.", "be a song-and-dance man.", "they did not receive a fair trial.", "Saturday", "wings", "Ross Perot", "put a lid on the marking of Ashura", "collaborating with the Colombian government,", "being evicted", "Iggy Pop", "17", "30-minute", "Sabina Guzzanti", "a hospital", "The Screening Room", "billions of dollars", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "a residential dike", "Almost all British troops in Iraq are being pulled out because the agreement that allows them to be there expires on Friday,", "last month's Mumbai terror attacks", "Christopher Savoie", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "Sunday.", "Garth Brooks", "2017", "New England Patriots", "Charlene Holt", "albert Einstein", "Parsley the Lion", "well.", "Rawlings", "four months in jail", "December 23, 1977", "orange", "Willa Cather", "the Persian Gulf", "bat"], "metric_results": {"EM": 0.703125, "QA-F1": 0.771856535942238}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.14634146341463414, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9411764705882353, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.32, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3474", "mrqa_naturalquestions-validation-9246", "mrqa_triviaqa-validation-6649"], "SR": 0.703125, "CSR": 0.49020127118644063, "EFR": 1.0, "Overall": 0.7085090042372881}, {"timecode": 59, "before_eval_results": {"predictions": ["Wayne Conley,", "barcode", "Michael Seater", "New York University School of Law", "2017", "Orange County, Florida, United States", "USC Marshall School of Business", "terrorist activity", "New York City", "Max Kellerman", "September 30, 2017", "Adelaide's number one Newstalk radio station", "November 11, 1901", "Schaffer", "Jahseh Dwayne Onfroy", "Boyd Gaming", "Port Clinton", "Cleopatra", "May 5, 1939", "1895", "DS Virgin Racing Formula E Team", "Miss Universe 2010", "black cat", "Buddha\\'s delight", "seventh", "gGmbH", "James Gregory", "Silvia Navarro", "Daphnis et Chlo\u00e9", "Richard Wayne Snell", "December 19, 1998", "1960", "about 26,000.", "Tuesday, November 8, 2016", "Joshua Rowley", "drummers", "Landstreitkr\u00e4fte \u00d6sterreich-Ungarns", "beer", "Black Friday", "Bologna, on the Po di Volano, a branch channel of the main stream of the Po River, located 5 km north.", "bioelectromagnetics", "British comedian", "St. Louis Cardinals", "Kalahari Desert", "The Gold Coast", "Danielle Fernandes Dominique Schuelein- Steel", "mastered recordings for many well known musicians,", "Julia Kathleen McKenzie", "Stephen Crawford Young", "Cody Miller", "1,521", "Caparra", "between 2 World Trade center and 3 World Trade Center", "Zeus", "binder", "peter guillam", "dudweis", "Jeddah, Saudi Arabia,", "Harare", "Krishna Rajaram,", "Tuscany", "Spanish American War", "a police car", "dillung"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6624503968253967}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.2, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.6666666666666665, 0.4444444444444445, 0.8, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-3803", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-2711", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-4134", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-5449", "mrqa_hotpotqa-validation-1206", "mrqa_hotpotqa-validation-5832", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-10088", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-3661", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-1137", "mrqa_triviaqa-validation-4861"], "SR": 0.578125, "CSR": 0.4916666666666667, "EFR": 0.9629629629629629, "Overall": 0.7013946759259259}, {"timecode": 60, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1491", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-1987", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-255", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2711", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3098", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3453", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3991", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4476", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4592", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5479", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-845", "mrqa_hotpotqa-validation-848", "mrqa_hotpotqa-validation-86", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-510", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9870", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1467", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2007", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2181", "mrqa_newsqa-validation-22", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2775", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-531", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-725", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-10431", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-10617", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10714", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-1121", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11461", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-12081", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-1231", "mrqa_searchqa-validation-12399", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12836", "mrqa_searchqa-validation-13054", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-1400", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-14626", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-15265", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15408", "mrqa_searchqa-validation-15525", "mrqa_searchqa-validation-15762", "mrqa_searchqa-validation-1592", "mrqa_searchqa-validation-16051", "mrqa_searchqa-validation-16057", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16670", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-16857", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1726", "mrqa_searchqa-validation-1841", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2250", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2786", "mrqa_searchqa-validation-31", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3578", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-3717", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5465", "mrqa_searchqa-validation-5469", "mrqa_searchqa-validation-5945", "mrqa_searchqa-validation-6156", "mrqa_searchqa-validation-6277", "mrqa_searchqa-validation-6586", "mrqa_searchqa-validation-6604", "mrqa_searchqa-validation-6774", "mrqa_searchqa-validation-6896", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8289", "mrqa_searchqa-validation-8506", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-8700", "mrqa_searchqa-validation-9166", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-9702", "mrqa_searchqa-validation-996", "mrqa_squad-validation-2728", "mrqa_squad-validation-2832", "mrqa_squad-validation-3986", "mrqa_squad-validation-4711", "mrqa_squad-validation-5315", "mrqa_squad-validation-7333", "mrqa_squad-validation-7421", "mrqa_squad-validation-8224", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1234", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-1285", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-1966", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-2043", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-2172", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-248", "mrqa_triviaqa-validation-2520", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-2799", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-2908", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-2953", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3103", "mrqa_triviaqa-validation-315", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3381", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-357", "mrqa_triviaqa-validation-3679", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-461", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-5146", "mrqa_triviaqa-validation-5317", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5344", "mrqa_triviaqa-validation-5388", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-5473", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-5701", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6836", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-6919", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7108", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7211", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7677", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7764", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-889", "mrqa_triviaqa-validation-912", "mrqa_triviaqa-validation-974", "mrqa_triviaqa-validation-978"], "OKR": 0.818359375, "KG": 0.46015625, "before_eval_results": {"predictions": ["Home Rule League", "Disha Patani", "Mineola, New York, United States", "Jimmy Ellis", "Vilius Storostas-Vyd\u016bnas", "Mario Lemieux", "Dennis Weaver as Kenneth Yarborough \"K.Y. or Kentucky\" Jones,", "Sugar Ray Robinson", "Pyotr Ilyich Tchaikovsky", "Darth Vader", "Todd Emmanuel Fisher", "Roger Thomas Staubach", "86 ft", "New Zealand", "David Patrick Griffin", "15 October 1988", "People v. Turner", "Lucille D\u00e9sir\u00e9e Ball", "Do Kyung-soo", "Cherokee River", "City of Westminster, London", "Lehmber Hussainpuri", "Eric Arthur Blair", "1974", "Old World fossil representatives", "fourth-ranking", "\"The Tonight Show\"", "Rebirth", "Peter Chelsom", "Aaron Hall", "Eli Manning", "1983", "Douglas Jackson", "more than 40 million", "Valley Falls", "Naked Soldier", "Bisexuality", "May 5, 1939", "1959", "his home town, the seaside resort of Morecambe", "Secretary of Defense", "Ambroise Thomas", "Belgian", "the first Saturday in May,", "Loch Moidart, Lochaber, Highland, Scotland", "Yellowcraigs", "28,776", "SARS", "8,648", "Moselle", "Cristian S\u00e1ez Vald\u00e9s Castro", "United States Ship '' ( USS )", "The Hunger Games : Mockingjay -- Part 1 ( 2014", "in and around an unnamed village", "dennis talavera de la Reina", "john", "Paraguay", "to see my kids graduate from this school district.", "Daniel Wozniak, 27,", "A witness", "a dust storm", "Dead Ringers", "ethanol", "April 2010."], "metric_results": {"EM": 0.53125, "QA-F1": 0.64140625}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6, 1.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.1, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-3237", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-528", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-2827", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-2558", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1570", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-4681", "mrqa_naturalquestions-validation-3095", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-4140", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-944", "mrqa_searchqa-validation-3931"], "SR": 0.53125, "CSR": 0.49231557377049184, "retrieved_ids": ["mrqa_squad-train-35912", "mrqa_squad-train-80047", "mrqa_squad-train-6795", "mrqa_squad-train-82129", "mrqa_squad-train-5993", "mrqa_squad-train-6559", "mrqa_squad-train-53313", "mrqa_squad-train-55991", "mrqa_squad-train-57598", "mrqa_squad-train-432", "mrqa_squad-train-83071", "mrqa_squad-train-46381", "mrqa_squad-train-18281", "mrqa_squad-train-65704", "mrqa_squad-train-65144", "mrqa_squad-train-1726", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-3563", "mrqa_searchqa-validation-9466", "mrqa_triviaqa-validation-796", "mrqa_naturalquestions-validation-4593", "mrqa_newsqa-validation-629", "mrqa_triviaqa-validation-6547", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-5606", "mrqa_newsqa-validation-268", "mrqa_naturalquestions-validation-5634", "mrqa_newsqa-validation-2105", "mrqa_naturalquestions-validation-5465", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-15056", "mrqa_naturalquestions-validation-4561"], "EFR": 1.0, "Overall": 0.7026037397540984}, {"timecode": 61, "before_eval_results": {"predictions": ["\"new chapter\" of improved governance", "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "Somali", "Pastor Paula White", "204,000", "Government Accountability Office", "antihistamine", "partying", "22", "Lebanese", "Rod Blagojevich", "Joe Jackson", "Los Angeles", "Haiti", "mated", "apologized", "$7.8 million", "the most high-profile amalgamation of Indian and western talent yet,", "\"a striking blow to due process and the rule of law.\"", "the Airbus A330-200", "Hurricane Gustav", "\"The people kill him with the blocks, because the people are angry. They are not hungry, they are angry,\"", "Japan", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "acid attack", "30,000", "opium", "Afghanistan", "15,000", "Barack Obama", "undated photo of Alexandros Grigoropoulos,", "Ralph Lauren", "Euna Lee,", "Consumer Reports", "curfew", "Kurdistan Workers' Party,", "the insurgency,", "U.S. President-elect Barack Obama", "Russia", "Spaniard", "CNN", "Aniston, Demi Moore and Alicia Keys", "Dan Parris, 25, and Rob Lehr, 26,", "U.S. Navy helicopter crew", "\"The public endorsement comes one day after the Register -- Iowa's largest newspaper -- backed Romney in his bid for the Republican presidential nomination", "drug cartels", "Philip Markoff,", "Marie-Therese Walter.", "they did not receive a fair trial.", "Anil Kapoor.", "last week", "privatized", "routing information base ( RIB )", "the south", "Walt Whitman", "Benjamin Kinglsey", "the Oaks", "Leon Marcus Uris", "Charlie Wilson", "1905", "a screwdrivers", "Elisabetta", "ground traction", "140 million"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7233752806333451}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false], "QA-F1": [0.8333333333333333, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.1904761904761905, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.06451612903225806, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-3936", "mrqa_newsqa-validation-1848", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-834", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-3784", "mrqa_triviaqa-validation-3764", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-13231", "mrqa_hotpotqa-validation-4810"], "SR": 0.609375, "CSR": 0.4942036290322581, "EFR": 1.0, "Overall": 0.7029813508064515}, {"timecode": 62, "before_eval_results": {"predictions": ["he has helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "punish participants in this week's bloody mutiny,", "the legitimacy of that race.", "five", "at \"CNN Heroes: An All-Star Tribute\" as", "$500,000", "Larry Zeiger", "Thabo Mbeki,", "Cambodia", "cortisone.", "provide security as needed.", "Miss USA Rima Fakih", "France's famous Louvre museum", "a man's lifeless, naked body", "June 6, 1944,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "I have been able to build a commitment to carbon neutrality into the heart of our business operations.", "semiconductors", "An immigration judge with the U.S. Justice Department", "President Clinton", "The Neptune Pool at Hearst Castle is 104 feet long and 95 feet wide at the alcove.", "Haiti", "\"The Cycle of Life,\"", "Fullerton, California,", "southern city of Naples", "\"it was the sport of kings.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "his past and his future", "Buenos Aires.", "three", "was killed in an attempted car-jacking", "3 p.m. Wednesday", "breast cancer", "1969", "Thabo Mbeki,", "America's infrastructure.", "African National Congress Deputy President Kgalema Motlanthe,", "Meredith Kercher.", "Angola", "Kurt Cobain", "between 1917 and 1924", "In fashionable neighborhoods of Tokyo", "Jaipur", "the Juarez drug cartel.", "my recent 12-day trip to Iran to film a public-television show.", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "\"The New Promised Land: Silicon Valley.\"", "suicides", "The new government has two important tasks before it: the writing of a new constitution within a year, and integration of 19,600 Maoist combatants into the security forces.", "10", "how health care can affect families.", "One day", "Sebastian Lund ( Rob Kerkovich )", "2001", "HMS amethyst", "oceania", "worked", "I, (Annoyed Grunt)-bot", "\"Menace II Society\"", "Dancing with the Stars", "beef", "termites", "(Sir Winston) Churchill", "a tortoise"], "metric_results": {"EM": 0.5, "QA-F1": 0.6013671875}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.04761904761904762, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6875000000000001, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-965", "mrqa_newsqa-validation-3933", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-2401", "mrqa_triviaqa-validation-2132", "mrqa_hotpotqa-validation-3504", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-9042", "mrqa_searchqa-validation-4054"], "SR": 0.5, "CSR": 0.4942956349206349, "EFR": 0.96875, "Overall": 0.6967497519841269}, {"timecode": 63, "before_eval_results": {"predictions": ["Sue Miller", "Ha Ha Ha by Roddy Doyle", "the Zambezi River", "hepatitis A", "anemia", "Washington", "Zen", "President of the United States", "Amherst", "Rand McNally & Company", "Pocahontas", "Calumet Farm", "Jan Hus", "nomenclature", "roulette", "Chesapeake Bay", "Golden", "fabric", "Sandhurst", "Western fiction", "dog", "the Battle of Verdun", "Carole King", "Guy Ritchie", "Betty the Ugly", "Zbigniew Brzezinski", "Caliber Killer", "Svengali", "Drumline", "Colorado", "the comet", "Hilary Swank", "These Boots Are Made for Walkin", "Vermont", "a paddock", "water levels", "Macbeth", "The Dying Swan", "the Cotton Bowl", "Sun-Hwa Kwon", "Dracula", "Wind Gods", "Chemistry", "Disturbia", "a crown", "Frank Zappa", "Hypothermia", "axel", "poet", "a bolt", "Rookwood", "1924", "1830", "rocks and minerals", "Sabena", "cocktails", "the Black Sea", "Margaret Thatcher", "Hirsch index rating", "Massachusetts", "whether he should be charged with a crime,", "The Palestinian Islamic Army,", "President Mohamed Anwar al-Sadat", "1972"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7256696428571429}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4433", "mrqa_searchqa-validation-11483", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-7825", "mrqa_searchqa-validation-9353", "mrqa_searchqa-validation-7900", "mrqa_searchqa-validation-16303", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-11075", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-10426", "mrqa_searchqa-validation-16550", "mrqa_searchqa-validation-9134", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-5136", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-14572", "mrqa_naturalquestions-validation-1003", "mrqa_triviaqa-validation-6373", "mrqa_hotpotqa-validation-3165", "mrqa_newsqa-validation-2431"], "SR": 0.640625, "CSR": 0.49658203125, "retrieved_ids": ["mrqa_squad-train-26297", "mrqa_squad-train-68681", "mrqa_squad-train-41754", "mrqa_squad-train-78046", "mrqa_squad-train-3919", "mrqa_squad-train-20191", "mrqa_squad-train-6560", "mrqa_squad-train-67967", "mrqa_squad-train-68341", "mrqa_squad-train-73999", "mrqa_squad-train-24751", "mrqa_squad-train-72364", "mrqa_squad-train-52270", "mrqa_squad-train-66489", "mrqa_squad-train-56080", "mrqa_squad-train-82459", "mrqa_naturalquestions-validation-7080", "mrqa_newsqa-validation-2105", "mrqa_searchqa-validation-1592", "mrqa_hotpotqa-validation-2588", "mrqa_triviaqa-validation-214", "mrqa_searchqa-validation-1867", "mrqa_triviaqa-validation-4140", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-3117", "mrqa_newsqa-validation-3783", "mrqa_searchqa-validation-6300", "mrqa_hotpotqa-validation-599", "mrqa_searchqa-validation-7874", "mrqa_triviaqa-validation-6070", "mrqa_hotpotqa-validation-618", "mrqa_naturalquestions-validation-6012"], "EFR": 0.9565217391304348, "Overall": 0.6947613790760869}, {"timecode": 64, "before_eval_results": {"predictions": ["The Bomb Factory", "Captain Cook's Landing Place", "Oneida Limited", "Naomi Campbell", "1967", "the Tallahassee City Commission", "John Mark Galecki (born April 30, 1975)", "Titus Lucretius Carus", "23 July 1989", "Half Hollow Hills Central School District", "sea lochs", "six", "12", "the Northrop F-15 Reporter", "Chief Strategy Officer", "the Beatles", "Italy", "a lauded intellectual and a controversial public figure", "cruiserweight", "Baylor 56\u201367.", "People!", "Shane Meadows", "tempo", "Pope John X", "Mollie Elizabeth King", "the \"Home of the Submarine Force\"", "1957", "two", "\"Grimjack\"", "Battle of Dresden", "Apsley George Benet Cherry-Garrard", "Kansas\u2013Nebraska Act", "1970", "a large green dinosaur", "November 10, 2017", "Joachim Trier", "11,163", "The Sound of Music", "Disha Patani", "Trey Parker and Matt Stone", "elderships", "the Ruul", "Beno\u00eet Jacquot", "Todd Fisher", "Ben Ainslie", "James Dean", "Amazonas", "Robert Arthur Mould", "Nickelodeon Animation Studio", "Michael Edwards (born 5 December 1963), best known as \"Eddie the Eagle\"", "Longford Town", "`` new version ''", "on location", "September 4, 2000", "South Pacific", "ironworking", "Venezuela", "Ewan McGregor", "1-1", "Brian Smith.", "Bank One", "a rectangle", "the slave trade", "Dr. Lexie Grey"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7184657356532356}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6, 0.5, 1.0, 0.4444444444444444, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3500", "mrqa_hotpotqa-validation-4024", "mrqa_hotpotqa-validation-3403", "mrqa_hotpotqa-validation-2879", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-993", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-1939", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-606", "mrqa_hotpotqa-validation-3595", "mrqa_naturalquestions-validation-1178", "mrqa_triviaqa-validation-3684", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2472", "mrqa_searchqa-validation-16522", "mrqa_searchqa-validation-8371"], "SR": 0.609375, "CSR": 0.49831730769230764, "EFR": 1.0, "Overall": 0.7038040865384615}, {"timecode": 65, "before_eval_results": {"predictions": ["United States", "Russell T Davies", "Squam Lake", "the journal has a 2015 impact factor of 0.500", "1982", "former England international striker", "Peel Holdings", "Dialogues des Carm\u00e9lites", "Red", "Ding Sheng", "small forward", "Homer Hickam, Jr.", "Courteney Cox", "McLaren", "Lt. Gen. Ulysses S. Grant", "Apalachees", "Bill Curry", "1993", "Wu-Tang Clan", "David Pajo", "his virtuoso playing techniques and compositions in orchestral fusion.", "Philip Mark Quast", "\"magnus\"", "Commerce", "Axl Rose", "banjo player", "Laurel, Mississippi", "Vladimir Valentinovich Menshov", "Sergeant First Class", "Minnesota", "Dupont Plaza Hotel", "#364", "Kentucky, Virginia, and Tennessee", "White Horse", "David Irving", "Minnesota", "Waylon Albright", "Bit Instant", "Austin, Texas", "Kevin Spacey", "Nikita Sergeyevich Khrushchev", "Algernod Lanier Washington", "ITV", "17", "Kegeyli tumani", "the fourth Thursday", "Larry Eustachy", "Richard Arthur", "the first and only U.S. born world grand prix champion", "University of Nevada, Reno", "Leona Lewis", "10.5 %", "John Goodman", "Rodney Crowell", "france", "dove", "cathedral", "super-yacht designers", "Brian Mabry", "a man's lifeless, naked body", "Dean Cain", "20 feet", "pediatric centers", "river usk"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6487580128205128}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-1763", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5446", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4394", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4715", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-2616", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-800", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-496", "mrqa_triviaqa-validation-7155", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-10998", "mrqa_triviaqa-validation-4272"], "SR": 0.546875, "CSR": 0.4990530303030303, "EFR": 1.0, "Overall": 0.703951231060606}, {"timecode": 66, "before_eval_results": {"predictions": ["44,300", "Walldorf", "Franklin, Indiana", "Levittown", "William Cavendish", "Peter 'Drago' Sell", "three or more", "Forbes", "2009", "CBS News", "Phelan Beale", "Philip K. Dick", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "Indooroopilly Shopping Centre", "zoonotic", "Dan Conner", "Westland Mall", "187th", "twice", "High school", "Yasiin Bey", "Graham Payn", "28 November 1973", "Charles Quinton Murphy", "Caesars Palace Grand Prix", "Yekaterinburg", "Young adult fiction", "Dorothy", "riders are turned upside-down and then back upright", "Wayne Rooney", "1st Marquess of Westminster", "Bill Lewis", "actress and singer", "Marco Hietala", "Mani", "Everton", "4 April 1963", "the western end of the National Mall in Washington, D.C.", "Jay Gruden", "Emad Hashim", "Washington", "Ronnie Schell", "Alan Tudyk", "heavier than a feather", "New Orleans Saints", "London", "hiphop", "Lommel differential equation", "1938", "Indian", "in the state of Maryland", "Washington", "203", "in dicots such as buttercups and oak trees, and gymnosperms such as pine trees", "Hartford", "copper", "Red Rock West", "Kurdistan Workers' Party", "Lashkar-e-Tayyiba", "the Dalai Lama's current \"middle way approach,\"", "trenchcoat", "by Arthur Homeshaw", "Stars", "Norfolk Island"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6873267357642358}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.3333333333333333, 0.4, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.28571428571428575, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4204", "mrqa_hotpotqa-validation-4578", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-5144", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3255", "mrqa_naturalquestions-validation-8220", "mrqa_newsqa-validation-1170", "mrqa_searchqa-validation-32", "mrqa_triviaqa-validation-3945"], "SR": 0.609375, "CSR": 0.5006996268656716, "retrieved_ids": ["mrqa_squad-train-81109", "mrqa_squad-train-47071", "mrqa_squad-train-50416", "mrqa_squad-train-16955", "mrqa_squad-train-38720", "mrqa_squad-train-73119", "mrqa_squad-train-49711", "mrqa_squad-train-55199", "mrqa_squad-train-35828", "mrqa_squad-train-68769", "mrqa_squad-train-18009", "mrqa_squad-train-17182", "mrqa_squad-train-24531", "mrqa_squad-train-12086", "mrqa_squad-train-36585", "mrqa_squad-train-64759", "mrqa_naturalquestions-validation-3721", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-3307", "mrqa_searchqa-validation-8261", "mrqa_triviaqa-validation-4759", "mrqa_newsqa-validation-3114", "mrqa_searchqa-validation-16823", "mrqa_triviaqa-validation-287", "mrqa_searchqa-validation-9111", "mrqa_naturalquestions-validation-7115", "mrqa_searchqa-validation-7203", "mrqa_triviaqa-validation-7440", "mrqa_newsqa-validation-2102", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-2503"], "EFR": 1.0, "Overall": 0.7042805503731342}, {"timecode": 67, "before_eval_results": {"predictions": ["7.6 mm", "Dr. Sachchidananda Sinha", "Setsuko Thurlow, an 85 - year - old survivor of the 1945 atomic bombing of Hiroshima, and ICAN Executive Director Beatrice Fihn", "Sumitra", "tomato pur\u00e9e, and tomato sauce", "the United States", "Puerto Rico ( Rich Port )", "John Locke", "a premalignant flat ( or sessile ) lesion", "a virtual reality simulator", "October 29 - 30, 2012", "2003", "a judicial officer, of a lower or puisne court, elected or appointed by means of a commission ( letters patent ) to keep the peace", "Kansas and Oklahoma", "Coconut Cove", "The Enchantress", "John Goodman", "1990", "Tom Selleck", "pick yourself up and dust yourself off and keep going", "capillaries, alveoli, glomeruli, outer layer of skin", "Gravity", "Terrell Suggs", "Cheryl Campbell", "from 13 to 22 June 2012", "Peter Klaven ( Paul Rudd )", "the inferior thoracic border", "`` Can't Change Me, ''", "Phillip Schofield and Christine Bleakley", "Nashville, Tennessee", "Evermoist", "the concentration of a compound exceeds its solubility", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor )", "seven", "1988", "Missi Hale", "1773", "U.S. Bank Stadium", "1997", "British Army soldiers shot and killed people while under attack by a mob", "a scythe", "the right to be served in facilities which are open to the public", "Algeria", "420", "1948", "more than a million", "1840s", "California, Utah and Arizona", "pit road speed", "deceased - donor ( formerly known as cadaveric ) or living - donor transplantation", "warning sign", "The Sea of Azov", "chop suey", "weather improved", "National Association for the Advancement of Colored People", "Clarence Nash", "Roslyn Castle", "trading goods and services without exchanging money", "state senators", "Kyra and Violet,", "a djembe or jembe", "teapots", "Winston Rodney", "Muslim north of Sudan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5795813857302827}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [0.0, 0.3333333333333333, 0.08, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8333333333333333, 0.35294117647058826, 1.0, 1.0, 0.0, 0.5, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.7272727272727273, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 0.4, 1.0, 0.7499999999999999, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-7270", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-5674", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-10271", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-2262", "mrqa_hotpotqa-validation-1720", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-10401", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-4479"], "SR": 0.453125, "CSR": 0.5, "EFR": 0.9428571428571428, "Overall": 0.6927120535714286}, {"timecode": 68, "before_eval_results": {"predictions": ["\"I will be asking questions,\"", "Rio de Janeiro,", "different women coping with breast cancer", "can play an important role in Afghanistan as a reliable NATO ally.", "burned over 65 percent of his body", "AbdulMutallab,", "April 6, 1994,", "opium", "partying your face off in public is not the way to reclaim your good guy image.", "183", "Stoke City.", "Angels", "between 1917 and 1924", "calls for Reid's dismissal.", "103", "Ameneh Bahrami", "President Obama and Britain's Prince Charles", "Columbia, Illinois,", "Seattle", "Unseeded Frenchwoman Aravane Rezai", "Wednesday.", "The station", "\"made a brutal choice to step up attacks against innocent civilians.\"", "Bright Automotive,", "the Russian air force,", "tie salesman from the Bronx.", "identity documents", "Silvio Berlusconi.", "$8.8 million", "movahedi,", "five", "Elena Kagan", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "\"a striking blow to due process and the rule of law.\"", "her son has strong values.", "a cancer-causing toxic chemical.", "baseball bat", "64,", "gasoline", "\"We thought we were doing a humanitarian transport,\"", "Appathurai", "Fort Bragg in North Carolina.", "The Louvre", "Ashley \"A.J.\" Jewell,", "prison inmates.", "up three of the last four months.", "Jenny Sanford,", "Saturday.", "Canada.", "34", "Drew Kesse,", "by chlorine and bromine from manmade organohalogens", "compound sentence", "2010", "1825", "caspian miehei", "m", "Statue of Liberty", "XXIV Summer Universiade", "Mickey\\'s PhilharMagic", "Captain Kangaroo", "Stevenson", "a tiger", "BBC Formula One coverage on TV, radio and online"], "metric_results": {"EM": 0.546875, "QA-F1": 0.697975331959707}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.8571428571428571, 0.8333333333333333, 1.0, 1.0, 0.5, 1.0, 0.125, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.4, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-4188", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-1891", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-3331", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-2682", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6986", "mrqa_hotpotqa-validation-1870", "mrqa_searchqa-validation-2953", "mrqa_hotpotqa-validation-2473"], "SR": 0.546875, "CSR": 0.5006793478260869, "EFR": 1.0, "Overall": 0.7042764945652173}, {"timecode": 69, "before_eval_results": {"predictions": ["VoteWoz.com", "inmates", "Georgia prosecutor's microscope.", "Mutassim,", "Mumbai suburb of Chembur,", "The sailboat, named Cynthia Woods,", "the administration's progress,", "Bill Klein,", "95.", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "significant skeletal remains", "Dr. Cade", "Arsene Wenger can expect an apology from Premier League referees chief Keith Hackett following his dismissal in the closing seconds of Saturday's 2-1 English Premier League defeat to Manchester United.", "solitary confinement", "$40 and a bread.", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "keystroke", "former French Open champion", "initiative to develop a common approach to combat global warming", "set up headquarters in Dublin.", "buckling under pressure from the ruling party.Mokotedi Mpshe,", "highest ranking former of Saddam Hussein's regime still at large,", "southern port city of Karachi,", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Cash for Clunkers", "between June 20 and July 20.", "Nicole", "Congress", "Shanghai", "18", "killing her daughter.", "gasoline", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Sheikh Sharif Ahmed", "heavy turbulence", "Malcolm X", "eight", "11th year in a row.", "2-1", "The meter reader", "Abdullah Gul,", "2009", "in her home", "writing and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "Max Foster,", "concentration camps", "Web", "UNICEF", "Citizens are picking members of the lower house of parliament,", "Secretary of State", "Basel", "Rafael Nadal", "Number 4, Privet Drive, Little Whinging in Surrey, England", "International System of Units", "bantu", "Martin Luther King", "stars", "Perth, Western Australia", "Rocky Boy\\'s Indian Reservation", "gwailou", "(Freddie) Aldam", "Golden Girls", "R", "hillsborough"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6426097722742943}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.4444444444444445, 1.0, 0.35294117647058826, 1.0, 1.0, 0.13333333333333333, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 0.7692307692307693, 0.7692307692307692, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.375, 0.8571428571428571, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2070", "mrqa_naturalquestions-validation-5820", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-834", "mrqa_hotpotqa-validation-2241", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-9679", "mrqa_triviaqa-validation-1046"], "SR": 0.515625, "CSR": 0.5008928571428571, "retrieved_ids": ["mrqa_squad-train-26890", "mrqa_squad-train-10842", "mrqa_squad-train-27320", "mrqa_squad-train-81620", "mrqa_squad-train-1143", "mrqa_squad-train-55989", "mrqa_squad-train-14822", "mrqa_squad-train-48239", "mrqa_squad-train-38899", "mrqa_squad-train-26935", "mrqa_squad-train-48424", "mrqa_squad-train-85603", "mrqa_squad-train-83975", "mrqa_squad-train-30632", "mrqa_squad-train-56908", "mrqa_squad-train-68286", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-2818", "mrqa_triviaqa-validation-3901", "mrqa_newsqa-validation-2519", "mrqa_searchqa-validation-10845", "mrqa_naturalquestions-validation-8478", "mrqa_searchqa-validation-1336", "mrqa_searchqa-validation-5321", "mrqa_searchqa-validation-14646", "mrqa_triviaqa-validation-5893", "mrqa_triviaqa-validation-569", "mrqa_triviaqa-validation-3158", "mrqa_naturalquestions-validation-4990", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-2705", "mrqa_newsqa-validation-3661"], "EFR": 1.0, "Overall": 0.7043191964285713}, {"timecode": 70, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1894", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-1939", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2827", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3498", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-368", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4205", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4388", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5634", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-927", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3281", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-3881", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1213", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3445", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-603", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-998", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-10038", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-1020", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-13065", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13535", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2908", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-3201", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-3868", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4796", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5332", "mrqa_searchqa-validation-5409", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-7455", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3523", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3559", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3813", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4748", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5411", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-7525", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-995"], "OKR": 0.828125, "KG": 0.48828125, "before_eval_results": {"predictions": ["a billionaire", "the Boer War", "Louisiana", "A Christmas Story", "Golden Hind", "astronomer", "Montana", "Marlon Brando", "Hilary Swank", "Return of the Jedi", "(Rahhardt) Jr.", "Billy Graham", "Tupac", "a rice measuring cup", "I Am Legend", "October", "Swamp Thing", "(John) Hancock", "Matt Leinart", "Nirvana", "a gulls", "nitrous oxide", "Martin Luther", "Swaziland", "Nutty Professor II", "Donna Summer", "Haiti", "\"Let there be light\"", "(Carver) Carver", "the Bastille", "a dog", "a message", "speed", "St. Francis of Assisi", "a zoom lens", "Russia", "green cards", "Long Island Sound", "Gatsby", "Brazil", "Bryant", "Death Watch", "a Mercedes-Benz", "Harvard University", "( Claire L. Chennault)", "Jack Sparrow", "Godot", "Christmas", "an injury caused by thermal, chemical, electrical, or radiation energy", "High Tor", "Frank Sinatra", "Thirty years after the Jedi Civil War", "al - khimar", "15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "Vancouver Island", "Nobel Prize in Literature", "silica sand", "Humberside Airport", "Don Johnson", "Premier League club Liverpool and the England national team", "killing rampage.", "dental work", "issued his first military orders as leader of North Korea", "sewing machines"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5891407279314889}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8333333333333334, 0.0, 0.9565217391304348, 1.0, 0.4, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 0.0, 0.18181818181818182, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-2914", "mrqa_searchqa-validation-16873", "mrqa_searchqa-validation-5502", "mrqa_searchqa-validation-7325", "mrqa_searchqa-validation-9352", "mrqa_searchqa-validation-16807", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-11506", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-13391", "mrqa_searchqa-validation-11411", "mrqa_searchqa-validation-10378", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-13563", "mrqa_searchqa-validation-2544", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-14784", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-9467", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-172", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-3862", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-2778", "mrqa_triviaqa-validation-3882"], "SR": 0.453125, "CSR": 0.5002200704225352, "EFR": 1.0, "Overall": 0.714106514084507}, {"timecode": 71, "before_eval_results": {"predictions": ["Laodicea", "Whiskey Shivers as Saddle Up", "the front of the body is called the chest", "Mitch Murray", "Kenny Gamble & Leon Huff", "Luke 6 : 12 -- 16, and Acts 1 : 13", "Hellenism", "Patrick Swayze", "letter series ''", "New England Patriots XX", "Jurchen Aisin Gioro clan", "Laura Jane Haddock", "Justin Bieber", "December 31, 1971", "Pasek & Paul", "1961", "colonialism", "Colman", "states", "Manhattan", "Claims adjuster", "the intersection of Del Monte Blvd and Esplanade Street", "Cody Fern", "247.3 million", "The vapor pressure chart ( right hand side )", "New Zealand and Australia", "Scarlett Johansson", "Article 1, Section 2, Clause 3", "908 mbar", "startup neutron source", "a premalignant flat ( or sessile ) lesion of the colon", "Andreas Vesalius", "JackScanlon", "Sachin Tendulkar", "Squamish, British Columbia, Canada", "2003", "Chandan Shetty", "Germany", "October 2008", "1983", "James Brown", "Kyla Coleman", "Coordinated Universal Time ( UTC \u2212 09 : 00 )", "the International Border ( IB )", "Andy", "Ernest Rutherford", "February 2017 in Japan and in March 2018 in North America and Europe", "June 12, 2018", "Pittsburgh", "775", "Amitabh Bachchan", "Norwich", "Tina Turner", "taka", "924", "Karl Kraus", "Smith Act", "22-year-old", "suicides", "alert patients of possible tendon ruptures and tendonitis.", "Jonah", "a Jaguar", "Morocco", "The St Andrews Agreement"], "metric_results": {"EM": 0.6875, "QA-F1": 0.8021111853832441}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.5714285714285715, 0.6, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.6666666666666666, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-6448", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1304", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-4993", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-3329", "mrqa_searchqa-validation-9123"], "SR": 0.6875, "CSR": 0.5028211805555556, "EFR": 1.0, "Overall": 0.714626736111111}, {"timecode": 72, "before_eval_results": {"predictions": ["Amanda Barrie", "Australia", "Helen Gurley Brown", "Sting", "Burma", "Maerten Tromp", "Isaac Newton", "the Etruscan army", "a raven", "10-inch", "Franz Joseph Haydn", "Mr. Brainwash", "Charles I", "Athina,", "month of", "Local Defence Volunteers (LDV)", "Julius Caesar", "Theresa May", "a linesider", "ourselves alone", "the TUC", "David Bowie", "the Liberator", "hedgehog", "Aaron", "Dik Browne", "Lyoness e", "soap", "Kevin Spacey", "yalta", "shoji", "comedo", "in the city of Leicester,", "snapdragons", "hongi", "Angela Merkel", "Scotland", "chess", "switzerland", "the pea,", "Fiji", "Ireland", "Essex Eagles", "a burrows", "archer", "The Longest Day", "1619", "President Obama", "Jimmy Carter", "gheimrat Dr. Max", "drag", "The Church of England", "1960", "by the French chemist Bernard Courtois", "Knoxville, Tennessee", "Patricia Veryan", "Melbourne Storm", "a controversial theory about Mary Magdalene and Jesus.", "not just to the islands, but to any resources that could be found there.", "May 4", "Solomon", "the Hawks", "Anthony Minghella", "Mark O'Connor"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6647321428571429}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-7413", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-6960", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4359", "mrqa_triviaqa-validation-3422", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-123", "mrqa_naturalquestions-validation-9054", "mrqa_hotpotqa-validation-626", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-1904", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-8757"], "SR": 0.59375, "CSR": 0.5040667808219178, "retrieved_ids": ["mrqa_squad-train-50870", "mrqa_squad-train-26000", "mrqa_squad-train-65685", "mrqa_squad-train-54686", "mrqa_squad-train-75992", "mrqa_squad-train-1669", "mrqa_squad-train-20603", "mrqa_squad-train-62769", "mrqa_squad-train-77324", "mrqa_squad-train-85078", "mrqa_squad-train-29216", "mrqa_squad-train-57086", "mrqa_squad-train-2273", "mrqa_squad-train-43013", "mrqa_squad-train-71116", "mrqa_squad-train-33716", "mrqa_naturalquestions-validation-6435", "mrqa_newsqa-validation-2877", "mrqa_naturalquestions-validation-5696", "mrqa_triviaqa-validation-5860", "mrqa_newsqa-validation-1893", "mrqa_hotpotqa-validation-1034", "mrqa_newsqa-validation-2590", "mrqa_hotpotqa-validation-2711", "mrqa_searchqa-validation-10522", "mrqa_hotpotqa-validation-3435", "mrqa_searchqa-validation-7995", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3625", "mrqa_triviaqa-validation-96", "mrqa_searchqa-validation-12069", "mrqa_naturalquestions-validation-2690"], "EFR": 1.0, "Overall": 0.7148758561643835}, {"timecode": 73, "before_eval_results": {"predictions": ["TOSLINK", "\"American Idol\"", "A Little Princess", "1986", "MGM Resorts International", "2,627", "Matt Groening", "Forbidden Quest", "Ronald Ryan", "Carson City", "Mossad", "Amy Jessicaup", "Carrefour", "ABC", "Southaven, Mississippi", "England", "diving duck", "uncle", "\"Darconville\u2019s Cat\"", "November of that year", "Jamel\u00e3o", "Swiss", "torpedoes", "Troma Entertainment", "heaviest album of all", "850 m", "Biola University in La Mirada, California", "various names", "Liz", "more than 40 million", "Via Port Rotterdam", "September 21, 2014", "Seth MacFarlane, Mike Barker, and Matt Weitzman", "Attack the Block", "Isabella II", "11,791", "three", "Shane Meadows", "David \"Zeb\" Cook", "win world titles", "Les Clark", "Montreal", "Venice", "Bass", "Newton County", "Military Band of Hanover", "Carnforth railway station", "Chiltern Hills", "Gold Coast in Queensland", "Luigi Segre", "Mark \"Chopper\" Read", "at least one double bond within the fatty acid chain", "food and clothing", "Bhupendranath Dutt", "HMS Conqueror", "avocados", "henry v", "Thursday", "asylum in Spain,", "FBI Special Agent Daniel Cain,", "Benjamin Harrison", "Galileo", "Rudolph Valentino", "M-98"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6656075261544012}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.6, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-50", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-4911", "mrqa_hotpotqa-validation-4957", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-2026", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-5721", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-8163", "mrqa_triviaqa-validation-5811", "mrqa_triviaqa-validation-6185", "mrqa_newsqa-validation-646", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-3369"], "SR": 0.546875, "CSR": 0.5046452702702703, "EFR": 1.0, "Overall": 0.714991554054054}, {"timecode": 74, "before_eval_results": {"predictions": ["William Shakespeare", "three", "Oklahoma", "Santa Fe, Missouri", "seven", "La vendedora de rosas", "Flavivirus", "14,673 at the 2010 census", "Crown Holdings", "E22", "Dorothy Zbornak", "Haleiwa, Hawaii", "1912", "quarterly", "a microbrewery", "Anna Clyne", "Debbie Harry", "Cuban descent", "Kneeland Street", "May 4, 2004", "NATO", "seven", "four sections", "69.7 million", "Lego", "Football Bowl Subdivision (FBS) of the National Collegiate Athletic Association (NCAA)", "MGM Grand Garden Special Events Center", "Acela Express", "the second", "Abidjan, Ivory Coast", "Jane Ryan", "\"Perfect Strangers,\"", "Hugh Grosvenor, 3rd Marquess of Westminster", "1985", "Melissa Ivy Rauch", "The Cosmopolitan of Las Vegas", "Vincent Anthony Guaraldi (July 17, 1928 \u2013 February 6, 1976), born Vincent Anthony Dellaglio,", "Sergeant First Class", "1.5 million", "The Times Higher Education Guide", "1936", "Tim Allen", "Rome", "World Famous Gold & Silver Pawn Shop", "New York Giants", "1932", "Scotiabank Saddledome", "Wolf Creek", "Electronic Attack Squadron 135 (VAQ-135) is a United States Navy electronic attack squadron that currently operates the EA-18G Growler carrier-based electronic warfare jet aircraft.", "Leona Lewis", "An All-Colored Vaudeville Show", "reared in South Africa", "under the supervision and control of Accounting Standards Board ( ASB )", "John Adams", "Playboy", "Perth", "Puerto Rico", "Russia's Tupolev TU-160,", "five", "Karen Floyd", "Re-Animator", "Do's", "Haggerty's", "muskets"], "metric_results": {"EM": 0.5, "QA-F1": 0.6524249137300607}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 0.5, 0.0, 1.0, 0.2, 1.0, 0.8, 1.0, 0.35294117647058826, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-1471", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-3545", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-4078", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-4637", "mrqa_hotpotqa-validation-2552", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-2058", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-4737", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4055", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-6962"], "SR": 0.5, "CSR": 0.5045833333333334, "EFR": 1.0, "Overall": 0.7149791666666667}, {"timecode": 75, "before_eval_results": {"predictions": ["December 23, 1977", "Ars Nova Theater", "Venice", "Dave Cook", "\"Slaughterhouse-Five\"", "Two escorts", "Marine Corps Air Station Kaneohe Bay", "paternalistic policies", "Philadelphia", "1916", "Carlos Coy", "The Fault in Our Stars", "Love Letter", "Eucritta melanolimnetes", "Catwoman", "Revengers Tragedy", "Al D'Amato 55% to 44%", "United States Navy", "National Hockey League", "October", "Oklahoma Sooners", "The individual chapters were published into 24 \"tank\u014dbon\" by Kodansha", "3,000", "Golden Globe Award", "United Healthcare", "The virus is zoonotic", "1987", "a municipality of Devbhoomi Dwarka district in the state of Gujarat in northwestern India", "Perth", "Kentucky Wildcats", "I Am Furious", "Brian Graden", "Tamara Ecclestone Rutland", "North Dakota and Minnesota to the south", "State House in Augusta", "first baseman", "John Boyd Dunlop", "Weare", "Mexico", "Prince Captain Peter Wooldridge Townsend", "Elbow", "five times", "2004", "Port Macquarie", "Wandsworth", "Alexander Martin Lippisch", "Univision", "110 films", "1874", "statistics", "Mani", "the president", "Mel Gibson", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor ) is a fictional character and one of the primary antagonists of the Star Wars franchise", "Lehman Bros International", "Bedser", "William Lamb", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "drama of the action in-and-around the golf course", "$40 and a loaf of bread.", "Alexander the Great", "Greece", "mass", "ten times"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7156927274114774}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.8, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4718", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-3529", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-1599", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-1240", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-1383", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-3086", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-4796", "mrqa_naturalquestions-validation-5986", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-6384", "mrqa_newsqa-validation-2481", "mrqa_naturalquestions-validation-225"], "SR": 0.578125, "CSR": 0.5055509868421053, "retrieved_ids": ["mrqa_squad-train-44880", "mrqa_squad-train-26968", "mrqa_squad-train-13325", "mrqa_squad-train-36817", "mrqa_squad-train-56123", "mrqa_squad-train-5958", "mrqa_squad-train-85107", "mrqa_squad-train-7453", "mrqa_squad-train-20811", "mrqa_squad-train-59252", "mrqa_squad-train-5991", "mrqa_squad-train-29000", "mrqa_squad-train-79808", "mrqa_squad-train-66572", "mrqa_squad-train-68271", "mrqa_squad-train-67243", "mrqa_newsqa-validation-1676", "mrqa_triviaqa-validation-834", "mrqa_hotpotqa-validation-2600", "mrqa_naturalquestions-validation-8652", "mrqa_hotpotqa-validation-5682", "mrqa_searchqa-validation-4862", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-1214", "mrqa_searchqa-validation-11506", "mrqa_triviaqa-validation-977", "mrqa_searchqa-validation-6241", "mrqa_squad-validation-8544", "mrqa_triviaqa-validation-3308", "mrqa_newsqa-validation-150", "mrqa_triviaqa-validation-317", "mrqa_naturalquestions-validation-2238"], "EFR": 1.0, "Overall": 0.715172697368421}, {"timecode": 76, "before_eval_results": {"predictions": ["third studio album, \"Nina\"", "James Harrison", "Paula D'Alessandris", "\"Guardians of the Galaxy Vol. 2\"", "Floyd Casey Stadium", "1948", "Stage Stores, Inc.", "Jane Mayer", "England", "Talib Kweli", "the Australian Defence Force", "Lowestoft, Suffolk", "Chrysler", "banjo", "The MGM Grand Las Vegas", "Michael Phelps", "2006", "Maurice Ravel", "C. H. Greenblatt", "48,982", "40 Days and 40 Nights", "Africa", "Neon City", "Althea Rae Janairo", "Roc-A-Fella Records and Priority Records", "The Catholic Church in Ireland", "the Netherlands", "The Division of Cook", "1919", "Serial (Bad) Weddings", "1970", "Canadian-American Association of Professional Baseball", "City and County of Honolulu", "Hirsch index rating", "Sydney", "1,925", "Las Vegas Strip in Paradise, Nevada", "Battleship", "Austrian", "John Duigan", "Chief Strategy Officer", "John M. Dowd", "Bruce R. Cook", "Hockey Club Davos", "4,000", "Neighbours", "Park Ye-jin", "Tennessee", "Dabistan-E-Mazahib", "\"Sh Shakespeare Wallah\"", "Trey Parker and Matt Stone", "The Forever People", "The period known as the Ubaid period ( c. 6500 to 3800 BC )", "Lord Banquo", "red", "Daedalus", "trout", "a fan", "Tom Hanks", "gym", "eastern Nevada", "The Jefferson Airplane", "The Dallas-Fort Worth-Arlington MSA", "Society of Union Employees"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6824337121212121}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-3286", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-3946", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-4159", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-4646", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-6519", "mrqa_triviaqa-validation-1300", "mrqa_newsqa-validation-1153", "mrqa_newsqa-validation-624", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-5822", "mrqa_triviaqa-validation-6174"], "SR": 0.578125, "CSR": 0.5064935064935066, "EFR": 0.9629629629629629, "Overall": 0.7079537938912939}, {"timecode": 77, "before_eval_results": {"predictions": ["\"Odorama\"", "Ryan Babel", "19th District", "Ginger Rogers", "The Number Twelve", "Timothy Allen Dick", "St Augustine\\'s Abbey", "My Father", "fennec fox", "Civic Arena", "Nine Inch Nails", "Lynyrd Skynyrd", "three years", "1995 to 2012", "middleweight", "Tudor City", "Sleeping Beauty", "various registries", "clockwise", "Prada", "November 6, 2009", "Nayvadius DeMun Wilburn", "India", "Albert Park", "Melissa George", "The Chamber", "1,467 rooms", "The Nassau Herald", "evangelical Christian", "276,170 inhabitants", "Pittsburgh, Pennsylvania", "51,271", "Ian Brayshaw", "Robert Downey", "FBI", "Peterborough", "1977", "Donald Sterling", "American", "Hanna", "Sleepy Hollow", "Westfield Old Orchard", "German", "Bolivian folk troupe", "An aircraft", "Lionsgate", "Roy Spencer", "Empire of Japan", "Akosua Busia", "A remake", "Unbreakable", "Stephen Foster", "hydrolysis reaction", "Yuzuru Hanyu", "viscount england", "temple", "Watford", "Brian Smith.", "Symbionese Liberation Army", "South African ministers and the deputy president", "Miles Davis III", "the Doge", "Zionism", "Cincinnati"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6501373626373627}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.8, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4040", "mrqa_hotpotqa-validation-4716", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-546", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-2935", "mrqa_naturalquestions-validation-7226", "mrqa_triviaqa-validation-7251", "mrqa_triviaqa-validation-2036", "mrqa_newsqa-validation-1380", "mrqa_searchqa-validation-3056", "mrqa_searchqa-validation-1978"], "SR": 0.578125, "CSR": 0.507411858974359, "EFR": 0.9259259259259259, "Overall": 0.700730056980057}, {"timecode": 78, "before_eval_results": {"predictions": ["the Reverse - Flash", "1995 Mitsubishi Eclipse", "The Canterbury Tales", "Rigg", "Simon Callow", "early - to - mid fourth century", "2.5 %", "IBM", "Speaker of the House of Representatives", "a solitary figure who is not understood by others, but is actually wise", "Claudia Grace Wells", "TC", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "`` Fix You ''", "Matt Jones", "Barry Bonds", "Teri Garr", "2.45 billion years ago", "1980", "conquistador Francisco Pizarro", "Ray Conniff", "In 1933", "product-market fit", "the end", "1924", "Executive chef Danny Veltri", "RMS Titanic", "Buddhism", "Americans who served in the armed forces and as civilians", "Eddie Van Halen", "foreign investors", "49 cents", "between the Eastern Ghats and the Bay of Bengal", "Vicente Fox", "Richard Crispin Armitage", "Edward IV", "December 19, 1971", "Thomas Edison", "four volumes", "Charles Habib Malik", "about 26,000 light - years", "13 May 1787", "2017 / 18 Divisional Round game", "Hathi Jr", "a young husband and wife", "Michael Crawford", "Thomas Jefferson", "the Israelites were encamped at the foot of biblical Mount Sinai", "West Norse sailors", "Holly", "142,907 residents", "germany", "Charlie cairoli", "wish FM", "American pharmaceutical company", "seven members", "Robert Digges Wimberly Connor", "a U.S. helicopter crashed in northeastern Baghdad as", "dismissed all charges", "November 26,", "C.S. Lewis", "the Monoceros", "the Constitution", "consumer price index"], "metric_results": {"EM": 0.546875, "QA-F1": 0.67144908271553}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4210526315789474, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8181818181818181, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4210526315789474, 0.0, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-5775", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-9707", "mrqa_triviaqa-validation-5097", "mrqa_hotpotqa-validation-4506", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-11100"], "SR": 0.546875, "CSR": 0.5079113924050633, "retrieved_ids": ["mrqa_squad-train-65239", "mrqa_squad-train-61412", "mrqa_squad-train-8913", "mrqa_squad-train-23045", "mrqa_squad-train-59609", "mrqa_squad-train-63620", "mrqa_squad-train-22389", "mrqa_squad-train-44467", "mrqa_squad-train-27965", "mrqa_squad-train-15277", "mrqa_squad-train-39486", "mrqa_squad-train-54537", "mrqa_squad-train-46881", "mrqa_squad-train-86559", "mrqa_squad-train-68168", "mrqa_squad-train-63888", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-47", "mrqa_hotpotqa-validation-1545", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-582", "mrqa_hotpotqa-validation-1570", "mrqa_naturalquestions-validation-6692", "mrqa_triviaqa-validation-1626", "mrqa_hotpotqa-validation-4024", "mrqa_newsqa-validation-2431", "mrqa_hotpotqa-validation-1599", "mrqa_newsqa-validation-3197", "mrqa_searchqa-validation-16871", "mrqa_searchqa-validation-15408", "mrqa_triviaqa-validation-3778", "mrqa_hotpotqa-validation-5784"], "EFR": 0.9655172413793104, "Overall": 0.7087482267568748}, {"timecode": 79, "before_eval_results": {"predictions": ["13 episodes", "Tim McGraw and Kenny Chesney", "Stephen Stills", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in the ark of the covenant", "mid-March", "at specific locations", "David Ben - Gurion", "Billy Idol", "Peking", "2016", "pathology", "Reginald Jeeves", "France", "Americans", "Tamara Drasin", "U.S. service members who have died without their remains being identified", "1996", "1923 and 1925", "Bob Dylan", "Shakespearean actresses and car salespeople", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Nawab Sir Sahibzada Abdul Qayyum", "dress shop", "Jason Lee", "Massachusetts", "Warren Hastings", "the part of maryland", "March 29, 2018", "Bob Dylan", "A simple majority", "within a dorsal root ganglion", "December 12, 2017", "3.45 billion years ago ( 2.45 Ga ), during the Siderian period, at the beginning of the Proterozoic eon", "1913", "16", "early 1980s", "across western North Carolina including Asheville, Cashiers and Saluda", "the winter solstice", "Don Henley and Glenn Frey", "Cheryl Campbell", "7 correct numbers", "R / T", "capillary action", "Bonnie Aarons", "twelve", "Bartolomeu Dias", "The axons of the tract cells cross over ( decussate ) to the other side of the spinal cord via the anterior white commissure", "The outermost layer of human skin", "Paracelsus", "season five", "table salt", "albatross", "Angiotensin Converting Enzyme (ACE) Inhibitors", "Erich Maria Remarque", "Danny Elfman", "SpongeBob SquarePants", "Irish capital.", "a student who admitted to hanging a noose in a campus library,", "a colonel in the Rwandan army,", "The War of the Worlds", "the Time Warp", "The Three Stooges", "Mikado"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6762945438308414}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.5714285714285715, 0.0, 0.7894736842105263, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.5714285714285715, 1.0, 0.896551724137931, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.14285714285714285, 0.25000000000000006, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-2671", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-3132", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-9047", "mrqa_naturalquestions-validation-7239", "mrqa_triviaqa-validation-918", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-393"], "SR": 0.546875, "CSR": 0.5083984375, "EFR": 0.9310344827586207, "Overall": 0.7019490840517241}, {"timecode": 80, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-368", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4205", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-774", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-603", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12413", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13391", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-1370", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2908", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3201", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-3868", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8606", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-996", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8157", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3540", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3559", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-995"], "OKR": 0.841796875, "KG": 0.48828125, "before_eval_results": {"predictions": ["Iden Versio", "Gayla Peevey", "November 27, 2013", "Britain", "22 November 1914", "students defensive techniques", "Jourdan Miller", "Pakhangba", "Bart Cummings", "David Motl", "Daya Jethalal Gada", "Kristy Swanson", "Oklahoma native Major General Clarence L. Tinker", "the homicidal thoughts of a troubled youth", "Castleford is a town in the metropolitan borough of Wakefield, West Yorkshire, England", "1969", "the National Assembly", "dispense summary justice", "South Africa", "needle - like", "regulatory site", "Ernest Rutherford", "Roger Federer", "Audrey II", "Walter Pauk", "1999", "Robin Cousins", "111", "a transformative change of heart ; especially : a spiritual conversion", "Rachel Sarah Bilson", "Mark Jackson", "the Mayor's son", "Procol Harum", "1,149 feet ( 350 m )", "Thawne", "1936", "Bart Millard", "User State Migration Tool ( USMT )", "4.5 pounds or 2.04 kg", "Bob Dylan", "Qutab Ud - Din - Aibak", "4 January 2011", "the digitization of social systems", "Dan Stevens", "a series of newsreel films depicting multiple alternative realities rather than a novel", "an Islamic shrine located on the Temple Mount", "at Tandi, in Lahaul", "Arnold Schoenberg", "a centre for international trade", "2010", "79", "table tennis", "crystal gayle", "horiz\u014dn", "Nick McCarthy", "Texas Tech University", "3", "Michelle Rounds", "The Bronx County District Attorneys Office", "will not support the Stop Online Piracy Act,", "Marat", "a fruitcake", "Paris", "3"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7450323879551821}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true], "QA-F1": [0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7058823529411764, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-7509", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-6887", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-4832", "mrqa_newsqa-validation-2655", "mrqa_searchqa-validation-6250"], "SR": 0.703125, "CSR": 0.5108024691358024, "EFR": 1.0, "Overall": 0.7166136188271605}, {"timecode": 81, "before_eval_results": {"predictions": ["Tessa Virtue", "eight episode series", "December 1, 2017", "Baker, California, USA", "B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "Kareem Abdul - Jabbar", "Roxette", "all transmissions", "Thomas being one of Jesus'disciples", "Bidar", "1924", "9.7 m ( 31.82 ft )", "Joanna Moskawa", "Tara / Ghost of Christmas Past", "runoff", "Hellenism", "Zoe McLellan", "Portugal. The Man", "the American Civil War", "Jenny", "Alan Shearer", "Kate Walsh", "France's Legislative Assembly", "Sunday", "com TLD", "Brooklyn, New York", "April 12, 2017", "Hercules", "Australia", "the Internal Revenue Service", "Tracy McConnell", "judges", "January 2012", "marks locations", "Janie Crawford, an African - American woman in her early forties", "a pop ballad", "T - Bone Walker", "in the thylakoid lumen", "Brian Johnson", "Mickey Mantle", "Norman Pritchard", "Conrad Lewis", "Exodus 20 : 7", "the reactor core", "3", "Bon Jovi", "Elvis Presley", "September", "a noble patrilineality", "in the Saronic Gulf", "the semilunar pulmonary valve", "france", "Italy", "David Lodge", "The Nikki Giovanni Poetry Collection", "playback singer, director, writer and producer", "Baudot code", "pipelines and hostage-taking", "Zimbabwe President Robert Mugabe", "Three aid workers", "The Cynic\\'s Word Book", "Johann Wolfgang von Goethe", "Band of Brothers", "the Rat"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6466860569985571}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, true, true], "QA-F1": [0.4, 0.5, 1.0, 0.8, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.24000000000000002, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.4, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-7385", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-367", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3947", "mrqa_searchqa-validation-6745", "mrqa_searchqa-validation-11427"], "SR": 0.5625, "CSR": 0.5114329268292683, "retrieved_ids": ["mrqa_squad-train-73448", "mrqa_squad-train-47903", "mrqa_squad-train-62220", "mrqa_squad-train-10258", "mrqa_squad-train-59816", "mrqa_squad-train-31461", "mrqa_squad-train-14807", "mrqa_squad-train-74166", "mrqa_squad-train-75209", "mrqa_squad-train-10468", "mrqa_squad-train-30975", "mrqa_squad-train-20544", "mrqa_squad-train-15115", "mrqa_squad-train-83479", "mrqa_squad-train-10157", "mrqa_squad-train-82491", "mrqa_naturalquestions-validation-7589", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-8159", "mrqa_newsqa-validation-4171", "mrqa_triviaqa-validation-506", "mrqa_searchqa-validation-11476", "mrqa_naturalquestions-validation-2179", "mrqa_newsqa-validation-2606", "mrqa_hotpotqa-validation-4483", "mrqa_searchqa-validation-14957", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3327", "mrqa_searchqa-validation-7203", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-3820", "mrqa_newsqa-validation-2646"], "EFR": 0.9642857142857143, "Overall": 0.7095968532229965}, {"timecode": 82, "before_eval_results": {"predictions": ["Ithna", "the New York Mets", "Thomas Jefferson", "sheep", "a sweatshirt", "cramming", "Alice", "The Basques", "Radames", "programming", "The Three Musketeers", "Buy the World a Coke", "(Marsha) Hunt", "(Auguste) Renoir", "Zhou Enlai", "the greater kudu", "Buddhism", "The Beagle", "Qwerty", "the vest", "a heart", "Saudi Arabia", "1.094", "Close Encounters of the Third Kind", "phloem", "The Policemen\\'s Little Run", "Carnation", "the Central Intelligence Agency", "a turquoise", "a nuclear submarine", "Alexander Graham Bell", "2,524,125", "Vaslav Nijinsky", "William Wordsworth", "mercury", "Sidney Sheldon", "an observer", "Charley", "The Weekly World News", "the Aleutian Islands", "the House of Representatives", "Roman Catholicism", "Jor-El", "Devo", "eponymos", "Middle-Earth", "embalming", "China", "ride", "Schwarzenegger", "Rachel Carson", "1922", "1994", "Anna Faris", "space station Mir", "Bobby Vinton", "fingers", "conservative", "Jennifer Aniston", "Otto Eduard Leopold", "NATO member states, Russia and India", "Newcastle", "misdemeanor assault", "Juan Francisco Antonio Hilari\u00f3n Zea D\u00edaz"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6513020833333334}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.8, 0.8]}}, "before_error_ids": ["mrqa_searchqa-validation-2709", "mrqa_searchqa-validation-6805", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-14721", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-8636", "mrqa_searchqa-validation-12933", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-3155", "mrqa_searchqa-validation-10832", "mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-13324", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-13739", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-1941", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-3253", "mrqa_hotpotqa-validation-2174"], "SR": 0.546875, "CSR": 0.5118599397590362, "EFR": 1.0, "Overall": 0.7168251129518073}, {"timecode": 83, "before_eval_results": {"predictions": ["\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "Al-Shabaab,", "1981,", "judge Shemsu Sirgaga", "Tillakaratne Dilshan scored his sixth Test century", "because its facilities are full.", "a skilled hacker", "near the George Washington Bridge,", "Greeley, Colorado,", "Alvaro Uribe", "Nirvana", "D.J. Knight of Pearlman, Texas,", "in his favor,", "34", "The Tennis Channel", "Sunday,", "Peshawar", "Saturday", "four", "Islamabad", "$273 million", "\"Beverly Hills Chihuahua\"", "President Obama", "California, Texas and Florida,", "nearly $162 billion in war funding", "Annie Duke", "Henrik Stenson", "March 24,", "collusion between the colossus of the North [the United States] and the col Colossus of the South [Brazil),\"", "shelling of the compound", "1918-1919.", "supply vessel Damon Bankston", "scored a hat-trick", "Matamoros, Mexico,", "environmental campaign video", "The Sopranos", "Both men were hospitalized and expected to survive,", "housing, business and infrastructure repairs,", "Nirvana", "it -- you know -- black is beautiful,\"", "Tukel", "\"It feels good for me to talk about her,\"", "International Polo Club Palm Beach in Wellington, Florida.", "\"We tortured (Mohammed al-) Qahtani,\"", "Hanin Zoabi,", "maintain an \"aesthetic environment\" and ensure public safety,", "Kerstin and two of her brothers,", "to clean up Washington State's decommissioned Hanford nuclear site,", "The last survivor of the Titanic, 97-year-old Millvina Dean,", "Robert Kimmitt", "21,", "10 June 1940", "2002", "c. 1000 AD", "Guru Nanak", "Tacitus", "gaseous", "Arsenal", "May 2011", "Dominican", "bowl game", "Baikal", "Beethoven", "France"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6551816739224827}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true], "QA-F1": [0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7692307692307693, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.6666666666666666, 0.0, 0.08, 0.9333333333333333, 1.0, 1.0, 1.0, 0.25, 0.9411764705882353, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-1188", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1234", "mrqa_triviaqa-validation-7444", "mrqa_hotpotqa-validation-3738", "mrqa_hotpotqa-validation-3836", "mrqa_searchqa-validation-9555", "mrqa_searchqa-validation-15010"], "SR": 0.5625, "CSR": 0.5124627976190477, "EFR": 1.0, "Overall": 0.7169456845238095}, {"timecode": 84, "before_eval_results": {"predictions": ["off Somalia's coast.", "30-minute recorded message was broadcast Wednesday on al-Raei Iraqi satellite television over an old picture of al-Douri,", "Suzan Hubbard, director of the Division of Adult Institutions, decided that Susan Atkins' request should not be sent to the sentencing court for consideration,", "drug trade", "Chris Robinson,", "Iran of trying to build nuclear bombs,", "the coalition", "the Revolutionary Armed Forces of Colombia, better known as FARC,", "Scarlett Keeling", "14", "\"extremely weak\" and said he weighs barely 100 pounds", "Larry King?\"", "strife in Somalia,", "U.S. Army", "Israel handed the United Nations Friday a report", "anti-Israeli sentiment in Egypt", "used-luxury market", "I was born in Nizhny Novgorod", "1616.", "from the Bronx.", "Rolling Stone", "energy-efficient", "The pilot,", "Iggy Pop formed a blues band called the Prime Movers.", "wildfires", "in the prestigious museum as \"bad taste\" and blamed the Louvre's directors for failing to prevent what could result in \"fragrances of fries drifting under Mona Lisa's nose\"", "Pakistan from Afghanistan,", "Matthew Fisher,", "Russian residents and worldwide viewers, in English or in Russian, what they think about Russia's role in the international community.", "economic opportunities.", "1825", "one-shot victory in the Bob Hope Classic", "Eleven people died and 36 were wounded", "Tim Clark, Matt Kuchar and Bubba Watson", "the Sadr City and Adhamiya districts of Baghdad City,\"", "Bill,", "\"Empire of the Sun,\"", "pelvis and sacrum -- the triangular bone within the pelvis.", "Gaslight Theater.", "Thabo Mbeki, the mediator in the talks,", "gun charges,", "Abhisit Vejjajiva", "The incident Sunday evening", "63", "that doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "clogs", "AbdulMutallab,", "Dr. Maria Siemionow,", "his father,", "its intention to set up headquarters in Dublin.", "India", "Atlanta, Georgia", "Matthew Gregory Wise", "the Pir Panjal Range in Jammu and Kashmir", "IN GOD WE TRUST", "The East Room", "Prussian Landsturm", "Australian", "iTunes", "Internet creators Guild", "Empire of the Sun", "Pocono", "the mitral valve", "Wonderwall"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5900021864344407}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.07142857142857142, 1.0, 0.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.8, 0.25, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.975609756097561, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-3364", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-3393", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-501", "mrqa_triviaqa-validation-5349", "mrqa_triviaqa-validation-1613", "mrqa_hotpotqa-validation-339", "mrqa_searchqa-validation-5505", "mrqa_triviaqa-validation-5896"], "SR": 0.484375, "CSR": 0.5121323529411765, "retrieved_ids": ["mrqa_squad-train-20962", "mrqa_squad-train-50598", "mrqa_squad-train-82089", "mrqa_squad-train-29789", "mrqa_squad-train-14143", "mrqa_squad-train-23868", "mrqa_squad-train-1104", "mrqa_squad-train-21638", "mrqa_squad-train-49404", "mrqa_squad-train-20531", "mrqa_squad-train-86030", "mrqa_squad-train-29869", "mrqa_squad-train-12218", "mrqa_squad-train-47202", "mrqa_squad-train-67615", "mrqa_squad-train-14805", "mrqa_searchqa-validation-2503", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-16591", "mrqa_naturalquestions-validation-9054", "mrqa_triviaqa-validation-2129", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3375", "mrqa_naturalquestions-validation-9650", "mrqa_searchqa-validation-3320", "mrqa_hotpotqa-validation-3738", "mrqa_hotpotqa-validation-2833", "mrqa_hotpotqa-validation-2580", "mrqa_triviaqa-validation-2990"], "EFR": 1.0, "Overall": 0.7168795955882353}, {"timecode": 85, "before_eval_results": {"predictions": ["2.5 million", "Mitt Romney", "Michael Schumacher", "25", "Nearly all", "Mitt Romney", "$150 billion", "5 percent", "release of the four men", "Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "in the killings this month of three people with ties to the U.S. Consulate in Ciudad Juarez, Mexico,", "Strategic Arms Reduction Treaty and nonproliferation.", "Kenneth Cole", "changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\" in 1995,", "eight.", "Aniston, Demi Moore and Alicia Keys", "Karthik Rajaram, 45.", "at the end of the season.", "Human Rights Watch organization", "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "Sen. Joe Lieberman, I-Connecticut,", "death", "1995", "he acted in self defense in punching businessman Marcus McGhee.", "Rev. Alberto Cutie", "D.J. Knight of Pearlman, Texas, decided to ride", "leaky valve", "11 to 12 year old", "a sixth member of a Missouri family", "Most of the 103 children that a French charity attempted to take to France from Chad for adoption", "1960", "President Obama's surge plan", "Nine out of 10 children", "AbdulMutallab", "Hoover Dam", "four other people", "A growing percentage of the Somali population has become dependent on humanitarian aid.", "Elena Kagan", "Golden Gate Yacht Club of San Francisco", "it should stay that way.", "Philippines", "Monday and Tuesday", "drafting a new constitution after three decades of Mubarak's rule.", "Manuel Mejia Munera was a drug lord with ties to paramilitary groups,", "bank robber John Dillinger,", "Steven Gerrard", "former Himalayan kingdom", "The pilot,", "Sunday", "Tuesday", "Missouri River", "supervillains", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "Dengue fever", "jellyfish", "jean", "Joseph Cotten", "ExCeL Exhibition Centre", "The Fault in Our Stars", "John Stuart Mill", "Smith", "elbow grease", "Alcorn State"], "metric_results": {"EM": 0.515625, "QA-F1": 0.647891865079365}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false], "QA-F1": [0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.09523809523809523, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-860", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-362", "mrqa_newsqa-validation-1267", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-4138", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-2040", "mrqa_naturalquestions-validation-8408", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-5460", "mrqa_searchqa-validation-16089", "mrqa_hotpotqa-validation-5644"], "SR": 0.515625, "CSR": 0.5121729651162791, "EFR": 0.967741935483871, "Overall": 0.71043610512003}, {"timecode": 86, "before_eval_results": {"predictions": ["\"Boots\"", "Just the Way You Are", "fluorescent lights", "Pat Sajak", "nuclear", "Hill Street Blues", "achaia", "nests", "cremation", "Bode Miller", "Mondrian", "The New York Times", "Elizabeth II", "Granite", "Montana", "the Amstel", "The New Yorker", "home is Where the Heart Is", "CCNA", "Medicaid", "Scotch eggs", "the Silk Road", "a resurfacer", "Russia", "(Vijay) Singh", "a hydra", "Stitch", "bones", "Mono", "a fish", "Mississippi", "words", "The Kiss", "Iran", "cheese", "Falcon Crest", "a cappuccino", "Graceland", "pearl", "Tulip", "Manhattan", "a flagellum", "The Monty Hall Problem", "Roosevelt", "Garfield", "blackjack", "The Lorax", "Anthony Perkins", "Chile", "Tusk", "John Roberts", "Hathi Jr", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Harlem River", "Cornwall", "polly", "Gianni Versace", "Lord's Resistance Movement", "100 million copies", "\"The Simpsons\"' thirteenth season", "southern port city of Karachi,", "Reid's dismissal.", "Donald Duck", "45 minutes,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6880952380952381}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-15302", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-5232", "mrqa_searchqa-validation-4901", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-15679", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-10383", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-11429", "mrqa_searchqa-validation-10057", "mrqa_searchqa-validation-12633", "mrqa_triviaqa-validation-5322", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-3820", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-4078"], "SR": 0.59375, "CSR": 0.5131106321839081, "EFR": 1.0, "Overall": 0.7170752514367816}, {"timecode": 87, "before_eval_results": {"predictions": ["Marxist guerrillas", "Marie-Therese Walter.", "diagnosed with skin cancer.", "two", "the federal chamber of deputies,", "650", "Bill Stanton", "Don Draper", "Kim Il Sung", "upper respiratory infection,\"", "The public endorsement", "two tickets to Italy", "iReport", "Expedia", "early 2008,", "Apple Inc.", "breast cancer.", "a planned training exercise designed to help the prince learn to fly in combat situations.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "A member of the group dubbed the \"Jena 6\"", "Democratic", "spend $60 billion on America's infrastructure.", "the Japanese navy and army.", "at least 12 months.", "David McKenzie", "three", "2.5 million copies,", "27-year-old", "Sen. Barack Obama", "braces.", "Sporting Lisbon", "10 percent", "304,000", "April.", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "shutting down buses, subways and trolleys that carry almost a million people daily.", "Arsene Wenger", "a one-shot victory in the Bob Hope Classic on the final hole", "the oceans", "Samson D'Souza,", "Virgin America", "\"disagreements\" with the Port Authority of New York and New Jersey,", "France's reputation as rugby's Jekyll and Hyde team", "restive provinces", "mental health and recovery.", "President Obama's", "Turkey,", "40-year-old", "he tried to throw a petrol bomb at the officers,", "Tehran,", "The father of Haleigh Cummings,", "Matthew Gregory Wise", "Asset = Liabilities + Equity", "during the American Civil War", "edward lear", "poller Rocky Graziano", "Alessandro Giuseppe Antonio Anastasio Volta", "2011", "baeocystin", "\" Theme Park\"", "the pronghorn", "The Old Man and the Sea", "Paul Revere", "24"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6847222222222222}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8666666666666666, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-2479", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-2741", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-3767", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-7957", "mrqa_triviaqa-validation-5210", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-2533", "mrqa_searchqa-validation-12578"], "SR": 0.578125, "CSR": 0.5138494318181819, "retrieved_ids": ["mrqa_squad-train-57688", "mrqa_squad-train-59458", "mrqa_squad-train-59143", "mrqa_squad-train-13864", "mrqa_squad-train-14356", "mrqa_squad-train-3197", "mrqa_squad-train-34947", "mrqa_squad-train-52419", "mrqa_squad-train-1400", "mrqa_squad-train-7977", "mrqa_squad-train-71103", "mrqa_squad-train-18030", "mrqa_squad-train-65644", "mrqa_squad-train-67334", "mrqa_squad-train-86190", "mrqa_squad-train-76505", "mrqa_searchqa-validation-1592", "mrqa_newsqa-validation-873", "mrqa_triviaqa-validation-5710", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-3495", "mrqa_newsqa-validation-4098", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-123", "mrqa_hotpotqa-validation-2596", "mrqa_naturalquestions-validation-714", "mrqa_hotpotqa-validation-3037", "mrqa_triviaqa-validation-7151", "mrqa_naturalquestions-validation-1279", "mrqa_newsqa-validation-3695", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-6497"], "EFR": 1.0, "Overall": 0.7172230113636363}, {"timecode": 88, "before_eval_results": {"predictions": ["empty water bottle down the touchline", "Venezuela", "2008,", "12 hours", "Abbey Road", "Thailand", "17", "doesn't get along with her co-star Kristin Davis,", "the club's board", "nuclear program.", "checkposts and military camps in the Mohmand agency,", "Manchester United's perfect start to the English Premier League season came to a halt on Saturday", "hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Museum-worthy pieces", "job training for all service members leaving the military.", "he should be charged with a crime,", "Shanghai", "Sen. Barack Obama", "the outdoors, particularly if they have a garden to eat from,", "a point for Bayern Munich", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "five", "portrait of Michael Jackson", "the governor's efforts to apparently raise campaign contributions in exchange for signing a horse-racing bill.", "1994,", "Bryant Purvis", "billions of dollars", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "The opposition group,", "A witness", "Burhanuddin Rabbani,", "4,000", "denied", "The Stooges comedic farce entitled \"Three Little Beers,\"", "Ma Khin Khin Leh,", "$40 and a loaf of bread.", "July 8 at London's 20,000-capacity O2 Arena.", "Sunday", "Cologne, Germany,", "15-year-old's", "10", "the Chao Phraya River", "British", "boyhood experience in a World War II internment camp", "21-year-old", "the two remaining crew members from the helicopter,", "President Robert Mugabe", "jaws of a crocodile in northern Australia", "She leaves her husband of 20 years, Asif Ali Zardari, two daughters and a son.", "Pakistani officials,", "stepped into the museum with a rifle and began firing.", "Action Jackson", "Icarus", "Masha Skorobogatov", "the Gulf of Mexico", "Toyota", "Peter Firmin", "Bolshoi Theatre", "Bardot", "Austin E. Knowlton School of Architecture", "the Enterprise", "soccer", "Christine", "The Twilight Zone"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6405248750527283}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.08695652173913043, 1.0, 0.0, 0.923076923076923, 1.0, 0.0, 0.125, 0.4, 0.8918918918918919, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-204", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-2145", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-66", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-2940", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-396", "mrqa_hotpotqa-validation-2716"], "SR": 0.53125, "CSR": 0.5140449438202247, "EFR": 0.9333333333333333, "Overall": 0.7039287804307116}, {"timecode": 89, "before_eval_results": {"predictions": ["Newcastle", "a crocodile", "Festival Foods", "Michael Jackson", "I, the chief executive officer, the one on the very top", "off the coast of Somalia,", "Osama bin Laden's sons", "11 healthy eggs", "the state's attorney", "3,000", "$1.5 million.", "Basilan", "American Civil Liberties Union", "Boys And Girls Alone", "Sen. Barack Obama", "11", "three", "Another high tide", "The new Touch", "August 19, 2007.", "a U.S. military helicopter", "Diego Milito", "\"People have lost their homes, their jobs, their hope,\"", "19-year-old", "a satellite.", "Larry King Live.", "tells Larry King her son has strong values.", "U.S.", "Nafees Syed", "75 percent", "of", "Alwin Landry's supply vessel Damon Bankston", "Egypt.", "200", "later apologized,", "Arkansas", "Kim Clijsters", "Brett Cummins", "11", "India", "6-2 6-1", "a bronze medal in the women's figure skating final,", "Nine out of 10 children", "North Korean leader had been plagued with health problems", "end of the season.", "social networking", "Samson D'Souza,", "Stoke City.", "Kellogg Brown and Root,", "5,600", "grossed $55.7 million during its first frame,", "George Harrison", "Deflection of an object due to the Coriolis force", "September 1972", "spitsbergen Current", "alberta", "five", "University of Kansas", "Salma Hayek", "Stalybridge Celtic Football Club", "the Borrowers", "a crossword", "Thomas Merton", "John Denver"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7440228174603174}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 0.28571428571428575, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-64", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-955", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2135", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-2978", "mrqa_newsqa-validation-1165", "mrqa_naturalquestions-validation-7242", "mrqa_triviaqa-validation-5995", "mrqa_triviaqa-validation-674", "mrqa_hotpotqa-validation-905", "mrqa_searchqa-validation-16474"], "SR": 0.671875, "CSR": 0.5157986111111111, "EFR": 0.8571428571428571, "Overall": 0.6890414186507936}, {"timecode": 90, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4796", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3026", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7065", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1505", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1821", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-204", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10159", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-12933", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13541", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14185", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14609", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-15669", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16185", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2909", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3002", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4105", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5688", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-8265", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-9176", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4289", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5530", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-995"], "OKR": 0.791015625, "KG": 0.4546875, "before_eval_results": {"predictions": ["South Florida International Airport ( RSW ), located southeast of the city", "Brazil", "restricted naturalization to `` free white persons '' of `` good moral character ''", "Palm Sunday", "Saphira", "Part 2", "1924", "Great G minor symphony ''", "1979 -- 80", "to transform Hitler's government into a legal dictatorship", "Japan", "MacFarlane", "John Cooper Clarke", "3,000", "United States", "1992", "c. 3000 BC", "the large area needed for effective gas exchange", "Kid Creole and the Coconuts", "France", "New York City", "sacroiliac joint or SI joint", "by the early 1980s", "British and French Canadian fur traders", "H CO ( equivalently OC ( OH ) )", "the Pir Panjal Range in Jammu and Kashmir", "Baker, California", "1983", "# 4 School of Public Health in the country", "Toronto Islands", "`` Heroes and Villains ''", "Eddie Murphy", "2003", "Bulgaria", "Francis Hutcheson", "Italian / Venetian John Cabot", "to bring", "Christina Aguilera", "the Indians", "October 27, 1904", "54 Mbit / s", "Queen M\u00e1xima of the Netherlands", "Richie Cunningham", "1937", "Mike Czerwien", "his cousin D\u00e1in", "Radiotelegraphy", "on the southeastern coast of the Commonwealth of Virginia in the United States", "2018", "Anthony Hopkins", "Peter Andrew Beardsley MBE", "john", "cheese", "cut\u012bcula", "Harlow Cuadra and Joseph Kerekes", "Lonely", "Sir Seretse Goitsebeng Maphiri Khama", "Gerfa Yeatts Lunsmann, 43, was found at Suba Kampong township", "United States, NATO member states, Russia and India", "off Somalia's coast.", "Sacha Baron Cohen", "Billy Bathgate", "Spider-Man", "2000"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6740149456521739}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.08695652173913043, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.6, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-2425", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-8182", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-1784", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-3276", "mrqa_hotpotqa-validation-1218", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2078", "mrqa_hotpotqa-validation-1893"], "SR": 0.5625, "CSR": 0.5163118131868132, "retrieved_ids": ["mrqa_squad-train-75593", "mrqa_squad-train-77084", "mrqa_squad-train-41837", "mrqa_squad-train-51481", "mrqa_squad-train-65402", "mrqa_squad-train-13363", "mrqa_squad-train-36292", "mrqa_squad-train-46968", "mrqa_squad-train-5754", "mrqa_squad-train-75191", "mrqa_squad-train-37697", "mrqa_squad-train-21776", "mrqa_squad-train-43434", "mrqa_squad-train-21736", "mrqa_squad-train-10244", "mrqa_squad-train-79370", "mrqa_naturalquestions-validation-10057", "mrqa_newsqa-validation-2067", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-4526", "mrqa_searchqa-validation-1408", "mrqa_hotpotqa-validation-153", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1604", "mrqa_searchqa-validation-11733", "mrqa_naturalquestions-validation-5984", "mrqa_hotpotqa-validation-2174", "mrqa_searchqa-validation-10383", "mrqa_triviaqa-validation-5883", "mrqa_newsqa-validation-1848", "mrqa_hotpotqa-validation-4069", "mrqa_naturalquestions-validation-4007"], "EFR": 0.9285714285714286, "Overall": 0.6857735233516483}, {"timecode": 91, "before_eval_results": {"predictions": ["Tom Brady", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "Lori McKenna", "The Stanley Hotel", "Cephalopoda", "has within it connotations of the passing of the year", "2007", "parthenogenic", "1908", "Sir Ronald Ross", "skeletal muscle", "pigs", "in front of only 700 fans", "Pyeongchang County, South Korea", "Set six months after Kratos killed his wife and child", "The Hudson River", "the chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Colon Street", "Justin Timberlake", "New York City", "Rick", "Kerry Shale as Tadheus `` Tad '' Stone", "Banzai", "the euro", "September 1959", "1890s", "the Alamodome and city of San Antonio", "John Smith", "ThonMaker", "Microfilaments", "the southeastern United States", "Interphase", "the Isle of FERNANDO 'S!, a fictional location based in Puerto de la Cruz, Tenerife", "1971", "September 19 - 22, 2017", "the pilot", "real - time chat", "alveolar bone", "lithium", "Tim Duncan", "2016", "deceased - donor ( formerly known as cadaveric )", "red", "turlough, or turlach", "lighter", "loosely on Eminem's actual upbringing", "sunny", "Microsoft Windows", "B.R. Ambedkar", "1975", "no longer a fundamental right", "birmingham", "denmark", "Bangladesh", "AVN Adult Entertainment Expo", "Henry Lau", "Mark Andrew Brayshaw", "Michael Jackson", "\"E! News\"", "Susan Boyle", "&quot", "William Robert Amis", "ukulele", "Emily Dickinson"], "metric_results": {"EM": 0.546875, "QA-F1": 0.692123674936175}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true], "QA-F1": [0.0, 0.09090909090909093, 1.0, 1.0, 0.6666666666666666, 0.7692307692307693, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.33333333333333337, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-7107", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2242", "mrqa_newsqa-validation-4130", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-15474"], "SR": 0.546875, "CSR": 0.5166440217391304, "EFR": 0.9310344827586207, "Overall": 0.6863325758995502}, {"timecode": 92, "before_eval_results": {"predictions": ["Al Gore", "Millay", "Augusta", "a Therapist", "the Tyger", "Elihu Root", "a helix", "the Tower of London", "Ho Chi Minh", "Hizbollah", "a fief", "Billy Joel", "Maurice Ravel", "Madagascar", "Cleveland", "violist", "Riboflavin", "Jack Johnson", "insurance", "Augusta", "a secret marriage", "Henry VIII", "Alien", "Athena", "Disney", "disease", "Secretary of Labor", "gravity", "Mystery Science Theater 3000", "the Thames", "Columbus", "Harry Houdini", "red", "the Ruby Gemstone", "fiberboard", "Heather Mills", "Robert Louis Stevenson", "Saints & Angels", "yellow", "Opal", "\"Mulholland Drive\"", "the Supreme Court", "Sherlock Holmes", "gallows", "John Edwards", "the King of Hearts", "shiatsu", "the Ark", "oxygen", "Porgy and Bess", "Gandhi", "pigs", "the President of India", "Asuka", "Elma's Pound", "plant with leaves that help us to relax after or even during a hectic day or as an aid for a good night\u2019s sleep", "Alexei Kosygin", "Shohola Falls", "Francis Egerton, 3rd Duke of Bridgewater", "The Guest", "Samson D'Souza, 29,", "the ancient Greek site of Olympia", "murder in the beating death of", "Atlanta"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7259672619047619}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2201", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-15264", "mrqa_searchqa-validation-8474", "mrqa_searchqa-validation-10947", "mrqa_searchqa-validation-10412", "mrqa_searchqa-validation-11958", "mrqa_searchqa-validation-7899", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-15364", "mrqa_searchqa-validation-12133", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-7843", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-10135", "mrqa_searchqa-validation-8663", "mrqa_triviaqa-validation-4954", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-117", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-1239", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-1159"], "SR": 0.59375, "CSR": 0.5174731182795699, "EFR": 0.8461538461538461, "Overall": 0.6695222678866831}, {"timecode": 93, "before_eval_results": {"predictions": ["Abigail Rokison", "heart", "$72", "4.37 light - years ( 1.34 pc ) from the Sun", "effectively overseeing the government of large numbers of people spread over extensive areas", "Marcus Aurelius", "Dan Stevens", "in a Norwegian town circa 1879", "Lager", "September 1993", "1916", "Randy VanWarmer", "the sixth series", "Dr. Rajendra Prasad", "Gary Grimes", "all transmissions", "late - night", "Sarah Silverman", "artificial insemination", "3 ( 55 -- 69 % ) & 4 ( 40 -- 54 % ), and fail ( below 40 % )", "comic", "Ace", "1994", "8.7 -- 9.2", "in its annual report on the implementation of the 2009 EU Budget", "U.S. Bank Stadium", "the state sector", "her abusive husband", "introverted Feeling ( Fi ) and Extroverted Intuition ( Te ) ) is an abbreviation used in the publications of the Myers -- Briggs Type Indicator ( MBTI ) to refer to one of sixteen personality types", "the National Football League ( NFL )", "union of 29 states", "by capillary action", "Persian", "Cee - Lo", "a premalignant flat ( or sessile ) lesion of the colon", "Mitch Murray", "the Fox Ranch in Malibu Creek State Park, northwest of Los Angeles", "Podujana Peramuna, led by former president Mahinda Rajapaksa", "native to Asia", "Teddy Randazzo", "in rocks and minerals", "The NFL Scouting combine", "1961", "Egypt", "Angel Benitez", "seven", "Mike Higham", "in the Arab World", "Turner Layton", "dromedary", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "timir Pinegin", "julian hansen", "Velazquez", "Kim Jong-hyun", "Allies of World War I", "1998", "Jamaleldine, a 31-year-old U.S. Army scout who proudly wears a Stetson hat and spurs on his boots, laughs.", "Akio Toyoda", "curfew", "Liechtenstein", "Jordan", "RSS", "Brad Pitt"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5724194822231741}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 1.0, 0.20689655172413793, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 0.9565217391304348, 0.3333333333333333, 0.5, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-1366", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-5687", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-8046", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-1414", "mrqa_hotpotqa-validation-4316", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-250", "mrqa_searchqa-validation-13551"], "SR": 0.453125, "CSR": 0.5167885638297872, "retrieved_ids": ["mrqa_squad-train-6654", "mrqa_squad-train-63688", "mrqa_squad-train-55476", "mrqa_squad-train-48177", "mrqa_squad-train-76003", "mrqa_squad-train-65481", "mrqa_squad-train-48932", "mrqa_squad-train-55806", "mrqa_squad-train-35274", "mrqa_squad-train-26386", "mrqa_squad-train-38413", "mrqa_squad-train-46102", "mrqa_squad-train-29746", "mrqa_squad-train-45381", "mrqa_squad-train-50475", "mrqa_squad-train-21529", "mrqa_newsqa-validation-3925", "mrqa_newsqa-validation-3936", "mrqa_naturalquestions-validation-4552", "mrqa_hotpotqa-validation-5833", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-3437", "mrqa_hotpotqa-validation-2860", "mrqa_triviaqa-validation-2430", "mrqa_naturalquestions-validation-4593", "mrqa_triviaqa-validation-7424", "mrqa_newsqa-validation-2392", "mrqa_searchqa-validation-16012", "mrqa_newsqa-validation-3307", "mrqa_naturalquestions-validation-3721", "mrqa_triviaqa-validation-5322"], "EFR": 0.9714285714285714, "Overall": 0.6944403020516717}, {"timecode": 94, "before_eval_results": {"predictions": ["L.K. Advani", "an adopted daughter of Thanos", "October 20, 1977", "the Pir Panjal Range in Jammu and Kashmir", "Bill's yacht in Monte Carlo", "1038", "Jughead Jones", "the student's transition from the study of preclinical to clinical health sciences", "Justin Timberlake", "Mahatma Gandhi", "Bhupendranath Dutt", "Bruce Mackinnon as the mummy", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product", "1599", "a French - American composer of film scores", "Mangal Pandey", "an unmasked and redeemed Anakin Skywalker ( formerly Darth Vader )", "the red - bed country of its watershed", "New England ( Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont )", "Nick Kroll", "Malina Weissman", "1980", "The Gupta Empire", "al - Mamlakah al - \u02bbArab\u012byah", "the first day of the New Year", "Gary Speed", "September 2000", "Middle Eastern alchemy", "the inferior thoracic border", "chimera ( a mixture of several animals )", "in 1960", "at Thunder Road", "James Arthur", "save, rescue, savior", "Frankel", "Nigel Lythgoe, Mia Michaels, and Adam Shankman", "1,350 at the 2010 census", "1839", "1787 -- 1869", "as early as January 3, and as late as February 12", "Kida", "Nearer, My God, to Thee", "August 22, 1980", "Nikita Khrushchev", "the northernmost point on the Earth", "listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "macOS High Sierra", "Queen M\u00e1xima of the Netherlands", "Rococo - era France", "Atlanta", "to collect menstrual flow", "nerve cell cluster", "house sparrows", "Gary Barlow", "Cold Spring", "Sam Raimi", "lambics", "hired translators to eavesdrop on a series of conversations in Arabic, Russian and Mandarin", "1-1 draw", "Wednesday", "gold", "bassoon", "J.S. Bach", "Taoism"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6213600490944241}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true], "QA-F1": [0.14814814814814814, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428572, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.0, 0.4, 0.4, 0.923076923076923, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.8, 1.0, 0.5, 0.0, 0.4444444444444445, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.12500000000000003, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8950", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-8217", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-234", "mrqa_triviaqa-validation-2705", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3591", "mrqa_newsqa-validation-3200", "mrqa_searchqa-validation-13034"], "SR": 0.46875, "CSR": 0.5162828947368421, "EFR": 0.8823529411764706, "Overall": 0.6765240421826625}, {"timecode": 95, "before_eval_results": {"predictions": ["Julius Caesar", "the liver", "Izzie Pick", "the Phantom of the Opera", "Otis Elevator Company", "Desert Pacific", "seabirds", "chicken pot pie", "Red Bull", "Dred Scott", "Birmingham", "trousseau", "a constitution", "rice", "a cad", "Nixon", "Canada", "Ellis Island", "solid", "Marie Antoinette", "France", "Thomas Stearns Eliot", "a tip-off", "Dustin Hoffman", "Captain James Cook", "Monopoly", "Frank Sinatra", "Handel", "Agatha Christie", "Ponce de Len", "Mr. Rogers", "bullion", "the Met", "Will Hunting", "Lebanon", "Puccini", "Arby\\'s", "a girl", "Wayne Gretzky", "October", "Pentecost", "Herod the Great", "Athens", "a rattlesnake", "World War II", "World War I", "Madison County", "John Jay", "Bojangles", "Jerry Garcia", "Phoenix", "behaves as an antagonist ( a substance that binds to a receptor but does not activate and can block the activity of other agonists )", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "April 26, 2005", "India", "The Watsons", "polish", "April 20, 1945", "Bishop\\'s Stortford", "My Cat from Hell", "Ricardo Valles de la Rosa,", "Sheikh Sharif Sheikh Ahmed", "Thursday.", "\"Thrilla in Manila\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.7069940476190476}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9142857142857143, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-13671", "mrqa_searchqa-validation-3143", "mrqa_searchqa-validation-15121", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-16186", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-6320", "mrqa_searchqa-validation-15613", "mrqa_searchqa-validation-11992", "mrqa_searchqa-validation-5579", "mrqa_searchqa-validation-11176", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-15715", "mrqa_searchqa-validation-7798", "mrqa_naturalquestions-validation-9749", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2624", "mrqa_hotpotqa-validation-4153", "mrqa_newsqa-validation-3181", "mrqa_triviaqa-validation-7401"], "SR": 0.609375, "CSR": 0.5172526041666667, "EFR": 1.0, "Overall": 0.7002473958333334}, {"timecode": 96, "before_eval_results": {"predictions": ["weather", "robert", "queen Elizabeth I", "jack Russell Terrier", "ruda", "man of manscothick", "Charlie Chan", "the Beatles", "mansowieckie", "pygmalion", "The Green Mile", "Richie Unterberger", "a Brat Pack", "atlas", "Mase", "december", "space Oddity", "Mikhail S. Gorbachev", "objects", "alberta", "dark", "rhododendron", "man of Congo,", "rugby", "the Blind Beggar", "man", "piglet's house", "haddock", "hons", "You're going to lose that girl", "antelope", "philias", "little dorrit", "William Butler Yeats", "spamalot", "wolf", "horseshoe", "horses", "kendo", "cabbage", "botulism", "small faces", "St. Louis", "a paddington bear", "Benfica", "30", "The Left Book Club", "The Iron Duke", "Virginia", "jocky Wilson", "\"Bubba\"", "the Ramones", "Sir Hugh Beaver", "the Seventeenth Amendment", "Aamir Khan", "1945", "Delacorte Press", "clogs", "Rany Freeman,", "probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives.", "man of war", "the small intestine", "Islamic", "Gene Wilder"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6609827898550724}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8695652173913044, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4461", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-7372", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-6161", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-7093", "mrqa_naturalquestions-validation-3848", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-98", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-1553"], "SR": 0.59375, "CSR": 0.518041237113402, "retrieved_ids": ["mrqa_squad-train-82296", "mrqa_squad-train-2350", "mrqa_squad-train-16314", "mrqa_squad-train-76362", "mrqa_squad-train-43490", "mrqa_squad-train-48657", "mrqa_squad-train-48146", "mrqa_squad-train-81325", "mrqa_squad-train-58575", "mrqa_squad-train-32991", "mrqa_squad-train-16433", "mrqa_squad-train-48292", "mrqa_squad-train-71836", "mrqa_squad-train-39243", "mrqa_squad-train-33362", "mrqa_squad-train-84218", "mrqa_newsqa-validation-4161", "mrqa_searchqa-validation-15456", "mrqa_hotpotqa-validation-3535", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-9005", "mrqa_newsqa-validation-3181", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2473", "mrqa_triviaqa-validation-2036", "mrqa_searchqa-validation-10175", "mrqa_hotpotqa-validation-2120", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-1840", "mrqa_searchqa-validation-10474"], "EFR": 1.0, "Overall": 0.7004051224226804}, {"timecode": 97, "before_eval_results": {"predictions": ["Julius caesar", "Mary Pickford", "the raffia", "roux", "the Netherlands", "pfeffernuesse", "Vienna", "Lake Champlain", "one", "dlre", "Slavic culture", "Mars", "Jim Jeffords", "Space Cadet", "Fen-phen", "Candice Bergen", "Andrew Johnson", "Venice", "Hairspray", "Tina Brown", "the Lone Ranger", "a fuel cell", "OK Go", "Ruben Amaro", "Bay of Fundy", "Savannah", "a Picts", "Pearl Jam", "wood", "Gaius Cassius Longinus", "Bob Dylan", "Maria Montessori", "microwave", "Peter Shaffer", "turquoise", "Casablanca", "White", "a potato", "the Chili Peppers", "Woody Guthrie", "thyroid", "Plutarch", "Hephaestus", "Iraq", "Whatchamacallit", "Tuscaloosa", "Pino Pino", "Mountain Dew", "tabula rasa", "pomegranate", "a mortar", "December 1, 2017", "Glenn Close", "diastema", "nairobi", "Heart of Oak", "Robinson Crusoe", "Galleria Vittorio Emanuele II", "51st", "Araminta Ross", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "football", "three", "Sunday,"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7572916666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14271", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-9329", "mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-14503", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-9453", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-12929", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-11766", "mrqa_naturalquestions-validation-3553", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2386"], "SR": 0.703125, "CSR": 0.5199298469387755, "EFR": 1.0, "Overall": 0.7007828443877551}, {"timecode": 98, "before_eval_results": {"predictions": ["the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah", "1804", "1987", "Guwahati", "parthenogenesis", "2018", "October 15, 1997", "Cheap Trick", "Massachusetts", "the base of the right ventricle", "the lumbar cistern", "Kingsford, Michigan", "taxonomy", "During Hanna's recovery masquerade celebration", "as - yet - unknown purpose", "In `` One Son ''", "RMS Titanic", "the British", "Matt Monro", "nearby objects show a larger parallax than farther objects when observed from different positions", "Certificate of Release or Discharge from Active Duty", "spot - type", "20 year - old Kyla Coleman", "Ben Faulks", "seven", "between 770,000 and 1.7 million nerve fibers", "Roger Nichols", "during the 1890s Klondike Gold Rush", "Ali", "71 -- 74 \u00b0 C ( 160 -- 165 \u00b0 F )", "Khrushchev", "1960", "John J. Flanagan", "France", "a `` part - Samoyed terrier", "Americans who served in the armed forces and as civilians during World War II", "Melissa Disney", "H CO ( equivalently OC ( OH ) )", "Tatsumi", "Pradyumna", "a medium", "Speaker of the House of Representatives", "the homicidal thoughts of a troubled youth", "Wales", "Buddhism", "UNICEF's global programing", "The Maidstone Studios in Maidstone, Kent", "between 3.9 and 5.5 litres / L ( 70 to 100 mg / dL )", "adenosine diphosphate", "Graham McTavish", "Orangeville, Ontario", "france", "Edward lear", "photographer", "Attorney General and as Lord Chancellor of England", "A1 Recordings", "churros", "at least 300", "2002", "U.S. Secretary of State Hillary Clinton,", "a topaz", "Mozart", "cuddle", "New York City Mayor Michael Bloomberg"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7234486867299368}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.25, 0.0, 0.0, 0.7272727272727272, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.14285714285714288, 0.46153846153846156, 0.0, 0.5714285714285715, 1.0, 0.5, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5714285714285715, 1.0, 0.8571428571428572, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-2966", "mrqa_naturalquestions-validation-6365", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-10260", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-2380", "mrqa_newsqa-validation-2408", "mrqa_searchqa-validation-11997", "mrqa_newsqa-validation-2212"], "SR": 0.578125, "CSR": 0.5205176767676767, "EFR": 0.8148148148148148, "Overall": 0.6638633733164983}, {"timecode": 99, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1847", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2572", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-2722", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3427", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4010", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4591", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5105", "mrqa_hotpotqa-validation-5270", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5842", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-5861", "mrqa_hotpotqa-validation-628", "mrqa_hotpotqa-validation-713", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10330", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3026", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-540", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6588", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-7065", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-9123", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9360", "mrqa_naturalquestions-validation-9392", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-99", "mrqa_newsqa-validation-1024", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1395", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1821", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-1870", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-204", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-830", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10005", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10355", "mrqa_searchqa-validation-10422", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10803", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-10947", "mrqa_searchqa-validation-11027", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-11281", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-11722", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12271", "mrqa_searchqa-validation-12557", "mrqa_searchqa-validation-12620", "mrqa_searchqa-validation-12647", "mrqa_searchqa-validation-12717", "mrqa_searchqa-validation-12853", "mrqa_searchqa-validation-12858", "mrqa_searchqa-validation-1297", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-14609", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-15291", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15669", "mrqa_searchqa-validation-15703", "mrqa_searchqa-validation-15803", "mrqa_searchqa-validation-15980", "mrqa_searchqa-validation-16185", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-16823", "mrqa_searchqa-validation-16942", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-2034", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-2390", "mrqa_searchqa-validation-2466", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-2909", "mrqa_searchqa-validation-2926", "mrqa_searchqa-validation-2990", "mrqa_searchqa-validation-3002", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3323", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-3448", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-4105", "mrqa_searchqa-validation-4162", "mrqa_searchqa-validation-4233", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-4478", "mrqa_searchqa-validation-4935", "mrqa_searchqa-validation-5234", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5642", "mrqa_searchqa-validation-5797", "mrqa_searchqa-validation-5806", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6060", "mrqa_searchqa-validation-6237", "mrqa_searchqa-validation-6322", "mrqa_searchqa-validation-6357", "mrqa_searchqa-validation-6631", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7319", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-7899", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-8616", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-9519", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-976", "mrqa_squad-validation-10333", "mrqa_squad-validation-7021", "mrqa_squad-validation-7080", "mrqa_squad-validation-8224", "mrqa_squad-validation-9899", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1414", "mrqa_triviaqa-validation-1460", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-2245", "mrqa_triviaqa-validation-2345", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3214", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-3554", "mrqa_triviaqa-validation-3571", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-3848", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4140", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-460", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5574", "mrqa_triviaqa-validation-5583", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-6215", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-631", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6592", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6869", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7069", "mrqa_triviaqa-validation-7106", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-801", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-995"], "OKR": 0.849609375, "KG": 0.5359375, "before_eval_results": {"predictions": ["absolute temperature", "optical smoke detector", "May 5, 1904", "Western cultures", "March 31, 2013", "October 27, 1904", "the courts", "in April 2011", "Pakhangba", "4 January 2011", "pulmonary heart disease ( cor pulmonale )", "Billie `` The Blue Bear '', a German ex-prostitute who has a reputation as a dirty fighter", "4 percent cumulative effect", "a solitary figure who is not understood by others, but is actually wise", "13", "Nueva Vizcaya", "the Supreme Court of Canada", "honey, tree and vine fruits, flowers, berries, and most root vegetables", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Bhupendranath Dutt", "October 2012", "Frederik Barth", "the Mishnah", "revolution or orbital revolution", "1979", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "2,000 mm ( 78.7 in )", "Billy Idol", "early 1974", "Patris et Filii et Spiritus Sancti", "Tanvi Shah", "administrative supervision over all courts and the personnel thereof", "interstellar medium", "Dr. Rajendra Prasad", "After the Reform Act of 1832", "Kiss", "775", "season four", "Akshay Kumar", "Mahatma Gandhi", "across western North Carolina including Asheville, Cashiers and Saluda", "Michael Crawford", "12 '' x 12 '' attached giant - sized booklet with state - of - the - art photography of the band's performance and outdoor session pictures", "Michael Phelps", "Matt Monro", "8 January 1999", "Henry Selick", "1", "94 by 50 feet", "a subduction zone", "2004", "Rudolf Hess", "alzheimer's", "arafura Sea", "Little Big League", "40 Days and 40 Nights", "The Handmaid\\'s Tale", "crocodile eggs", "1994", "the two-state solution to the Mideast", "grave", "a circle", "A Room With a View", "Bromley-By-Bow"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7231484661172161}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-10529", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-8737", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-1857", "mrqa_hotpotqa-validation-5478", "mrqa_newsqa-validation-406", "mrqa_searchqa-validation-708", "mrqa_triviaqa-validation-316"], "SR": 0.671875, "CSR": 0.52203125, "retrieved_ids": ["mrqa_squad-train-61367", "mrqa_squad-train-41688", "mrqa_squad-train-62528", "mrqa_squad-train-46807", "mrqa_squad-train-84305", "mrqa_squad-train-38581", "mrqa_squad-train-44518", "mrqa_squad-train-2906", "mrqa_squad-train-45025", "mrqa_squad-train-32075", "mrqa_squad-train-77763", "mrqa_squad-train-68991", "mrqa_squad-train-52400", "mrqa_squad-train-30366", "mrqa_squad-train-19112", "mrqa_squad-train-35779", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-3404", "mrqa_searchqa-validation-4054", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-3406", "mrqa_hotpotqa-validation-2744", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-3340", "mrqa_newsqa-validation-47", "mrqa_searchqa-validation-5136", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-7287", "mrqa_naturalquestions-validation-3208", "mrqa_searchqa-validation-14352"], "EFR": 0.9523809523809523, "Overall": 0.7227730654761905}]}