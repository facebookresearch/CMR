{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5210, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Jelme and Bo'orchu", "gauge bosons", "consumer prices", "Albert C. Outler", "a computational problem", "1521", "River Tyne", "Boston", "San Jose Marriott", "illegal boycotts", "Mitochondria", "bilaterians", "Alexandre Yersin", "Methodists today", "Beyonc\u00e9", "the Rhine and its downstream extension", "7\u20134\u20132\u20133", "Horniman Museum", "400 AD to 1914", "early 1526", "The individual is the final judge of right and wrong", "five", "Battle of B\u1ea1ch \u0110\u1eb1ng", "Time Lady", "oxygen-16", "The Day of the Doctor", "Sierra Sky Park", "James Clerk Maxwell", "Bill Clinton", "in areas its forces occupied in Eastern Europe", "20,000", "Queen Elizabeth II", "The Daleks", "gas turbines", "Newton", "Miasma theory", "Ealy", "several medals", "remaining in black and white", "computability theory", "autoimmune", "American Sweetgum", "Pleistocene epoch", "Feynman diagrams", "orange", "oxygen compounds", "four", "Fort Caroline", "counties or powiats", "chemical bonds", "2015", "France's claim to the region was superior to that of the British", "double", "helps many proteins bind the polypeptide", "Islamism", "lines or a punishment essay", "mercuric oxide", "released Islamists from prison and welcomed home exiles", "Washington and Thomas Gage", "The individual", "Bruno Mars", "the public", "Thomas Edison and George Westinghouse", "a freshwater lake"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8040364583333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6848", "mrqa_squad-validation-9923", "mrqa_squad-validation-100", "mrqa_squad-validation-9434", "mrqa_squad-validation-6966", "mrqa_squad-validation-7725", "mrqa_squad-validation-8538", "mrqa_squad-validation-1703", "mrqa_squad-validation-3511", "mrqa_squad-validation-3597", "mrqa_squad-validation-9567", "mrqa_squad-validation-6973", "mrqa_squad-validation-2025"], "SR": 0.796875, "CSR": 0.796875, "EFR": 0.8461538461538461, "Overall": 0.8215144230769231}, {"timecode": 1, "before_eval_results": {"predictions": ["the General Sejm", "232", "New Holland", "the \"Rhine knee\"", "the U.S. South", "the Schmalkaldic League", "January 1985", "an Executive Committee", "King Sancho VI of Navarre", "the Seattle Seahawks", "36", "Chloroplasts", "Sydney", "the Panic of 1901", "Muslim medicine", "the Silk Road", "silicon dioxide", "statocyst", "Several thousand", "the Fourth Intercolonial War and the Great War for the Empire", "medieval", "30\u201360% of Europe's total population", "the laws of physics", "the Ten Commandments", "the San Fernando Valley", "Roger NFL", "Hugh L. Dryden", "metals", "1.5 gigatons", "Denver Broncos", "\u00a31 of capital", "the 2007 Christmas special episode, \"Voyage of the Damned\"", "megaprojects", "1024-bit primes", "the portrait of Fran\u00e7ois, Duc d'Alen\u00e7on by Fran\u00e7ois Clouet, Gaspard Dughet", "rice, coffee, sisal, pyrethrum, corn, and wheat", "the Electorate of Saxony", "ice-sheets", "the lion, leopard, buffalo, rhinoceros, and elephant", "A plea of no contest is sometimes regarded as a compromise between the two", "seven", "Demaryius Thomas", "Napoleon's", "the Santa Clara Marriott", "kinematic measurements", "shipping", "12th", "helical thylakoid model", "the A69", "14%", "Thomas Edison", "Toshiba", "detention", "antigen presentation", "hunter's garb", "The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League", "British Gas plc", "Demaryius Thomas", "alcohol", "Joe Scarborough", "win world titles in four weight classes", "Gareth Jones", "Sivakumar, S. V. Subbaiah, Jayachitra, Srividya, Shubha, Kamal Haasan and Jayasudha", "the Rev. Jacquie Hood Martin"], "metric_results": {"EM": 0.734375, "QA-F1": 0.776943108974359}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.25000000000000006, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-1117", "mrqa_squad-validation-239", "mrqa_squad-validation-3664", "mrqa_squad-validation-85", "mrqa_squad-validation-4482", "mrqa_squad-validation-7758", "mrqa_squad-validation-8978", "mrqa_squad-validation-5490", "mrqa_squad-validation-8446", "mrqa_squad-validation-8278", "mrqa_squad-validation-6914", "mrqa_squad-validation-7155", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-2275"], "SR": 0.734375, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 2, "before_eval_results": {"predictions": ["Ugali with vegetables, sour milk, meat, fish or any other stew", "TFEU article 294", "over $20 billion", "was a major source of water pollution", "unity of God", "all war", "1000 and 1900", "Gamal Abdul Nasser", "viniculture and tourism", "minor", "1162", "the metal locking screw on the camera lens", "ABC Cable News", "22 May 2006", "Germany and Austria", "Golden Gate Bridge", "the Welsh", "acquiring nutrients", "The Muslims in the semu class", "the Chinese", "temperature and light", "12 May 1999", "1852", "the development of safety lamps", "stabilize the rest of the chloroplast genome", "The Mongols' extensive West Asian and European contacts", "24%", "the Milton Friedman Institute", "Donald Davies", "three", "student motivation and attitudes towards school", "1560", "1891", "Lutheran views", "an electron", "fear of their lives", "Science", "John Cardinal, Lord of Pelham Manor", "Cam Newton", "Osama bin Laden", "international drug suppliers", "President", "expelled", "arid and semi-arid", "Yosemite Freeway", "Annan and his UN-backed panel and African Union chairman Jakaya", "The Warsaw Stock Exchange", "the Presiding Officer submits it to the Monarch for royal assent", "certification by a recognized body", "a chain or screw stoking mechanism", "the Battle of B\u1ea1ch \u0110\u1eb1ng (1288)", "silver", "1947", "1860", "The club will participate in the Premier League, FA Cup, EFL Cup (as holders), UEFA Champions League and UEFA Super Cup.", "Detroit, Michigan", "the Emancipation Proclamation", "26,000", "Pakistan", "Ricky Skaggs", "Saturday Night Live", "the last living pilot of the X-15 program", "Jason Chaffetz is a conservative Republican married father of three who is sleeping on a cot in his congressional office to save money", "250,000"], "metric_results": {"EM": 0.734375, "QA-F1": 0.8162252376246941}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.9523809523809523, 1.0, 0.3636363636363636, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.1818181818181818, 0.17391304347826084, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9343", "mrqa_squad-validation-9093", "mrqa_squad-validation-1506", "mrqa_squad-validation-9388", "mrqa_squad-validation-5157", "mrqa_squad-validation-3319", "mrqa_squad-validation-8399", "mrqa_squad-validation-4562", "mrqa_squad-validation-8383", "mrqa_squad-validation-9499", "mrqa_squad-validation-8222", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-368"], "SR": 0.734375, "CSR": 0.7552083333333334, "EFR": 1.0, "Overall": 0.8776041666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["10 February 1763", "good, clear laws, fairly and democratically", "shaping ideas about the free market", "SAP Center in San Jose", "older", "the university and military academy", "Foreign Protestants Naturalization Act", "inequality", "jiggle TV", "three", "permafrost", "Silas B. Cobb", "the traditional salute of a knight winning a bout", "Jane Kim", "the Presiding Officer", "lipophilic alkaloid toxins", "one astronaut", "William Rainey Harper", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "June", "the Protestant cause with politics unpopular in France", "US President Barack Obama chose not to visit the country", "Ralph Woodward", "Susan Foreman", "clinical pharmacists", "a \"teleforce\" weapon", "The British failures in North America, combined with other failures in the European theater", "300", "66 million years ago", "in southern China in 1865", "commerce, schooling and government", "Krak\u00f3w", "France", "three", "a power outage", "easier and more efficient than anywhere else", "Muslim and Chinese", "free trade", "15,100", "Cuba", "high pressure shock waves", "28,000", "21 to 11", "Cam Newton", "128,843", "Van Gend en Loos v Nederlandse Administratie der Belastingen", "four years", "Howard Keel", "half of the Pangaea supercontinent", "Mel Gibson", "George Washington", "John Uhler Lemmon III", "six", "The words 'broccoli' and 'calabrese", "The Best Of Billy Preston", "Henry Kelly", "hedgehogs", "George III", "The Time Machine", "the Oil Capital of Europe", "the natural world and mysticism", "more funds", "the U.S. military", "$1.45 billion"], "metric_results": {"EM": 0.6875, "QA-F1": 0.744683614996115}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1831", "mrqa_squad-validation-4971", "mrqa_squad-validation-6223", "mrqa_squad-validation-9570", "mrqa_squad-validation-3044", "mrqa_squad-validation-8326", "mrqa_squad-validation-4975", "mrqa_squad-validation-1830", "mrqa_squad-validation-4065", "mrqa_squad-validation-973", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-1517", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-1148"], "SR": 0.6875, "CSR": 0.73828125, "retrieved_ids": ["mrqa_squad-train-69052", "mrqa_squad-train-61669", "mrqa_squad-train-4803", "mrqa_squad-train-2255", "mrqa_squad-train-55304", "mrqa_squad-train-58643", "mrqa_squad-train-45040", "mrqa_squad-train-67294", "mrqa_squad-train-74460", "mrqa_squad-train-32848", "mrqa_squad-train-65385", "mrqa_squad-train-82849", "mrqa_squad-train-8514", "mrqa_squad-train-13656", "mrqa_squad-train-34983", "mrqa_squad-train-83236", "mrqa_hotpotqa-validation-4018", "mrqa_squad-validation-4482", "mrqa_hotpotqa-validation-532", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-6973", "mrqa_squad-validation-100", "mrqa_squad-validation-8978", "mrqa_squad-validation-8446", "mrqa_squad-validation-8399", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-2423", "mrqa_squad-validation-9499", "mrqa_squad-validation-3319", "mrqa_squad-validation-9093", "mrqa_squad-validation-7725"], "EFR": 1.0, "Overall": 0.869140625}, {"timecode": 4, "before_eval_results": {"predictions": ["vote clerk", "estimated $200,000", "carbohydrates", "redistributive taxation", "the League of Nations", "two", "The Time of the Doctor", "Nairobi", "Missy", "free", "7:00 to 9:00 a.m.", "Professor Richard ( Dick) Geary", "lipid monolayer", "2009", "whether a state or threat of war existed", "the courts of member states", "Jin", "greater equality but not per capita income", "John Houghton", "carbohydrates", "immediately", "America's Funniest Home Videos", "42%", "19", "specialised education and training", "layered basaltic lava flows", "October 2007", "Robert Maynard Hutchins", "Shoushi Li", "clerical marriage", "40%", "Kevin Harlan", "200 Troupes de la marine and 30 Indians", "Half", "Independence Day: Resurgence", "duty", "carbon dioxide", "Worldvision Enterprises", "hunter's garb", "Channel Islands", "\"Blue Harvest\" and \"420\"", "1225", "739 georgie", "a traceable farm assured sheep that is at least two years of age has been finished", "andrew jackson", "\"Hey there Delilah, I know... God speed your love to me\"", "Octavian", "the Biblioteca Nacional Jose Marti", "Warren E. Burger", "nouns that modify them", "five reeds on the treble side and three on the bass", "comet man", "giant cephalopod", "andrew jackson Duchamp", "a pigeon named 'Narco Paloma'", "andrewhere", "the Navy blockaded the enemy coast, bombarded his shore fortresses", "Yogi Berra", "New York City", "Britomart", "Kerris Lilla Dorsey", "Islamabad", "Jefferson Memorial", "he was arrested earlier in the afternoon after a traffic stop south of the city, police said."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7045693277310925}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.23529411764705882]}}, "before_error_ids": ["mrqa_squad-validation-2491", "mrqa_squad-validation-8234", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-2118", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-14197", "mrqa_newsqa-validation-839"], "SR": 0.6875, "CSR": 0.728125, "EFR": 1.0, "Overall": 0.8640625}, {"timecode": 5, "before_eval_results": {"predictions": ["Justifying Grace", "neuronal dendrites", "an electrical generator", "Doctor Who", "coronary thrombosis", "his grandson", "San Francisco", "consumer prices", "2016", "as soon as they enter into force", "colonizing empires", "the \"hockey stick graph\"", "OpenTV", "intuition", "1720", "around 300", "2001", "The Chase", "cortisol and catecholamines", "Economist Intelligence Unit", "in simpler, more personal, Trinitarian language", "Paramount Pictures", "Thomas Coke", "The Neighbors", "the waldzither", "the United States", "\u20ac53,423", "the Helicosproidia", "the public network to reach locations not on the private network", "2001", "the High Rhine", "Justin Tucker", "Colorado Springs", "2004", "Newton", "26", "University College London", "Jerricho Cotchery", "(Freddy and the Ignormus)", "the cube root of a negative number", "Willa Cather", "manganese", "(Lewis and Clark)", "Eisenhower", "pope", "manganese", "a Fokker", "(Olav Trygvason)", "(2008)", "Ian Fleming", "manganese", "The Thing", "manganese", "a donkey", "the Bolshevik party", "Charlotte Russe", "(and later, into the evil G Kessler)", "The Tale of Genji", "The Daily Show", "Ant & Dec", "manganese", "Cherokee Nation", "manganese", "UFC 50: The War of '04"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6408854166666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999]}}, "before_error_ids": ["mrqa_squad-validation-6267", "mrqa_squad-validation-9865", "mrqa_squad-validation-7827", "mrqa_squad-validation-2391", "mrqa_squad-validation-4874", "mrqa_squad-validation-5214", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-8713", "mrqa_triviaqa-validation-3042", "mrqa_newsqa-validation-1583", "mrqa_hotpotqa-validation-1190"], "SR": 0.59375, "CSR": 0.7057291666666667, "EFR": 1.0, "Overall": 0.8528645833333334}, {"timecode": 6, "before_eval_results": {"predictions": ["ten", "the force of gravity", "Von Miller", "a downward pressure on wages", "Catholic", "nine", "11:28", "chest pains", "March 1896", "T cells", "economically", "private networks were often connected via gateways to the public network to reach locations not on the private network", "the college", "Yes\u00fcgei", "research", "toward the end of his life", "Bill Clinton", "Ollie Treiz", "esoteric", "San Andreas Fault", "30%\u201350% O2", "a double coronation", "ESPN", "p", "a plantar fasciitis injury in his heel", "Peter Capaldi", "6000 Da", "Queen Victoria and Prince Albert", "cortisol and catecholamines", "Manakintown", "1985", "a stronger, tech-oriented economy", "stream capture", "the general number field sieve", "Victor Mejia Munera", "The Bronx County District Attorneys Office", "two women", "Nothing But Love", "a man's lifeless, naked body", "Pakistan", "a legitimate forum for prosecution", "Ronnie White", "8 p.m. local time", "the New Jersey Economic Development Authority's 20% tax credit", "diabetes and hypertension", "six", "a city of romance", "Israel", "2009", "said, \"It has never been the policy of this president or this administration to torture.\"", "Himalayan", "Wednesday", "2,000 euros", "Siri", "88", "Sesame Street", "0300", "Russia", "Representatives", "Bangladesh", "Argentinian", "a lion", "Department of Homeland Security", "Moldova"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6615898569023568}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.5, 0.5454545454545454, 0.0, 1.0, 0.962962962962963, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6008", "mrqa_squad-validation-2122", "mrqa_squad-validation-3687", "mrqa_squad-validation-9213", "mrqa_squad-validation-620", "mrqa_squad-validation-6696", "mrqa_squad-validation-3193", "mrqa_squad-validation-2835", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3491", "mrqa_naturalquestions-validation-3569", "mrqa_triviaqa-validation-2240", "mrqa_hotpotqa-validation-985", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-9754"], "SR": 0.546875, "CSR": 0.6830357142857143, "retrieved_ids": ["mrqa_squad-train-61216", "mrqa_squad-train-41767", "mrqa_squad-train-58588", "mrqa_squad-train-7557", "mrqa_squad-train-75445", "mrqa_squad-train-79289", "mrqa_squad-train-45670", "mrqa_squad-train-69721", "mrqa_squad-train-66972", "mrqa_squad-train-10095", "mrqa_squad-train-43197", "mrqa_squad-train-77330", "mrqa_squad-train-62205", "mrqa_squad-train-73858", "mrqa_squad-train-33274", "mrqa_squad-train-80063", "mrqa_searchqa-validation-8507", "mrqa_squad-validation-1506", "mrqa_squad-validation-4874", "mrqa_squad-validation-9923", "mrqa_searchqa-validation-13963", "mrqa_squad-validation-3664", "mrqa_squad-validation-4482", "mrqa_hotpotqa-validation-1190", "mrqa_squad-validation-5214", "mrqa_searchqa-validation-5158", "mrqa_squad-validation-9570", "mrqa_squad-validation-8446", "mrqa_triviaqa-validation-3502", "mrqa_searchqa-validation-16704", "mrqa_squad-validation-9567", "mrqa_searchqa-validation-3966"], "EFR": 1.0, "Overall": 0.8415178571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["Blaydon Race", "The Central Region", "ten", "viral", "100\u2013150", "late 1886", "\u00a334m per year", "Alvaro Martin", "more efficient solutions", "Schedule 5", "BBC 1", "victory at Fort Niagara successfully cut off the French frontier forts further to the west and south", "cantatas", "January 30", "seven", "\u20ac25,000 per year", "St. Bartholomew's Day massacre", "9th", "principle of equivalence", "The Entertainment Channel", "when they improve society as a whole", "1735", "the incentive for the democratic changes", "The St. Johns River", "Life", "a Serbian Orthodox priest", "Jan Andrzej Menich", "Jane Kim", "~74,000 (BP = Before Present)", "biomass", "1562", "10 a.m.", "Empire of the Sun", "22", "four", "Ross Perot", "Baghdad", "$1.45 billion", "in her home", "35,000", "in a tenement in the Mumbai suburb of Chembur", "sniff out cell phones", "\"The Dr. Phil Show\"", "July 4", "Evan Bayh", "Chinese President Hu Jintao", "April 24", "Idriss Deby", "scientific reasons", "around 1918 or 1919", "the first or second week in April", "Pakistan", "Pakistan", "jazz", "appealed against the punishment", "the new-car market", "Steve Williams", "Ali", "February", "Mickey's PhilharMagic", "Sugar Ray", "Oedipus Rex", "guitar feedback", "(Cd, Cs-137, Co, Pu-238, Ra, Sr,"], "metric_results": {"EM": 0.75, "QA-F1": 0.8234651633089134}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.846153846153846, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.4444444444444445, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10269", "mrqa_squad-validation-2419", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2809", "mrqa_triviaqa-validation-955", "mrqa_hotpotqa-validation-3972"], "SR": 0.75, "CSR": 0.69140625, "EFR": 1.0, "Overall": 0.845703125}, {"timecode": 8, "before_eval_results": {"predictions": ["nolo contendere", "5,984", "Jacksonville", "over-fishing and long-term environmental changes", "applied force", "DuMont Television Network", "Amtrak San Joaquins", "teaching", "the traditional salute of a knight winning a bout.", "a ribosome in the cytosol", "British", "1887", "February 7, 2016", "Cargill Meat Solutions and Foster Farms", "Henry Plitt", "1978", "Von Miller", "about 3.5 billion people", "water", "Electronic Frontier Foundation", "Tyndale Bible", "specialised education and training", "Il milione", "Guo Shoujing", "1908", "1560", "reason", "only \"essentials\"", "30,000", "a Taliban member who had come for the talks about peace and reconciliation,", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "CNN/Opinion Research Corporation", "Al Gore.", "Rima Fakih", "his business dealings for possible securities violations", "summer", "the Dalai Lama's current \"middle way approach,\"", "the U.S. Holocaust Memorial Museum", "a body", "Brian Mabry", "completely changed the business of music", "more than 4,000", "consumer confidence", "autonomy", "1996", "1831", "Russia", "a ruthless cartel whose area of influence includes the eastern state of Veracruz", "clothes that are consistent and accessible", "Arizona", "Muslim festival of Eid al-Adha.", "BET", "FBI Special Agent Daniel Cain,", "the use of torture and indefinite detention", "Mugabe's opponents", "Casalesi Camorra clan", "\"It didn't matter if you were 60, 40 or 20 like I am.", "China", "the first locomotive to have reached that speed.", "Prime Minister Margaret Thatcher and her cabinet", "Mason-Dixon Line Segment.", "1898", "11 : 40 p.m. ship's time", "the 18th century"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7328458694083694}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1212121212121212, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.7272727272727273, 1.0, 0.5, 0.0, 0.16]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8960", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-776", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-1352", "mrqa_naturalquestions-validation-8279", "mrqa_triviaqa-validation-2510", "mrqa_hotpotqa-validation-3165", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-3505"], "SR": 0.671875, "CSR": 0.6892361111111112, "EFR": 0.9523809523809523, "Overall": 0.8208085317460317}, {"timecode": 9, "before_eval_results": {"predictions": ["the Yassa", "An attorney", "The Quasiturbine", "\u22122, \u22124,...", "creates immunological memory", "three", "coal", "pseudorandom", "average teacher salaries", "independently developed the same message routing methodology as developed by Baran.", "lost all influence on other Mongol lands across Asia,", "Temecula and Murrieta", "antibodies", "Korean", "the revolution could only succeed in Russia as part of a world revolution.", "the Williamite war", "the Horn of Africa", "lipid monolayer", "Hymn for the Weekend", "Colonel Monckton,", "Gymnosperms don't require light to form chloroplasts.", "7 January 1943", "1.7 million", "ABC News Now", "Boomer Esiason and Dan Fouts", "Eintracht Frankfurt", "former U.S. secretary of state", "1,073 immigration detainees", "10", "nearly 28 years of rule.", "Islamabad", "Haleigh", "90", "The Zimbabwe Electoral Commission,", "the opening round of the WGC-CA Championship", "terminal brain cancer.", "the conviction of their former president, Milton Obote, Ferdinand Marcos, Anastasio Somoza, Jean-Claude \"Baby Doc\" Duvalier and Mobutu Sese Seko,", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Leaders of more than 30 Latin American and Caribbean nations", "the Beatles", "Citizens are picking members of the lower house of parliament,", "almost 30 tunnels, including the 6.2-mile Moffat Tunnel,", "CNN's", "the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "led the weekend box office,", "The planned Kingdom City project", "Daryeel Bulasho Guud", "United Arab Emirates", "Republican Gov. Bobby Jindal", "Polo because \"it was the sport of kings.", "The military fired warning shots into the air and sprayed water cannons to disperse the crowd.", "made out of either heavy flannel or wool", "Jennifer Aniston, Marta Kauffman, co-creator of the series \"Friends\" and Kristin Hahn,", "The National Infrastructure Program", "The pair appeared relaxed in sneakers and almost matching black suits and joked frequently -- often at each other's expense.", "Pakistan", "William Strauss and Neil Howe", "Transvaginal ultrasonography", "Live and Let Die", "Ben Hogan", "400", "\"Nebo Zovyot\"", "the Chesapeake Bay", "the list of dos & don'ts"], "metric_results": {"EM": 0.53125, "QA-F1": 0.627451268828494}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.07407407407407407, 1.0, 0.125, 1.0, 0.6153846153846153, 0.0, 1.0, 0.631578947368421, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.1714285714285714, 0.6666666666666666, 0.23529411764705885, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8132", "mrqa_squad-validation-10128", "mrqa_squad-validation-9912", "mrqa_squad-validation-8880", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2879", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-2016", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-2219"], "SR": 0.53125, "CSR": 0.6734375, "retrieved_ids": ["mrqa_squad-train-52238", "mrqa_squad-train-11991", "mrqa_squad-train-1762", "mrqa_squad-train-16025", "mrqa_squad-train-34467", "mrqa_squad-train-10174", "mrqa_squad-train-3020", "mrqa_squad-train-48726", "mrqa_squad-train-71051", "mrqa_squad-train-13107", "mrqa_squad-train-57861", "mrqa_squad-train-78805", "mrqa_squad-train-21750", "mrqa_squad-train-58802", "mrqa_squad-train-28091", "mrqa_squad-train-50904", "mrqa_squad-validation-2835", "mrqa_newsqa-validation-2844", "mrqa_squad-validation-7758", "mrqa_newsqa-validation-2983", "mrqa_hotpotqa-validation-3165", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3816", "mrqa_squad-validation-1703", "mrqa_searchqa-validation-2057", "mrqa_squad-validation-5490", "mrqa_newsqa-validation-776", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1218", "mrqa_squad-validation-3597", "mrqa_newsqa-validation-1102"], "EFR": 1.0, "Overall": 0.83671875}, {"timecode": 10, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3412", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4048", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-100", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1886", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-8001", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9754", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10128", "mrqa_squad-validation-10155", "mrqa_squad-validation-10162", "mrqa_squad-validation-10167", "mrqa_squad-validation-1018", "mrqa_squad-validation-10198", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10269", "mrqa_squad-validation-10272", "mrqa_squad-validation-1029", "mrqa_squad-validation-103", "mrqa_squad-validation-10310", "mrqa_squad-validation-10315", "mrqa_squad-validation-10326", "mrqa_squad-validation-10345", "mrqa_squad-validation-1036", "mrqa_squad-validation-10380", "mrqa_squad-validation-10413", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10476", "mrqa_squad-validation-1048", "mrqa_squad-validation-1053", "mrqa_squad-validation-1088", "mrqa_squad-validation-1097", "mrqa_squad-validation-1119", "mrqa_squad-validation-1131", "mrqa_squad-validation-1197", "mrqa_squad-validation-1222", "mrqa_squad-validation-1231", "mrqa_squad-validation-1255", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-139", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1521", "mrqa_squad-validation-1537", "mrqa_squad-validation-1546", "mrqa_squad-validation-1561", "mrqa_squad-validation-1592", "mrqa_squad-validation-1611", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1831", "mrqa_squad-validation-1834", "mrqa_squad-validation-1876", "mrqa_squad-validation-1940", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-20", "mrqa_squad-validation-2048", "mrqa_squad-validation-2048", "mrqa_squad-validation-2087", "mrqa_squad-validation-2116", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2188", "mrqa_squad-validation-2235", "mrqa_squad-validation-2250", "mrqa_squad-validation-2374", "mrqa_squad-validation-239", "mrqa_squad-validation-2391", "mrqa_squad-validation-2403", "mrqa_squad-validation-2419", "mrqa_squad-validation-2422", "mrqa_squad-validation-2447", "mrqa_squad-validation-2462", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2580", "mrqa_squad-validation-2640", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2723", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-2797", "mrqa_squad-validation-282", "mrqa_squad-validation-2835", "mrqa_squad-validation-2848", "mrqa_squad-validation-2870", "mrqa_squad-validation-2873", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-30", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3048", "mrqa_squad-validation-3084", "mrqa_squad-validation-3086", "mrqa_squad-validation-3141", "mrqa_squad-validation-316", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3299", "mrqa_squad-validation-3304", "mrqa_squad-validation-3309", "mrqa_squad-validation-3319", "mrqa_squad-validation-3358", "mrqa_squad-validation-3368", "mrqa_squad-validation-3390", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3511", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3849", "mrqa_squad-validation-3932", "mrqa_squad-validation-3948", "mrqa_squad-validation-4032", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4165", "mrqa_squad-validation-4176", "mrqa_squad-validation-4186", "mrqa_squad-validation-4248", "mrqa_squad-validation-4265", "mrqa_squad-validation-4274", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4413", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4482", "mrqa_squad-validation-4488", "mrqa_squad-validation-4493", "mrqa_squad-validation-4562", "mrqa_squad-validation-4611", "mrqa_squad-validation-4623", "mrqa_squad-validation-4627", "mrqa_squad-validation-465", "mrqa_squad-validation-4698", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-4971", "mrqa_squad-validation-4976", "mrqa_squad-validation-501", "mrqa_squad-validation-506", "mrqa_squad-validation-5079", "mrqa_squad-validation-5113", "mrqa_squad-validation-5133", "mrqa_squad-validation-5150", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5214", "mrqa_squad-validation-5230", "mrqa_squad-validation-5295", "mrqa_squad-validation-5343", "mrqa_squad-validation-5355", "mrqa_squad-validation-5457", "mrqa_squad-validation-5478", "mrqa_squad-validation-5490", "mrqa_squad-validation-5499", "mrqa_squad-validation-55", "mrqa_squad-validation-5544", "mrqa_squad-validation-5563", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5642", "mrqa_squad-validation-5664", "mrqa_squad-validation-567", "mrqa_squad-validation-5698", "mrqa_squad-validation-5708", "mrqa_squad-validation-5762", "mrqa_squad-validation-5820", "mrqa_squad-validation-5835", "mrqa_squad-validation-586", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5978", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6008", "mrqa_squad-validation-6011", "mrqa_squad-validation-6079", "mrqa_squad-validation-6109", "mrqa_squad-validation-6124", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-616", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6223", "mrqa_squad-validation-6247", "mrqa_squad-validation-6267", "mrqa_squad-validation-6273", "mrqa_squad-validation-6284", "mrqa_squad-validation-6350", "mrqa_squad-validation-6362", "mrqa_squad-validation-6382", "mrqa_squad-validation-6421", "mrqa_squad-validation-6452", "mrqa_squad-validation-6475", "mrqa_squad-validation-6509", "mrqa_squad-validation-6535", "mrqa_squad-validation-6561", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6643", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6869", "mrqa_squad-validation-6879", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-7021", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7062", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7250", "mrqa_squad-validation-7306", "mrqa_squad-validation-7474", "mrqa_squad-validation-7521", "mrqa_squad-validation-7540", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7591", "mrqa_squad-validation-7592", "mrqa_squad-validation-7598", "mrqa_squad-validation-7653", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7733", "mrqa_squad-validation-7738", "mrqa_squad-validation-7751", "mrqa_squad-validation-7758", "mrqa_squad-validation-7775", "mrqa_squad-validation-778", "mrqa_squad-validation-7827", "mrqa_squad-validation-7842", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7937", "mrqa_squad-validation-7941", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8023", "mrqa_squad-validation-8028", "mrqa_squad-validation-8066", "mrqa_squad-validation-813", "mrqa_squad-validation-8132", "mrqa_squad-validation-8174", "mrqa_squad-validation-8213", "mrqa_squad-validation-8221", "mrqa_squad-validation-8222", "mrqa_squad-validation-824", "mrqa_squad-validation-8298", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8436", "mrqa_squad-validation-8446", "mrqa_squad-validation-8458", "mrqa_squad-validation-8466", "mrqa_squad-validation-8475", "mrqa_squad-validation-85", "mrqa_squad-validation-8505", "mrqa_squad-validation-8507", "mrqa_squad-validation-8533", "mrqa_squad-validation-8538", "mrqa_squad-validation-855", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8606", "mrqa_squad-validation-8636", "mrqa_squad-validation-8656", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8790", "mrqa_squad-validation-8790", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8836", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8880", "mrqa_squad-validation-890", "mrqa_squad-validation-8941", "mrqa_squad-validation-8960", "mrqa_squad-validation-8962", "mrqa_squad-validation-8978", "mrqa_squad-validation-9008", "mrqa_squad-validation-9101", "mrqa_squad-validation-9144", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9297", "mrqa_squad-validation-9308", "mrqa_squad-validation-9343", "mrqa_squad-validation-9388", "mrqa_squad-validation-9431", "mrqa_squad-validation-9470", "mrqa_squad-validation-9499", "mrqa_squad-validation-9567", "mrqa_squad-validation-9638", "mrqa_squad-validation-9661", "mrqa_squad-validation-9692", "mrqa_squad-validation-973", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9865", "mrqa_squad-validation-9912", "mrqa_squad-validation-9923", "mrqa_squad-validation-9935", "mrqa_squad-validation-9975", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-955"], "OKR": 0.91796875, "KG": 0.44296875, "before_eval_results": {"predictions": ["France", "Turkey", "League of Augsburg", "Marburg Colloquy", "1951", "nearly three hundred years", "enter the priesthood", "32", "Southwest Fresno", "receptions, gatherings or exhibition purposes", "marry one of his wife's ladies-in-waiting", "Prime numbers", "a six membraned chloroplast", "Timucua", "capturing prey", "Presiding Officer", "1521", "around 5 million,", "supervisory church body", "the member state cannot enforce conflicting laws", "Ed Asner", "bitstrings", "\"Quiet Nights,\"", "Friday,", "\"momentous discovery\"", "We Found Love", "five", "\"It was a comment that shouldn't have been made and certainly one that he wished he didn't make.\"", "Karl Kr\u00f8yer", "Austin Wuennenberg,", "133", "Lana Clarkson", "Mad Men", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Ice jams in rivers have been a major factor in the flooding there.", "Barney Stinson", "Mutassim,", "Jennifer Arnold and husband Bill Klein,", "Coptic church in the town of Alexandria was bombed on New Year's Day,", "ambassadors", "they don't feelMisty Cummings has told them everything she knows.", "the picturesque Gamla Vaster neighborhood", "17", "she also told FBI agents Lisa's parents never mentioned anyone wanting to harm them.", "women and breast cancer.", "walk slowly between the rows and rows of headstones, looking for a familiar name.", "e-mails", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "Fullerton, California,", "the BBC's central London offices", "45 minutes, five days a week.", "his past and his future", "Arab Emirates", "Lashkar-e-Tayyiba", "July", "a U.S. military helicopter made a hard landing in eastern Afghanistan,", "Alamodome and city of San Antonio", "February 6, 2005", "Falkland Islands,", "pH values below seven (7)", "\"Lions for Lambs\"", "Efrem Zimbalist Jr.", "better grades", "American funk rock band"], "metric_results": {"EM": 0.625, "QA-F1": 0.6849476304945055}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.47619047619047616, 0.0, 1.0, 1.0, 0.923076923076923, 0.125, 1.0, 0.7, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9047", "mrqa_squad-validation-2468", "mrqa_squad-validation-4272", "mrqa_squad-validation-1676", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-2195", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-976", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1473", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-5158", "mrqa_hotpotqa-validation-1398", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-15716"], "SR": 0.625, "CSR": 0.6690340909090908, "EFR": 0.9166666666666666, "Overall": 0.7487026515151515}, {"timecode": 11, "before_eval_results": {"predictions": ["manned lunar landings", "the $2.25 billion package", "the perceived difficulty of its tune", "zero", "Hymn for the Weekend", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "\"Blue Harvest\" and \"420\",", "$105 billion", "Samarkand", "27", "an occupancy permit", "1774", "circuit switching", "TEU articles 4 and 5", "plastoglobulus", "Pakistan", "San Jose State", "the mouth and pharynx", "local area networks", "erosion", "Kim Clijsters", "African National Congress Deputy President", "12-hour-plus shifts", "Kit of Elsinore", "about 2,000", "the president", "Kim Clijsters", "not feelMisty Cummings has told them everything she knows.", "Sub-Saharan Africa", "short- and medium-range missile tests,", "the man was dead,", "Dr. Jennifer Arnold and husband Bill Klein,", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "U.S. Vice President Dick Cheney as a \"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "parents", "International Polo Club", "a drop in blood pressure, lightheadedness or hives all over.", "UNICEF", "Casey Anthony,", "skilled hacker", "Leo Frank,", "the Niger Delta", "Cash for Clunkers", "Turkey,", "the i report form", "everyone can use solar and renewable energy at home everyday,\"", "41,280", "Natalie Cole", "not", "Fayetteville, North Carolina", "43,000 people", "Rolling Stone", "shark River Park", "\"Stagecoach\"", "Chinese", "The Man", "to ensure party discipline in a legislature", "The wood", "La Toya", "Lincoln Memorial University", "Ant Timpson, Ted Geoghegan and Tim League", "Ural owl", "sackcloth", "over a 20 - year period"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6893272480443533}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.25, 0.4, 0.4444444444444445, 0.1904761904761905, 0.3, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-7226", "mrqa_squad-validation-6248", "mrqa_squad-validation-4789", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3862", "mrqa_naturalquestions-validation-8434", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-319", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-2726", "mrqa_naturalquestions-validation-7253"], "SR": 0.515625, "CSR": 0.65625, "EFR": 0.967741935483871, "Overall": 0.7563608870967742}, {"timecode": 12, "before_eval_results": {"predictions": ["green spaces", "it was developed to explore alternatives to the early ARPANET design and to support network research generally", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "15 June 1899", "Ismail El Gizouli", "by department", "the Rhine-Ruhr region", "the trans-Atlantic wireless telecommunications facility", "a bachelor's degree", "two", "Death wish Coffee", "Huguenot refugees", "Edward Poynter", "Francisco de Orellana", "\" missing self.\"", "Bruno Mars", "Sybilla of Normandy", "the population growth", "plate tectonics", "the liver and kidneys", "InterContinental Hotels Group", "Walter Brennan", "Warren Hastings", "Poems : Series 1", "April 1917", "the New Testament", "Aristotle", "in a thousand years", "a No. 16 seed", "2015", "2018", "help batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship", "Zoe", "16 June", "the head of Lituya Bay in Alaska", "Thomas Jefferson", "India", "request line", "Labour Party", "Roger Dean Stadium", "May 2002", "northern Europe", "a ranking used in combat sports", "the portal tomb", "The National Football League Draft", "mainland greece", "159", "Missouri River", "Brazil", "318", "Kim", "Domhnall Gleeson", "the chest, back, shoulders, torso and / or legs", "Internal epithelia", "religious Hindu musical theatre styles", "Corey Pavin", "duck", "Super Junior", "Nanyue", "18", "Iran", "hockey", "the New York City Russian-Jewish community", "howling"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6384620610367893}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.24000000000000002, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1040", "mrqa_squad-validation-4786", "mrqa_squad-validation-1384", "mrqa_squad-validation-3229", "mrqa_squad-validation-5422", "mrqa_squad-validation-1020", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-6137", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-976", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-4596", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-3679", "mrqa_searchqa-validation-9108"], "SR": 0.546875, "CSR": 0.6478365384615384, "retrieved_ids": ["mrqa_squad-train-15546", "mrqa_squad-train-62869", "mrqa_squad-train-83174", "mrqa_squad-train-66065", "mrqa_squad-train-70459", "mrqa_squad-train-37241", "mrqa_squad-train-18238", "mrqa_squad-train-18128", "mrqa_squad-train-34020", "mrqa_squad-train-2459", "mrqa_squad-train-62488", "mrqa_squad-train-40246", "mrqa_squad-train-34359", "mrqa_squad-train-1595", "mrqa_squad-train-50560", "mrqa_squad-train-62690", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-3451", "mrqa_squad-validation-4065", "mrqa_searchqa-validation-16774", "mrqa_squad-validation-8960", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-3159", "mrqa_squad-validation-9434", "mrqa_newsqa-validation-2070", "mrqa_searchqa-validation-4995", "mrqa_newsqa-validation-439", "mrqa_triviaqa-validation-955", "mrqa_newsqa-validation-2431", "mrqa_squad-validation-9196"], "EFR": 0.9655172413793104, "Overall": 0.7542332559681697}, {"timecode": 13, "before_eval_results": {"predictions": ["Ominde Commission", "2,000", "2005", "the university's off-campus rental policies", "seven", "Thomas Edison and George Westinghouse", "southern Suriname", "sell prescription drugs without requiring a prescription", "automobiles", "landed on the Moon on the last mission", "Cam Newton", "KGPE", "difference in potential energy", "prime elements", "Julia Butterfly Hill", "five", "a coomb or combe", "Vienna", "40", "President Mitterrand", "Spice Girls", "the hose", "Sandi Toksvig, 57", "Salvador Allende", "Paris", "Arkansas", "\"Blind Side\"", "a negative effect on your quality of life", "Dennis Potter", "Burma", "peregrines", "ethephon", "a wild vagrant", "MauritaniaMauritania", "the piano", "Angus Hudson", "Bill Clinton's", "Charlie Sheen", "a full fat pasteurised cow's milk soft cheese", "an udder cell", "Concepcion", "Republic of Upper Volta", "Laurie Lee", "Karl Marx and Friedrich Engels", "John Mortimer", "Beaujolais", "Humphrey Bogart", "Guns N' Roses", "Kansas", "Amy", "Carl Sagan and his wife and co-writer, Ann Druyan", "Alex Turner", "a St. Tropez drag-show nightclub", "commitment", "his brother", "2001 -- 2002 season", "\"Back to December\"", "Michael Lewis Greenwell", "an independent homeland", "an angry mob.", "'Attab of the Omayyad", "an obstetrician", "an Academy Award in the category Best Sound", "International Boxing Federation"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7160984848484849}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-3899", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-4630", "mrqa_hotpotqa-validation-3507", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-530", "mrqa_searchqa-validation-3375", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-47"], "SR": 0.640625, "CSR": 0.6473214285714286, "EFR": 0.9565217391304348, "Overall": 0.7523311335403726}, {"timecode": 14, "before_eval_results": {"predictions": ["Iberia", "contemporary accounts were exaggerations", "lectures", "Victoria", "erosion", "Cricket", "2", "Gabriel Zwilling", "\"The Day of the Doctor\"", "through confirmation and sometimes the profession of faith", "in the city of Deabolis", "Wijk bij Duurstede", "June 6, 1951", "24 September 2007", "18", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "Teri Garr", "Malayalam", "lamb", "the red - bed country of its watershed", "DNA was the genetic material", "Steve Russell", "Pangaea", "Lord's", "Dalveer Bhandari", "boy", "Humphrey Bogart", "Michael Jackson and Lionel Richie", "6 March 1983", "11 : 15 p.m.", "Glenn Close", "Andreas Vesalius", "1959", "in a forest", "2026", "1901", "Gorakhpur", "House of Representatives", "1834", "longitude", "New York University", "The Ecology", "Sophia Akuffo", "Hagrid", "2", "April 29, 2009", "regulatory", "restarting play after a minor infringement", "Pearl Harbor attack", "Wembley Stadium", "1992", "Jonathan Cheban", "blue", "4", "dav davs", "Arabah", "May 27, 2016", "frigate", "\"There are about 100 different types of human papillomavirus,\"", "Lonnie", "a drey", "clinton", "Lake Placid, New York", "Russell T Davies"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6171388507326008}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1188", "mrqa_squad-validation-1076", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-3977", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-11598", "mrqa_hotpotqa-validation-2357"], "SR": 0.53125, "CSR": 0.6395833333333334, "EFR": 0.9333333333333333, "Overall": 0.7461458333333334}, {"timecode": 15, "before_eval_results": {"predictions": ["Writers Guild of America", "every five years", "NFIL3", "826", "Treaty of Logstown", "mid-Cambrian period", "Cabot Science Library, Lamont Library, and Widener Library", "Palestine", "PNU and ODM camps", "The waxy cuticle of many leaves", "2,200", "KMBC-TV and KQTV", "whether he stood by their contents", "jedoublen", "Hudson River", "mutualism", "Helen Keller", "The old-Egyptian zodiac of Dendera", "Southern elephant", "Sally Field", "1908-1993", "the Civil War", "ruby slippers", "jedoublen/jeopardy", "Atreus of Mycenae", "\"Washington's southern tour, 1791\"", "The show was founded and hosted by actor and comedian Jerry Lewis,", "sambal", "Boeing", "Alberta", "Kareem Abdul-Jabbar", "on top of a mountain plateau called Masada", "undercard", "Willa Cather", "a piece of furniture", "Keith Urban", "French", "deer", "jedoublen", "loss of sleep sooner than", "change to a boat in midstream", "parrots, gorillas, and tarantulas", "Rick Springfield", "jabal", "70% isopropyl alcohol", "musical, or pleasant to hear", "Tecumseh,", "phantom limb syndrome", "kabbalah", "palomino", "piedmont", "French Lgion d' Honneur", "Elisabeth", "Rent", "Camping World Stadium in Orlando, Florida", "John Cooper Clarke", "Woodrow Wilson", "Nicola Adams", "1943", "the American rock band Pearl Jam", "\"It appears that they just made those numbers up,\"", "there is not a process to ensure that auto owners comply with recalls.", "two", "16\u201321"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5056962568681318}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.15384615384615385, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6435", "mrqa_searchqa-validation-12940", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-9570", "mrqa_searchqa-validation-8796", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-10319", "mrqa_searchqa-validation-14466", "mrqa_searchqa-validation-4536", "mrqa_searchqa-validation-1446", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-4133", "mrqa_searchqa-validation-16715", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-13578", "mrqa_searchqa-validation-1624", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-2200", "mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-14789", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2358", "mrqa_hotpotqa-validation-5438"], "SR": 0.46875, "CSR": 0.62890625, "retrieved_ids": ["mrqa_squad-train-31506", "mrqa_squad-train-15721", "mrqa_squad-train-72366", "mrqa_squad-train-2479", "mrqa_squad-train-78972", "mrqa_squad-train-30136", "mrqa_squad-train-12416", "mrqa_squad-train-44909", "mrqa_squad-train-162", "mrqa_squad-train-27676", "mrqa_squad-train-21528", "mrqa_squad-train-6561", "mrqa_squad-train-5744", "mrqa_squad-train-34985", "mrqa_squad-train-19989", "mrqa_squad-train-35788", "mrqa_triviaqa-validation-5973", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2590", "mrqa_squad-validation-9434", "mrqa_newsqa-validation-1920", "mrqa_squad-validation-8978", "mrqa_triviaqa-validation-6251", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-4061", "mrqa_squad-validation-9388", "mrqa_naturalquestions-validation-1834", "mrqa_searchqa-validation-16774", "mrqa_squad-validation-1040", "mrqa_squad-validation-8538", "mrqa_hotpotqa-validation-2357", "mrqa_newsqa-validation-1372"], "EFR": 1.0, "Overall": 0.75734375}, {"timecode": 16, "before_eval_results": {"predictions": ["within the Church of England", "Lenin", "a qualified majority vote, if not consensus", "36", "Brough Park in Byker", "2012", "Stress", "the physics", "Jonathan Stewart", "George Westinghouse", "human", "the limited forces it had in New France", "Aquitaine", "the Lord of the Rings", "Florida", "the Tongass National Forest", "the Netherlands", "the Ritz-Carlton", "drink wine", "masks", "the Declassified School Survival Guide", "the Sons of Liberty", "movie house", "National Security Agency", "Ugly Betty", "a shackle for the free", "the Key deer", "The All-New Blue Ribbon Cookbook", "flowers", "Glendale", "Amy Fisher", "A Portrait of the Artist as a Young Man", "a tumbler", "guttural", "polio", "Meg Tilly", "the Mausoleum of Halicarnassus", "King George III", "Annie Braddock", "the catechism of the Council of Trent", "the earth", "(Tharawal) Caves", "KENNY", "Wendy Beckett", "Ferris B Mueller's Day Off", "the catcher", "the Lady is a Tramp", "the Situation Room", "Samuel Goldwyn", "Annika Sorenstam", "an emulsified sauce", "Thurman Munson", "Washington, D.C.", "flip-flops", "the International Border", "1783", "chile", "Ray Robinson", "McComb, Mississippi", "Dorothy Zbornak", "Tutsi ethnic minority", "last week.", "18", "\"Twilight\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5667534722222223}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.08333333333333333, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4216", "mrqa_squad-validation-5456", "mrqa_squad-validation-10388", "mrqa_squad-validation-10158", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1961", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-11531", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-16832", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9957", "mrqa_searchqa-validation-4484", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-2627", "mrqa_hotpotqa-validation-959", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-608"], "SR": 0.453125, "CSR": 0.6185661764705883, "EFR": 1.0, "Overall": 0.7552757352941176}, {"timecode": 17, "before_eval_results": {"predictions": ["49\u201315", "113", "protein structure prediction", "Deformational events", "2004", "Department of State Affairs", "Prague", "were immediately arrested", "governments", "M\u00f6ngke Khan", "by using net wealth (adding up assets and subtracting debts),", "Danny Lee", "snow", "Cyril J. O'Brien", "Romeo and Juliet", "Jane Addams", "Rand McNally", "Harry S. Truman", "the dollar", "Auguste Rodin", "the Andes", "Sherlock Holmes", "the Taj Mittal", "the trampoline", "an axe", "rice", "Constantine", "Daniel Inouye", "krypton", "a Zen monastery", "Kung Fu", "The Star-Crossed Stars of Showgirls", "thecocktail", "The GNTCE", "Milwaukee", "silver", "Bangkok", "the Soviet Union", "a Scotch Cocktail", "Henry V", "a doses", "Frank Sinatra", "Christopher Columbus", "the King of the Hill", "Schtze Benjamin", "Stephen King", "Lord Byron", "Korea", "Joan of Arc", "Jaguar", "the Oompa-Loompas", "a beast", "R. Stanton Avery", "U.S. Bank Stadium", "The Golden Gate Bridge", "Jim hacker", "Jim Branning", "1976", "(IATA: VNO, ICAO: EYVI)", "Cipro, Levaquin, Avelox, Noroxin and Floxin", "murder", "Parlophone Records", "five times", "Nazi concentration camps"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6338541666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2819", "mrqa_squad-validation-6702", "mrqa_squad-validation-7554", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-2546", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-4402", "mrqa_naturalquestions-validation-5674", "mrqa_hotpotqa-validation-3728", "mrqa_newsqa-validation-57", "mrqa_hotpotqa-validation-5499"], "SR": 0.546875, "CSR": 0.6145833333333333, "EFR": 1.0, "Overall": 0.7544791666666666}, {"timecode": 18, "before_eval_results": {"predictions": ["hunter's garb", "from the official declaration of war in 1756 to the signing of the peace treaty in 1763", "two", "a computational problem", "10 to 15 million", "deliver duty as high as 25 million,", "Establishing \"natural borders\"", "his Houston Street lab", "Excellent job opportunities", "\"Turks\" (Muslims) and Catholics", "provides the public with financial information about a nonprofit organization", "the Northeast Monsoon", "April 3, 1973", "Fa Ze", "July 14, 1969", "Krypton", "Mandarin", "in the bone marrow", "Ukraine", "Coldplay", "Yugoslavia", "head coach", "May 19, 2017", "T - Bone Walker", "April 2, 2018", "Rose Stagg ( Valene Kane )", "Doug Diemoz", "Iran", "southern Anatolia", "classical architecture", "the duodenum", "1546", "100,000", "the cella of the Parthenon", "16 seasons", "Long Island", "A lacteal", "Teri Hatcher", "1987", "Elliot Scheiner", "in beer making", "Jikji", "Panning", "the RAF", "Pepsi", "Detective Superintendent Dave Kelly", "Isabela Moner", "Ray Henderson", "provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "New Orleans", "The Outback", "an unmasked and redeemed Anakin Abrams", "Russell Huxtable", "Namibia", "tender", "all-time leader", "Rawlings", "23", "Los Angeles", "Mount Rushmore", "Naples", "Etna", "Edward VI", "North Rhine-Westphalia"], "metric_results": {"EM": 0.5, "QA-F1": 0.5809712047212048}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.962962962962963, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.07999999999999999, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10121", "mrqa_squad-validation-3300", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-3727", "mrqa_hotpotqa-validation-466", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-1828", "mrqa_searchqa-validation-9438"], "SR": 0.5, "CSR": 0.6085526315789473, "retrieved_ids": ["mrqa_squad-train-6829", "mrqa_squad-train-59600", "mrqa_squad-train-72437", "mrqa_squad-train-40315", "mrqa_squad-train-69821", "mrqa_squad-train-51766", "mrqa_squad-train-13476", "mrqa_squad-train-10974", "mrqa_squad-train-41926", "mrqa_squad-train-3394", "mrqa_squad-train-71899", "mrqa_squad-train-42244", "mrqa_squad-train-84985", "mrqa_squad-train-64965", "mrqa_squad-train-56449", "mrqa_squad-train-59575", "mrqa_searchqa-validation-8507", "mrqa_naturalquestions-validation-3505", "mrqa_hotpotqa-validation-532", "mrqa_searchqa-validation-9957", "mrqa_searchqa-validation-5752", "mrqa_newsqa-validation-3451", "mrqa_searchqa-validation-13963", "mrqa_newsqa-validation-3678", "mrqa_searchqa-validation-3038", "mrqa_squad-validation-2391", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-3246", "mrqa_searchqa-validation-5158"], "EFR": 0.9375, "Overall": 0.7407730263157895}, {"timecode": 19, "before_eval_results": {"predictions": ["Thomas Vasey and Richard Whatcoat", "reminding their countrymen of injustice", "the sex offenders register", "Kenya became a republic under the name \"Republic of Kenya\"", "the violence that subsequently engulfed the country", "aristocracy", "1905", "Saul Alinsky", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "U + 002A * Asterisk", "February 1834", "Jonathan Goldstein", "Yahya Khan", "potatoes", "BC Jean and Toby Gad", "Nacio Herb Brown", "Anatomy", "Pyeongchang County, Gangwon Province, South Korea", "Jacqueline MacInnes Wood", "Duckie '' Dale", "2017", "follows a child with Treacher Collins syndrome trying to fit in", "Andaman and Nicobar Islands", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals ), blood plasma and lymph in the '' intravascular compartment ''", "member states", "in the town of Carcassonne in Aude, France", "Rocinante", "1978", "May 1, 2018", "Hans Christian Andersen", "De pictura", "Cheitharol Kummaba", "1", "alveolar", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F ) at Vostok Station", "October 14, 2017", "16 for females and 18 for males", "July 21, 1861", "an EU state ( Italy )", "45 %", "in the Christian biblical canon", "bypasses, to cross major bridges, and to provide direct intercity connections", "Niketa Calame", "Robert E. Lee", "Clarence L. Tinker", "ended Russia's participation in World War I", "Paul", "federal republic", "Thomas Edison", "mitosis", "an oxidant, usually atmospheric oxygen, that produces oxidized, often gaseous products, in a mixture termed as smoke", "pit road speed", "Pyeongchang County, Gangwon Province, South Korea", "the Loop", "Gianni Versace", "David Simon", "1999", "Madrid's Barajas International Airport", "in Seoul", "paratrooper", "an attendant", "three people", "the Dalai Lama", "Christopher Savoie"], "metric_results": {"EM": 0.4375, "QA-F1": 0.615402160578607}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.9473684210526316, 0.8, 0.43750000000000006, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8421052631578948, 1.0, 0.25, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.4615384615384615, 0.7999999999999999, 0.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8370", "mrqa_squad-validation-9640", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-8075", "mrqa_triviaqa-validation-2196", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-84", "mrqa_searchqa-validation-10274", "mrqa_searchqa-validation-2516", "mrqa_newsqa-validation-1535"], "SR": 0.4375, "CSR": 0.6, "EFR": 0.9444444444444444, "Overall": 0.7404513888888888}, {"timecode": 20, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1705", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4329", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-466", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-12", "mrqa_naturalquestions-validation-1436", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3376", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9809", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3578", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10708", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-11360", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11598", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16636", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1924", "mrqa_searchqa-validation-1928", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2887", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-3899", "mrqa_searchqa-validation-4133", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4750", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6233", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9561", "mrqa_searchqa-validation-9570", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10158", "mrqa_squad-validation-10162", "mrqa_squad-validation-10198", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10471", "mrqa_squad-validation-1076", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1188", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1330", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1424", "mrqa_squad-validation-1506", "mrqa_squad-validation-1540", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1611", "mrqa_squad-validation-1703", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1834", "mrqa_squad-validation-1908", "mrqa_squad-validation-1976", "mrqa_squad-validation-2015", "mrqa_squad-validation-2025", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2111", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2250", "mrqa_squad-validation-2395", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2532", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-3001", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3193", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-331", "mrqa_squad-validation-3368", "mrqa_squad-validation-3449", "mrqa_squad-validation-3493", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3626", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3948", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4159", "mrqa_squad-validation-4176", "mrqa_squad-validation-4248", "mrqa_squad-validation-4248", "mrqa_squad-validation-4272", "mrqa_squad-validation-4274", "mrqa_squad-validation-4301", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4686", "mrqa_squad-validation-4698", "mrqa_squad-validation-4765", "mrqa_squad-validation-4789", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-501", "mrqa_squad-validation-5133", "mrqa_squad-validation-5157", "mrqa_squad-validation-5214", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-55", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5664", "mrqa_squad-validation-5715", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5897", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6251", "mrqa_squad-validation-6253", "mrqa_squad-validation-6264", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6435", "mrqa_squad-validation-6452", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7191", "mrqa_squad-validation-7226", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7592", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7751", "mrqa_squad-validation-7775", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7889", "mrqa_squad-validation-7932", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8010", "mrqa_squad-validation-8019", "mrqa_squad-validation-8199", "mrqa_squad-validation-8213", "mrqa_squad-validation-826", "mrqa_squad-validation-8278", "mrqa_squad-validation-8298", "mrqa_squad-validation-830", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8383", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-861", "mrqa_squad-validation-8612", "mrqa_squad-validation-8636", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8786", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9308", "mrqa_squad-validation-9315", "mrqa_squad-validation-9322", "mrqa_squad-validation-9388", "mrqa_squad-validation-9405", "mrqa_squad-validation-9431", "mrqa_squad-validation-9495", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9640", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9865", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1646", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5140", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6531", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-88"], "OKR": 0.880859375, "KG": 0.44375, "before_eval_results": {"predictions": ["the dukes", "saw the Muslim faith as a tool of the devil", "Ed Lee", "40%", "post-World War I", "10,000", "can produce both eggs and sperm at the same time", "\u00d6gedei Khan", "June 24, 1935", "Kohlberg K Travis Roberts", "John McClane", "I Write What I Like", "1910", "between 11 or 13 and 18", "Song Il-gon", "Let's Make Sure We Kiss Goodbye", "Martin \"Marty\" McCann", "Moon Shot", "Paul John Kushner Jr.", "December 17, 1974", "\"The Royal Family\"", "Lord Baden-Powell", "Mike Holmgren", "S7", "S. F. Newcombe", "Donald Richard \"Don\" DeLillo", "University of Nevada, Las Vegas (UNLV)", "a co-op of grape growers,", "South America", "five", "Kramer", "Bank of China ( Hong Kong)", "Andrew \"Combo\" Gascoigne", "Bisexuality", "\"Winnie the Pooh\"", "Excalibur Hotel and Casino", "Rigoletto", "Knoxville, Tennessee", "Americana Manhasset", "Omega SA", "1978", "Pim Fortuyn", "eastern shore of the Firth of Clyde, Scotland,", "Todd McFarlane", "M. Night Shyamalan", "Magdalen College, Oxford", "Eric Allan Kramer", "1969", "Province of New York", "a palace", "Ghana", "Ashley Leggat", "1933", "mind your manners", "Charles Darwin", "Brisbane Road", "Queen Katherine Parr", "his past and his future", "London and Buenos Aires", "Engelbert Humperdinck", "Colorado River", "John Denver", "The Princess Bride", "a republic in W Africa"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6114701704545455}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666665, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.375, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2293", "mrqa_squad-validation-5236", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-456", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-891", "mrqa_naturalquestions-validation-4109", "mrqa_newsqa-validation-3889", "mrqa_triviaqa-validation-6564"], "SR": 0.515625, "CSR": 0.5959821428571428, "EFR": 1.0, "Overall": 0.7317745535714286}, {"timecode": 21, "before_eval_results": {"predictions": ["a touchdown", "\"the disbelieving (Kafir) colonial powers\" working through Turkish modernist Mustafa Kemal Atat\u00fcrk.", "pharmacy practice science and applied information science", "thought it may have been a combination of anthrax and other pandemics", "1912", "rubisco", "the Huguenot rebellions", "Psych", "German", "August 11, 1946", "Protestant Christian", "Queens, New York", "Erreway", "Oakland County", "Madeleine L' Engle", "FAI Junior Cup", "1966", "February 18, 1965", "Sydney", "Cuban descent", "1898", "Tricia Helfer", "Mickey Mouse series characters with 3D computer animation", "Eielson Air Force Base", "Taylor Swift", "Italy", "Tel Aviv University", "the Flamingo Las Vegas", "John of Gaunt", "William Clark Gable", "Kmart", "C. J. Cherryh", "Spanish", "CBS affiliate KWCH-DT", "Axl Rose", "Daniil Shafran", "Sharman Joshi", "1912", "four", "13 October 1958", "70", "3 May 1958", "the Women's World Curling Classic", "Vancouver", "Marlborough", "Fountains of Wayne", "Ubba", "sulfur mustard H or HD blister gas", "The Saturdays", "the Chick tract of the same name", "southwestern", "The Campbell Soup Company", "The Gang", "2010", "outside", "A", "dark, spicy", "Rod Blagojevich", "Iraq", "kriya yoga", "Harley-Davidson", "75", "$60 billion", "can also taste a hamburger and pizza, and drink coffee from a cup,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5504267331932773}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.7777777777777778, 1.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444444, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.24999999999999994]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_squad-validation-9918", "mrqa_squad-validation-5029", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-1287", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-51", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-8220", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2301", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1784", "mrqa_searchqa-validation-7453", "mrqa_searchqa-validation-2193", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-1093"], "SR": 0.421875, "CSR": 0.5880681818181819, "retrieved_ids": ["mrqa_squad-train-81844", "mrqa_squad-train-74832", "mrqa_squad-train-12328", "mrqa_squad-train-84804", "mrqa_squad-train-61181", "mrqa_squad-train-80677", "mrqa_squad-train-41862", "mrqa_squad-train-23510", "mrqa_squad-train-53256", "mrqa_squad-train-65771", "mrqa_squad-train-62391", "mrqa_squad-train-82648", "mrqa_squad-train-68076", "mrqa_squad-train-56666", "mrqa_squad-train-66440", "mrqa_squad-train-63792", "mrqa_newsqa-validation-2", "mrqa_triviaqa-validation-2627", "mrqa_hotpotqa-validation-4982", "mrqa_naturalquestions-validation-2100", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-10732", "mrqa_newsqa-validation-3827", "mrqa_naturalquestions-validation-6214", "mrqa_squad-validation-3229", "mrqa_squad-validation-8383", "mrqa_naturalquestions-validation-3182", "mrqa_hotpotqa-validation-4511", "mrqa_naturalquestions-validation-7857", "mrqa_searchqa-validation-11855", "mrqa_triviaqa-validation-3042", "mrqa_newsqa-validation-3891"], "EFR": 1.0, "Overall": 0.7301917613636364}, {"timecode": 22, "before_eval_results": {"predictions": ["one of the daughters of former King of Thebes, Oedipus", "environmental determinism", "1110 AM", "Articles 106 and 107", "infected corpses", "Cobb Lecture Hall", "Cortina d'Ampezzo", "Eric Edward Whitacre", "1983", "EQT Plaza in Pittsburgh, Pennsylvania", "Hugh Hefner", "Giotto", "Nickelodeon", "Brad Wilk", "Bobby Hurley", "11", "PEN America", "March 19, 2017", "Disney California Adventure", "Anah\u00ed", "Nic Cester", "Anne, Princess Royal", "Harry Robbins \"Bob\" Haldeman", "247,597", "various deities, beings, and heroes", "directed several episodes of the popular sitcom \"Friends\"", "40 million", "Billund, Denmark", "Africa", "William Cavendish", "20 March to 1 May 2003", "23 July 1989", "Kinnairdy Castle", "Javed Miandad", "\"The Catcher in the Rye\"", "Indianapolis Motor Speedway", "Marjorie Jacqueline \"Marge\" Simpson", "May 4, 1924", "Transporter 3", "Richard Street", "drama \"Queen In-hyun's Man\"", "Steve Carell", "Green Chair", "The Frog Prince", "1882", "Germany", "November 10, 2017", "CTV Television Network", "Australia", "Rafael Palmeiro", "Eric Allan Kramer", "Orchard County", "can raise the relative humidity to 100 % and create clouds and, under the right conditions, precipitation", "Reproductive system", "Coroebus of Elis", "\u201cMy Favorite Martian,\u201d", "Disraeli", "Barry McGuigan", "Thursday.", "5 1/2-year-old", "For weeks,", "supersonic", "the Supreme Court", "the Pledge of Allegiance"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6992559523809523}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.8571428571428571, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5636", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-4204", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-3629", "mrqa_naturalquestions-validation-581", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-4189", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-13691"], "SR": 0.578125, "CSR": 0.5876358695652174, "EFR": 1.0, "Overall": 0.7301052989130434}, {"timecode": 23, "before_eval_results": {"predictions": ["Article 17(3)", "G\u00fcy\u00fck", "Department of Justice", "tidal currents", "was sacked by DeMarcus Ware", "The Eleventh Doctor", "Malware", "Claims adjuster", "1995", "Caleb", "Arunachal Pradesh", "1998", "Richard of Shrewsbury", "the Roman Empire", "1985", "Rufus and Chaka Khan", "18", "to form a higher alkane", "Percy Jackson", "Ali", "the final scene of the fourth season", "UMBC", "the customer's account", "the New Croton Reservoir", "Elena Anaya", "January 2018", "as a prison", "biscuit - sized", "Woodrow Wilson", "Commander in Chief of the United States Armed Forces", "Waylon Jennings", "the Italian / Venetian John Cabot", "the New York Yankees", "protect the genome from lethal chemical and physical agents", "Turducken", "balance sheet", "San Francisco Bay", "Mickey Mantle", "left sleeve pocket flap", "1956", "2011", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "April 2011", "David Gahan", "the Italian pignatta", "South Asia", "deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "stuffing", "the fourth C key from left on a standard 88 - key piano keyboard", "New England Patriots", "Tristan Rogers", "novella", "the Brady Bunch", "Oliver Stone", "Mexico", "Apsley George Benet Cherry-Garrard", "the Ecumenical Award", "Brig Gen Augustine Warner Robins", "Florida", "Department of Homeland Security Secretary Janet Napolitano", "Kerstin", "Colorado", "gusto", "Ulysses"], "metric_results": {"EM": 0.609375, "QA-F1": 0.705765775302369}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9411764705882353, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6976744186046512, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8230", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-2830", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2908"], "SR": 0.609375, "CSR": 0.5885416666666667, "EFR": 0.96, "Overall": 0.7222864583333334}, {"timecode": 24, "before_eval_results": {"predictions": ["the first two series", "Doctor in Bible", "c1750", "60%", "a deterministic Turing machine", "King Henry III of England", "Henkel", "Bowie", "George Frideric Handel", "Pol Pot, butcher of Cambodia", "the Pyrenees Mountains", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Motel 6", "a downtown restaurant", "Virginia", "Isaac,", "a crystal ball", "the Houyhnhnm", "the gallbladder", "Thabo Mbeki", "1921", "a dog", "Robert Schumann", "The Benedictine Order", "linda evans", "translator", "King County Executive", "Scotland", "the Penguin", "The Great Victoria Desert", "Mata Hari", "rings", "the Brisbane River", "The Aidensfield Arms", "armada", "Liechtenstein", "the Rolling Stones", "Rodney", "the team consisted of prominent lawyers such as F. Lee Bailey, Robert Kardashian and Alan Dershowitz.", "the UK", "Prokofiev", "the Great War", "treats", "The State of Kansas", "Australia", "EGBDF", "\"Little Red Rented Rowboat\"", "smell", "Saul's son Jonathan, one of those who love David,", "driving Miss Daisy", "the Isles of the Blessed", "Anthropocene", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Darren McGavin as Mr. Parker ( The Old Man )", "Sir Henry Cole", "Scottish", "Sarah Hurst", "July 1, 1916", "Real Madrid", "Jason Chaffetz", "if Gadhafi suffered the wound in crossfire or at close-range", "analog", "Tootsie", "Alexander Haig Jr."], "metric_results": {"EM": 0.5, "QA-F1": 0.5896453373015873}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.7499999999999999, 1.0, 0.6666666666666666, 0.4, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.3, 1.0, 0.5714285714285715, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-1819", "mrqa_triviaqa-validation-5569", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-1702", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-3189", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-1053", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-2385", "mrqa_searchqa-validation-1228"], "SR": 0.5, "CSR": 0.585, "retrieved_ids": ["mrqa_squad-train-37157", "mrqa_squad-train-69835", "mrqa_squad-train-64131", "mrqa_squad-train-16783", "mrqa_squad-train-44649", "mrqa_squad-train-70504", "mrqa_squad-train-11223", "mrqa_squad-train-54114", "mrqa_squad-train-50842", "mrqa_squad-train-38029", "mrqa_squad-train-69353", "mrqa_squad-train-85097", "mrqa_squad-train-3135", "mrqa_squad-train-57004", "mrqa_squad-train-24001", "mrqa_squad-train-67772", "mrqa_naturalquestions-validation-2506", "mrqa_squad-validation-2819", "mrqa_newsqa-validation-2431", "mrqa_triviaqa-validation-5973", "mrqa_naturalquestions-validation-9078", "mrqa_searchqa-validation-1624", "mrqa_searchqa-validation-7116", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1640", "mrqa_squad-validation-8230", "mrqa_squad-validation-9093", "mrqa_searchqa-validation-1013", "mrqa_newsqa-validation-112", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-959"], "EFR": 0.96875, "Overall": 0.723328125}, {"timecode": 25, "before_eval_results": {"predictions": ["commissioned to the north west of the garden the five-storey School for Naval Architects (also known as the science schools), now the Henry Cole Wing", "floor function", "QuickBooks", "Westinghouse Electric", "The Bachelor", "The Statue of Freedom", "2018", "2017 Georgia Bulldogs", "Honor\u00e9 Mirabeau", "England and Wales", "Charles Habib Malik", "radioisotope thermoelectric generator", "1908", "honey bees", "Samantha Jo `` Mandy '' Moore", "Edward Douglass White", "Mangal Pandey", "Woodrow Strode", "Woodrow Wilson", "four", "tissues", "season two", "January 17, 1899", "Heliocentric model of Copernicus", "Left Behind", "British Ultra code", "520", "a concave mirror", "5.7 million", "Vincent Price", "Steve Russell", "Janis Joplin", "1871", "September 9, 2012", "Ahmad Givens", "Haiti", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "October 14, 2017", "2026", "2018", "The Outback", "the President", "January 2004", "the South Pacific Ocean", "eleven", "Master Christopher Jones", "Around 1200", "9.7", "RMS Titanic", "2017", "St. Theodosius Russian Orthodox Cathedral", "2011", "Italy", "to be performed", "george ii", "12 Play", "Indian", "Sofia the First", "software magnate", "Hurricane Gustav", "21", "Kurt Vonnegut", "baptism", "ore"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5747395833333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false], "QA-F1": [0.08333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-8004", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-2781", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7007", "mrqa_hotpotqa-validation-3423", "mrqa_hotpotqa-validation-3842", "mrqa_newsqa-validation-1234", "mrqa_searchqa-validation-9088", "mrqa_searchqa-validation-2294"], "SR": 0.484375, "CSR": 0.5811298076923077, "EFR": 1.0, "Overall": 0.7288040865384615}, {"timecode": 26, "before_eval_results": {"predictions": ["Saudi Arabia", "Thomas Commerford Martin", "seven", "labor inputs (workers)", "1996", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Indraprastha", "John Quincy Adams ( / \u02c8kw\u026anzi / ( listen )", "third season", "Christopher Columbus", "a child with Treacher Collins syndrome trying to fit in", "an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "U.S. Senate", "administrative supervision over all courts and the personnel thereof ''", "in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form, he made the hair more `` wild '' and covered Frieza's body in red fur", "Mandy", "king Harold Godwinson", "during meiosis", "producer Norman Whitfield", "Spanish / Basque", "in early evenings to call ( in spring and summer ) and hunt for food", "not being pushed around by big labels, managers, and agents and being true to yourself creatively", "January 2, 1971", "Hirschman", "in the books of Exodus and Deuteronomy", "Lucknow", "Neuropsychology", "User State Migration Tool ( USMT )", "Michelle to which Effy ended up flatting with Michelle", "May 1, 2018", "291 episodes in Japan, and 276 episodes in the United States originally, although all 291 episodes were later broadcast when content from the first 67 episodes was restored", "naturalization law for the United States, the Naturalization Act of 1790", "flawed democracy", "in Pashto and Persian as \u0647\u0646\u062f\u0648\u06a9\u0634 \u202c", "last Ice Age", "Ren\u00e9 Descartes", "in people and animals that collects and stores urine from the kidneys before disposal by urination", "Confederate", "linda sparrow", "after turning to the dark side of the Force", "Germany", "London, United Kingdom", "Jeff East", "President Yahya Khan", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "6th century AD", "Schoenberg", "3.5 million years old", "111", "when a population temporarily exceeds the long term carrying capacity of its environment", "Mike Leeson & Peter Vale and produced by Josh Deutsch", "amount to a crime and deserve punishment", "Lucille Fay LeSueur", "Principality of Andorra", "Ned Sherrin", "Band-e Amir National Park", "Cartoon Network", "Chrysler", "Kabul in the eastern Afghan province of Logar,", "more than 100 tribal members", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "roanoke Colony", "Ballet Master in Chief Peter Martins", "apples"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6262324531525507}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.9, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.8, 0.08, 0.8205128205128205, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.07407407407407407, 0.2105263157894737, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3636363636363636, 0.3870967741935484, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.7499999999999999, 0.4210526315789474, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3771", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9812", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-1694", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-7787"], "SR": 0.453125, "CSR": 0.5763888888888888, "EFR": 0.9714285714285714, "Overall": 0.7221416170634921}, {"timecode": 27, "before_eval_results": {"predictions": ["eight", "after sustaining an injury which would be fatal to most other species", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "direct repeats", "Naples", "black, red or white,", "2009", "cowardly lion who is the main character -- the protagonist -- in this novel called \"Wicked.\"", "Diego Maradona", "an \"unnamed international terror group\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "\"It's you against the world and the CBT will bring out a dark side you might not have even realized you possessed.", "golf", "Used Acura", "Floxin", "a Muslim", "February 12", "Roberto Micheletti,", "last few months,", "a group of college students of Pakistani background", "\"It is I, the chief executive officer, the one on the very top,", "was arrested in a federal sting after his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "African-Americans", "environmental and political events", "\"I'm going to deny that motion,\"", "three empty vodka bottles,", "Euna Lee,", "Angela Merkel", "the Ku Klux Klan", "some dental work done, including removal of his diamond-studded teeth.", "Manuel Mejia Munera", "required police to question people if there's reason to suspect they're in the United States illegally", "UNICEF", "club managers,", "used-luxury market", "John Auer,", "\"I'm certainly not nearly as good of a speaker as he is.\"", "suicides", "$15 billion", "Karen Floyd", "Leo Frank,", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "Cogentin and Haldol,", "$2 billion", "Both men were hospitalized and expected to survive,", "Krishna Rajaram,", "flooding was so fast that the thing flipped over,\"", "Chievo", "Dubai", "up three of the last four months.", "Sunday,", "12-hour-plus", "three times", "Canada, Mexico and the United States of America", "South Dakota", "Homo sapiens", "potash", "Something In The Air", "1987", "Peter Kay's Car Share", "Cyclic Defrost", "pfeffernuesse", "\"Jack Reacher: Never Go Back\"", "14"], "metric_results": {"EM": 0.5, "QA-F1": 0.5517123361426334}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.2105263157894737, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.09523809523809525, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.42857142857142855, 0.07692307692307691, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4648", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2325", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-2456", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-441", "mrqa_naturalquestions-validation-7659", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-1223", "mrqa_searchqa-validation-2197"], "SR": 0.5, "CSR": 0.5736607142857143, "retrieved_ids": ["mrqa_squad-train-24670", "mrqa_squad-train-7850", "mrqa_squad-train-27457", "mrqa_squad-train-53517", "mrqa_squad-train-52776", "mrqa_squad-train-10307", "mrqa_squad-train-63208", "mrqa_squad-train-9430", "mrqa_squad-train-38142", "mrqa_squad-train-15856", "mrqa_squad-train-4432", "mrqa_squad-train-85890", "mrqa_squad-train-17373", "mrqa_squad-train-22748", "mrqa_squad-train-60202", "mrqa_squad-train-78117", "mrqa_hotpotqa-validation-2271", "mrqa_naturalquestions-validation-2067", "mrqa_squad-validation-5490", "mrqa_naturalquestions-validation-1801", "mrqa_triviaqa-validation-1595", "mrqa_naturalquestions-validation-7624", "mrqa_hotpotqa-validation-834", "mrqa_squad-validation-5236", "mrqa_triviaqa-validation-7464", "mrqa_newsqa-validation-4061", "mrqa_searchqa-validation-8796", "mrqa_squad-validation-3193", "mrqa_squad-validation-6267", "mrqa_newsqa-validation-3170", "mrqa_naturalquestions-validation-1838", "mrqa_triviaqa-validation-1065"], "EFR": 1.0, "Overall": 0.7273102678571429}, {"timecode": 28, "before_eval_results": {"predictions": ["deserts", "political parties", "discipline problems with the Flight Director's orders during their flight", "William Goldman", "The Cherokee Nation", "Caesars Entertainment Corporation", "Galt\u00fcr avalanche", "Niger\u2013Congo", "2013", "Luis Resto", "the Salzburg Festival", "Jay Park", "Blackpool F.C.", "New Orleans Saints", "2012", "The WB supernatural drama series \"Charmed\" from 2001 to 2006.", "Dundalk", "Ashley Jensen", "Syracuse", "Eileen Atkins", "Mollie Elizabeth King", "Esteban Ocon", "Gallery of flags of dependent territories", "Ouse and Foss", "Emilia-Romagna Region in Northern Italy", "Casablanca", "The Go-Go's", "1943", "Sleepy Hollow", "Ronnie Schell", "Wandsworth, London", "Christopher Lloyd Smalling", "Chevron", "World Music Awards", "La Liga", "Australian", "prominent artists such as Justin Bieber, Monica, Britney Spears, Usher, Keri Hilson, T.I.,", "Fort Hood, Texas", "Michael Fred Phelps II", "Lauren Alaina", "1961", "Droga5", "Preston, Lancashire, UK", "Prudential Center in Newark, New Jersey", "\"The Clash of Triton\"", "Mach number", "1945 to 1951", "Mexico", "Chevy", "wooden", "Disco", "Theodor W. Adorno", "re-education", "Sir Rowland Hill", "compound sentence", "Jimmy Carter", "John Logie Baird", "1066", "Joan Rivers", "did not go into further detail about her heart condition or the medical procedure.", "auction off one of the earliest versions of the Magna Carta later this year,", "wall of the Moon and Wall of the Sun", "Vista", "lids"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6727205086580086}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.8571428571428571, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2791", "mrqa_squad-validation-4060", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-1751", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-1926", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1593", "mrqa_naturalquestions-validation-8329", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-9671", "mrqa_searchqa-validation-14497", "mrqa_searchqa-validation-14791"], "SR": 0.546875, "CSR": 0.5727370689655172, "EFR": 0.9655172413793104, "Overall": 0.7202289870689655}, {"timecode": 29, "before_eval_results": {"predictions": ["some teachers and parents", "July 1969", "glaucophyte", "\"God and the just cause\"", "Mercury Records", "Evgeni Platov", "Nye County", "Karolina Dean", "Firestorm", "ten", "White Knights of the Ku Klux Klan", "Chechen Republic", "Green Lantern", "Kauffman Stadium", "2003", "English", "the Food and Agriculture Organization", "Idaho", "Jeff Meldrum", "Crossed: Psychopath", "Romance language", "Philip K. Dick", "exercise power directly or elect representatives from among themselves to form a governing body, such as a parliament", "English", "Cartoon Network Too", "David Starkey", "Cherokee River", "pop music and popular culture", "Field Marshal Lord Gort", "Hopeless Records", "Razor Ramon", "Godspell", "8 August 1907", "Donald J. Trump's", "7.63\u00d725mm Mauser", "Bangkok", "51st Disney animated feature film.", "his exploration and settlement", "August 28, 1774", "Afghanistan", "British", "Potomac River", "the Netherlands", "Love the Way You Lie", "Rio Ferdinand", "Boston", "Summerlin", "actor, producer, and director", "Rockbridge County", "St. Louis, Missouri", "Tsung-Dao Lee", "Bay Ridge, Brooklyn", "Human anatomy", "Tyler", "Pebble Beach", "AFC Wimbledon", "The Duke of Plaza Toro Francis Egerton", "1", "Meira Kumar", "U.S. Food and Drug Administration", "bartering", "spoiled", "malted", "Iceland"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6996225845410629}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-5306", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-3362", "mrqa_triviaqa-validation-6131", "mrqa_triviaqa-validation-4462", "mrqa_searchqa-validation-11933"], "SR": 0.640625, "CSR": 0.575, "EFR": 1.0, "Overall": 0.7275781250000001}, {"timecode": 30, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1079", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1524", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2823", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3382", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3950", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4058", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4334", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4953", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-5313", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-932", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-999", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9867", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1582", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-719", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-958", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-1228", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1828", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9613", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1379", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1546", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1600", "mrqa_squad-validation-1751", "mrqa_squad-validation-1819", "mrqa_squad-validation-1908", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-2025", "mrqa_squad-validation-2106", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-2848", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-3001", "mrqa_squad-validation-3103", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3449", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-4065", "mrqa_squad-validation-4132", "mrqa_squad-validation-4159", "mrqa_squad-validation-4216", "mrqa_squad-validation-4248", "mrqa_squad-validation-4274", "mrqa_squad-validation-4472", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4698", "mrqa_squad-validation-4736", "mrqa_squad-validation-4765", "mrqa_squad-validation-4772", "mrqa_squad-validation-4789", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5270", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5908", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6382", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7043", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7217", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7564", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7775", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7937", "mrqa_squad-validation-8010", "mrqa_squad-validation-8023", "mrqa_squad-validation-826", "mrqa_squad-validation-8298", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8466", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-8612", "mrqa_squad-validation-8665", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9308", "mrqa_squad-validation-9499", "mrqa_squad-validation-9594", "mrqa_squad-validation-9638", "mrqa_squad-validation-9918", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-4504", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6173", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7452", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-88"], "OKR": 0.849609375, "KG": 0.446875, "before_eval_results": {"predictions": ["the State Department", "immediately", "a second Gleichschaltung", "Las Vegas Hilton", "the Recording Industry Association of America (RIAA)", "between 7,500 and 40,000 words", "mountaineer", "Belgian", "Eve Hewson", "\"Slaughterhouse-Five\"", "Allan McNish (born 29 December 1969) is a British former racing driver, commentator, and journalist from Scotland.", "William Jefferson Clinton (born William Jefferson Blythe III; August 19, 1946) is an American politician who served as the 42nd President of the United States from 1993 to 2001.", "Oldham County", "sandstone", "Channel 4", "25 December 2009", "a priest", "punk rock", "Lufthansa heist", "Lord Byron", "Laura Dern", "Carrefour", "American burlesque", "Argentina", "Forever Living Products International, Inc. (FLPI)", "FBI", "The Saturdays", "Indianapolis", "French", "1968", "Edinburgh", "Charles Bronson", "Oklahoma Sooners", "Orson Welles", "Sharyn McCrumb", "National Archives", "1.23 million", "Ford Motor Company", "J. K. Rowling", "University of Kentucky College of Pharmacy", "Zola", "New Zealand", "January 28, 2016", "Martin Scorsese", "1979", "Being John Malkovich", "Merrimack County", "RAF Tangmere, West Sussex", "\"Brotherly Leader\"", "Suicide Kings", "North Kesteven, Lincolnshire, England", "a cobblestone-size (10x10 cm ) concrete cube bearing a brass plate inscribed with the name and life dates of victims of Nazi extermination or persecution.", "Wyatt", "Montgomery", "Bart Howard", "South Park", "a sound-reflecting target", "Andre Agassi", "10 below", "Asashoryu", "heavy turbulence", "gary pizzarelli", "Clarence", "The Secret"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6858716475095786}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.20689655172413793, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727272, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4854", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4470", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-5278", "mrqa_hotpotqa-validation-528", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-3325", "mrqa_naturalquestions-validation-7912", "mrqa_triviaqa-validation-7167", "mrqa_newsqa-validation-1077", "mrqa_searchqa-validation-12237"], "SR": 0.59375, "CSR": 0.5756048387096775, "retrieved_ids": ["mrqa_squad-train-71404", "mrqa_squad-train-49965", "mrqa_squad-train-75192", "mrqa_squad-train-67279", "mrqa_squad-train-32174", "mrqa_squad-train-38858", "mrqa_squad-train-53575", "mrqa_squad-train-2428", "mrqa_squad-train-40065", "mrqa_squad-train-66956", "mrqa_squad-train-67568", "mrqa_squad-train-24253", "mrqa_squad-train-58304", "mrqa_squad-train-85250", "mrqa_squad-train-1851", "mrqa_squad-train-21477", "mrqa_triviaqa-validation-3551", "mrqa_searchqa-validation-15593", "mrqa_squad-validation-1076", "mrqa_naturalquestions-validation-613", "mrqa_hotpotqa-validation-4018", "mrqa_naturalquestions-validation-4930", "mrqa_squad-validation-818", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-5393", "mrqa_naturalquestions-validation-2758", "mrqa_squad-validation-239", "mrqa_squad-validation-5456", "mrqa_naturalquestions-validation-9078", "mrqa_searchqa-validation-4861", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-1904"], "EFR": 1.0, "Overall": 0.7189490927419355}, {"timecode": 31, "before_eval_results": {"predictions": ["NCAA Division I", "1985", "mistress of the Robes", "2006", "25 million", "age 15", "Brian Yorkey", "Cressida Bonas", "Hermione Youlanda Ruby Clinton-Baddeley", "Game Informer", "Fort Albany", "Thorgan Ganael Francis Hazard", "Dennis Hull", "Love Actually", "Larnelle Harris", "Australia", "Southbank", "Commanding General", "2017", "Sean Yseult", "2001", "Benjamin Andrew \"Ben\" Stokes", "newspapers, television, radio, cable television", "Francis Keogh Gleason", "Royal Navy", "\"The Land of Enchantment\".", "$10\u201320 million", "Cumberland Plain", "Formula E", "residential", "Province of Canterbury", "Anhaltisches Theater", "Alemannic", "1932", "128", "Telugu", "1937", "Windermere Hotel", "Curtis James Martin Jr.", "Marco Fu", "horror film", "Isabella (Belle) Baumfree", "Kate Millett", "the fifth season of the American comedy-drama series \"Gilmore Girls\"", "Premier League", "Aqua", "St. Louis, Missouri", "Dan Castellaneta", "Bury St Edmunds", "Philip K. Dick", "the Labour Party", "Bury Football Club", "Toby Keith", "Lady Olenna Tyrell", "January 15, 2010", "Kenya", "Macbeth", "Sir William Hamilton", "Las Vegas", "Simon Cowell", "an \"unnamed international terror group\"", "the Church of Christ, Scientist", "Ronald Reagan Presidential Library", "East Germany"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6747767857142857}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.5, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2793", "mrqa_hotpotqa-validation-4223", "mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-3488", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-3892", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-3798", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5482", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-8909", "mrqa_triviaqa-validation-2828", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-4128", "mrqa_searchqa-validation-1275", "mrqa_searchqa-validation-1396"], "SR": 0.53125, "CSR": 0.57421875, "EFR": 1.0, "Overall": 0.718671875}, {"timecode": 32, "before_eval_results": {"predictions": ["'Licensed Local Pastor'", "a power outage", "13", "Hebrew", "Blenheim Palace", "\"Why did David choose five smooth stones before going to fight Goliath?", "Edith Louisa Cavell", "c. 1068\u20131099", "De Lorean DMC-12", "Cuban Missile Crisis", "Action Comics", "Queen Elizabeth II", "The Merchant of Venice", "Northwestern University", "curling", "Cole Porter", "Colorado", "Google", "Aviva plc", "oil pressure to the engine in most all conditions", "Project Gutenberg", "surf", "Dr John Sentamu", "Kiel Canal", "General Sir Herbert Kitchener", "Cevennes", "Eggs Benedict", "Luigi Pirandello", "Sheffield United", "Ken Harrison", "Midtown", "Today", "Hugh Laurie", "a cappella", "Japan", "Rick Astley", "red squirrels", "Punjab", "Francois Quesnay", "Model T", "Spice Girls", "Richard IV", "Michael Caine", "Sebastian Beach", "pig", "Bank of England", "Isaac Newton", "Pet Sounds by The Beach Boys", "the monarch", "Bangladesh", "St Clements", "Castor", "Nalini Negi", "last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "Norway", "CommunityAmerica Ballpark", "University of Mississippi", "The Rebirth", "\"Bagosora was the highest authority in the Ministry of Defense and exercised effective control of the Rwandan army and gendarmerie,\"", "Apple", "Akio Toyoda", "Hamlet", "a reddish-orange nose", "chicken Kiev"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5987925543024227}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.10526315789473684, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3469", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-6905", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-373", "mrqa_triviaqa-validation-1359", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-907", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-1538", "mrqa_hotpotqa-validation-217", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-250", "mrqa_searchqa-validation-16717", "mrqa_searchqa-validation-10619"], "SR": 0.546875, "CSR": 0.5733901515151515, "EFR": 1.0, "Overall": 0.7185061553030303}, {"timecode": 33, "before_eval_results": {"predictions": ["NASA Administrator Webb", "Duval County", "the Atlantic Ocean", "Richard Branson", "temperature", "tibet", "yorkshire", "1709", "Tutankhamun", "Morgan Spurlock", "the iris", "Massachusetts", "Andre Agassi", "Jane Austen", "Dutch", "tibet", "nacre", "yellow", "Tbilisi", "Mrs Merton", "cricketer", "Wyoming", "emsworth", "Hugh Quarshie", "Flanagan", "\"My Sweet Lord\"", "lord sugar", "smith", "Henri Paul", "Red Sea", "Helen Gurley Brown", "Wash", "ship", "smith", "Mark Carney", "Cassidy", "c\u00f4te d'Or", "Utah", "Toy Story", "David Lloyd George", "Italy", "lord Nelson", "George Osborne", "August 10, 1960", "Apollo", "Gentlemen Prefer Blondes", "smith", "smithers", "Paul Gauguin", "United Republic of Tanzania", "proton", "Demi Moore", "28 July 1914", "Spike", "1995", "Elizabeth Claire Kemper", "7 June 1954", "Oryzomyini", "Ryder Russell,", "\"The missile defense system is not aimed at Russia,\"", "The Washington Post", "smith", "ship", "ship"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5859375}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3929", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-377", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7241", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-5636", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1375", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-5950", "mrqa_naturalquestions-validation-5304", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-5041", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-2642"], "SR": 0.53125, "CSR": 0.5721507352941176, "retrieved_ids": ["mrqa_squad-train-17658", "mrqa_squad-train-7053", "mrqa_squad-train-6741", "mrqa_squad-train-46475", "mrqa_squad-train-83120", "mrqa_squad-train-38161", "mrqa_squad-train-71677", "mrqa_squad-train-34001", "mrqa_squad-train-61511", "mrqa_squad-train-17942", "mrqa_squad-train-55816", "mrqa_squad-train-63140", "mrqa_squad-train-52521", "mrqa_squad-train-44640", "mrqa_squad-train-54421", "mrqa_squad-train-6996", "mrqa_hotpotqa-validation-2076", "mrqa_newsqa-validation-223", "mrqa_hotpotqa-validation-2960", "mrqa_squad-validation-4272", "mrqa_hotpotqa-validation-5499", "mrqa_newsqa-validation-1971", "mrqa_hotpotqa-validation-380", "mrqa_searchqa-validation-8713", "mrqa_squad-validation-1384", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-8386", "mrqa_hotpotqa-validation-2837", "mrqa_newsqa-validation-1386", "mrqa_searchqa-validation-10319", "mrqa_searchqa-validation-16717", "mrqa_triviaqa-validation-373"], "EFR": 1.0, "Overall": 0.7182582720588235}, {"timecode": 34, "before_eval_results": {"predictions": ["international metropolitan region", "petroleum", "Crystal Pepsi", "New Orleans", "Dan", "Truthful or creditable", "GIGO", "Pawn center", "adam ballinger", "silk", "stakes", "Arthur C. Clarke", "rice", "fox", "the Wise", "tibet", "adam smith", "Led Zeppelin", "Alderney", "Charles Lindbergh", "River Phoenix", "Oz", "heroes", "Krntnertor", "Jason", "La", "the Marine Band", "AbeBooks.com", "The Lady", "Profiles in Courage", "bogota", "biony", "Bea Arthur", "Pop", "dijon", "borat", "coal", "Jean Foucault", "Hanna Glawari", "ulnar nerve", "Harriet Tubman", "horse", "Louisa May Alcott", "Big Ben", "smith", "Margaret Atwood", "cacique", "Khartoum", "Joaquin Phoenix", "Winslow Homer", "Moby Dick", "The Hot Chick", "iOS", "Mars Hill", "Aaron Harrison", "Gwyneth Paltrow", "gollumoides", "aromatherapy", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "140 million", "1923", "Saturday", "7-1", "Karen Floyd"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5697916666666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-1570", "mrqa_searchqa-validation-2853", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-12487", "mrqa_searchqa-validation-15135", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-12354", "mrqa_searchqa-validation-9156", "mrqa_searchqa-validation-15325", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-1882", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7832", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4753", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-30", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2996"], "SR": 0.515625, "CSR": 0.5705357142857144, "EFR": 1.0, "Overall": 0.7179352678571429}, {"timecode": 35, "before_eval_results": {"predictions": ["an arrow", "the Chicago Bears", "Floridians", "green and yellow", "Galicia", "Regional Rural Bank", "M2M", "American 3D computer-animated comedy film", "authoritarian tendencies", "The Division of Cook", "July 16, 1971", "13 May 2018", "Kentucky River", "Barbara Niven", "the Ferengi bartender Quark", "Yellowcraig", "Santiago del Estero Province", "a super-regional shopping mall", "Messiah Part II", "Abbey Road", "Mel Blanc", "\"\u010cesk\u00e9 kr\u00e1lovstv\u00ed\"", "2000", "Jim Jones", "Alfred Preis", "Terry Malloy", "Countess of Lovelace (\"n\u00e9e\" Byron) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine", "The interview", "various registries", "20th", "Lord Lucan", "January 15, 1975", "Chiwetel Umeadi Ejiofor, CBE", "Appleby-in-Westmorland", "27 November 1956", "Charles de Gaulle Airport", "The St Andrews Agreement", "the Seasiders", "Victorian College of the Arts and Melbourne Conservatorium of Music", "south", "Nick Cassavetes", "Cate Blanchett", "John David Souther", "January 28, 2016", "Hopi", "John Meston", "Romeo Montague (Italian: \"Romeo Montecchi\" )", "\u00c6thelwald Moll", "The Albanian Coalition \"Perspective\" is an Albanian political party in Montenegro", "the University College of North Staffordshire", "Battle of the Rosebud", "Jaffrey, New Hampshire", "1998", "the Red Sea", "Sara Gilbert", "Elkie Brooks", "acrostic", "blue", "Asashoryu", "computer problems", "December 7, 1941", "Peter Pan", "beryl", "Luxor Las Vegas"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6625496031746032}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5283", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-3867", "mrqa_naturalquestions-validation-6452", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-1457", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-15743"], "SR": 0.59375, "CSR": 0.5711805555555556, "EFR": 1.0, "Overall": 0.718064236111111}, {"timecode": 36, "before_eval_results": {"predictions": ["the potential energy stored in an H+ or hydrogen ion gradient to generate ATP energy", "the Caesars Palace Grand Prix", "A compact car (North America), or small family car", "Benjam\u00edn", "Koch Industries", "Enigma", "Yellow fever", "Julia Compton Moore", "Lord's Resistance Movement", "Workers' Party", "Talib Kweli", "By Britain's Royal Proclamation of 1763", "Bulgaria", "the Swiss national team", "the remake", "Oldham County, Kentucky", "The Captain Matchbox Whoopee Band", "Alec Berg", "wild boar, and red, fallow and roe deer", "the Ghanaian national team", "Nikolai Morozov", "Rabies", "Switzerland", "eastern Tennessee", "Godiva", "October 21, 2016", "August", "Lawrence of Arabia", "The Ansonia Hotel", "1937", "Irish Government's Health Service Executive", "Leona Louise Lewis", "John Robert Cocker", "$7.3 billion", "Rob", "his most brilliant student.", "the \"Black Abbots\"", "Sarah Kerrigan, the Queen of Blades", "German", "Katy Perry", "Bharat Ratna", "Wilderness Road", "12 mi east-southeast of Bridgeport", "\"Nebo Zovyot\"", "\"Orchard County\"", "The Kree", "110", "The shortest player ever to play in the National Basketball Association", "The authorship of \"Titus Andronicus\"", "Denmark", "Tim Burton", "James Corden", "Audrey II ''", "1998", "red", "Shakyamuni", "Jim Braddock", "\"He is more American than German.\"", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Brazilian supreme court judge", "the Yankees", "the Washington Redskins", "( Samuel Taylor Coleridge)", "h2g2"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6230097720906544}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8333333333333333, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.4, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8903", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-4571", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-667", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-3582", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-4569", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4184", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-11491", "mrqa_triviaqa-validation-768"], "SR": 0.515625, "CSR": 0.5696790540540541, "retrieved_ids": ["mrqa_squad-train-6459", "mrqa_squad-train-47655", "mrqa_squad-train-66242", "mrqa_squad-train-11489", "mrqa_squad-train-7303", "mrqa_squad-train-59012", "mrqa_squad-train-56098", "mrqa_squad-train-85783", "mrqa_squad-train-11354", "mrqa_squad-train-39859", "mrqa_squad-train-84729", "mrqa_squad-train-55276", "mrqa_squad-train-37237", "mrqa_squad-train-14165", "mrqa_squad-train-68317", "mrqa_squad-train-81880", "mrqa_hotpotqa-validation-1926", "mrqa_hotpotqa-validation-1852", "mrqa_triviaqa-validation-3713", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-13367", "mrqa_naturalquestions-validation-3658", "mrqa_searchqa-validation-15593", "mrqa_newsqa-validation-3772", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-5636", "mrqa_newsqa-validation-3752", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-13337", "mrqa_squad-validation-3899", "mrqa_hotpotqa-validation-5041", "mrqa_hotpotqa-validation-303"], "EFR": 1.0, "Overall": 0.7177639358108108}, {"timecode": 37, "before_eval_results": {"predictions": ["the plain moraine plateau", "londres", "cauliflower", "Nizhny Novgorod", "James Bond", "shekel", "keeper of the Longstone (Fame Islands) lighthouse", "Carlos the Jackal", "Australia", "Annelies Marie Frank", "Belgium", "Sufjan Stevens", "the isthmus", "Benny Hill", "Roddy Doyle", "Kevin Spacey", "Alexandria", "Chad", "1215", "the neck", "David Hockney", "Rudyard Kipling", "lactic acid", "Fleet Street", "the Netherlands", "fractal geometry", "Cosmos: A spacetime Odyssey", "the duck-billed echidna", "Aquaman", "Jean-Paul Sartre", "a novel", "a carburetors", "the Esmeralda\\'s Barn night", "Scotch", "Switzerland", "a sheep", "trumpet", "Lou Gehrig", "(James) Dumas", "a film", "Heston Blumenthal", "Tony Curtis", "U2", "a woman", "the invasion of Russia", "Charlie Harper", "Morgan Choir", "Canada", "Buster Edwards", "Chief Inspector of Prisons", "Henley Royal Regatta", "Paul Lynde", "Irsay", "Cairo, Illinois", "the Lazio region", "Charles Ellis Schumer", "SKUM", "BET", "HSH Nordbank Arena", "October 2007", "first woman mayor", "Nicolae Ceausescu", "the Siberian Husky", "pronghorn"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6380208333333333}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-953", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-2340", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6211", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-4500", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-7109", "mrqa_hotpotqa-validation-3529", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-731", "mrqa_searchqa-validation-10306", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-10797", "mrqa_hotpotqa-validation-2366"], "SR": 0.59375, "CSR": 0.5703125, "EFR": 1.0, "Overall": 0.717890625}, {"timecode": 38, "before_eval_results": {"predictions": ["rocketry and manned spaceflight", "Dutch", "Peter Yarrow", "Washington", "Zack Snyder", "Mondays", "The Cosmopolitan", "Anna Clyne", "Meghan Markle", "exploitation of natural resources particularly North Sea oil", "Commissioner", "August 1973", "Burnley", "Teen Titan", "Evey's mother in the Wachowskis", "Love and Theft", "1978", "SKUM", "Edmonton, Alberta", "Seattle", "\"The School Boys\"", "Orchard Central", "The Kennedy Center", "commanders of the Great Army", "Environmental Protection Agency (EPA)", "Humberside", "Diamond Rio", "charlie", "Northampton, England", "Mike Greenwell", "2017", "SAS Fr\u00f6sundavik", "polka", "Irish Chekhov", "1860", "2006", "Ghana's Asamoah Gyan", "Coronation Street", "\"The Dragon\"", "Cold Spring", "Never Paradox", "Sophie Monk", "The Supremes", "Sunset Publishing Corporation", "Melbourne Storm", "twenty", "9 November 1967", "Retina", "technical director", "Cincinnati", "Captain B.J. Hunnicutt", "19 June 2018", "naos", "31", "kendo", "Jeffrey Archer", "germany", "little blue booties", "Majid Movahedi,", "Espinoza Patron's", "Feb 2, 2016", "ninjitsu", "witch", "Mozart"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6497195512820513}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.6153846153846153, 1.0, 0.4, 0.5, 1.0, 1.0, 0.4, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3812", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-668", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3208", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-2868", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-1300", "mrqa_hotpotqa-validation-2157", "mrqa_hotpotqa-validation-1264", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-5545", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-16163", "mrqa_searchqa-validation-3163"], "SR": 0.53125, "CSR": 0.5693108974358974, "EFR": 1.0, "Overall": 0.7176903044871794}, {"timecode": 39, "before_eval_results": {"predictions": ["Much of the city's tax base dissipated", "Aly Raisman", "40,400", "Seoul, South Korea", "American", "frontman Shane MacGowan", "William Powell \"Bill\"Leary", "Distinguished Service Cross", "Revolver", "Samuel M \"Sam\" Raimi", "\"Martian Manhunter\"", "Eden Valley Railway", "Wolfgang Amadeus Mozart", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Mercer Bears", "Dame Eileen June Atkins", "May 1801", "Little Dixie", "1979", "1905", "member of the Kansas House of Representatives", "Loughborough University", "Los Angeles", "2.1 million", "Sierra Nevada mountains", "Alfred Joel Horford Reynoso", "\"The Young Ones\"", "Mauritian", "Tony Burke", "Lorman, Mississippi", "Wilderness Road", "Alfred Edward Housman", "The Killer", "London", "4 km", "25 October 1921", "\"War & Peace\"", "2016 U.S. Senate election", "Joseph Conrad", "Mickey Gilley's Club", "Jedi", "Edward Albert Heimberger", "Akosua Busia", "Gian Carlo Menotti", "Indian", "Target Corporation", "last Roman Catholic Archbishop of Canterbury", "sub-Saharan Africa", "City of Newcastle", "gamecock", "Aaliyah Dana Haughton", "Mad - Eye Moody", "one - point perspective, or `` eye point ''", "16,801", "Lady Gaga", "October", "Georgetown", "The Palm Jumeirah", "consumer confidence", "Peter Garrett", "Scarlett Johansson", "Fried Green Tomatoes", "Bon Jovi", "American teenage singer, Marcie Blane"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6706473214285714}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666665, 1.0, 0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-4577", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-4770", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1074", "mrqa_newsqa-validation-101", "mrqa_searchqa-validation-2773", "mrqa_naturalquestions-validation-7367"], "SR": 0.546875, "CSR": 0.56875, "retrieved_ids": ["mrqa_squad-train-58371", "mrqa_squad-train-35338", "mrqa_squad-train-22230", "mrqa_squad-train-14206", "mrqa_squad-train-48204", "mrqa_squad-train-21045", "mrqa_squad-train-7108", "mrqa_squad-train-54897", "mrqa_squad-train-66189", "mrqa_squad-train-65328", "mrqa_squad-train-3335", "mrqa_squad-train-56128", "mrqa_squad-train-47383", "mrqa_squad-train-55349", "mrqa_squad-train-39587", "mrqa_squad-train-13203", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-959", "mrqa_naturalquestions-validation-7124", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-2921", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-922", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-1004", "mrqa_squad-validation-4216", "mrqa_hotpotqa-validation-562", "mrqa_naturalquestions-validation-3297", "mrqa_squad-validation-2419", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-955", "mrqa_hotpotqa-validation-4606"], "EFR": 1.0, "Overall": 0.717578125}, {"timecode": 40, "UKR": 0.779296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1611", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2331", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3188", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5538", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-5705", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-987", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4998", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-809", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12714", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5071", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9480", "mrqa_squad-validation-10044", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-10326", "mrqa_squad-validation-10425", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1231", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1472", "mrqa_squad-validation-1608", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2006", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2819", "mrqa_squad-validation-297", "mrqa_squad-validation-3001", "mrqa_squad-validation-3262", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3812", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4078", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4543", "mrqa_squad-validation-4611", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5079", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5355", "mrqa_squad-validation-5563", "mrqa_squad-validation-5597", "mrqa_squad-validation-5616", "mrqa_squad-validation-5881", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6223", "mrqa_squad-validation-6251", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-7952", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8199", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9768", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-244", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6607", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.88671875, "KG": 0.5015625, "before_eval_results": {"predictions": ["gilt bronze", "\"The oceans are kind of the last frontier for use and development,\"", "President Obama's surge plan", "said they would not be making any further comments, citing the investigation.", "Pakistan's North West Frontier Province,", "$250,000", "The Valley swim Club", "the actor who created one of British television's most surreal thrillers", "in a stream in Shark River Park", "helicopters and unmanned aerial vehicles", "governor Mark Sanford", "between 1917 and 1924", "Frank Ricci", "Janet Napolitano", "Mahmoud Ahmadinejad", "\"utterly baseless.\"", "Jacob", "Francisco X. Pacheco,", "European Commission", "lousiana", "prostate cancer,", "RheinEnergieStadion.", "Tsvangirai", "the FBI", "\"GoldenEye\"", "necropsy", "a new puppy", "Kurdish group", "77-year-old Oscar winner", "Barack Obama supporter", "it has not intercepted any Haitians attempting illegal crossings", "immigration", "17", "rally at the State House", "an empty water bottle", "in Val d'Isere, France", "a head injury", "News of the World tabloid.", "Swat Valley", "200.", "Manny Pacquiao", "1964", "Pixar", "officers at a Texas  airport", "environmental", "80,", "EU naval force", "Angelo Nieve,", "in a ceremony at the ancient Greek site of Olympia", "Gary Player", "200", "Adam", "Erika Mitchell Leonard", "December 1, 1969", "Vaclav Havel", "right", "immigration", "London", "703", "20 October 1980", "glaciers", "Rahm Emanuel", "Cerberus", "1967"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5608109721161192}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.18181818181818182, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.05714285714285715, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1159", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-2427", "mrqa_searchqa-validation-2894"], "SR": 0.484375, "CSR": 0.5666920731707317, "EFR": 1.0, "Overall": 0.7468540396341463}, {"timecode": 41, "before_eval_results": {"predictions": ["Business Connect", "for an \"explosion of violence.\"", "40-year-old", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "six", "Kuranyi's", "Kenyan and Somali governments", "Aung San Suu Kyi", "legitimacy of that race.", "think, symbolic mistake of the president declining to meet the Dalai Lama before his own visit to China later next month.\"", "for the New Yorker, as well as a quarter century as an advocate for social activism.", "Mashhad", "Islamabad", "North Korea's reclusive leader Kim Jong- IlThe missiles can travel about 3,000 kilometers (1,900 miles),", "pesos", "Steven Green", "American collective consciousness", "1000 square meters", "sailor", "they'd get to bring a new puppy with them to the White House in January.", "27-year-old's", "Friday,", "Los Ticos", "Seasons of My Heart", "helping on the sandbags lines", "$17,000", "opium", "for not doing more since taking office.\"", "10 years", "$55.7 million", "Noida, located in the outskirts of the capital New Delhi.", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "two and a half hours.", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England this weekend.", "MDC offshoot is part of larger deal that has not been signed by anyone.", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "of the busiest transport hubs in the world,", "104 feet long and 95 feet wide at the alcove.", "Transit Workers Union leaders", "state senators who will decide whether to remove him from office", "84-year-old", "1/2 hours", "the Southern Baptist Convention,", "Sen. Barack Obama", "a mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Anil Kapoor", "think that someday, they'll try to take over your brain.", "Marie-Therese Walter.", "South African police", "The eye of Hurricane Gustav", "Felix Baumgartner ( German : ( \u02c8fe\u02d0l\u026aks \u02c8ba\u028a\u032fm\u02cc\u0261a\u0250\u032ftn\u0250 )", "July 2012", "1273.6 cm", "(Frank) Miller, trans.).", "a Dutch painter", "kievan Rus", "Toronto", "1993", "Robert L. Stone", "Nike", "Barbara Bush", "a keyboard layout designed to be a practical alternative to the QWERTY and Dvorak keyboard layouts.", "Corpulent"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5777224511599511}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.15384615384615385, 0.13333333333333336, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.28571428571428575, 0.8, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.75, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.1111111111111111, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-3430", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2301", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-7458", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2480", "mrqa_hotpotqa-validation-582", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-2542"], "SR": 0.453125, "CSR": 0.5639880952380952, "EFR": 1.0, "Overall": 0.746313244047619}, {"timecode": 42, "before_eval_results": {"predictions": ["Wittenberg", "Prohibition", "Bligh", "Parkinson's", "Changing Places", "linda", "Moscow", "germany", "Portugal", "first among equals", "Friedrich Nietzsche", "the moon", "Moldova", "Simon Townshend", "Craggy", "the Suez Canal", "otters", "shagbark", "Port Talbot", "Rapa Nui", "The West Wing", "Charlie Cairoli", "Salvador Allende", "Mike Tyson", "J. M. W. Turner", "conductor", "( Robert) Boyle", "divination", "a winter fur hat", "Tony Blair", "Adolf Hitler", "Jamaica", "June Brae", "hypertension", "1066", "Euryoneme", "Jesse James", "Purple Heart Medal", "cephalus", "Jessica Simpson", "a bacterial genus", "the South Saskatchewan River", "Robert Devereux", "NASCAR", "Canada", "Pennsylvania", "Louis Daguerre", "Argentina", "Kwame Nkrumah", "The Color Purple", "terrorism", "lithium", "can be produced with constant technology and resources per unit of time", "Mariah Carey", "lifetime achievements", "unidentified flying objects", "Chicago Bears", "a grizzly bear", "Turkey", "Pew Research Center", "inflammation", "Ferrari", "Smoky Mountains National Park", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.53125, "QA-F1": 0.621214257233994}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.21052631578947367]}}, "before_error_ids": ["mrqa_squad-validation-2165", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-882", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-797", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-9911", "mrqa_naturalquestions-validation-4915"], "SR": 0.53125, "CSR": 0.5632267441860466, "retrieved_ids": ["mrqa_squad-train-33972", "mrqa_squad-train-78766", "mrqa_squad-train-2922", "mrqa_squad-train-66115", "mrqa_squad-train-68659", "mrqa_squad-train-12325", "mrqa_squad-train-80235", "mrqa_squad-train-4790", "mrqa_squad-train-75553", "mrqa_squad-train-46449", "mrqa_squad-train-52078", "mrqa_squad-train-70373", "mrqa_squad-train-14534", "mrqa_squad-train-38147", "mrqa_squad-train-31304", "mrqa_squad-train-69116", "mrqa_naturalquestions-validation-3505", "mrqa_newsqa-validation-1175", "mrqa_hotpotqa-validation-4770", "mrqa_searchqa-validation-9438", "mrqa_squad-validation-7758", "mrqa_searchqa-validation-14466", "mrqa_naturalquestions-validation-4798", "mrqa_squad-validation-6640", "mrqa_squad-validation-4562", "mrqa_squad-validation-9567", "mrqa_hotpotqa-validation-3489", "mrqa_naturalquestions-validation-337", "mrqa_squad-validation-620", "mrqa_naturalquestions-validation-9253", "mrqa_newsqa-validation-1391", "mrqa_squad-validation-6973"], "EFR": 1.0, "Overall": 0.7461609738372093}, {"timecode": 43, "before_eval_results": {"predictions": ["John M. Grunsfeld", "dancing with the Stars", "psychotropic drugs", "opium", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "10 below", "Democrat", "whites", "16", "Rany Freeman,", "forgery and flying without a valid license,", "President Bush", "15-year-old", "seven", "an upper respiratory infection,\"", "543", "Kevin Kuranyi", "gopi Podila,", "Susan Atkins,", "Ameneh Bahrami", "Virgin America", "$1,500", "Al Nisr Al Saudi", "We Found Love", "his parents", "Ralph Lauren", "Joe Patane", "humanitarian mission.", "North Korea", "beetles", "Old Trafford", "Lillo Brancato Jr.", "Arabic, French and English", "Britain.", "Arsene Wenger", "The FBI's Baltimore field office", "Michael Jackson", "all day starting at 10 a.m.,", "Ma Khin Khin Leh,", "South African police", "an American who entered the country illegally from China on Christmas Eve.", "Palestinian Islamic Army,", "same-sex civil unions,", "was killed", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "consumer confidence", "Phil Spector", "the District of Columbia National Guard,", "Australia and New Zealand", "Steven Gerrard", "new clashes in Cairo's Tahrir Square that stretched into Wednesday.", "with an armature of piped masonry often carved in decorative patterns", "Hermann Ebbinghaus", "southern whites", "a pig", "myxoma virus", "Wisconsin", "Battle of Britain and the Battle of Malta", "Viacom Media Networks", "five", "carbon fiber", "the Star-Spangled Banner", "Bing Crosby", "d"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6526498538011696}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.9473684210526316, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-540", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2065", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9516", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-2853", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11764"], "SR": 0.59375, "CSR": 0.5639204545454546, "EFR": 1.0, "Overall": 0.7462997159090909}, {"timecode": 44, "before_eval_results": {"predictions": ["Supreme Court of the United Kingdom", "Seal", "on to other parts of the brain.", "coffin-maker", "63 to 144 inches", "Wye", "a Californio nobleman", "USS Thresher", "the Last Post", "Chongqing", "BMW", "eagle", "Morgan", "Tito Jackson", "of worker motivation, enabling better managerial practices and higher job satisfaction", "Prague", "Yellowstone", "Watford", "Nevada", "muezzin", "sheep", "Rihanna", "Tintin", "Alexandrina", "22", "Hector BERLIOZ", "Azerbaijan", "Ireland", "Ash", "Madness", "the Dalton gang", "Australia", "Phil Woolas", "bats", "United States", "Penelope Keith", "Alexei Kosygin", "John Galsworthy", "lieutenant general", "James Van Allen", "on the bowl-shaped crater of a narrow inlet", "the Mediterranean", "purse", "Nicaragua", "Passepartout", "on the Quadriga of Victory", "Lancashire", "Edouard Manet", "Burger King", "Thebes", "Hyundai", "Xiu Li Dai and Yongge Dai", "President of the sitting Governors", "David Ben - Gurion", "Chow Tai Fook Enterprises", "an organ", "Point of Entry", "1973's", "a violent government crackdown seeped out.", "Robert Mugabe", "Hungarian", "China", "China", "Isabella (Belle) Baumfree"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6182291666666666}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-7579", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-5745", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4657", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-7893", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3391", "mrqa_searchqa-validation-15957"], "SR": 0.578125, "CSR": 0.5642361111111112, "EFR": 1.0, "Overall": 0.7463628472222222}, {"timecode": 45, "before_eval_results": {"predictions": ["trespassing at a nuclear-missile installation", "The Hague", "2", "skull", "new York", "morgan goon", "Idaho", "Nuuk", "the Philippines", "pool", "China", "Graham Henry", "yarn", "South Pacific", "king henry vtoroy", "soybean pods", "Leeds", "wood", "Elizabeth II", "dog", "llamas", "London Underground Piccadilly Line", "Oklahoma", "la Boh\u00e9me", "Nepal", "scurvy", "cutter", "Indonesia", "purpurea", "elgar", "keane", "gauteng", "crimson", "Pakistan", "Uranus", "rodbottom", "morgan joe", "my Favorite martian", "niki lauda", "barawak", "The Daily Mirror", "Eric Morley", "radio waves", "york", "Manchester City", "football", "Reform Club", "William Shakespeare", "Snark", "Trimdon", "Queensland", "approximately 26,000 years", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "Scottish post-punk band Orange Juice", "thirteenth", "Plato", "Joan Feynman", "Florida", "his grandfather,", "400 years", "freelance", "mouth", "barbara dole", "on bail set at $500,000 each."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5446924603174603}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5555555555555556, 0.5714285714285715, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-6714", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-7559", "mrqa_triviaqa-validation-2995", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-4038", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-5775", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-2379", "mrqa_hotpotqa-validation-1169", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-4100", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-11241", "mrqa_newsqa-validation-3806"], "SR": 0.46875, "CSR": 0.5621603260869565, "retrieved_ids": ["mrqa_squad-train-25245", "mrqa_squad-train-21007", "mrqa_squad-train-40893", "mrqa_squad-train-26317", "mrqa_squad-train-26466", "mrqa_squad-train-20400", "mrqa_squad-train-26286", "mrqa_squad-train-16546", "mrqa_squad-train-70392", "mrqa_squad-train-50915", "mrqa_squad-train-20603", "mrqa_squad-train-70838", "mrqa_squad-train-1809", "mrqa_squad-train-30936", "mrqa_squad-train-85121", "mrqa_squad-train-72411", "mrqa_hotpotqa-validation-3423", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-985", "mrqa_searchqa-validation-4484", "mrqa_newsqa-validation-2627", "mrqa_naturalquestions-validation-1640", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-3771", "mrqa_hotpotqa-validation-3356", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-2128", "mrqa_searchqa-validation-6843", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-1852", "mrqa_squad-validation-3597"], "EFR": 0.9411764705882353, "Overall": 0.7341829843350383}, {"timecode": 46, "before_eval_results": {"predictions": ["Konwiktorska Street", "a crust of mashed potato", "Lalo Schifrin", "16 November 2001", "Don McMillan", "7", "Billy Hill", "Paul Lynde", "Chlorofluorocarbons", "body - centered cubic ( BCC ) lattice", "May 2002", "2010", "virtual reality simulator", "beneath the liver", "1885", "pre-Columbian times", "early 2014", "Most days are sunny throughout the year", "caused by chlorine and bromine from manmade organohalogens", "Gamora", "1973", "differs in ingredients", "homicidal thoughts of a troubled youth", "The Hunger Games : Mockingjay -- Part 1 ( 2014 )", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear )", "October 22, 2017", "Amitabh Bachchan", "1998", "nine", "Jeff Bezos", "in response to a perceived harmful event, attack, or threat to survival", "A lymphocytes is one of the subtypes of white blood cell in a vertebrate's immune system", "electrons from electron donors to electron acceptors via redox", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "produced with constant technology and resources per unit of time", "Lewis Carroll", "2004", "Charles Carroll", "Cetshwayo", "early 1980s", "Sasha Banks", "Erica Rivera", "New York", "September 2001", "to demonstrate rounded vowel sounds", "Chris Rea", "mashed potato", "disputes between two or more states", "three", "Louisa Johnson", "The Mongol", "candy bars", "1948", "Ruth Rendell", "Hidden America with Jonah Ray", "the United Kingdom", "people working in film and the performing arts", "Henrik Stenson", "between 1917 and 1924", "lawyers trying to save their client from the death penalty", "Cops is On", "The Virgin Spring", "cookies", "his uncle Juan Nepomuceno Guerra"], "metric_results": {"EM": 0.546875, "QA-F1": 0.66963893461256}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.4, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.5, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.7692307692307692, 1.0, 0.08695652173913042, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4196", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-6321", "mrqa_triviaqa-validation-7778", "mrqa_newsqa-validation-2156", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-7061", "mrqa_hotpotqa-validation-4241"], "SR": 0.546875, "CSR": 0.5618351063829787, "EFR": 0.9310344827586207, "Overall": 0.7320895428283198}, {"timecode": 47, "before_eval_results": {"predictions": ["Acadia National Park", "Earl Long", "the sauna European style", "paiyar", "the Space Shuttle Challenger", "the Rolling Stones", "F Thomas G Nazareth", "lapis lazuli", "the Pentagon", "a valley fold", "snails", "bamboo", "the Vietnam War", "the Palisadoes", "Gerard Mercator", "this Mr. stay Puft", "the Barnum & Bailey Circus", "Dizzy Gillespie", "Marcia Brady", "Ernie Els", "former Yugoslav Republic of Macedonia", "gestation", "skin", "there Will Be Blood", "Herb Alpert", "John Adams", "the Rolling Stones", "Bernard", "Zeus", "Fast Food Nation", "the Tasmanian devil", "the British flag", "a glass container", "the cyclotron", "Priscilla", "a brothel", "Barcelona", "a Yellow Ribbon", "the Indy 500", "\"The Hills\"", "lowest", "porter", "Rhode Island", "the Atlantic Ocean", "Baton Rouge", "Kamehameha I", "menudo", "Alan Alda", "a science lab", "a park", "sirloin", "in late - 17th century New England", "Walter Pauk", "the left atrium of the heart", "the American Civil War", "the Governing Body of Jehovah\\'s Witnesses", "the Soviet Union", "the Uranian moon Titania", "Obafemi Martins", "\"Twice in a Lifetime\"", "two-state solution", "Republican Party", "a communications breakdown at a Federal Aviation Administration facility,", "The Tempest"], "metric_results": {"EM": 0.5, "QA-F1": 0.5796874999999999}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2144", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-847", "mrqa_searchqa-validation-11698", "mrqa_searchqa-validation-9392", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-13199", "mrqa_searchqa-validation-4024", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-15464", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-16787", "mrqa_searchqa-validation-6078", "mrqa_searchqa-validation-6431", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-4236", "mrqa_searchqa-validation-11015", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-3996", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-10881", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3174", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-109", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-904"], "SR": 0.5, "CSR": 0.560546875, "EFR": 1.0, "Overall": 0.745625}, {"timecode": 48, "before_eval_results": {"predictions": ["optical", "31", "''", "The Satavahanas", "a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "axons", "bohrium", "London", "Chernobyl Nuclear Power Plant", "Dalveer Bhandari", "Bush", "northernmost point on the Earth", "Exodus", "David Tennant", "China", "to life", "on Chesapeake Bay, south of Annapolis in Maryland", "A footling breech", "the 1964 Republican National Convention in San Francisco, California", "1926", "Splodgenessabounds", "1990", "sport utility vehicles", "Bob Dylan", "qualitative data, quantitative data or both", "Johannes Gutenberg", "to collect menstrual flow", "a single prefix for all warships of a nation's navy, and other prefixes for auxiliaries and ships of allied services, such as coast guards", "William the Conqueror", "Sir Ernest Rutherford", "Nicole Gale Anderson", "William Chatterton Dix", "April 12, 2017", "Gupta Empire", "Freedom Day", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "Gustav Bauer", "uterus", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "in technical journals, the abstract", "Antarctica", "Walter Scheib", "revenge and karma", "the efferent nerves that directly innervate muscles", "1986", "1546", "a crime", "1942", "Bachendri Pal", "John Adams", "2014", "Ballets Russes", "Jessica Smith", "Edward Elgar", "\"Big Mamie\"", "six", "\"First Family of Competitive eating\"", "$2 billion", "the player", "on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "St. Valentine's Day", "Minnelli", "Gabriel", "opposition parties"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5805260870029015}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.4, 0.0, 0.0, 0.0, 0.9500000000000001, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.29629629629629634, 0.06451612903225806, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-4741", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-4001", "mrqa_hotpotqa-validation-1210", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-1290", "mrqa_searchqa-validation-2795", "mrqa_searchqa-validation-16493"], "SR": 0.484375, "CSR": 0.5589923469387755, "retrieved_ids": ["mrqa_squad-train-76595", "mrqa_squad-train-23594", "mrqa_squad-train-55340", "mrqa_squad-train-13398", "mrqa_squad-train-16286", "mrqa_squad-train-66150", "mrqa_squad-train-59501", "mrqa_squad-train-6352", "mrqa_squad-train-34007", "mrqa_squad-train-32126", "mrqa_squad-train-44645", "mrqa_squad-train-40696", "mrqa_squad-train-74227", "mrqa_squad-train-31618", "mrqa_squad-train-26491", "mrqa_squad-train-42601", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-5873", "mrqa_squad-validation-2122", "mrqa_searchqa-validation-3375", "mrqa_newsqa-validation-530", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-4206", "mrqa_squad-validation-6914", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9419", "mrqa_hotpotqa-validation-2042", "mrqa_newsqa-validation-1391", "mrqa_triviaqa-validation-896", "mrqa_squad-validation-4971", "mrqa_searchqa-validation-2118", "mrqa_searchqa-validation-5745"], "EFR": 1.0, "Overall": 0.7453140943877551}, {"timecode": 49, "before_eval_results": {"predictions": ["1912", "Standard Oil", "Kabuki", "These Boots Are Made for Walkin", "pachycephalosaurs", "united states", "Nancy Lopez", "ozone", "Who\\'s the Boss", "Donnie Wahlberg", "Tasmania", "Oriole Park", "Tunisia", "Queen Mary 2", "Zionism", "Prague", "horse-breaking", "(Isa) Newton", "Toby Keith", "the accordion", "the Black swan", "Edith Piaf", "the Stratosphere Tower", "parkinsonism", "Strings", "Burger", "Guinevere", "Department of Energy", "tangerine", "united states", "Dead Ringers", "Johann Strauss II", "Solidarity", "(Charlie) Carter", "love", "Charles Lindbergh", "the Lotus", "Haunted Mansion", "blog", "the Golden Bear", "a war between political factions", "a vacuum flask", "Teen Titans Go!", "a swan", "Istanbul", "Mary Poppins", "St. Louis", "the Amish", "the CIA", "Levi\\'s", "Badminton", "Canada", "broadcast `` as live '' on the Sunday", "autopistas", "Phar Lap", "a dove", "Lewis Carroll", "1983", "Excalibur Hotel and Casino", "Czech Kingdom", "Arsenal", "Abdullah Gul,", "humans", "Action Comics"], "metric_results": {"EM": 0.625, "QA-F1": 0.7255208333333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-3755", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-767", "mrqa_searchqa-validation-13380", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-1337", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-6473", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-8129", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-15313", "mrqa_naturalquestions-validation-8350"], "SR": 0.625, "CSR": 0.5603125, "EFR": 1.0, "Overall": 0.745578125}, {"timecode": 50, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1338", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1604", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7716", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12265", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3025", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-3764", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8267", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9911", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3635", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.86328125, "KG": 0.528125, "before_eval_results": {"predictions": ["Aerosmith", "Willa Cather", "Senate", "the Who", "a science fiction novel", "Bismarck", "a quartz watch", "a Tricer Programme", "Luisa Tetrazzini", "Renoir", "the polio vaccine", "THUMPER", "William", "a bar exam", "Dostoevsky", "Smucker", "a Spanish conquistador", "Canada", "grease", "Hollandaise", "Esau", "Dry Ice", "Martin Luther King", "oxygen", "a catalyst", "Kansas City", "a sergeant in the Palm Beach County sheriff`s Organized Crime Bureau", "Uganda", "senators", "Sappho", "the Battle of Thermopylae", "Maccabees", "John Paul Jones", "Hamlet", "a flounder", "Ganga", "New Brunswick", "Copacabana", "I Write the Songs", "We Own the Night", "Shimon Peres", "Mr. & Mrs. Smith", "fever", "a cake", "Memphis", "Thomas Mann", "Krackel", "a dog eat dog world", "Dmitri Mendeleev", "Azkaban", "tea leaves", "thia Weil", "an Arabic masculine given name and occasional surname with the meaning `` beloved ''", "the heads of federal executive departments who form the Cabinet of the United States", "Microsoft", "General John J. Pershing", "Ross Kemp", "Taylor Swift", "September 23, 1935", "invoicing the employees' work based on an hourly rate, measuring the work effectiveness and project management", "Martin \"Al\" Culhane,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Dangjin", "2015"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6520689596861472}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.25000000000000006, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0625, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15858", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-130", "mrqa_searchqa-validation-803", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-7278", "mrqa_searchqa-validation-14038", "mrqa_searchqa-validation-13163", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-14305", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-16283", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-2780", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-7741", "mrqa_naturalquestions-validation-998", "mrqa_triviaqa-validation-6411", "mrqa_hotpotqa-validation-5801", "mrqa_newsqa-validation-762"], "SR": 0.578125, "CSR": 0.5606617647058824, "EFR": 1.0, "Overall": 0.7451011029411765}, {"timecode": 51, "before_eval_results": {"predictions": ["cupcake", "Wilkie Collins", "Chief of Staff", "the Bible", "Pizarro", "the Civil War", "the Nobel Prize", "Taft", "Dracula", "Technetium", "Nazareth", "867-5309", "Miss Havisham", "Thailand", "Taft", "opal", "Taft", "dense", "air pressure", "echidna", "Taft", "porcelain", "Synchronicity", "Taft", "bees", "dark energy", "Reptiles", "La Scala", "William Herschel", "POMPEIA", "Barbara Walters", "Jubal Anderson", "Taft", "pumice", "watermelon", "Cole Porter", "Sock", "Taft", "Louis Comfort Tiffany", "Cosmopolitan", "Democratic Republic of Madagascar", "C.G. Jung", "Carnarvon", "Ontario", "Olympia", "Ptolemy", "Lovesexy", "Candlestick", "the Black Death", "Google", "Defense", "they believed that it violated their rights as Englishmen to `` No taxation without representation", "1898", "Brevet Colonel Robert E. Lee", "Don Revie", "Gilda", "Buddha", "Lancia-Abarth", "1963", "Brittany Anne Snow", "Christopher Savoie", "authorizing killings and kidnappings by paramilitary death squads.", "\"Watchmen\"", "Cork"], "metric_results": {"EM": 0.453125, "QA-F1": 0.558578431372549}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5490196078431372, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-3888", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-1496", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-440", "mrqa_searchqa-validation-5000", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-10284", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-9206", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11468", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-4585", "mrqa_searchqa-validation-4971", "mrqa_naturalquestions-validation-7223", "mrqa_triviaqa-validation-7153", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-5582"], "SR": 0.453125, "CSR": 0.55859375, "retrieved_ids": ["mrqa_squad-train-27657", "mrqa_squad-train-14295", "mrqa_squad-train-60121", "mrqa_squad-train-36397", "mrqa_squad-train-39859", "mrqa_squad-train-49529", "mrqa_squad-train-13181", "mrqa_squad-train-71592", "mrqa_squad-train-63493", "mrqa_squad-train-47981", "mrqa_squad-train-31448", "mrqa_squad-train-12542", "mrqa_squad-train-6081", "mrqa_squad-train-39607", "mrqa_squad-train-82105", "mrqa_squad-train-61619", "mrqa_triviaqa-validation-2337", "mrqa_hotpotqa-validation-1926", "mrqa_newsqa-validation-2128", "mrqa_naturalquestions-validation-8446", "mrqa_squad-validation-7758", "mrqa_newsqa-validation-3720", "mrqa_hotpotqa-validation-5018", "mrqa_squad-validation-5270", "mrqa_naturalquestions-validation-859", "mrqa_newsqa-validation-1784", "mrqa_searchqa-validation-5965", "mrqa_naturalquestions-validation-8903", "mrqa_hotpotqa-validation-2117", "mrqa_triviaqa-validation-3042", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-8766"], "EFR": 1.0, "Overall": 0.7446875000000001}, {"timecode": 52, "before_eval_results": {"predictions": ["Deere", "(Ella) VICTORIA", "an electron", "the Missouri River", "brandy", "George Babbitt", "GIGO", "Gioachino Rossini", "Ophelia", "Rome", "the Isle of Wight", "Colorado Springs", "hay", "Possession", "(Sir) Scott", "contact lenses", "km", "the Buk", "Vibe", "Pulp Fiction", "yelping", "Frederick Forsyth", "India", "Princess Leia", "Vietnam", "Vince Lombardi", "globalization", "Dubliners", "Sudan", "Kwanzaa", "Warren Buffett", "Charlie\\'s Angels", "President Lincoln", "imagism", "whimper", "obsolete", "Algonquin", "Taiwan", "Mickey Spillane", "Buzz Lightyear", "Jack Bauer", "scissors", "calcium", "Necessity", "a skull", "President Eisenhower", "Atlanta", "California, New York, Texas, Florida, Pennsylvania", "glucose", "hours", "tigre", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Sir Donald Bradman", "The Caucasus Mountains", "Rambo", "Darby and Joan", "antelope", "Jung Yun-ho", "The 7.63\u00d725mm Mauser", "a united Ireland", "rape and murdering a woman in Missouri.", "tara Livesay", "a series of wildfires from late summer through autumn in Bastrop County.", "Estadio Victoria"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6330357142857143}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-3292", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-10292", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-14015", "mrqa_searchqa-validation-9935", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-12531", "mrqa_searchqa-validation-13465", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5962", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-9744", "mrqa_searchqa-validation-4660", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-8817", "mrqa_searchqa-validation-14998", "mrqa_triviaqa-validation-6223", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3364"], "SR": 0.578125, "CSR": 0.5589622641509434, "EFR": 1.0, "Overall": 0.7447612028301887}, {"timecode": 53, "before_eval_results": {"predictions": ["to encourage rebellion against the British authorities", "Debbie Gibson", "three", "either February 28 or March 1", "Ireland", "December 2, 1942", "an expression of unknown origin", "the heart", "March 26, 1973", "Necator americanus", "June 11, 2002", "St Pancras International", "Games played", "climate", "Frank Langella", "The Maidstone Studios in Maidstone, Kent", "Human fertilization", "Aslan", "16 seasons", "Bill Russell", "the Washington Redskins", "Donald Trump", "The vascular cambium", "Epidemiology", "1895", "a contestant", "between the Mediterranean sea to the north and the Red Sea to the south", "a Border Collie", "Washington metropolitan area", "Julie Adams", "Gatiman", "John Young", "Kevin Spacey", "novella", "uterus", "Gene MacLellan", "a flash music video featuring an animated dancing banana was created", "2010", "on location", "Frankie Muniz", "Latin alphabet", "Jason Lee", "Bailey Graffman", "about the hardships of growing older and has no relationship to drug - taking", "2017", "1978", "a loanword of the Visigothic word guma `` man ''", "air moisture", "a tree", "Colonel Robert E. Lee", "brothers Henry, Jojo and Ringo Garza", "Annie Walker", "tides", "Roman history", "Buddha\\'s delight", "Matt Kemp", "bushwhackers", "not feel Misty Cummings has told them everything she knows.", "30,000", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Picasso", "(Scott) Peterson", "(Charles) Bulfinch", "blood pressure"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6171911002304147}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.06451612903225806, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-5363", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1009", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-4240", "mrqa_hotpotqa-validation-3154", "mrqa_hotpotqa-validation-458", "mrqa_newsqa-validation-3873", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-5691"], "SR": 0.578125, "CSR": 0.5593171296296297, "EFR": 0.9629629629629629, "Overall": 0.7374247685185186}, {"timecode": 54, "before_eval_results": {"predictions": ["Norway", "Ecuador", "Home Improvement", "cobalt", "the salmon", "Berlin", "Othello", "Fidel Castro", "Patrick Floyd Garrett", "Montana", "Harvard University", "General Custer", "an arboretum", "Marie Curie", "Abnormal Psychology", "love", "Vice President of the United States", "the Italian flag", "Samuel Butler", "Kitty Kelley", "Abraham Lincoln", "teddy bears", "Crouching Tiger", "baseball", "upsilon", "banknotes", "arizonensis", "Jupiter", "conformation", "the Ziegfeld Girl", "David Cassidy", "the Volcanoes", "the Louvre", "Cyrillic", "opera house", "oxygen", "a house of prayer", "the Wessex", "the snowmobile", "Aaron Copland", "red", "the voltage", "the sleeping Beauty", "Tesla", "Lil Jon", "the diamond", "the plum", "Lizzie Borden", "Hockey", "Pop-Tarts", "bovine spongiform encephalopathy", "Henry Purcell", "Mankombu Sambasivan Swaminathan", "eight", "Ethiopia", "a root", "Argentina", "Real Madrid and the Spain national team", "1983", "Richard Street", "Brazil forward Ronaldinho", "mpire of the Sun", "whether he should be charged with a crime,", "four"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6348958333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-9879", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-8125", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-8825", "mrqa_searchqa-validation-16878", "mrqa_searchqa-validation-10404", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-13693", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9692", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-4573", "mrqa_searchqa-validation-5952", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-13757", "mrqa_triviaqa-validation-1529", "mrqa_hotpotqa-validation-4436", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-3273", "mrqa_triviaqa-validation-261"], "SR": 0.53125, "CSR": 0.5588068181818182, "retrieved_ids": ["mrqa_squad-train-42249", "mrqa_squad-train-53377", "mrqa_squad-train-51785", "mrqa_squad-train-4205", "mrqa_squad-train-68222", "mrqa_squad-train-2317", "mrqa_squad-train-42978", "mrqa_squad-train-36404", "mrqa_squad-train-38997", "mrqa_squad-train-13012", "mrqa_squad-train-23071", "mrqa_squad-train-86044", "mrqa_squad-train-46613", "mrqa_squad-train-67415", "mrqa_squad-train-59867", "mrqa_squad-train-65140", "mrqa_searchqa-validation-1438", "mrqa_squad-validation-4060", "mrqa_hotpotqa-validation-528", "mrqa_naturalquestions-validation-581", "mrqa_hotpotqa-validation-1190", "mrqa_triviaqa-validation-5271", "mrqa_searchqa-validation-847", "mrqa_naturalquestions-validation-833", "mrqa_squad-validation-9213", "mrqa_triviaqa-validation-1702", "mrqa_naturalquestions-validation-6333", "mrqa_squad-validation-9912", "mrqa_triviaqa-validation-4212", "mrqa_searchqa-validation-1446", "mrqa_naturalquestions-validation-4196", "mrqa_hotpotqa-validation-2796"], "EFR": 1.0, "Overall": 0.7447301136363637}, {"timecode": 55, "before_eval_results": {"predictions": ["Santa Fe", "a Beanie Baby", "kick drum", "chess", "cola", "Berlin", "Comedy Central", "USA Swimming", "Bonnie Prince Billy", "Romeo and Juliet", "Cerberus", "death", "the Nile", "bullion", "Plutarch", "17-year-old", "the submarine", "St. Augustine", "the White Sands Missile Range", "Fiji", "the burnoose", "Thomas Edison", "the Mekong", "the 36th", "Valentina Tereshkova", "Canada", "the acrophobia", "Missouri", "ribonucleic acid", "Rubeus Hagrid", "Manitoba", "Arthur Miller", "chocolate", "inshallah", "jordan", "Pamela Anderson", "weight loss", "Idaho", "tootsie", "Edward VI", "the Empire State Building", "love", "Tennessee", "the Constitution", "Toronto", "University of Exeter", "percy pecos", "Thomas Edward Lawrence", "Andy Warhol", "take Me Out to the Ball Game", "Karrueche Tran", "Pac - 12 Conference Champions Stanford Cardinal", "moist temperate climates", "B.F. Skinner", "\"Nokia tune\"", "General Paulus", "Cyprus", "2010", "Tufts University", "the Crips", "U.S. District Judge Ricardo Urbina", "he fears a desperate country with a potential power vacuum that could lash out.", "Monday.", "Cpl. Cesar Laurean"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5638392857142857}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-13435", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9266", "mrqa_searchqa-validation-6438", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-13316", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-10897", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-7386", "mrqa_searchqa-validation-5672", "mrqa_searchqa-validation-1520", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-438", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2517"], "SR": 0.515625, "CSR": 0.5580357142857143, "EFR": 1.0, "Overall": 0.7445758928571429}, {"timecode": 56, "before_eval_results": {"predictions": ["former English county of Humberside", "the Federal Bureau of Prisons", "\"Dumb and Dumber\"", "the first trans-Pacific flight from the United States to Australia", "John Hunt", "Walt Disney Productions", "Reinhard Heydrich", "an Australian-born actor", "\"Sheen Michaels Entertainment\"", "her sixth studio album", "1770", "Sunflower County", "2005", "A Bug's Life", "the U.S. military", "6 mi", "the Qin dynasty", "Kentucky River", "fourth", "\"The Bob Edwards Show\"", "The S7 series", "White Knights of the Ku Klux Klan", "Key West, Florida", "Charles Otto Puth", "Fort Oranje", "Best Actress \u2013 Motion Picture Comedy or Musical", "Soviet Union", "the National Society of Daughters of the American Revolution (NSDAR)", "Martin Scorsese", "General Sir John Monash", "Protestant", "Cardinals RiverBats", "\"Supergirl\"", "Agra", "close to 50 million", "Henry II", "Tyler \"Ty\" Mendoza", "Dalcroze Eurhythmics", "An agricultural cooperative", "Kairi", "Texas", "the Democratic Unionist Party (DUP)", "five", "Candice Swanepoel", "the greater risk-adjusted return of value stocks over growth stocks", "Mark Wahlberg", "multiple awards", "McComb, Mississippi", "\"King of Cool\"", "2009", "\"Losing My Religion\"", "The Royalettes", "six", "1982", "Triumph", "'Q'", "Aintree", "\"Britain's Got Talent\"", "Turkey", "Atlanta", "the Charleston", "emerald", "El burlador de Sevilla", "Pandora"], "metric_results": {"EM": 0.5, "QA-F1": 0.6044270833333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 0.0, 1.0, 0.9333333333333333, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-557", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-983", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-5716", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1229", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-7140"], "SR": 0.5, "CSR": 0.5570175438596492, "EFR": 1.0, "Overall": 0.7443722587719298}, {"timecode": 57, "before_eval_results": {"predictions": ["Italian architect and art theorist Leon Battista Alberti", "in a counter clockwise direction", "The Sixth Extinction II", "December 2, 1942", "Ray Charles", "mid November", "The period of being a junior doctor starts when they qualify as a medical practitioner following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Daniel Suarez", "its population", "the Central and South regions", "Taylor Beyga -- Rhys Jones", "Archie Marries Betty", "Dan Stevens", "9.7 m", "Guwahati", "Muno, Foofa, Brobee, and Toodee", "the modern state system", "efferent nerves", "William Wyler", "Dragon Ball GT", "American country music group The Nitty Gritty Dirt Band", "Donald Fauntleroy Duck", "2013", "the investment bank Friedman Billings Ramsey", "2018", "skeletal muscle", "Joe Spano", "Spanish moss", "Georges Auguste Escoffier", "Nodar Kumaritashvili", "December 27, 2015", "Madeline Reeves", "New England Patriots", "Ashoka", "Britain of Florida", "a charbagh", "an unknown recipient", "Andy Warhol", "Elected Emperor of the Romans", "Dalveer Bhandari", "Middle Eastern alchemy", "Vienna", "Nalini Negi", "Rightly Guided Caliphs", "Lituya Bay in Alaska", "The Demon Barber of Fleet Street", "2003", "George Strait", "A vanishing point", "The United States is a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "Emma Chambers", "Hans Lippershey", "Thermopylae", "August 6, 1845", "an album", "Spain", "Kim Kardashian's", "A family friend of a U.S. soldier", "60 euros", "Blue", "lump", "a knish", "CBS"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6816107693077231}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.28571428571428575, 0.08163265306122448, 0.0, 0.0, 0.6666666666666666, 0.0, 0.25, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.7843137254901961, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-31", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-419", "mrqa_searchqa-validation-416"], "SR": 0.578125, "CSR": 0.5573814655172413, "retrieved_ids": ["mrqa_squad-train-10068", "mrqa_squad-train-7446", "mrqa_squad-train-12379", "mrqa_squad-train-61778", "mrqa_squad-train-1589", "mrqa_squad-train-13400", "mrqa_squad-train-77477", "mrqa_squad-train-41143", "mrqa_squad-train-31635", "mrqa_squad-train-47287", "mrqa_squad-train-66307", "mrqa_squad-train-16117", "mrqa_squad-train-79318", "mrqa_squad-train-61226", "mrqa_squad-train-49750", "mrqa_squad-train-19898", "mrqa_searchqa-validation-16205", "mrqa_hotpotqa-validation-891", "mrqa_naturalquestions-validation-10571", "mrqa_hotpotqa-validation-4210", "mrqa_squad-validation-9567", "mrqa_triviaqa-validation-3058", "mrqa_hotpotqa-validation-5840", "mrqa_newsqa-validation-439", "mrqa_hotpotqa-validation-3455", "mrqa_naturalquestions-validation-7736", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-47", "mrqa_squad-validation-6966", "mrqa_naturalquestions-validation-6917", "mrqa_newsqa-validation-6"], "EFR": 0.9259259259259259, "Overall": 0.7296302282886333}, {"timecode": 58, "before_eval_results": {"predictions": ["the five - year time jump", "111", "every president since Woodrow Wilson, with the notable exception of Herbert Hoover", "Uralic", "22", "IBM", "John Adams", "2018", "Jesus Christ", "14 November 2001", "along the Californian coast at The Inn at Newport Ranch", "24", "Coriolis force", "Hugh S. Johnson", "Paul Lynde", "Erica Rivera", "Malina Weissman", "The Han", "Bo\u00f6tes / bo\u028a\u02c8o\u028ati\u02d0z", "1970", "DeWayne Warren", "the nucleus", "1996", "German", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Tom Brady", "in pilgrimages to Jerusalem", "1996", "Coconut Cove", "Curtis Armstrong", "Dolby Theatre in Hollywood, Los Angeles, California", "Category 4", "Rust", "Karen Gillan", "$19.8 trillion", "1,228 km / h ( 763 mph )", "Tommy Shaw", "the warships and the naval bases of the belligerents", "Nigel Lythgoe, Mia Michaels, and Adam Shankman", "Central Germany", "Atlanta", "Ricky Nelson", "James Chadwick", "in Welch, West Virginia", "Tristan Rogers", "15 February 1998", "Houston Astros", "Americans who served in the armed forces and as civilians during World War II", "in the west by the east coast of Queensland", "in the middle of the 15th century", "Norman Whitfield", "Lake Nicaragua", "sacerdotal", "rue", "Delphi Lawrence", "Juan Manuel Mata Garc\u00eda", "Edward James Olmos", "vitamin injections", "Scotland", "President Obama and Britain's Prince Charles", "a heart", "James Stewart", "Frank Sinatra", "Salisbury"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7271058108558108}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.3076923076923077, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6399999999999999, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.46153846153846156, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-10452", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-3325", "mrqa_searchqa-validation-8636", "mrqa_searchqa-validation-9866", "mrqa_hotpotqa-validation-3324"], "SR": 0.59375, "CSR": 0.5579978813559322, "EFR": 0.9230769230769231, "Overall": 0.7291837108865711}, {"timecode": 59, "before_eval_results": {"predictions": ["Idriss Deby", "at least 300", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "U.S. Supreme Court.", "anti-doping", "Wednesday.", "Linda Hogan", "be silent", "Crandon, Wisconsin", "Turkey", "John Demjanjuk", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "eight", "U.S. Agency for International Development", "Missouri", "\"perezagruzka,\"", "Haiti", "9 percent", "many as 250,000", "Maj. Nidal Malik Hasan,", "Former Mobile County Circuit Judge Herman Thomas", "to stop the Afghan opium trade", "Nick Adenhart", "order", "his father", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Operation Pipeline Express.", "death of cardiac arrest", "Susan Boyle", "10", "April.", "heist follows the recent theft in Switzerland of two paintings by Pablo Picasso,", "promotes fuel economy and safety while boosted the economy", "gasoline", "to do jobs that Arizonans wouldn't do.", "a \"prostitute\"", "digging", "Tottenham", "U.S. diplomacy to prevent Iran from developing nuclear weapons", "Obama and McCain", "3-2", "\"remained at the bottom of the hill surviving on leaves and water from a nearby creek,\"", "\"I didn't think I was going to learn so much about myself through the process,\"", "56", "in an interview Tuesday on CNN's \"Larry King Live.\"", "15-month", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "summer", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "heavy turbulence", "Zac Efron", "drivers who were Daytona Pole Award winners, former Daytona 500 pole winners who competed full - time in 2017, and drivers who qualified for the 2017 Playoffs", "the spectroscopic notation for the associated atomic orbitals", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "son et lumi\u00e8re", "6", "John Buchan's grandson", "NCAA Division I Football Bowl Subdivision", "Kristoffer Kristofferson", "Ben Savage", "The Lion, the Witch and the Wardrobe", "Bering Strait", "Dame Ninette de Valois", "gadsunheit"], "metric_results": {"EM": 0.375, "QA-F1": 0.4910329363420396}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [0.0, 0.0, 0.08, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.25, 0.12500000000000003, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.8571428571428571, 0.0, 0.08695652173913043, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.6111111111111112, 1.0, 0.8205128205128205, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-920", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4133", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-318", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4387", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-7371", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-3871", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-13957"], "SR": 0.375, "CSR": 0.5549479166666667, "EFR": 0.975, "Overall": 0.7389583333333333}, {"timecode": 60, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2665", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10461", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11693", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11890", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-11925", "mrqa_searchqa-validation-12105", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12441", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13915", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14450", "mrqa_searchqa-validation-14481", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16382", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-2634", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5612", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9852", "mrqa_searchqa-validation-9911", "mrqa_searchqa-validation-9935", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1876", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1145", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5886", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937"], "OKR": 0.853515625, "KG": 0.49375, "before_eval_results": {"predictions": ["$7.8 million", "prostate cancer,", "Donald Duck", "Whitney Houston", "The attorney general will announce his decision early next week,", "South Africa", "consumer confidence", "Saturn", "Prague", "35,000.", "Osama", "The EU naval force", "Police say he confessed to holding Elisabeth captive since 1984 and raping her repeatedly,", "threatening messages", "in the west African nation", "misdemeanor", "firefighter", "$273 million", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Haiti", "air support.", "20", "$250,000 for Rivers' charity: God's Love We Deliver.", "March 22,", "12", "Blacks and Hispanics", "Australian officials", "Alberto Espinoza Barron,", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "The Rev. Alberto Cutie", "meter reader", "the man facing up, with his arms out to the side.", "Garth Brooks", "Monday", "Friday,", "10,000", "Ryan Adams.", "$1.5 million", "three out of four", "Burrell Edward Mohler", "a cancerous tumor.", "50", "his health and about a comeback.", "up", "\"utterly baseless.\"", "employment aid, family finances, competitiveness, infrastructure, and actions toward public spending that is more transparent and efficient.", "Caylee,", "1-0", "her boyfriend", "Lonnie", "first", "in English - speaking countries", "Tokyo", "access to US courts", "Doncaster Rovers", "john ley-Ann Fraser", "1973", "2,099", "PPG Paints Arena", "2017", "lactic acid", "The Greatest Show on Earth", "The republic", "Germany"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7211805555555555}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-110", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-3229", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-3833"], "SR": 0.6875, "CSR": 0.5571209016393442, "retrieved_ids": ["mrqa_squad-train-46235", "mrqa_squad-train-16453", "mrqa_squad-train-78379", "mrqa_squad-train-48238", "mrqa_squad-train-47907", "mrqa_squad-train-82237", "mrqa_squad-train-53845", "mrqa_squad-train-68721", "mrqa_squad-train-30810", "mrqa_squad-train-55481", "mrqa_squad-train-42858", "mrqa_squad-train-41450", "mrqa_squad-train-1464", "mrqa_squad-train-84555", "mrqa_squad-train-64444", "mrqa_squad-train-78074", "mrqa_triviaqa-validation-1429", "mrqa_hotpotqa-validation-5496", "mrqa_naturalquestions-validation-2100", "mrqa_searchqa-validation-16715", "mrqa_triviaqa-validation-501", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-2459", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-3430", "mrqa_hotpotqa-validation-466", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-2862", "mrqa_squad-validation-1040", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-2195", "mrqa_hotpotqa-validation-2130"], "EFR": 1.0, "Overall": 0.7332210553278689}, {"timecode": 61, "before_eval_results": {"predictions": ["Windows Easy Transfer", "John Cooper Clarke", "Charlotte of Mecklenburg - Strelitz", "two", "Judi Dench", "accomplish the objectives of the organization", "Omar Khayyam", "P.V. Sindhu", "1665 to 1666", "Saturday", "1982", "Siddharth Arora / Vibhav Roy", "1948", "mitosis", "Butter Island", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Pat McCormick", "10,605", "U.S. Electoral College", "Linda Davis", "Kid Creole & The Coconuts", "provinces along the Yangtze River and in provinces in the south", "New York City", "July 21, 1861", "Nashville, Tennessee", "Paspahegh Indians", "cella", "about 375 miles ( 600 km ) south of Newfoundland", "April 12, 2017", "October 2012", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect in season eight", "John Joseph Patrick Ryan", "49 cents", "counter clockwise", "Kit Harington", "Human anatomy", "above the light source and under the sample in an upright microscope", "divergent tectonic", "prokaryotic", "neutral", "Speaker of the House of Representatives", "1877", "18", "active absorption of water from the soil by the root", "norm that sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "winter festivals", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "the 1820s", "The Royalettes", "Kate '' Mulgrew", "Fats Waller", "john Terry", "\"Land of the Rising Sun\"", "James Hogg", "Lawrence", "\"Realty Bites\"", "national aviation branch", "Thessaloniki and Athens,", "\"extremely weak\"", "on the edge our cinema seats.\"", "chicken Little", "Saturn", "Russia", "Anna Caterina Antonacci"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7184481579467045}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6428571428571429, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.9302325581395349, 0.5, 0.625, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-3163", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-1975", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3093", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-3859", "mrqa_triviaqa-validation-6243"], "SR": 0.609375, "CSR": 0.5579637096774194, "EFR": 0.88, "Overall": 0.7093896169354839}, {"timecode": 62, "before_eval_results": {"predictions": ["1983", "Bacon", "from 1922 to 1991", "79", "Gibraltar", "in 1 January 1904", "Thebes", "Brooke Wexler", "October 2", "in the 1980s", "Alabama's capital is Montgomery", "Evermoist", "in the mid - to late 1920s", "differential erosion", "Kanawha River", "Graham McTavish", "Thomas Alva Edison", "since been adopted by five other countries", "due to a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "Richard Masur", "Frankie Valli", "one", "JackScanlon", "Saturday", "the human body", "sometime in 2018", "2015", "Sarah Josepha Hale", "Nickelback", "Ledger", "The India and Pakistan Border", "in a 1945 NCAA game between Columbia and Fordham", "2017", "permanently absorbed the superhuman powers and the psyche of Carol Danvers", "in a brownstone in Brooklyn Heights", "Pink Floyd", "On 6 March 1983", "during Christmas season in the late 1970s", "in 1986", "1939", "Himadri Station", "at the 2012 ceremony", "birch", "in February 2017 in Japan and in March 2018 in North America and Europe", "FaZe Rug", "at the fictional elite conservative Vermont boarding school Welton Academy", "April 3, 1973", "9 February 2018", "The long - hair gene is recessive", "94 by 50 feet", "Tom\u00e1s de Torquemada", "in 1985", "Kent", "Nicaraguan Sign Language", "Dusty Dvoracek", "South America", "Los Angeles", "has an inspiration: U.S. President Barack Obama.", "the foyer of the BBC building in Glasgow, Scotland", "to disrupt the inauguration,", "Frederic Chopin", "spring", "pesos", "The Rev. Alberto Cutie"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6485837232620322}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.2, 0.0, 0.5, 0.8181818181818181, 0.5333333333333333, 0.0, 0.4, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.9411764705882353, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-4432", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-761", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-219", "mrqa_newsqa-validation-1330"], "SR": 0.515625, "CSR": 0.5572916666666667, "EFR": 0.967741935483871, "Overall": 0.7268035954301075}, {"timecode": 63, "before_eval_results": {"predictions": ["adat budaya melayu", "Billy Martin", "seaport", "chainmaille", "Strindberg's", "osca tosca", "Morocco", "hearth Menotti", "WolfeScience.com", "tosca", "Pop art", "embalming", "Portland", "Mariah Carey", "Dionysus", "an eel", "symbiosis", "Planets", "46th Vice President", "Hold On", "The Lost World", "Prince Edward Island", "New York Presbyterian Hospital", "the Bosporus", "Red Heat", "the Atlas Mountains", "kafkaesque", "Heather Mills", "snow", "Paris", "Mont Blanc On", "Lacoste", "preemption", "the Nobel", "summer", "osca", "Jawaharlal Nehru", "end of day", "the Agony", "a cat", "congruent", "Spain", "toad", "San Francisco", "A Brief History of Time", "a crossword", "Macy's", "spouse of Bath's Tale", "a midget", "Hillary's America", "Benazir Bhutto", "CBS", "Gibraltar", "Orographic lift", "Virginia Plain", "strawberry", "Venice", "Republic of Chad", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Stephen King", "Pat Quinn", "murder in the beating death of a company boss who fired them.", "\"Walk -- Don't Run\"", "Comeng and Clyde Engineering"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6490327380952381}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.28571428571428575, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-11842", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-14989", "mrqa_searchqa-validation-303", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-16392", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-15031", "mrqa_searchqa-validation-4348", "mrqa_hotpotqa-validation-3558", "mrqa_hotpotqa-validation-5688", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2308"], "SR": 0.578125, "CSR": 0.5576171875, "retrieved_ids": ["mrqa_squad-train-35770", "mrqa_squad-train-21682", "mrqa_squad-train-46945", "mrqa_squad-train-67490", "mrqa_squad-train-81773", "mrqa_squad-train-60107", "mrqa_squad-train-24670", "mrqa_squad-train-72659", "mrqa_squad-train-30311", "mrqa_squad-train-14926", "mrqa_squad-train-60579", "mrqa_squad-train-42160", "mrqa_squad-train-48131", "mrqa_squad-train-66428", "mrqa_squad-train-2798", "mrqa_squad-train-14715", "mrqa_hotpotqa-validation-2935", "mrqa_newsqa-validation-2391", "mrqa_naturalquestions-validation-3745", "mrqa_newsqa-validation-3961", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-16774", "mrqa_naturalquestions-validation-9383", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-14323", "mrqa_hotpotqa-validation-1716", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-373", "mrqa_searchqa-validation-9038", "mrqa_naturalquestions-validation-6321", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-11764"], "EFR": 1.0, "Overall": 0.7333203125}, {"timecode": 64, "before_eval_results": {"predictions": ["a person of Latin American or Iberian ancestry", "Bonnie Elizabeth Parker", "Forrest Gump", "a relationship with a man who proves to", "The Crossing Guard", "Thomas Beekman", "The original foreword", "Friday Night Lights", "contractions", "Skull and Crossbones", "large country in the world", "Florida State", "Ukraine", "a ship", "a Tibetan antelope", "The Godfather", "a bolt", "Australia", "Napalm", "Roald Dahl", "Mount Kenya", "John Lennon", "the Stamp Act", "Princeton University", "CO2", "The Battle of Thermopylae", "Buenos Aires", "Mulberry Street", "Romeo & Juliet", "Prescott", "Helen Hayes", "Wesley Clark", "iron", "Sing Sing", "salmon", "Catch A Falling Star", "(Jules) Verne", "Abercrombie & Fitch", "Beatrix Potter", "The Romaunt", "a cassowary", "the Gadsden Purchase", "the umbilical cord", "trees", "Sweden", "British Parliament", "the Red Cross", "terrorists", "The Sunshine Band", "Vimy Ridge", "Graceland", "David Tennant", "on the southeastern coast of the Commonwealth of Virginia in the United States", "pulmonary heart disease ( cor pulmonale )", "9", "squash", "website", "Gospel Starlighter", "500-room", "Wal-Mart Canada Corp.", "Tuesday", "Diversity", "were directly involved in an Internet broadband deal with a Chinese firm.", "gang rape"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6562088815789473}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9473684210526316, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11200", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-6468", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-16325", "mrqa_searchqa-validation-691", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-16100", "mrqa_searchqa-validation-8741", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-11743", "mrqa_searchqa-validation-2681", "mrqa_searchqa-validation-6512", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-12394", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-15724", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-14787", "mrqa_searchqa-validation-3727", "mrqa_naturalquestions-validation-1680", "mrqa_hotpotqa-validation-2565", "mrqa_newsqa-validation-3111"], "SR": 0.546875, "CSR": 0.557451923076923, "EFR": 1.0, "Overall": 0.7332872596153845}, {"timecode": 65, "before_eval_results": {"predictions": ["Brown Mountain Lights", "3,384,569", "Vishal Bhardwaj", "around 169 CE", "Ed O'Neill", "Milwaukee Bucks", "138,535", "Dennis Hull,", "Max Martin, Savan Kotecha and Ilya Salmanzadeh", "alternate", "Love Letter", "Brazil", "1968", "2005", "Stacey Kent", "Shenandoah", "Regionalliga Nord", "Campeonato Brasileiro S\u00e9rie A", "Samantha Spiro", "William Shakespeare", "over 1.6 million", "the Yule goat", "West Africa", "West Tambaram", "Portal A Interactive", "Graduados", "Sada Carolyn Thompson", "World Health Organization", "Chow Tai Fook Enterprises", "Michelle Anne Sinclair", "2011", "2012", "Honolulu", "Lalit", "Kal Ho Naa Ho", "in Srinagar", "Ronald Wilson Reagan", "musicology", "left", "1835", "1926 Paris during the period of the Lost Generation", "Erreway", "Forbes", "January 28, 2016", "69.7 million", "500-room", "in Sochi, Russia", "2027 Fairmount Avenue", "New Hampshire Gospel Radio", "Black Panther Party", "globetrotters", "1980", "intermembrane space", "Butter Island off North Haven, Maine in the Penobscot Bay", "eye", "Tolstoy", "gizzard", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "2,000", "Some have complained that his wins are too routine, and purists grouse that he does not poses the quality of \"hinkaku,\"", "Sir Loin", "Eleanor (Nell) Taylor", "Sue Miller", "Dan Parris,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.598998708010336}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.4444444444444445, 1.0, 0.4, 1.0, 0.8, 1.0, 0.8, 1.0, 0.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5581395348837209, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-4973", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-180", "mrqa_triviaqa-validation-3498", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1122", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-1265", "mrqa_newsqa-validation-2296"], "SR": 0.46875, "CSR": 0.5561079545454546, "EFR": 0.9705882352941176, "Overall": 0.7271361129679145}, {"timecode": 66, "before_eval_results": {"predictions": ["musician", "Captain Hans Geering", "the 50JJB Sports Fitness Clubs", "October 2015", "American", "Han Sung-soo", "Bhushan Patel", "Pamela Chopra", "Mark O'Connor", "Kinnairdy Castle", "South African", "Barbara Feldon", "The 2008\u201309 UEFA Champions League", "National Hockey League", "intelligent design", "Parlophone Records", "Soldier in Truck", "eight", "Cuban", "arts manager", "\"The Royal Family\".", "girls aged 11 to 18", "Jackie Harris", "water", "National Basketball Development League", "Operation Overlord", "invoice", "Sir Christopher Wren", "1851", "the first month of World War I", "12-year", "World War II", "Graham Payn", "Martin Truex Jr.", "twice", "Malayalam cinema", "47,818", "Every Rose Has Its Thorn", "13", "Scarborough, Maine", "1953", "German", "ScMartinat Mountain", "brothers Malcolm and Angus Young", "Boston Celtics", "1912", "Jewish", "Bill Lewis", "Transgender rights activist", "1968", "311", "Jason Lee", "Steve Mazzaro & Missi Hale", "slavery", "the Florida Current", "The History Boys", "a tabby", "Mexico", "three", "'overcharged.'\"", "Melba", "Gary", "Henry Hudson", "the optic disc"], "metric_results": {"EM": 0.5, "QA-F1": 0.6187499999999999}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-1698", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4061", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-2842", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5294", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5004", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-7417", "mrqa_newsqa-validation-2935", "mrqa_searchqa-validation-7226", "mrqa_naturalquestions-validation-3368"], "SR": 0.5, "CSR": 0.5552705223880596, "retrieved_ids": ["mrqa_squad-train-39515", "mrqa_squad-train-78846", "mrqa_squad-train-33646", "mrqa_squad-train-43866", "mrqa_squad-train-30417", "mrqa_squad-train-69244", "mrqa_squad-train-41954", "mrqa_squad-train-78063", "mrqa_squad-train-16551", "mrqa_squad-train-15562", "mrqa_squad-train-65724", "mrqa_squad-train-44792", "mrqa_squad-train-36943", "mrqa_squad-train-4761", "mrqa_squad-train-49764", "mrqa_squad-train-24131", "mrqa_naturalquestions-validation-3926", "mrqa_hotpotqa-validation-2459", "mrqa_naturalquestions-validation-5452", "mrqa_hotpotqa-validation-1413", "mrqa_squad-validation-3597", "mrqa_searchqa-validation-10585", "mrqa_triviaqa-validation-1595", "mrqa_naturalquestions-validation-3841", "mrqa_newsqa-validation-1104", "mrqa_hotpotqa-validation-3343", "mrqa_triviaqa-validation-1794", "mrqa_naturalquestions-validation-8903", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-3329", "mrqa_searchqa-validation-3221", "mrqa_squad-validation-6914"], "EFR": 1.0, "Overall": 0.7328509794776119}, {"timecode": 67, "before_eval_results": {"predictions": ["classical", "Dr. Alberto Taquini", "democracy and personal freedom", "Rudolf Kehrer", "2015", "Fu\u00dfballklub Austria Wien", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Edward Albert Heimberger", "Squam Lake", "Lindemans Brewery", "Croatian", "Harpe brothers", "Adam Rex", "Marvel's Agent Carter", "Everton", "\"Holinshed's Chronicles\"", "XII", "coal town in McDowell County, West Virginia", "1975", "Deathwatch", "Ted Bundy", "1943", "doctorates", "URO VAMTAC", "Malta", "East Knoyle", "Philadelphia", "Maria Brink", "Jyothika", "Sippin' on Some Syrup", "24 January 76 \u2013 10 July 138", "Leonard Cohen", "Simon Bolivar Buckner", "BraveStarr", "25 million records", "Paul Avery", "Sunflower County", "848 km", "Ellesmere Port", "Homer Hickam, Jr.", "South America", "Montreal, Quebec", "Eugene", "Chief of the Operations Staff of the Armed Forces High Command", "CBS News", "Philadelphia, Pennsylvania", "Parlophone", "June", "the best known globetrotters", "Henry Lau", "John Schlesinger", "Pasek & Paul", "Diary of a Wimpy Kid", "Ed Sheeran", "Sarah Palin", "Ub Iwerks", "Seven", "college campus.\"", "14 bodies", "one American diplomat to a \"prostitute\"", "drop-down", "The Muppet Show", "Iberia", "arson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6248579545454546}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false], "QA-F1": [0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.4, 0.5, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-705", "mrqa_hotpotqa-validation-259", "mrqa_hotpotqa-validation-5082", "mrqa_hotpotqa-validation-5433", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-5385", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-6828", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-3337", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-5330", "mrqa_newsqa-validation-1398"], "SR": 0.453125, "CSR": 0.5537683823529411, "EFR": 1.0, "Overall": 0.7325505514705882}, {"timecode": 68, "before_eval_results": {"predictions": ["Stephen T. Kay", "Trey Parker", "Tampa Bay Storm", "Wayman Tisdale", "Manchester\u2013Boston Regional Airport", "the Corps of Discovery", "sarod", "Fleetwood Mac", "County Louth", "Chelmsford", "2009", "Comedy Central", "five", "Lazio", "Mick Jackson", "The Livingston family", "U.S. saloon-keeper", "Fort Orange", "\"Kitty Hawk\"", "the Qin dynasty", "best known as the star of the self-produced sitcoms \"I Love Lucy\", \"The Lucy Show\", \"Here's Lucy\", and \"Life with Lucy\".", "Charles de Gaulle Airport", "Francophone", "Matthew Perry", "cricket fighting", "Jaguar Land Rover Limited", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "Noel Gallagher", "1966", "Love", "band director", "Ford Field in Detroit, Michigan", "drake v. Goldberg", "Florence Nightingale", "Bolton, England", "February 9, 1994", "Las Vegas", "Kongo", "Missouri River", "World War II", "Ector County", "Norse", "Mercedes-Benz Superdome", "1979 to 2013", "He first came to prominence when he and his first cousin, WWE wrestler John Cena, collaborated on the 2005 album \"You Can't See Me\",", "October 12, 1962", "rural", "Lombardy", "August 14, 1848", "Punjabi/Pashtun", "43rd", "Shalmaneser V", "He chose to charter a plane to reach their next venue in Moorhead, Minnesota", "1948", "Mallard", "Sherlock Holmes", "Tokyo", "in September internet giant Twitter announced its intention to set up headquarters in Dublin.", "near Warsaw, Kentucky,", "Marxist guerrillas", "(W. Somerset) Maugham", "a pine", "a typing instructor is concerned that students who pass her typing classes can literally be the deal... in today's computerized world, passing a typing wpm test can't even say where the keys are", "a cemetery"], "metric_results": {"EM": 0.5, "QA-F1": 0.6294935966810966}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.13333333333333333, 0.3333333333333333, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-78", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-1316", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7939", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-2573", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-16102", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-1104"], "SR": 0.5, "CSR": 0.5529891304347826, "EFR": 1.0, "Overall": 0.7323947010869565}, {"timecode": 69, "before_eval_results": {"predictions": ["November 1999", "the senior-most judge of the supreme court", "the Norman given name Robert", "March 16, 2018", "the Maryland Senate's actions", "The Battle of Britain is about to begin", "relieves the driving motor from the load of holding the elevator cab", "The federal government received only those powers which the colonies had recognized as belonging to king and parliament.", "Matt Monro", "December 2, 2013", "over most of the sixth century", "the original title of the novelization of the 1977 film Star Wars", "In 1998", "Walter Pauk", "Lindsey Buckingham and Richard Dashut", "1975", "maquila", "1959", "Schwarzenegger", "Salman Khan", "2015", "1965", "Isekai wa Sum\u0101tofon", "Representatives and Delegates serve for two - year terms, while the Resident Commissioner serves for four years", "a recognized group of people who jointly oversee the activities of an organization", "New Zealand", "a large roasted turkey", "currently a free agent", "Joe Pizzulo and Leeza Miller", "Times Square in New York City west to Lincoln Park in San Francisco", "1 immediately follows the year 1 BC", "Captaincy General of Guatemala", "the 1960s", "Florida and into the town of Coconut Cove", "the economy", "tropical desert climate", "1988", "in the 1970s", "anembryonic gestation", "February 27, 2015", "John Hancock", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "lakes or reservoirs at high altitudes", "March 5, 2014", "FIGG Bridge Engineers, a Tallahassee - based firm", "Carol Ann Susi", "a hydrolysis reaction", "a crown cutting of the fruit", "Chris Rea", "1998", "16 August 1975", "a conifer", "Goliath", "Frankenstein", "Dutch", "The MGM Grand fire", "India Today", "Jet Republic", "Thousands", "Kabul", "Prison Break", "the Lone Ranger", "Egypt", "President Obama and Britain's Prince Charles"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5024461813234066}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.25, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 0.13333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.42857142857142855, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.26666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9328", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-3386", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-8183", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-6372", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-2261", "mrqa_hotpotqa-validation-886", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-12778", "mrqa_newsqa-validation-2497"], "SR": 0.421875, "CSR": 0.5511160714285714, "retrieved_ids": ["mrqa_squad-train-36829", "mrqa_squad-train-13152", "mrqa_squad-train-489", "mrqa_squad-train-64411", "mrqa_squad-train-72040", "mrqa_squad-train-2965", "mrqa_squad-train-33558", "mrqa_squad-train-61850", "mrqa_squad-train-44711", "mrqa_squad-train-31862", "mrqa_squad-train-28058", "mrqa_squad-train-54624", "mrqa_squad-train-54759", "mrqa_squad-train-52212", "mrqa_squad-train-5026", "mrqa_squad-train-26148", "mrqa_searchqa-validation-4179", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-1313", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-9343", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-2754", "mrqa_searchqa-validation-5691", "mrqa_hotpotqa-validation-5565", "mrqa_searchqa-validation-6523", "mrqa_naturalquestions-validation-5538", "mrqa_squad-validation-6640", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-3527"], "EFR": 0.9459459459459459, "Overall": 0.7212092784749035}, {"timecode": 70, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4068", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-757", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8993", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10731", "mrqa_searchqa-validation-10872", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16049", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-226", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7278", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1330", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1949", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3687", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4301", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-562", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-6362", "mrqa_squad-validation-66", "mrqa_squad-validation-6962", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7693", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-855", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_squad-validation-9923", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4657", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-7726"], "OKR": 0.849609375, "KG": 0.5203125, "before_eval_results": {"predictions": ["aikido", "Peter Stuyvesant", "Cornell", "cactus", "NASCAR", "Vivaldi", "seven", "Grace Slick", "London", "Sweden", "Phil Lynott", "purple", "Zachary Taylor", "Kempton Park", "Leonardo Da Vinci", "John Everett Millais", "Belfast", "coconut shy", "Fulham", "Kent", "bryophyta", "Margot Fonteyn", "jordan", "fondue", "glockenspiel", "Drew Carey", "William Shakespeare", "Mackinac Bridge", "a 965-foot ocean liner", "Sunny After afternoon", "tea", "Joan Crawford", "red", "Alexandria", "1969", "the queen", "boxing", "(Lord) Beaconsfield", "Earl", "Babylon", "Nottingham", "George III", "25", "(S. Jimmy) McGovern", "Antoine Lavoisier", "New Zealand", "Agenor", "X-Men Origins: Wolverine", "Jimmy Carter", "David Mitchell", "King William IV", "December 15, 2017", "Gracyn Shinyei", "Fox Ranch in Malibu Creek State Park", "Loretta Lynn", "National Association for the Advancement of Colored People", "gGmbH", "Second seed Fernando Gonzalez", "Alaska or Hawaii.", "\"falling space debris,\"", "jury dutyserve", "an egg", "Brooke Shields", "Nepal"], "metric_results": {"EM": 0.625, "QA-F1": 0.7120339912280702}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.7368421052631579, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-968", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6959", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6308", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-468", "mrqa_hotpotqa-validation-4252", "mrqa_hotpotqa-validation-1720", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-3347", "mrqa_searchqa-validation-9986"], "SR": 0.625, "CSR": 0.552156690140845, "EFR": 0.9583333333333334, "Overall": 0.7280355046948357}, {"timecode": 71, "before_eval_results": {"predictions": ["four", "Wyoming", "tobacco", "Oprah", "Phil Spector", "Margaret Beckett", "Robin Hood Men in Tights", "Rapa Nui", "Fringillidae", "Greyfriars", "millais", "pasta", "Ritchie", "Harry Palmer", "Augustus", "Kelly Gang", "Yorkshire", "China", "catherine cookson", "baron-on-Trent", "trumpet", "$100", "Dionysus", "sheep", "the French Open", "cactus", "a child", "Peach", "Greece", "hip joint", "bruise", "barber", "sienna", "baron", "Washington, D.C.", "Daedalus", "Tommy Roe", "baron villa", "barleycorn", "komando Pasukan Khusus", "Uranus", "Amalthea", "exploits", "behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing, or engaging in sexual activity", "pascal", "brain", "ash", "1985", "sisyphus", "Kathryn C. Taylor", "Hawaii", "usually in May", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "diurnal and insectivorous", "Derry City F.C.", "URO VAMTAC", "Christian", "Stop the War Coalition", "U.S. program to assassinate terrorists in Iraq.", "\"procedure on her heart,\"", "lexicographer", "a sandhill crane", "Billy the Kid", "mayor of Seoul"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6023155663780664}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7878787878787877, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5335", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-7199", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-55", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-1222", "mrqa_hotpotqa-validation-1630", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2547", "mrqa_searchqa-validation-16275", "mrqa_newsqa-validation-3686"], "SR": 0.515625, "CSR": 0.5516493055555556, "EFR": 0.9354838709677419, "Overall": 0.7233641353046595}, {"timecode": 72, "before_eval_results": {"predictions": ["20th feature-length film, is going to be a downer.", "Colombia", "Afghan homes and compounds,", "has refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "they'd get to bring a new puppy with them to the White House in January.", "three", "Kurdistan Freedom Falcons, known as TAK,", "closing these racial gaps.", "Barbara Streisand's", "U.S. President-elect Barack Obama", "phone calls or by text messaging,", "travels four hours to reach a government-run health facility that provides her with free drug treatment.", "July 18, 1994,", "off the coast", "Pixar's", "Friday", "state senators who will decide whether to remove him from office", "\"The Rosie Show,\"", "Louela Binlac", "South African captain Graeme Smith", "Brazil", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds,", "Clifford Harris,", "Jeffrey Jamaleldine", "to sniff out cell phones.", "21 percent", "five", "Carol Fowler", "around 8 p.m. local time Thursday", "15-year-old's", "150", "has broken no laws, and what he did doesn't affect us at all.", "Ennis, County Clare", "haitians", "to end her trip in Crawford and hoped to arrive on September 15 after hitting the road from the White House in July.", "Daniel Radcliffe", "1 million people", "55-year-old", "Southeast,", "Aravane Rezai", "Russian bombers", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi, according to Karachi Police Chief Waseem Ahmad.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "Caylee Anthony", "March 22,", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "Steve Young", "prisoners", "Choi", "Apple employees", "September 1973", "Norman Greenbaum", "in November 1975", "kiki", "Leeds", "prisca", "Matthew Ward Winer", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Sun Woong", "darts", "Joe Louis", "16th", "\"Basileia t\u014dn Rh\u014dmai\u014dn\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5824892406155706}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.09523809523809525, 1.0, 1.0, 0.962962962962963, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0689655172413793, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.28571428571428575, 0.0, 0.923076923076923, 0.19354838709677416, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6956521739130436, 0.0, 1.0, 1.0, 0.27027027027027023, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4104", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-1433", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-81", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-857", "mrqa_triviaqa-validation-3654", "mrqa_hotpotqa-validation-3757", "mrqa_searchqa-validation-2301", "mrqa_searchqa-validation-10852", "mrqa_hotpotqa-validation-1958"], "SR": 0.5, "CSR": 0.5509417808219178, "retrieved_ids": ["mrqa_squad-train-33744", "mrqa_squad-train-77109", "mrqa_squad-train-71007", "mrqa_squad-train-15358", "mrqa_squad-train-44431", "mrqa_squad-train-14237", "mrqa_squad-train-44158", "mrqa_squad-train-66798", "mrqa_squad-train-55120", "mrqa_squad-train-82228", "mrqa_squad-train-47415", "mrqa_squad-train-70442", "mrqa_squad-train-49549", "mrqa_squad-train-49494", "mrqa_squad-train-47254", "mrqa_squad-train-54274", "mrqa_searchqa-validation-2542", "mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-1368", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-206", "mrqa_newsqa-validation-219", "mrqa_squad-validation-3899", "mrqa_naturalquestions-validation-3432", "mrqa_squad-validation-8538", "mrqa_naturalquestions-validation-7223", "mrqa_searchqa-validation-130", "mrqa_squad-validation-2293", "mrqa_naturalquestions-validation-3745", "mrqa_hotpotqa-validation-1190", "mrqa_naturalquestions-validation-5452", "mrqa_hotpotqa-validation-3833"], "EFR": 0.96875, "Overall": 0.7298758561643834}, {"timecode": 73, "before_eval_results": {"predictions": ["12", "Sodra nongovernmental organization,", "the United States", "not feel Misty Cummings has told them everything she knows.", "Mogadishu", "suicide car bombing", "more than 100", "The three gunshot wound, Van Hollen said, struck Peterson in the left bicep and appeared to have been fired from a rifle \"at some distance.\"", "Scarlett Keeling", "took on water", "two", "Omar", "Dr. Cade", "Christian farmer", "Twitter", "new field, said Fanny Douvere, a co-principal investigator at UNESCO's Intergovernmental Oceanographic Commission.", "relatives of the five suspects,", "165-room", "U.S. Holocaust Memorial Museum,", "simple puzzle video game", "curfew in Jaipur", "40-year-old", "ask the AU contingent to leave once there is a solid political solution to the conflict.", "U.S. 93 in White Hills, Arizona, near Hoover Dam.", "different women coping with breast cancer in five vignettes.", "two years", "269,000", "in Harare", "the UK", "found Julissa Brisman, 26, unconscious with multiple gunshot wounds on April 14.", "because the guerrillas are out to harm indigenous populations.", "fighters", "they'd get to bring a new puppy with them to the White House in January.", "as he tried to throw a petrol bomb at the officers,", "Austin, Texas", "forgery and flying without a valid license,", "Kurt Cobain", "ALS6", "he and the other attackers were from Pakistan and asked for a meeting with Pakistan's High Commission.", "a rabbit hole,", "Larry Ellison", "anesthetic", "10", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "39,", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "on the bench", "12.3 million", "\"private client\" list,", "Seasons of My Heart", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "Justice A.K Mathur", "somatic cell nuclear transfer ( SCNT )", "1956", "The Parson Russell Terrier", "Quito", "Nicolas cage", "Don DeLillo", "10 Years", "\"Queen City\"", "Carl Sagan", "Israel", "Copacabana", "50\u201340\u201390"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6780100432174786}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.8750000000000001, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.06896551724137931, 1.0, 0.888888888888889, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-534", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2397", "mrqa_naturalquestions-validation-5109", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-345", "mrqa_hotpotqa-validation-3869", "mrqa_searchqa-validation-8780", "mrqa_hotpotqa-validation-1757"], "SR": 0.609375, "CSR": 0.5517314189189189, "EFR": 0.96, "Overall": 0.7282837837837837}, {"timecode": 74, "before_eval_results": {"predictions": ["brazil", "north yorkshire", "Lou Gehrig", "Goat Island", "Loretta Lynn", "a bat", "email sent to you in bulk by an unknown sender that you do in fact welcome", "Andrew Lloyd Webber", "east of Eden", "georgia boston", "Mark Hamill", "Aslan", "kabaddi", "The Merchant of Venice", "kvetching", "colombia", "Harold Wilson", "colombia", "Handley Page", "Tina Turner", "puffer fish", "Leeds United", "Capricorn", "john k Kennedy", "the bluebird", "Toy Story", "(Dean) Wareham", "white", "the Kiel Canal", "colombia", "Avro", "Sarah Vaughan", "Abu Dhabi", "33 miles", "Emily Davison", "Marc Brunel", "Aberystwyth", "oasis", "Peter Sellers", "the Indus valley", "colombia Belgica", "a even break", "david bowie", "Lorne Greene", "1655", "fusilli", "Thai", "Duke Viola", "\u00e1stron", "Ramadan", "sewing machines", "Montgomery County", "11 January 1923", "the Roman Empire", "1994", "Two Pi\u00f1a Coladas", "Tamara Ecclestone Rutland", "Kaka,", "comments he made after his new boss, golfer Adam Scott, defeated Woods at the Bridgestone Invitational in Ohio in August.", "Three", "a Corporal", "William Henry Harrison", "Shelley", "1982"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7067708333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-1135", "mrqa_triviaqa-validation-1712", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6722", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-3882", "mrqa_hotpotqa-validation-4672", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-9552"], "SR": 0.65625, "CSR": 0.553125, "EFR": 0.9545454545454546, "Overall": 0.727471590909091}, {"timecode": 75, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "reached an agreement late Thursday to form a government of national reconciliation.", "Dead Weather's \"Horehound\"", "two", "Oprah Winfrey's school", "twice.", "Haleigh Cummings,", "eradication of the Zetas cartel", "Hamas,", "a bank", "Miss USA Rima Fakih", "against using injectable vitamin supplements because the quantities are not regulated.", "edward", "Arthur E. Morgan III,", "job training", "moved into her rental house", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "22", "1969", "diabetes and hypertension,", "Tillakaratne Dilshan scored his sixth Test century", "Tuesday", "100,000", "The remains of Cologne's archive building following the collapse on Tuesday afternoon.", "$17,000", "President Obama", "Caylee Anthony", "Glasgow office", "journalists and the flight crew will be freed,", "AMD,", "returning combat veterans", "September 21.", "Thursday.", "246", "40", "stole", "fine", "elephant sanctuary", "At least 15", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "Arabic, French and English", "$40", "many as 250,000", "state senators", "Iran", "Mohammed Mohsen Zayed,", "1950s,", "Orbiting Carbon Observatory", "2.5 million", "5,600", "Yemen,", "in desperation, with only a small chance of success and time running out on the clock", "amphetamines", "contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "(B\u00e9la) Bart\u00f3k", "The Fridge", "blackcurrant liquor", "Premier League club Everton", "Magic Band", "Fat Man", "messenger", "a beaver", "Jan Hus", "R2-D2"], "metric_results": {"EM": 0.671875, "QA-F1": 0.730738750496815}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true], "QA-F1": [0.4444444444444445, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1904761904761905, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060606060606060615, 1.0, 0.06451612903225806, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-817", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-2207", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-5388"], "SR": 0.671875, "CSR": 0.5546875, "retrieved_ids": ["mrqa_squad-train-1911", "mrqa_squad-train-27109", "mrqa_squad-train-28113", "mrqa_squad-train-14614", "mrqa_squad-train-48326", "mrqa_squad-train-32908", "mrqa_squad-train-38876", "mrqa_squad-train-5706", "mrqa_squad-train-22277", "mrqa_squad-train-50436", "mrqa_squad-train-33528", "mrqa_squad-train-56309", "mrqa_squad-train-55424", "mrqa_squad-train-17679", "mrqa_squad-train-86157", "mrqa_squad-train-59449", "mrqa_naturalquestions-validation-7939", "mrqa_searchqa-validation-691", "mrqa_searchqa-validation-10168", "mrqa_hotpotqa-validation-3421", "mrqa_triviaqa-validation-7618", "mrqa_newsqa-validation-1920", "mrqa_triviaqa-validation-2016", "mrqa_newsqa-validation-4130", "mrqa_hotpotqa-validation-5587", "mrqa_searchqa-validation-3269", "mrqa_naturalquestions-validation-6577", "mrqa_triviaqa-validation-626", "mrqa_hotpotqa-validation-203", "mrqa_newsqa-validation-185", "mrqa_naturalquestions-validation-5607", "mrqa_searchqa-validation-16717"], "EFR": 0.9523809523809523, "Overall": 0.7273511904761906}, {"timecode": 76, "before_eval_results": {"predictions": ["Hawaii", "\"The Real Housewives of Atlanta\"", "commission, led by former U.S. Attorney Patrick Collins,", "the number of new cases is falling.", "said he was released Friday and taken to the Australian embassy in Bangkok, where he stayed until leaving for Australia at about midnight.", "Sheikh Sharif Sheikh Ahmed", "two", "golf", "ended his playing career", "Nigeria", "Ameneh Bahrami", "Akio Toyoda", "Daytime Emmy Lifetime Achievement Award", "test-launched a rocket capable of carrying a satellite,", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "2,800", "Roger Federer", "March 22,", "Venezuela", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "sanctions 17 entities, including three government-owned or controlled companies used by Mugabe and his government \"to illegally siphon revenue and foreign exchange from the Zimbabwean people,\" as well as one individual.\"", "\"Operation Crank Call,\"", "the body of the aircraft", "Rima Fakih", "more than 100", "Obama and McCain camps", "an Iranian court", "about 5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "President Bill Clinton", "Haleigh Cummings,", "Transportation Security Administration", "pulling on the top-knot of an opponent,", "response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "J. Crew,", "was killed", "Kindle Fire", "Islamabad", "three", "Nigeria", "She also has had a leg amputated,", "fill a million sandbags and place 700,000 around our city,\"", "Brinley", "15-year-old's", "attempting illegal crossings into U.S. waters.", "edward d Bowie", "More than 15,000", "depressed", "Franklin, Tennessee,", "said such joint exercises between nations are not unusual.", "Authorities in Fayetteville, North Carolina,", "Saturday", "Lake Powell", "Lew Brown", "Rockwell", "edward Stuart", "Neighbours", "Pesach", "Walcha", "Player's No 10, Skol, Leyland Cars, Daily Mirror, TNT Sameday and Dunlop", "Hern\u00e1n Crespo", "Heimskringla", "(Somerset) Verdi", "Serengeti National Park", "Switzerland"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6556290486654451}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5806451612903226, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.9473684210526316, 0.0, 1.0, 1.0, 1.0, 0.0, 0.05128205128205128, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.18181818181818185, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2396", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-2519", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-648", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-5708", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-1821", "mrqa_searchqa-validation-11541"], "SR": 0.5625, "CSR": 0.554788961038961, "EFR": 0.9285714285714286, "Overall": 0.7226095779220779}, {"timecode": 77, "before_eval_results": {"predictions": ["Sunday's", "Charlotte Gainsbourg and Willem Dafoe", "Patrick McGoohan,", "flooding and debris", "Vicente Dale Coutinho, commander of Brazil's 4th Army, reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Woosuk Ken Choi,", "a head injury.", "1994,", "Mawise Gumba", "his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "25 dead", "At least 15", "at least nine", "shows the world that you love the environment and hate using fuel,\"", "\"falling space debris,\"", "Transportation Security Administration", "$10 billion", "\"The Orchid Thief,\"", "102", "his business dealings", "Jaime andrade", "A receptionist with a gunshot wound in her stomach", "innovative, exciting skyscrapers", "the United States", "financial gain,", "Arnold Drummond", "trading goods and services without exchanging money", "Yang Hyong Sop, and Kim Kye Gwan, the vice foreign minister,", "Bangkok's Chinatown area -- normally hopping with activity on a Saturday afternoon -- was largely wasteland, with few passers-by wading in knee-deep water.", "July", "16", "the Register -- Iowa's largest newspaper -- backed Romney in his bid for the Republican presidential nomination", "100 to 150", "reached an agreement late Thursday", "Zelaya and Roberto Micheletti,", "one of Africa's most stable nations.", "the end", "prison inmates.", "Cannes Film Festival,", "Mark Obama Ndesandjo", "Ronald Reagan UCLA Medical Center,", "one of five", "the Afghan opium trade", "Belfast's Odyssey Arena.", "executive director of the Americas Division of Human Rights Watch,", "\"Empire of the Sun,\"", "Basel", "\"@\"", "Jeffrey Jamaleldine", "Heshmat Tehran Attarzadeh", "At least 14", "Daryl Sabara", "July 2014", "Gibraltar", "Barry White", "george george", "Anita Brookner", "9", "a Canadian comedian", "consulting", "flamboyant", "Orson Welles", "a fence", "loyal"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6363682644110276}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.7894736842105263, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3928", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-2803", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-1497", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-795", "mrqa_naturalquestions-validation-9330", "mrqa_triviaqa-validation-635", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-3222"], "SR": 0.5625, "CSR": 0.5548878205128205, "EFR": 0.9642857142857143, "Overall": 0.729772206959707}, {"timecode": 78, "before_eval_results": {"predictions": ["dress shop", "callable bonds", "British Columbia, Canada", "45 %", "September 14, 2008", "to prevent further offense by convincing the offender that their conduct was wrong", "Mike Alstott", "the closing of the atrioventricular valves and semilunar valves", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "a compiler", "Dalveer Bhandari", "bone marrow", "to collect menstrual flow", "the Kansas City Chiefs", "`` Killer Within ''", "Filipino", "the 15th century", "Wakanda", "the Roman Empire", "Joel", "a young girl ( an illustration by Everest creative Maganlal Daiya back in the 1960s )", "Gertrude Niesen", "Australia", "1983", "Kathleen Erin Walsh", "March 12, 2013", "The photoelectric ( optical ) smoke detector", "the head", "Chicago and to New York", "sedimentary", "$2 million", "Michael Buffer", "Eric Clapton", "115", "c. 3000 BC", "Andrew Garfield", "each team", "1898", "a routing table", "the rez", "Twickenham", "Glenn Close", "David H. Splane", "Tim Russert", "the anterolateral system", "letter series '' ; a large, high - performance luxury coupe sold in very limited numbers", "positive, zero, or negative scalar quantity", "the Second Continental Congress", "the winter solstice", "phencyclidine and cocaine", "an integral membrane protein that builds up a proton gradient across a biological membrane", "Lewis Carroll", "lithium", "The Apprentice", "A123 Systems, LLC", "President of the United States", "Gareth", "34 civilians and 16 police officers", "Caylee Anthony", "Michael Brewer,", "a bugle", "Williams", "Nathaniel Hawthorne", "Friday,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.700915750915751}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9333333333333333, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 1.0, 0.9, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-5536", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-3438", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-7894", "mrqa_searchqa-validation-16420"], "SR": 0.546875, "CSR": 0.5547863924050633, "retrieved_ids": ["mrqa_squad-train-67738", "mrqa_squad-train-65037", "mrqa_squad-train-20125", "mrqa_squad-train-2807", "mrqa_squad-train-51558", "mrqa_squad-train-41246", "mrqa_squad-train-4586", "mrqa_squad-train-23592", "mrqa_squad-train-57327", "mrqa_squad-train-72526", "mrqa_squad-train-75907", "mrqa_squad-train-16908", "mrqa_squad-train-23927", "mrqa_squad-train-53928", "mrqa_squad-train-30533", "mrqa_squad-train-35052", "mrqa_newsqa-validation-23", "mrqa_hotpotqa-validation-4711", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-762", "mrqa_searchqa-validation-1607", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-514", "mrqa_hotpotqa-validation-3865", "mrqa_newsqa-validation-2730", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-2548", "mrqa_squad-validation-10388", "mrqa_searchqa-validation-16205", "mrqa_hotpotqa-validation-562"], "EFR": 1.0, "Overall": 0.7368947784810127}, {"timecode": 79, "before_eval_results": {"predictions": ["Bury, Greater Manchester, England", "the Battle of the Rosebud", "Rabat", "Potomac River", "Hermione Baddeley", "Harmony Korine", "December 1993", "New Jersey", "rock and roll", "the Red and Assiniboine Rivers", "King George IV", "June 24, 1935", "Donald Parks Baker", "odd-eyed", "2001", "Perth's", "1999", "Tempo", "Presbyterian", "2002", "English", "Southaven", "Anheuser-Busch", "Kansas City, Missouri", "Donald David Dixon Ronald O\u2019Connor", "Leslie James \"Les\" Clark", "Francesco Maria Piave", "County Louth", "Gal Gadot", "Kurt Vonnegut", "top division", "film", "February 10, 1927", "Vince Guaraldi", "2007", "Grave Digger", "Mulberry", "Isabella", "Jay Park", "The final of 2011 AFC Asian Cup", "Mel Blanc", "Centers for Medicare & Medicaid Services", "Pakistan", "Godspell", "Steven Selling", "Scunthorpe", "Australian", "Argentinian", "Jack Kilby", "throughout the 1970s and 1980s", "Personal History", "Timothy B. Schmit", "Tim Duncan", "the direction from which the wind is blowing", "France", "Shropshire", "trees", "Alfredo Astiz,", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip,", "Kim", "Catherine of Aragon", "Little Boy Blue", "Cheyenne", "March 27, 2017"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7650974025974027}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-598", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-1453", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-2518", "mrqa_naturalquestions-validation-601", "mrqa_newsqa-validation-433", "mrqa_searchqa-validation-5939", "mrqa_naturalquestions-validation-5649"], "SR": 0.671875, "CSR": 0.55625, "EFR": 1.0, "Overall": 0.7371874999999999}, {"timecode": 80, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1288", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.814453125, "KG": 0.52421875, "before_eval_results": {"predictions": ["John Nash", "Greedy Italians", "Russ Conway", "in the form (x,y), an ordered pair", "Mel Brooks", "Agent 007", "Edward Woodward", "Scotland", "Fiat", "three-stringed", "Bob Anderson", "Andre Agassi", "Eugene", "Pakistan", "pieve di San Pietro a Romena", "Mark Darcy", "California Chrome", "Jason Bennetto", "Milan", "Tony Meo", "Bash Street", "Leonard Rossiter", "Robin Hood", "\"Dancing the Lambeth Walk\",", "johannam", "Augustus", "Shepherd Neame", "Titanic", "Stuart Whitman", "tax collector", "Robert Maxwell", "Mikhail Gorbachev", "Pocahontas", "Noah Beery, Jr.", "Argentina", "the peripheral nerves", "myxomatosis", "a fruit with green, reddish-purple or blackish skin", "World War I", "Captain America", "HARIBO", "bolognese", "New Zealand", "Eva Braun", "Sindh", "Devon Loch", "\"One Country Two Systems\"", "Bruce Willis", "Kwame Nkrumah", "Fifth", "cording", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "pneumonoultramicroscopicsilicovolcanoconiosis", "Hercules", "Philip", "2010", "\"The Process\"", "Ameneh Bahrami", "Nearly eight in 10", "a tenement in the Mumbai suburb of Chembur,", "camels", "Shrek", "Seoul", "Asia"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6739583333333334}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5157", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-2223", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-5713", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-5149", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-1693", "mrqa_hotpotqa-validation-616", "mrqa_searchqa-validation-11588", "mrqa_searchqa-validation-457"], "SR": 0.640625, "CSR": 0.5572916666666667, "EFR": 0.9565217391304348, "Overall": 0.7216689311594203}, {"timecode": 81, "before_eval_results": {"predictions": ["London", "his writings about the outdoors, especially mountain-climbing", "50 Greatest Players in National Basketball Association History", "Roger Staubach", "World Health Organization", "Pulitzer Prize", "Argentine", "Pittsburgh Steelers", "Kim Jong-hyun", "Las Vegas", "Scandinavian design", "romantic comedy", "New Orleans Pelicans", "Tsung-Dao Lee", "December 19, 1967", "South African-born", "Barney Fife", "1822", "1926 Paris", "Bulgarian", "Quahog, Rhode Island", "Landing Barge, Kitchen", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Roslyn Castle", "The Battle of Dresden", "2015", "Violet", "Free Range Films", "Mondays", "full-sized nameplates such as Plymouth Fury", "Edinburgh", "Laurel, Mississippi", "Matt Flynn", "Camber Sands", "Jaguar Land Rover", "the Socialist Republic of Vietnam", "Adelaide", "Shepardson Microsystems", "The Fault in Our Stars", "sixteen", "Crips", "Deftones", "Doctor of Philosophy", "Donald Richard \"Don\" DeLillo", "Claude Mak\u00e9l\u00e9l\u00e9", "Blue Valley Northwest High", "Magic Band", "Salzburg Festival", "German princely Battenberg", "German political and military leader as well as one of the most powerful figures in the Nazi Party (NSDAP) that ruled Germany from 1933 to 1945", "The Nassau Herald", "in northern Arizona", "You are a puzzle", "pineapple", "euthanasia", "peter", "Poland", "Former U.S. soldier Steven Green", "Steve Williams", "Umar Farouk AbdulMutallab", "Dr. No", "legs", "Big Brown", "ties to paramilitary groups,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7338789682539683}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.2857142857142857, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-712", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-2485", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-376", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2925", "mrqa_triviaqa-validation-5053", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-1205", "mrqa_searchqa-validation-11199", "mrqa_newsqa-validation-877"], "SR": 0.65625, "CSR": 0.5584984756097561, "retrieved_ids": ["mrqa_squad-train-78193", "mrqa_squad-train-13452", "mrqa_squad-train-34540", "mrqa_squad-train-21942", "mrqa_squad-train-18559", "mrqa_squad-train-23958", "mrqa_squad-train-80765", "mrqa_squad-train-41352", "mrqa_squad-train-4101", "mrqa_squad-train-69942", "mrqa_squad-train-83444", "mrqa_squad-train-9071", "mrqa_squad-train-25783", "mrqa_squad-train-60035", "mrqa_squad-train-19424", "mrqa_squad-train-59560", "mrqa_searchqa-validation-9879", "mrqa_triviaqa-validation-1025", "mrqa_searchqa-validation-4971", "mrqa_naturalquestions-validation-6931", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-10364", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-776", "mrqa_searchqa-validation-1821", "mrqa_newsqa-validation-1382", "mrqa_naturalquestions-validation-8444", "mrqa_newsqa-validation-419", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-9392", "mrqa_triviaqa-validation-841"], "EFR": 1.0, "Overall": 0.7306059451219513}, {"timecode": 82, "before_eval_results": {"predictions": ["Kenny Young", "40 million", "the fourth season of \"American Idol\"", "father", "Martin Scorsese", "Keith Crofford", "Daimler-Benz", "Edward Moore \"Ted\" Kennedy", "private Ivy League research university", "Lee Seok-hoon", "Sevens", "strongly associated with Gaia and Cybele, who have similar functions", "Most observers viewed the election as blatantly unfair", "Eucritta melanolimnetes", "Umberto II", "1866", "1860", "Attorney General and as Lord Chancellor of England", "Sexred, or Sexr\u00e6d, (d. 626?)", "British", "Westfield Tea Tree Plaza", "924", "1951", "Darci Kistler", "1966", "Potomac River", "Europe", "Harry Potter series, The Boy in the Striped Pyjamas", "Gateways", "England", "Black Panthers", "An extended play record", "\"Pineapple Express\"", "May 5 to July 8, 2014", "The Supremes", "What You Will", "Wolf Creek", "Sky News", "High Court of Admiralty", "Chelsea Does", "bobsledder", "Double Agent", "Oregon Ducks", "Aksel Sandemose", "Sim Theme Park", "the Earth", "Eric Whitacre", "Sullivan University College of Pharmacy", "24 January 76 \u2013 10 July 138", "Marvel Comics", "Shire River", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "1961", "September 14, 2008", "Harry S. Truman", "Kentucky Derby", "Gianni Versace", "London City airport", "Diego Maradona,", "his club", "pink", "Beaker", "Claddagh", "Emperor Concerto"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7533195970695972}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.2666666666666667, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3076923076923077, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-5066", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-4293", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4974", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1461", "mrqa_searchqa-validation-10992"], "SR": 0.65625, "CSR": 0.5596762048192772, "EFR": 1.0, "Overall": 0.7308414909638554}, {"timecode": 83, "before_eval_results": {"predictions": ["Thunder Road", "from the former Kingdom of Strathclyde who spoke Cumbric, a close relative of the Welsh language, or possibly an incomer from Wales, or the Welsh Marches", "between the Eastern Ghats and the Bay of Bengal", "Lincoln Park in San Francisco", "New York University", "amylase", "Eydie Gorm\u00e9", "Werner Ruchti", "Stephen Lang", "used obscure languages as a means of secret communication during wartime", "the English", "Himadri Station", "1959", "Sun Tzu ( `` Master Sun '', also spelled Sunzi )", "their son Jack ( short for Jack - o - Lantern )", "The Grasshopper Lies Heavy", "Pope Gregory I the Great", "1966", "December 20, 1951", "2017", "Warren Hastings", "sea water", "water ice", "June 1992", "John Vincent Calipari", "1975", "Charles Darwin and Alfred Russel Wallace", "the pretribulation, premillennial, Christian eschatological interpretation of the Biblical apocalypse", "1997", "Judith Cynthia Aline Keppel", "the 1820s", "May 18, 2018", "Russia", "France ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "relators", "Fusajiro Yamauchi", "Etienne de Mestre", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "pre-Christian festivals that were celebrated around the winter solstice", "Speaker of the House of Representatives", "the NFL", "Italy", "Detroit Red Wings", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral", "Captain Jones", "Blue with a harp of gold", "Florida and into the town of Coconut Cove", "S - shaped", "September 9, 2010", "at least 18 or 21 years old ( or have a legal guardian present )", "Lori Rom", "Jennifer Eccles", "Sinclair Lewis", "Akon", "Lily Hampton", "Havenhurst", "Gregg Harper", "23-year-old", "more than 200.", "Transportation Security Administration", "the Vaio Z Canvas 2-in-1", "Newman", "The Color Purple", "East Knoyle"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7520196759259259}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7200000000000001, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-6308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-1447", "mrqa_searchqa-validation-8173"], "SR": 0.703125, "CSR": 0.5613839285714286, "EFR": 0.9473684210526315, "Overall": 0.720656719924812}, {"timecode": 84, "before_eval_results": {"predictions": ["as a relay between the brain and spinal cord and the rest of the body", "the lungs", "Grand Inquisition", "Theodore Roosevelt", "the Western Bloc ( the United States, its NATO allies and others )", "Anna Faris", "the s - block", "late - night", "Tim Rice", "April 10, 2018", "the New York Yankees", "Pope Gregory I the Great", "the West", "Ireland", "Saphira", "Pre-evaluated, strategic planning, operative planning, implementation, and post-evaluation", "Saint Peter", "Active absorption", "1983", "Congress in 1790 passed the first naturalization law for the United States, the Naturalization Act of 1790", "Tachycardia, also called tachyarrhythmia", "Ptolemy", "the advent of the Super Bowls XXI ( 1986 ), XXV ( 1990 ), XLII ( 2007 ), and XLVI ( 2011 )", "the Battle of Antietam", "Bill Condon", "1939", "upon braking to a full stop", "Thomas Mundy Peterson", "Filipino Americans", "Glynis Johns", "The Continental Congress", "eusebeia", "Napoleon Bonaparte", "Lagaan", "A blighted ovum or anembryonic gestation", "September 19 - 22, 2017", "in the New Testament", "Kirsten Simone Vangsness", "asphyxia", "in 1966", "Austin", "A patent", "1999", "10,605", "the East Coast", "Reverse - Flash", "March 1930", "John Donne", "Category 4", "season four", "During the reign of King Beorhtric of Wessex", "Rugby School", "gold", "Doctor Dolittle", "goalkeeper", "Christian Kern", "\"50 best cities to live in.\"", "regulators in the agency's Colorado office", "Miguel Cotto", "The tale told by Average", "Rocky", "the Cumberland Gap", "salinity", "George Fox"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6272864008751106}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [0.7096774193548387, 0.0, 0.5, 0.8, 0.3636363636363636, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.16666666666666666, 0.28571428571428575, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.8333333333333333, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-1910", "mrqa_naturalquestions-validation-1688", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-4863", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2653"], "SR": 0.515625, "CSR": 0.5608455882352941, "retrieved_ids": ["mrqa_squad-train-84312", "mrqa_squad-train-38158", "mrqa_squad-train-1636", "mrqa_squad-train-24770", "mrqa_squad-train-84676", "mrqa_squad-train-60965", "mrqa_squad-train-41050", "mrqa_squad-train-54222", "mrqa_squad-train-7387", "mrqa_squad-train-68807", "mrqa_squad-train-5490", "mrqa_squad-train-27125", "mrqa_squad-train-15312", "mrqa_squad-train-21193", "mrqa_squad-train-67143", "mrqa_squad-train-12527", "mrqa_hotpotqa-validation-5582", "mrqa_naturalquestions-validation-7939", "mrqa_triviaqa-validation-3270", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-4039", "mrqa_searchqa-validation-16878", "mrqa_searchqa-validation-4971", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-1470", "mrqa_newsqa-validation-4128", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3826", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-6378"], "EFR": 0.9354838709677419, "Overall": 0.7181721418406072}, {"timecode": 85, "before_eval_results": {"predictions": ["John Marshall", "Pirates of the Caribbean: Dead Man\\'s Chest", "Samuel de Champlain", "Louis XIV", "Lady Jane Grey", "the Barbary Coast", "Iceland", "the owl", "Richard Cory", "Volkswagen", "baldness", "Athens", "rum", "tea rose", "Radames", "give love a bad name", "the Banni", "Madame Dficit", "rotunda", "the magnolia", "India", "a bicentennial", "Gallico", "eBay", "the peace sign", "Michael Dell", "Pizza Hut", "the Titanic", "1972", "a hurricane", "Amish", "the Rocky Mountains", "carbon", "the Sword", "Boston", "Wu-Tang Clan", "king", "(Jose) Martin", "a whale", "Salt Lake City", "Luxembourg", "Texas", "drag", "Python", "toi aussi", "Las Vegas", "Laura", "The New Yorker", "a manuscript", "a spoiled-brat", "Tufts", "The results of the Avery -- MacLeod -- McCarty experiment, published in 1944, suggested that DNA was the genetic material, but there was still some hesitation within the general scientific community to accept this", "the cast", "the buttock", "Amy Tan", "carbon", "The Truman Show", "Robert Redford", "Great Lakes and Midwestern", "three", "five", "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "last March 3,", "Elizabeth Birnbaum"], "metric_results": {"EM": 0.625, "QA-F1": 0.697702205882353}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-12392", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-14721", "mrqa_searchqa-validation-6995", "mrqa_searchqa-validation-7272", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-15481", "mrqa_searchqa-validation-14552", "mrqa_searchqa-validation-4152", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-11933", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-2130", "mrqa_newsqa-validation-4210"], "SR": 0.625, "CSR": 0.5615915697674418, "EFR": 1.0, "Overall": 0.7312245639534884}, {"timecode": 86, "before_eval_results": {"predictions": ["granite", "Bull", "Horse Feathers", "Bleak House", "Chaillot", "Do the Right Thing", "painting", "Asteroids", "a bad peace", "Yves Saint Laurent", "Iceland", "England", "the Lend-Lease Act", "Spanglish", "Monica Lewinsky", "Friday Night Lights", "Google", "Medusa", "the vest", "Prince", "a gull", "Hammurabi", "Nixon", "rain", "(Prince) Kesselring", "falling asleep", "Ned", "the 747-400", "Terry Bradshaw", "Chris Evert", "Azerbaijan", "Mamma Mia!", "Fallingwater", "Alanis Morissette", "dashes", "a barrel", "Etna", "a law clerk", "the Faneuil Hall", "Louisiana", "(George) Orwell", "tea", "(Thunnus) thynnus", "Stalin", "Metallica", "change horses", "Get Smart", "Lafayette", "(Daisy) Baker", "Captain Kangaroo", "Kosher Wines", "1996", "The 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII", "Gabrielle - Suzanne Barbot de Villeneuve", "paste", "nitrogen", "South Africa", "Cartoon Network Too", "Rotterdam Square", "Bruce Grobbelaar", "Tehran, Iran", "The Palm Jumeirah", "35,000.", "Mashhad"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7194353070175439}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.21052631578947367, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-7312", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-2504", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-16075", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-16876", "mrqa_searchqa-validation-4740", "mrqa_searchqa-validation-15515", "mrqa_searchqa-validation-13220", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-4307", "mrqa_naturalquestions-validation-288", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-1471", "mrqa_hotpotqa-validation-337", "mrqa_newsqa-validation-3141"], "SR": 0.640625, "CSR": 0.5625, "EFR": 1.0, "Overall": 0.73140625}, {"timecode": 87, "before_eval_results": {"predictions": ["Oblivion", "a chiffon", "Corpus Christi", "Grover Cleveland", "an eye", "the Federalist Papers", "Martin Luther King, Jr.", "transitive", "California", "the Central Pacific", "ACTIVE", "Tom Cruise", "New York City pizza", "a panda", "Risk", "brown rice", "Kansas State", "antonyms", "1945", "Bourbon", "a stork", "Towers", "a rat", "anime", "Daisy Miller", "Icelandic", "Russell Crowe", "Mercury", "Zapan", "The Stars and Stripes Forever", "One Hundred Years of Solitude", "lethal", "Henry Cavendish", "vanilla", "terminal", "Italy", "Night of the Iguana", "Lucky", "cricket", "Anne Rice", "the root", "the Book of the Wars of Jehovah", "1066", "Sir John Soane", "a Bull", "the hip", "a hearse", "City Slickers", "Ned Kelly", "Souci", "the Doge of Venice", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "the southeastern United States", "a generic engine provided by V8 Supercars", "a sovereign state", "Air Algerie", "sculpture", "1998", "\"Peshwa\" (Prime Minister)", "\"Histoires ou contes du temps pass\u00e9\"", "the Beatles", "Adidas", "President Bush", "Consumer Reports"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7447296626984127}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-4561", "mrqa_searchqa-validation-2042", "mrqa_searchqa-validation-5164", "mrqa_searchqa-validation-15006", "mrqa_searchqa-validation-13444", "mrqa_searchqa-validation-11517", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-2206", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-8657", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12454", "mrqa_searchqa-validation-5566", "mrqa_naturalquestions-validation-1438", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3663", "mrqa_hotpotqa-validation-4588", "mrqa_newsqa-validation-663"], "SR": 0.65625, "CSR": 0.5635653409090908, "retrieved_ids": ["mrqa_squad-train-60342", "mrqa_squad-train-47927", "mrqa_squad-train-26608", "mrqa_squad-train-15181", "mrqa_squad-train-12615", "mrqa_squad-train-80692", "mrqa_squad-train-30145", "mrqa_squad-train-37498", "mrqa_squad-train-62581", "mrqa_squad-train-10202", "mrqa_squad-train-83351", "mrqa_squad-train-49822", "mrqa_squad-train-14557", "mrqa_squad-train-27088", "mrqa_squad-train-76335", "mrqa_squad-train-78387", "mrqa_newsqa-validation-2168", "mrqa_squad-validation-6696", "mrqa_searchqa-validation-4024", "mrqa_newsqa-validation-743", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-1423", "mrqa_searchqa-validation-7891", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2208", "mrqa_searchqa-validation-10881", "mrqa_naturalquestions-validation-2506", "mrqa_newsqa-validation-3503", "mrqa_triviaqa-validation-7057", "mrqa_hotpotqa-validation-656"], "EFR": 1.0, "Overall": 0.7316193181818182}, {"timecode": 88, "before_eval_results": {"predictions": ["Sputnik", "Let It Snow!", "The Kinks", "Mikhail S. Gorbachev", "Jerez", "Buncefield Depot", "royal court", "cable", "Westminster Abbey", "Oasis and Blur", "the king of hearts", "Hawaii", "World War II", "aromatherapy", "s Sierra One from Sierra Oscar", "Downton Abbey", "Bobby Darin", "France", "Montmorency", "Kent", "Cliff Thorburn", "Paris", "four", "cymbals", "violin", "Ireland", "Venus", "beetles", "paralysis", "eight", "Japanese silvergrass", "swindon town", "Billy Preston", "Happy birthday to You", "Everton", "Awning window", "Marc", "Staraya Russa", "Makepeace", "Mud", "Dumbo", "Jimmy Knapp", "\" taking of Pelham One Two Three\"", "4", "7,926 miles", "Gianfranco Ferre", "malekith", "smith seddon", "Chiricahua", "Enda Kenny", "Aug. 24, 1572", "in the very late 1980s", "Taylor Michel Momsen", "Malayalam", "2008", "Western Canada", "Dutch", "the peace with Israel", "Jiverly Wong,", "28", "Shirley Jackson", "Hannibal", "jam", "Mike Mills"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6838541666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-5008", "mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-7013", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-4019", "mrqa_hotpotqa-validation-3566", "mrqa_searchqa-validation-3464"], "SR": 0.640625, "CSR": 0.5644311797752809, "EFR": 0.9130434782608695, "Overall": 0.71440118160723}, {"timecode": 89, "before_eval_results": {"predictions": ["18", "A third beluga whale belonging to the world's largest aquarium has died", "Los Ticos", "reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Aniston, Demi Moore and Alicia Keys", "Haiti,", "Rwanda", "dance", "repair", "Azzam", "10", "\u00a341.1 million", "between 5 and 10 knots an hour.", "helping to plan the September 11, 2001, terror attacks,", "to provide security as needed.", "Los Ticos", "some dental work done,", "the hiring of hundreds of foreign workers for a construction project", "fill a million sandbags", "helping on the sandbag lines", "Immigration Minister Eric Besson", "since 1983", "133", "promotes fuel economy and safety while boosting the economy.", "planning processes are urgently needed", "tabasco", "12.3 million", "The Ski Train", "two years", "Robert Park", "Piedad Cordoba", "eight", "Itawamba County School District", "Frank Ricci", "1959", "$249", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "1983", "13", "The Valley Swim Club", "\"still trying to absorb the impact of this week's stunning events.\"", "West Palm Beach, Florida", "relatives of the five suspects,", "Sharon Bialek", "not", "John Dillinger", "18", "Queen Elizabeth's birthday", "Iggy Pop", "two", "opryland", "Garfield Sobers", "R.E.M.", "San Jose, California", "john donne", "127 Hours", "kiel Canal", "\"Slaughterhouse-Five\"", "1885", "Germanic", "hearsay", "buffoon", "the 17th century", "linda"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6390000318309141}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.11764705882352942, 0.8571428571428571, 0.0, 0.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.9090909090909091, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-268", "mrqa_naturalquestions-validation-878", "mrqa_hotpotqa-validation-4986", "mrqa_triviaqa-validation-6731"], "SR": 0.5625, "CSR": 0.5644097222222222, "EFR": 1.0, "Overall": 0.7317881944444444}, {"timecode": 90, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.83984375, "KG": 0.5171875, "before_eval_results": {"predictions": ["15,000", "Jezebel.com's Crap E-mail From A Dude", "Mark Sanford", "the Russian air force,", "Nigeria, Africa's largest producer.", "one", "serving its fast burgers and fries", "Melbourne.", "allergen-free", "opium", "Tuesday.", "acute stress disorder", "that a 15-year-old can't vote, can't join the armed forces and cannot buy alcohol,", "Severalteen years ago", "fastest circumnavigation of the globe in a powerboat", "order", "Saturn", "Windsor, Ontario,", "\"The Da Vinci Code,\"", "Nearly eight in 10", "\"The Closer.\"", "no motive has been determined for the killing,", "Opry Mills,", "striker", "Jan Brewer,", "oceans", "Columbia", "pun", "weren't taking it well.", "Leo Frank", "Ralph Lauren", "the American Civil Liberties Union", "Islamabad", "evokes childhood memories in", "June 2004", "Olympia", "\"Percy Jackson & The Olympians,\"", "Frank Ricci,", "U.S. Navy", "the Russian air force,", "they are co-chair of the Genocide Prevention Task Force.", "she wonders if part of the appeal of plus-sized", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "Climatecare,", "Jaipur", "Venezuela", "likening one American diplomat to a \"prostitute\"", "meeting with the president to discuss her son.", "Toffelmakaren.", "she also believed police were trying to cover up the truth behind her daughter's murder,", "buckling under pressure from the ruling party.", "16 seasons", "12.9 - kilometre ( 8 mi )", "The Wizard of Oz", "India", "Alessandro Allori", "61", "Nicolas Winding Refn", "Father Dougal McGuire", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "alfalfa", "Thomas Jefferson", "\"Here\\'s Johnny\"", "Adolphe Adam"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6592742925999406}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true], "QA-F1": [0.5, 0.0, 0.8, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.10526315789473685, 0.5714285714285715, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.26086956521739124, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-1368", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-2983", "mrqa_naturalquestions-validation-10284", "mrqa_triviaqa-validation-2805", "mrqa_triviaqa-validation-3262", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-15070"], "SR": 0.5625, "CSR": 0.5643887362637363, "retrieved_ids": ["mrqa_squad-train-83295", "mrqa_squad-train-37068", "mrqa_squad-train-80360", "mrqa_squad-train-1387", "mrqa_squad-train-15641", "mrqa_squad-train-79512", "mrqa_squad-train-48010", "mrqa_squad-train-56460", "mrqa_squad-train-84727", "mrqa_squad-train-486", "mrqa_squad-train-61776", "mrqa_squad-train-40385", "mrqa_squad-train-83424", "mrqa_squad-train-1268", "mrqa_squad-train-61664", "mrqa_squad-train-27559", "mrqa_hotpotqa-validation-3401", "mrqa_triviaqa-validation-907", "mrqa_hotpotqa-validation-5804", "mrqa_triviaqa-validation-4455", "mrqa_newsqa-validation-1926", "mrqa_triviaqa-validation-7641", "mrqa_naturalquestions-validation-10525", "mrqa_searchqa-validation-11985", "mrqa_newsqa-validation-2431", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-6473", "mrqa_naturalquestions-validation-2894", "mrqa_searchqa-validation-8125", "mrqa_newsqa-validation-3686", "mrqa_hotpotqa-validation-1161", "mrqa_searchqa-validation-7517"], "EFR": 1.0, "Overall": 0.7393621222527472}, {"timecode": 91, "before_eval_results": {"predictions": ["British Airways,", "Linus van Pelt", "chestnut", "bitter almond", "Mark Twain", "Oslo", "Humphrey Bogart", "Hawaii", "flute", "British writer George Orwell.", "goldtrail", "The Archers", "Ben Franklin", "jack Nicholson", "photography", "lemon", "Taiwan", "Willem de Zwijger", "Oliver Stone", "astronauts Neil Armstrong and Edwin \" Buzz\" Aldrin", "Oregon", "your Excellency", "Nikola Tesla", "Thomas De Quincey", "Susie Dent", "Pancho Villa", "the Crusades", "Ivan Owen", "1919", "copper", "Pickwick", "Bluebell Girls", "Columbus,", "afghanistan", "Ann Darrow", "blue", "the Flying Pickets", "St Moritz", "glamorgan", "Vietnam", "1985", "Bogota", "the United States", "James Murdoch", "Crystal Palace", "Belfast", "Moulin Rouge", "Thermopylae", "Reginald Dwight", "Seattle", "Marshalsea", "card verification value", "\u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "Tandi", "94", "August 6, 1845 - October 6, 1931", "Ted 2", "24", "Tuesday", "opium", "Ham", "resuscitation", "the Hudson River", "Subway"], "metric_results": {"EM": 0.625, "QA-F1": 0.6941176470588235}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1, 0.8235294117647058, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3978", "mrqa_triviaqa-validation-4026", "mrqa_triviaqa-validation-6408", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-6962", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-7195", "mrqa_triviaqa-validation-5332", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-6887", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-4434", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-2175", "mrqa_searchqa-validation-13767"], "SR": 0.625, "CSR": 0.5650475543478262, "EFR": 0.9583333333333334, "Overall": 0.7311605525362319}, {"timecode": 92, "before_eval_results": {"predictions": ["orangutans", "lowestoft", "jesse", "new Zealand", "fay Moore", "Ernest Hemingway", "charles II", "Godfather of Italian cooking", "the Hubble Space Telescope", "France", "greece", "a window", "british schnit", "a coffee house", "the little dog laugh'd", "baseball cards", "george das farbige", "harry swank", "Neighbours", "kursk", "Jessica Simpson", "six", "blind beggar", "Mark Darcy", "bacofoil", "Homo floresiensis", "brian blair", "Yuri Andropov", "surrey", "greece", "anabaptists", "british greece", "Noah Beery, Jr.", "oregon", "petula Clark", "greece colquhoun", "dice", "Saturn", "Sinclair Lewis", "the river", "surrey", "james chadwick", "sealion", "1879", "a polecat", "table tennis", "bison", "Alberich", "geomorphology", "martin", "Tina Turner", "com TLD", "abdicated", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Guangzhou, China", "29, 1985", "Jane Mayer", "a storm,", "a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.", "American", "a Cruiser", "Coors Field", "the Chrysler Building", "the USS \"Enterprise\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.5720920138888889}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.125, 0.22222222222222218, 1.0, 0.0, 1.0, 1.0, 0.09999999999999999, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4018", "mrqa_triviaqa-validation-5157", "mrqa_triviaqa-validation-1795", "mrqa_triviaqa-validation-654", "mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-6329", "mrqa_triviaqa-validation-904", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4741", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-1588", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3737", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-2717", "mrqa_newsqa-validation-3990", "mrqa_searchqa-validation-6690"], "SR": 0.53125, "CSR": 0.5646841397849462, "EFR": 0.9, "Overall": 0.7194212029569893}, {"timecode": 93, "before_eval_results": {"predictions": ["Robert Barnett,", "a public housing project,", "hospital in Amstetten,", "new Zealand", "Zimbabwe,", "a one-shot victory in the Bob Hope Classic", "the administration's progress,", "\"an accomplished pilot\"", "Her husband and attorney, James Whitehouse,", "80,", "her children \"have no problems about the school, they are happy about everything.", "Iraqi", "Passers-by", "subscribers to a daily publication which is the primary service of Stratfor,\"", "Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "Former Mobile County Circuit Judge Herman Thomas", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "out of either heavy flannel or wool -- fabrics that would not be transparent when wet -- and covered the entire body from neck to toe.", "Diego Milito's", "Kit of Elsinore", "The worst snowstorm to hit Britain", "potential revenues from oil and gas", "the wars in Iraq and Afghanistan", "two", "finance", "Indonesian", "The Charlie Daniels Band,", "three out of four", "trading goods and services without exchanging money", "gasoline", "Cambodia", "South Africa", "Harrison Ford", "Jacob,", "Cash for Clunkers", "100 meter", "Shenzhen in southern China.", "fluoroquinolone", "Chinese and international laws", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "\"Great Charter\" in Latin.", "Fifteen years ago Wednesday,", "five", "Secretary of State", "the American Civil Liberties Union.", "it was unjustifiable \"for a project which does nothing more than perpetuate misconceptions about the state and its citizens.", "his father", "\"The Sopranos,\"", "finance", "$60 billion", "managing his time.", "merengue", "California and South Carolina", "currently a free agent", "john smith", "bill bryson", "argument form", "1961", "Rodolfo Arizaga", "fascist misinformation", "the Yangtze River", "Ouija", "the Andes", "season"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6423506319566102}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7407407407407407, 1.0, 1.0, 0.29629629629629634, 1.0, 1.0, 0.33333333333333337, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307693, 0.08695652173913043, 0.0, 1.0, 1.0, 0.0, 0.3, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3233", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-3716", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-1958", "mrqa_newsqa-validation-4204", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-4073", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4841", "mrqa_triviaqa-validation-3004", "mrqa_hotpotqa-validation-3981", "mrqa_hotpotqa-validation-1657", "mrqa_searchqa-validation-13597", "mrqa_triviaqa-validation-2535"], "SR": 0.53125, "CSR": 0.5643284574468085, "retrieved_ids": ["mrqa_squad-train-28093", "mrqa_squad-train-83878", "mrqa_squad-train-24189", "mrqa_squad-train-46291", "mrqa_squad-train-17042", "mrqa_squad-train-83662", "mrqa_squad-train-71268", "mrqa_squad-train-40464", "mrqa_squad-train-63084", "mrqa_squad-train-3496", "mrqa_squad-train-62360", "mrqa_squad-train-39500", "mrqa_squad-train-19385", "mrqa_squad-train-28509", "mrqa_squad-train-8210", "mrqa_squad-train-42280", "mrqa_newsqa-validation-4100", "mrqa_naturalquestions-validation-7084", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-9286", "mrqa_newsqa-validation-820", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-598", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-2301", "mrqa_searchqa-validation-2929", "mrqa_triviaqa-validation-4654", "mrqa_naturalquestions-validation-4367", "mrqa_searchqa-validation-10306", "mrqa_triviaqa-validation-1138", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-9552"], "EFR": 1.0, "Overall": 0.7393500664893617}, {"timecode": 94, "before_eval_results": {"predictions": ["Larry King", "Immigration Minister Eric Besson", "$500,000", "humanitarian", "airlines around the world shut down every year.", "At least 38", "laid 11 healthy eggs", "Eleven people", "41,", "McDonald's", "The outline of a general election plan: Create a wide coalition to bring new voters to the polls in record numbers.", "10.1,\"", "Democrats and Republicans", "Zac Efron", "The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,", "Sylt", "\"Piers Morgan Tonight\"", "Congress", "Cash for Clunkers", "the 11th year in a row.", "three thousand", "said the \"face of the peace initiative has been attacked,\"", "after Wood went missing off Catalina Island,", "grossed $55.7 million", "Tulsa, Oklahoma.", "almost 100", "Janet Napolitano", "56,", "There's no chance", "Barack Obama", "as part of its 18-month journey around the world.", "a month ago", "30,000", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "$14.1 million.", "of the Movement for Democratic Change,", "cartel from the state of Veracruz, Mexico,", "Orbiting Carbon Observatory,", "the IAAF", "militants in Afghanistan", "Kim", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Daryeel Bulasho Guud", "onto the college campus.", "a collapsed apartment building in Cologne, Germany,", "in the Gaslight Theater.", "Caylee Anthony", "in the neighboring country of Djibouti,", "18", "school,", "Hayden", "to collect menstrual flow", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors", "two of whom were his sons", "Moldova", "Muhammad Ali", "Ub Iwerks", "Croatan, Nantahala, and Nebo", "4,613", "English rock band", "Raytheon", "the Lion King", "Radiohead", "Johnny Got His Gun"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6703142877574371}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.08695652173913043, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.2631578947368421, 0.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-335", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2341", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-644", "mrqa_triviaqa-validation-2079", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3036"], "SR": 0.5625, "CSR": 0.5643092105263158, "EFR": 0.9642857142857143, "Overall": 0.732203359962406}, {"timecode": 95, "before_eval_results": {"predictions": ["Ford", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "around 3,000 - 5,000 program - erase cycles, but some flash drives have single - level cell ( SLC ) based memory that is good for around 100,000 writes", "a legal case in certain legal systems", "providing torque to all its wheels simultaneously", "October 6, 2017", "autopistas", "Bemis Heights", "Fusajiro Yamauchi", "1854", "Tim McGraw and Kenny Chesney", "Qutab - ud - din Aibak", "Devastator", "Abraham Gottlob Werner", "lacteal", "X&Y", "In some cases, there was a transitional stage where toilets were built into the house but accessible only from the outside", "Britain and France", "originate in the House of Representatives", "architecture", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "the arms of Ireland", "pop ballad", "2018", "2017", "Annette Strean", "Charles Path\u00e9", "cell surface ( particularly caveolae internalization ) as well as at the Golgi apparatus", "World War II", "Bactrian", "2003", "Thomas Hobbes in his Leviathan, though with a somewhat different meaning ( similar to the meaning used by the British associationists )", "August 2015", "songwriter", "Spanish moss", "Guant\u00e1namo", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "961", "Lightning Thief", "milling", "The Cornett family", "Allison Janney", "around 2011", "Patrick Swayze", "McFerrin, Robin Williams, and Bill Irwin", "Ann Gillespie", "Lula", "2017", "March 15, 1945", "Adwaita", "Oliver Goldsmith", "Brussels", "iron", "Seventeen", "Cartoon Network", "What You Will", "Kurt Cobain", "Madonna", "a 12-year veteran of the Utah state police,", "Shakespeare", "Bob Hope", "three", "$40 and a loaf of bread."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6741856234043734}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8181818181818181, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.25, 1.0, 0.45454545454545453, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-3309", "mrqa_triviaqa-validation-3684", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4098", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-7262"], "SR": 0.546875, "CSR": 0.5641276041666667, "EFR": 0.9310344827586207, "Overall": 0.7255167923850575}, {"timecode": 96, "before_eval_results": {"predictions": ["the game", "\"It seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "Another high tide", "April 22,", "\"Hawaii Five-O\"", "a number of calls,", "30", "U.S. senators", "The son of Gabon's former president", "will not support the Stop Online Piracy Act,", "could be secretly working on a nuclear weapon", "Abu Sayyaf,", "Islamabad", "to travel,", "explosives.", "hot and humid", "a hospital", "in Fayetteville, North Carolina,", "\"We're not going to forget you in Washington, D.C.\"", "Tom Hanks, Ayelet Zurer and Ewan McGregor join director Ron Howard at the film's premier.", "September,", "\"This is not something that anybody can reasonably anticipate,\"", "genocide", "any person who has been abused by any priest of the Diocese of Cloyne", "Barney Stinson,", "19-year-old", "Dr. Death in Germany", "collaborating with the Colombian government,", "the U.S. Holocaust Memorial Museum,", "sportswear,", "peppermint oil, soluble fiber, and antispasmodic drugs", "outside the municipal building of Abu Ghraib in western Baghdad", "Venus Williams", "researchers", "The Oka boys", "30-minute", "Grease.", "in good spirits, especially comforted to be receiving care from talented doctors in a world-class hospital named in honor of her late husband,\"", "gunned down four Lakewood, Washington, police officers Sunday.", "the estate with its 18th-century sights, sounds, and scents.", "that students often know ahead of time when and where violence will flare up on campus.", "between 1917 and 1924", "work rule issues.", "Kerstin", "salary", "\"CNN Heroes: An All-Star Tribute\"", "clean up Washington State's decommissioned Hanford nuclear site,", "over 1,000 pounds", "3,000 kilometers (1,900 miles),", "UH-60 Blackhawk helicopters", "Robert & Blad (Norra Grangesbergsgatan 4)", "Lorazepam", "the Coppolas and, technically, the Farrow / Previn / Allens", "To capitalize on her publicity", "ecclesiastical", "The Men Behaving Badly", "tall", "\u00c6thelwald Moll", "The Bears", "Prince Nikolai Sergeyevich Trubetzkoy", "sorority", "Charles Dana Gibson", "the Constitution", "a snail"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5542878562409812}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.5, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5454545454545455, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.2857142857142857, 0.22222222222222224, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-1918", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2130", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-3543", "mrqa_hotpotqa-validation-5590", "mrqa_searchqa-validation-114"], "SR": 0.4375, "CSR": 0.5628221649484536, "retrieved_ids": ["mrqa_squad-train-11017", "mrqa_squad-train-81729", "mrqa_squad-train-6796", "mrqa_squad-train-6512", "mrqa_squad-train-6651", "mrqa_squad-train-18886", "mrqa_squad-train-21744", "mrqa_squad-train-30326", "mrqa_squad-train-9322", "mrqa_squad-train-84434", "mrqa_squad-train-17838", "mrqa_squad-train-53937", "mrqa_squad-train-18474", "mrqa_squad-train-43659", "mrqa_squad-train-68234", "mrqa_squad-train-60197", "mrqa_searchqa-validation-5672", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-14046", "mrqa_hotpotqa-validation-4223", "mrqa_naturalquestions-validation-902", "mrqa_newsqa-validation-1613", "mrqa_triviaqa-validation-1028", "mrqa_newsqa-validation-1281", "mrqa_triviaqa-validation-3531", "mrqa_hotpotqa-validation-667", "mrqa_hotpotqa-validation-840", "mrqa_newsqa-validation-2259", "mrqa_hotpotqa-validation-4430", "mrqa_newsqa-validation-3947", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-6679"], "EFR": 0.9722222222222222, "Overall": 0.7334932524341352}, {"timecode": 97, "before_eval_results": {"predictions": ["Michael Schumacher", "Andy Murray", "Six", "girls", "David Bowie,", "1957,", "at Lifeway Christian Stores", "'We want to reset our relationship and so we will do it together.'\"", "heavy brush,", "1959.", "Phoenix, Arizona,", "\"project work\"", "Buddhism", "40", "that the company's products are roadworthy.", "Rwanda is now considered one of Africa's most stable nations.", "Molotov cocktails, rocks and glass.", "Alfredo Astiz,", "on the 11th anniversary of the September 11, 2001,", "Mashhad", "the wars in Iraq and Afghanistan", "$106,482,500", "Obama should have met with the Dalai Lama.", "Matthew Fisher,", "autonomy", "the head", "control and censorship remain rife", "Guinea, Myanmar, Sudan and Venezuela.", "Brazil's", "Uzbekistan.", "45 minutes, five days a week.", "urged NATO to take a more active role in countering the spread of the narcotics trade,", "took on water", "ashes", "July", "Visitors aren't allowed onto the property to view the elephants,", "an auxiliary lock", "Diego Milito's", "music video", "Robert Mugabe", "gang rape", "\"wacko.\"", "threatening messages", "Mexico", "more than 1.2 million people.", "Sunday.", "tennis", "Osama bin Laden's sons", "Chesley \"Sully\" Sullenberger", "Gary Player,", "Karen Floyd", "Asuka", "Authority", "Prince William, Duke of Cambridge", "the Astor family", "Gryffendor", "Ben Miller", "841", "Adelaide", "gender queer", "Windsor Castle", "Tad Hamilton", "Israel", "Bill Irwin"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6819974296536797}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1362", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-1775", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-2337", "mrqa_triviaqa-validation-5311", "mrqa_triviaqa-validation-5472", "mrqa_triviaqa-validation-6776", "mrqa_hotpotqa-validation-2217"], "SR": 0.578125, "CSR": 0.5629783163265306, "EFR": 1.0, "Overall": 0.7390800382653061}, {"timecode": 98, "before_eval_results": {"predictions": ["Cambodian officials", "monarchy's", "\"pleased\"", "AbdulMutallab", "in a stream in Shark River Park in Monmouth County", "consumer confidence", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Madonna", "Egyptian striker", "Iran's parliament speaker", "18", "2050,", "U.S. State Department and British Foreign Office", "Janet Napolitano", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "make sure water continues flow through the river channel and not spread out over land.", "likely", "in Iraq and Afghanistan through the remainder of his presidency and into spring 2009.", "Islamabad", "the 3rd District of Utah.", "75.", "Nineteen", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "nearly $2 billion", "Jeddah, Saudi Arabia,", "Al-Shabaab,", "February 12", "Kenneth Cole", "Clifford Harris,", "heavy turbulence", "Saturday's killing of a 15-year-old boy", "to sniff out cell phones.", "California, Texas and Florida,", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "\"He focuses on Ozzy, and he gets through his torture hour.\"", "Revolutionary Armed Forces of Colombia,", "suicides", "Jaime Andrade", "Aung San Suu Kyi", "$81,8709.", "Patrick McGoohan,", "\"GoldenEye\"", "J.Crew,", "Dublin.", "1995", "Chester Arthur Stiles,", "the journalists and the flight crew will be freed,", "a skilled hacker", "$7.8 million", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "remains committed to British sovereignty", "Lee Thompson Young", "September 2017", "Norman occupational surname ( meaning tailor ) in France", "\"Landlord\u2019s Game", "vino veritas", "congenital hydrocephalus", "India Today", "Eliot Cutler", "7 January 1936", "Bran Mak Morn", "Santiago", "Washington Redskins", "\"Mrs. Miniver\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6766285553453031}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.4444444444444445, 0.9565217391304348, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947364, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-3888", "mrqa_naturalquestions-validation-8858", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-3864", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-1798", "mrqa_searchqa-validation-6956"], "SR": 0.609375, "CSR": 0.5634469696969697, "EFR": 0.92, "Overall": 0.7231737689393939}, {"timecode": 99, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2871", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1961", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-739", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-2079", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4698", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-5339", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-904"], "OKR": 0.83203125, "KG": 0.490625, "before_eval_results": {"predictions": ["bacteria", "1648 - 51", "England", "Scott Schwartz", "by one of the United States courts of appeals", "Lana Del Rey", "semi solid", "Madison", "May 29, 2018", "Dan Stevens", "in Ephesus in AD 95 -- 110", "Bulgaria", "zinc", "1986", "2018", "U + 2234 \u2234 therefore ( HTML & # 8756 ; &there4 ; )", "July 8, 1998", "September 2017", "Clarence Anglin", "Ariana Clarice Richards", "James Intveld", "Darlene Cates", "one", "mashed potato", "2020", "dress shop", "George Harrison", "Seton Hall Pirates", "second season", "Universal Pictures", "Cyanea capillata", "enabled business applications to be developed with Flash", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "Lee County, Florida", "digestive systems", "Theodosius I", "Christopher Allen Lloyd", "reared", "Mark Jackson", "Fall 1998", "TLC", "Inequality of opportunity was higher", "In 1929", "Abid Ali Neemuchwala", "Bhupendranath Dutt", "May 19, 2017", "the people of France", "two to three barrel vaults", "Brazil, Bolivia, Paraguay and Argentina", "Gustav Bauer", "the largest part of the brain", "leicestershire", "\"The curse is come upon me,\"", "15", "Mike Pence", "Soviet Union", "Heinkel Flugzeugwerke", "she returned to Pakistan in October after President Pervez Musharraf signed an amnesty lifting corruption charges.", "Zac Efron", "Aryan Airlines Flight 1625", "Ocean\\'s Twelve", "Trajan", "Anne Rice", "Wilkie Collins"], "metric_results": {"EM": 0.609375, "QA-F1": 0.725730294011544}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.19047619047619047, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.5714285714285715, 0.6666666666666666, 0.761904761904762, 0.5, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.08000000000000002, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-10706", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-1395", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-9791", "mrqa_triviaqa-validation-5094", "mrqa_newsqa-validation-846"], "SR": 0.609375, "CSR": 0.56390625, "retrieved_ids": ["mrqa_squad-train-77191", "mrqa_squad-train-66862", "mrqa_squad-train-34483", "mrqa_squad-train-46884", "mrqa_squad-train-60923", "mrqa_squad-train-15490", "mrqa_squad-train-11753", "mrqa_squad-train-39151", "mrqa_squad-train-42217", "mrqa_squad-train-13613", "mrqa_squad-train-64238", "mrqa_squad-train-33444", "mrqa_squad-train-3952", "mrqa_squad-train-45155", "mrqa_squad-train-64692", "mrqa_squad-train-4404", "mrqa_triviaqa-validation-58", "mrqa_newsqa-validation-1983", "mrqa_searchqa-validation-12155", "mrqa_newsqa-validation-2214", "mrqa_naturalquestions-validation-601", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-16787", "mrqa_naturalquestions-validation-10525", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1202", "mrqa_searchqa-validation-11035", "mrqa_hotpotqa-validation-5082", "mrqa_hotpotqa-validation-380", "mrqa_searchqa-validation-8602", "mrqa_naturalquestions-validation-7624", "mrqa_newsqa-validation-1271"], "EFR": 0.96, "Overall": 0.72478125}]}