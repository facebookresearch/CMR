{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]_result.json', stream_id=4, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5470, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["fall of 1937", "The Skirmish of the Brick Church", "beliefs of Sunni Islamic thinkers", "\"The Lodger\"", "Londonistan", "a high-level marketing manager", "Houston, Texas", "cone-shaped", "San Francisco Bay Area's Levi's Stadium", "Ren\u00e9 Lalique", "absolution", "$105 billion", "ABC Cable News", "trial division", "their belief in the validity of the social contract", "Hyde Park", "four years", "Grey Street", "most of the items in the collection, unless those were newly accessioned into the collection", "literacy and numeracy", "Luther", "prime elements", "the Aveo", "one week", "Steymann v Staatssecretaris van Justitie", "1937", "The governments of the United States, Britain, Germany and France", "mother-of-pearl", "cholera", "Tower District", "ring theory", "Euclid's fundamental theorem of arithmetic", "Tony Hawk", "Beyonc\u00e9", "The Book of Discipline", "USSR", "Schmalkaldic League", "2006", "70%", "Einstein", "Genghis Khan", "four half-courses per term", "2011", "Brownlee", "Tracy Wolfson", "the wisdom and prudence of certain decisions of procurement", "1971", "the Uighurs surrendered to the Mongols first", "cnidarians", "CBS", "842 pounds", "two of Tesla's uncles", "up to \u00a332,583", "the City council", "three", "shopping", "4 weeks", "propulsion, electrical power and life support", "William Smilie", "George Westinghouse", "1279", "complexity classes", "\"everything that smacks of sacrifice\"", "a system to function"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7404040404040404}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9630", "mrqa_squad-validation-7687", "mrqa_squad-validation-4836", "mrqa_squad-validation-131", "mrqa_squad-validation-2297", "mrqa_squad-validation-5505", "mrqa_squad-validation-1802", "mrqa_squad-validation-9136", "mrqa_squad-validation-9061", "mrqa_squad-validation-116", "mrqa_squad-validation-5877", "mrqa_squad-validation-6294", "mrqa_squad-validation-7214", "mrqa_squad-validation-3699", "mrqa_squad-validation-8247", "mrqa_squad-validation-4419", "mrqa_squad-validation-553", "mrqa_squad-validation-3811", "mrqa_squad-validation-2092"], "SR": 0.703125, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 1, "before_eval_results": {"predictions": ["the Pulfrich effect", "semi-legal", "Westinghouse Electric", "the Pax Mongolica (Mongol Peace)", "The date of 2035", "internal strife", "the Marburg Colloquy", "Northumbria University", "non-cryogenic", "the defense and justification of empire-building", "the Carmichael numbers", "1999", "Scorpion", "October 16, 2012", "a commune (gmina)", "the metal locking screw on the camera lens", "Eldon Square Shopping Centre", "type III secretion system", "$680 billion", "296", "New Collegiate Division", "four levels", "18 million volumes", "15,100", "Warner Bros. Presents", "V\u03b39/V\u03b42 T cells", "1985", "the Augustinian friars", "third", "2012", "gold", "friction", "378", "tourism", "the Jews", "all", "many celebrated seasons", "Charles-Fer Ferdinand University", "The Nationals", "a computational problem where a single output (of a total function) is expected for every input", "Katharina von Bora", "1888", "the middle of the continent", "the Schmalkaldic League", "4:51", "Knaurs Lexikon", "constant pressure", "detective shows", "mainly in the southern and central parts of France", "Maria Fold and thrust Belt", "making it seem like climate change is more serious by overstating the impact", "1945", "1876", "Elway", "spring of 1349", "Extreme Makeover: Home Edition", "the Wesleyan Holiness Consortium", "it has settled as one of the pillars of history, has little foundation in truth", "There are four railroad properties in the Monopoly board game: Pennsylvania Railroad, B&O Railroad, Reading Railroad and Short Line Railroad", "The Sphinx would devour anyone who could not answer her riddle in the story of.... Sphinx", "The Smashing Pumpkins are an American alternative rock band from Chicago, Illinois, formed... After the breakup of his gothic rock band", "On August 10, 1993, Ruth Bader Ginsburg was sworn in as the 107th justice on... by a vote of ninety-six to three", "a man identifying himself as Dan... The note went on to say that he wanted $200,000 in twenty dollar bills and two... and twenty-one pounds of ransom money in hand, Dan Cooper jumped into history.", "If the citizen's heart was heavier than a feather they would face torment in a lake of fire"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8538103070175438}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.2666666666666667, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.1904761904761905, 0.13333333333333333, 0.19999999999999998, 0.09523809523809523, 0.05714285714285715, 0.2105263157894737]}}, "before_error_ids": ["mrqa_squad-validation-8546", "mrqa_squad-validation-1028", "mrqa_squad-validation-1030", "mrqa_squad-validation-1189", "mrqa_squad-validation-1600", "mrqa_squad-validation-4287", "mrqa_squad-validation-2975", "mrqa_squad-validation-2166", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-1274", "mrqa_hotpotqa-validation-3713"], "SR": 0.78125, "CSR": 0.7421875, "EFR": 0.9285714285714286, "Overall": 0.8353794642857143}, {"timecode": 2, "before_eval_results": {"predictions": ["Solim\u00f5es Basin", "the seal of the Federal Communications Commission", "Islamization", "the E. W. Scripps Company", "Grand Canal d'Alsace", "food security", "exothermic", "Newcastle Diamonds", "the wisdom and prudence of certain decisions of procurement", "D\u00fcrer", "Grover Cleveland", "private individuals", "concrete", "a program of coordinated, evolving projects sponsored by the National Science Foundation (NSF) beginning in 1985 to promote advanced research and education networking in the United States", "The Newlywed Game", "the German-Swiss border", "the \"blurring of theological and confessional differences in the interests of unity.\"", "microbes", "300", "the electrostatic force", "Jim Nantz and Phil Simms", "1530", "from 12:00 to 6:00 p.m. Eastern Time", "employ consultant pharmacists and/or provide consulting services", "the murder of Christ", "1708", "the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "the Religious Coalition for Reproductive Choice", "The Arrow", "intractable problems", "yellow fever", "silver and inlaid with gold", "Richard Wilkinson and Kate Pickett", "the Northern United Kingdom", "president and CEO", "Von Miller", "the weak force", "a diverse phylum of bacteria capable of carrying out photosynthesis", "vaccination", "Yersinia pestis", "loss of soil fertility and weed invasion", "the fact (Fermat's little theorem)", "11th", "the most popular show", "Robert of Jumi\u00e8ges", "student populations", "German", "Persia", "superheaters", "two", "Johann von Staupitz", "lectures", "a Twinkie", "What a wonderful World", "the place", "Dugout canoe", "Ganges", "the Heritage 1981 brand", "Smell This", "the title", "the Don", "Ford Motor Company", "Fidenza", "Charles Scribner"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7248397435897436}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9768", "mrqa_squad-validation-5521", "mrqa_squad-validation-6981", "mrqa_squad-validation-4847", "mrqa_squad-validation-597", "mrqa_squad-validation-5828", "mrqa_squad-validation-10460", "mrqa_squad-validation-8777", "mrqa_squad-validation-9023", "mrqa_squad-validation-1188", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-16779", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-9859", "mrqa_searchqa-validation-6857", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-3869"], "SR": 0.671875, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 3, "before_eval_results": {"predictions": ["Winter Film Capital of the World", "sarcasm and attempts to humiliate pupils", "collenchyma tissue", "24 March 1879", "Scottish Constitutional Convention", "constant factors and smaller terms", "1996", "CBS", "(genetic branches)", "a religious basis", "14,000", "Killer T cells", "11 million", "third-most watched", "The Earth's mantle", "expansions", "Necessity-based", "glaucophyte chloroplasts", "artisans and farmers", "inverted repeat regions", "pharmacists", "September 2007", "a declining state of mind", "the G mission", "civil disobedience", "Sociologist", "World News Tonight", "the carriage of their respective basic channels", "Type I \u2013 IV", "Liao, Jin, and Song", "Tyneside Classical", "nine", "18 million volumes", "four", "Christian Whiton", "his mother", "Johann Gerhard", "Korean", "The Time of the Doctor", "7 January 1900", "90\u00b0", "badly disposed towards the French, and are entirely devoted to the English", "steal", "Centrum", "$200,000", "4,686 Mau Mau", "economic ties", "1950s", "Karl Marx", "President of the U.S.", "Phantom Manor", "Wittenberg", "South Africa", "fibre optics", "(The Color Purple)", "a cone-shaped utensil", "Yasser Arafat", "a pigeon", "vodou", "a dowry", "the title character", "Piper Halliwell", "Prince James, Duke of York and of Albany ( later King James II & VII )", "Charles Perrault"], "metric_results": {"EM": 0.625, "QA-F1": 0.6883680555555556}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9337", "mrqa_squad-validation-1714", "mrqa_squad-validation-9597", "mrqa_squad-validation-104", "mrqa_squad-validation-6409", "mrqa_squad-validation-3958", "mrqa_squad-validation-6670", "mrqa_squad-validation-6884", "mrqa_squad-validation-7083", "mrqa_squad-validation-2406", "mrqa_squad-validation-10186", "mrqa_squad-validation-2576", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-350", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-495"], "SR": 0.625, "CSR": 0.6953125, "retrieved_ids": ["mrqa_squad-train-63207", "mrqa_squad-train-27227", "mrqa_squad-train-81864", "mrqa_squad-train-39884", "mrqa_squad-train-30194", "mrqa_squad-train-27433", "mrqa_squad-train-55748", "mrqa_squad-train-50580", "mrqa_squad-train-12649", "mrqa_squad-train-62804", "mrqa_squad-train-79802", "mrqa_squad-train-38001", "mrqa_squad-train-28707", "mrqa_squad-train-65782", "mrqa_squad-train-37024", "mrqa_squad-train-15550", "mrqa_squad-validation-1802", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-8978", "mrqa_squad-validation-7687", "mrqa_squad-validation-3699", "mrqa_searchqa-validation-13088", "mrqa_squad-validation-8247", "mrqa_squad-validation-1600", "mrqa_squad-validation-4419", "mrqa_squad-validation-9630", "mrqa_squad-validation-131", "mrqa_squad-validation-8546", "mrqa_squad-validation-10460", "mrqa_squad-validation-597", "mrqa_squad-validation-8777"], "EFR": 0.9583333333333334, "Overall": 0.8268229166666667}, {"timecode": 4, "before_eval_results": {"predictions": ["Mars", "education", "Kingdom of Prussia", "May 18, 1756", "odd prime", "economic growth", "quantum electrodynamics", "topographic", "Hassan al Banna", "regional burden sharing", "Indianapolis Colts", "Pole Mokotowskie", "a school or other place of formal education", "Francis Blackburne", "black earth", "photolysis of ozone by light of short wavelength", "smart ticketing", "State Route 99", "F and \u2212F are equal in magnitude and opposite in direction", "free", "Air", "1,548", "whether the bill is within the legislative competence of the Parliament", "two", "Galileo Galilei", "Catholic", "patient care rounds drug product selection", "vicious and destructive", "greater scarcity", "The Eleventh Doctor", "86", "Arizona Cardinals", "not", "2015", "one hunting excursion", "a bishop", "to destroy the antichrist", "socialist realism", "July 23, 1963", "1162", "a method which pre- allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "Barbara Walters", "2050", "Red River", "the hundreds", "British", "Sub-Saharan Africa", "(Doogie Howser, M.D)", "the Carrousel du Louvre", "$1.5 million", "3 to 17", "Isabella", "Obama", "Tara Livesay", "Preah Vihear temple", "Ralph Lauren", "Noriko Savoie", "T.I.", "Zed", "work together to stabilize Somalia and cooperate in security and military operations", "battles", "oldpatricktoe-end", "Mulberry", "Shaft"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7357743818681319}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.25, 1.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8634", "mrqa_squad-validation-1891", "mrqa_squad-validation-10333", "mrqa_squad-validation-3706", "mrqa_squad-validation-10068", "mrqa_squad-validation-2564", "mrqa_squad-validation-4746", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-2234", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-712", "mrqa_searchqa-validation-8929"], "SR": 0.671875, "CSR": 0.690625, "EFR": 0.9523809523809523, "Overall": 0.8215029761904762}, {"timecode": 5, "before_eval_results": {"predictions": ["visitation of the Electorate of Saxony", "United States", "11", "May", "employ limited coercion", "1798", "3D printing technology", "Waterlogged", "Tim Allen", "more wealth and income", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "40", "LeGrande", "filaments", "Joanna Lumley", "Energiprojekt AB in Sweden", "DuMont Television Network", "1913", "Egyptian Islamic Jihad organization", "consumer prices", "petroleum", "1870", "27.7 million tons", "the remainder of the British Isles", "Jean Auguste Dominique Ingres,", "Eliot Ness", "as the devil's work", "Edgar", "experience and extra responsibilities", "chameleon circuit", "All-Channel Receiver Act", "under the wing of the secular powers", "opposite end from the mouth", "areas controlled by Russia in 1914,", "kinescope", "University Athletic Association", "three", "Chebyshev", "an American reality television series", "Disco", "Christopher Lloyd Smalling", "Mary Harron", "Polk", "Minette Walters", "1983", "79 AD", "1993 to 2001", "Major League Soccer", "1669", "University of Vienna", "\"lo Stivale\"", "Richa Sharma", "Centennial Olympic Stadium", "Violet", "Vernier, Switzerland", "October 21, 2016", "an Indian cricketer and former captain of the Indian cricket team", "international association football competitions", "The conversation", "9 February 2018", "Canada", "15", "an increase in dew point", "Donna Mills"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8394097222222222}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3345", "mrqa_squad-validation-5519", "mrqa_squad-validation-2322", "mrqa_squad-validation-2288", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4614", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-2982", "mrqa_searchqa-validation-4118"], "SR": 0.765625, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 6, "before_eval_results": {"predictions": ["woodblocks", "General Hospital", "the circle logo", "Denver Broncos", "129 MSPs", "Lenin", "the completed (or local) fields", "achievement-oriented", "alone", "stem cells", "John Pell, Lord of Pelham Manor", "quickly", "an induction motor", "non-religious", "pressure terms", "ten million", "Sting", "nine factors", "13.34% (116.7 sq mi or 302 km2)", "kilopond", "water level", "1981", "rules that conflict with morality", "sixteenth century", "R\u00fcdesheim am Rhein", "an epidemiological account of the plague", "time or space", "inner city blacks", "Aristotle", "1724", "mid-Cambrian period", "Canada", "Stanford", "Reuben Townroe", "small forward", "Alamo Bowl", "The King of Chutzpah", "Charles Russell", "German", "Minette Walters (born 26 September 1949)", "St. Patrick's Day in 1988", "Dulwich", "Michael Sheen, and Laurence Fishburne", "Hungary", "ITV", "Ella Fitzgerald", "\"To Save a Life\"", "EBSCO Information Services", "Dutch", "Marc Bolan", "\"Frankenstein\"", "John Mills and Tom Courtenay", "1992", "Saint-Domingue", "Airline Deregulation Act", "\"Kill Your Darlings\" (2006)", "University of Kansas", "The Land of Enchantment", "2001", "the Hanging Gardens of Babylon", "Texas", "Billy Bob Thornton", "Charles Martel", "The Blues Brothers"], "metric_results": {"EM": 0.703125, "QA-F1": 0.742125496031746}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9430", "mrqa_squad-validation-7317", "mrqa_squad-validation-7614", "mrqa_squad-validation-7476", "mrqa_squad-validation-7295", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-2431", "mrqa_triviaqa-validation-2856", "mrqa_newsqa-validation-696", "mrqa_searchqa-validation-12796"], "SR": 0.703125, "CSR": 0.703125, "retrieved_ids": ["mrqa_squad-train-35401", "mrqa_squad-train-14081", "mrqa_squad-train-56513", "mrqa_squad-train-51911", "mrqa_squad-train-67762", "mrqa_squad-train-84151", "mrqa_squad-train-58355", "mrqa_squad-train-6184", "mrqa_squad-train-76458", "mrqa_squad-train-81010", "mrqa_squad-train-982", "mrqa_squad-train-38942", "mrqa_squad-train-79344", "mrqa_squad-train-49668", "mrqa_squad-train-5447", "mrqa_squad-train-80777", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-217", "mrqa_squad-validation-5828", "mrqa_squad-validation-2576", "mrqa_newsqa-validation-3635", "mrqa_squad-validation-6294", "mrqa_squad-validation-1600", "mrqa_squad-validation-597", "mrqa_newsqa-validation-2234", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-6857", "mrqa_squad-validation-1714", "mrqa_squad-validation-6884", "mrqa_squad-validation-6981", "mrqa_squad-validation-8546", "mrqa_newsqa-validation-1899"], "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 7, "before_eval_results": {"predictions": ["Germany", "1985", "William Iron Arm", "Muhammad Abd al-Salaam Farag", "comedies", "lack of understanding of the legal ramifications", "British East Africa", "Pacific", "Jingshi Dadian", "economic instability", "Jean- Marc Bosman", "Ismailiyah, Egypt", "Wiesner", "polynomial time", "quarterback", "ten times their own weight", "primes", "light energy", "successfully", "\"TFIF\"", "adapted quickly and often married outside their immediate French communities", "George Westinghouse", "the force that acts between nucleons in atomic nuclei", "certification by a recognized body", "Alberto Calder\u00f3n", "Mercury", "Marconi successfully transmitted the letter S from England to Newfoundland", "private", "Cadeby", "Ten", "vice president", "Steven Gerrard", "the international community", "South African police", "Adam Yahiye Gadahn", "Berlusconi", "London Heathrow's Terminal 5", "in the mouth", "football", "a strict interpretation of the law", "JBS", "President Obama", "the Kurdish Freedom Falcons", "The pilot", "Wednesday", "$50", "back at work", "flooding", "1969", "composer", "50,000", "to many drivers, these words are exactly all they are looking for", "Bangladesh", "SSM Cardinal Glennon Children's Medical Center", "military", "he has no plans to fritter his cash away on fast cars", "Secretary of State Hillary Clinton", "7th century", "Wigan", "1974", "Mormon Tabernacle Choir", "Hagai Amir", "The Little Foxes", "The Holy Grail"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6570215071949197}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.29629629629629634, 0.25, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.08695652173913045, 0.05263157894736842, 0.33333333333333337, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6034", "mrqa_squad-validation-6925", "mrqa_squad-validation-8339", "mrqa_squad-validation-4692", "mrqa_squad-validation-2160", "mrqa_squad-validation-6001", "mrqa_squad-validation-3069", "mrqa_squad-validation-10445", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_naturalquestions-validation-8664", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-1341"], "SR": 0.578125, "CSR": 0.6875, "EFR": 0.9259259259259259, "Overall": 0.806712962962963}, {"timecode": 8, "before_eval_results": {"predictions": ["2 July 1505,", "beginning in early September and ending in mid-May", "high risk preparations and some other compounding functions", "Non Governmental and Intergovernmental Organizations", "chief electrician position", "1671", "United States", "12 January", "Naimans (Naiman Mongols)", "Sonia Shankman Orthogenic School", "Innate immune systems", "service sector", "Wardenclyffe Tower project", "horizontal compression", "Edgar Scherick", "Imperial", "Moscone Center in San Francisco", "Dai \u00d6n Ulus,", "the 1980s", "oxygen-16", "Lessing", "at least one advanced course every three years.", "city centre roads", "civil disobedience", "ca. 22,000\u201314,000 yr BP,", "two", "Lek", "New England Patriots", "work rule issues", "28", "late May,", "the Catholic League", "more than two years,", "Newark's Liberty International Airport,", "We Found Love", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "a kidney", "to foster national reconciliation between religious and ethnic groups.", "Addis Ababa,", "five female pastors", "military commissions", "to launch a group that will serve as an alternative to the Organization of American States.", "a monthly allowance", "Tutsi and Hutu rivalry", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Draquila -- Italy Trembles.", "hooked up with Mildred, a younger woman of about 80, in March.", "his former Boca Juniors teammate and national coach Diego Maradona,", "17 Again", "five", "75", "Kgalema Motlanthe,", "preventing our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia.", "about 3,000 kilometers (1,900 miles)", "two courses", "NATO fighters", "Austin, Texas,", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Royals", "Anah\u00ed", "Dictator Still Casts A Shadow In Romania", "Nikkei 225 Stock Average", "Surrey", "David Bowie"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6166769704184109}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1818181818181818, 0.8, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.058823529411764705, 0.2608695652173913, 0.0, 0.28571428571428575, 0.06451612903225806, 0.5, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.6060606060606061, 0.0, 1.0, 0.2857142857142857, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8526", "mrqa_squad-validation-1279", "mrqa_squad-validation-3113", "mrqa_squad-validation-589", "mrqa_squad-validation-6128", "mrqa_squad-validation-7377", "mrqa_squad-validation-503", "mrqa_squad-validation-2405", "mrqa_squad-validation-5357", "mrqa_squad-validation-6671", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-490", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-93", "mrqa_searchqa-validation-8602", "mrqa_triviaqa-validation-1"], "SR": 0.484375, "CSR": 0.6649305555555556, "EFR": 1.0, "Overall": 0.8324652777777778}, {"timecode": 9, "before_eval_results": {"predictions": ["The Private Education Student Financial Assistance", "philanthropy", "in an unknown location", "Daily Mail", "heard her songs; he followed the fishermen and captured the mermaid.", "equal in magnitude", "Combined Statistical Area", "orogenic wedges", "eleven", "Grissom, White, and Chaffee", "Amtrak San Joaquins", "two", "1", "Barbara Walters", "temperate", "girls", "by citizens", "by up to 3 pence in the pound", "Johann Gerhard", "the courts of member states and the Court of Justice of the European Union", "1887", "new laws or amendments to existing laws as a bill", "487", "the Presiding Officer", "expansion", "between 3.9 and 5.5 kW / L ( 70 to 100 mg / dL )", "the axial skeleton", "Thebes", "moral", "The Maidstone Studios", "mongrel", "the National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1986", "Charles Darwin", "the historical Saint Nicholas", "in Poems : Series 1", "1956", "merengue", "lamina dura", "Jennifer O'Neill as Hermie's mysterious love interest, and Katherine Allentuck and Christopher Norris as a pair of girls whom Hermie and Oscy attempt to seduce", "Proposition 103", "1997", "the intersection of Mud Mountain Road and Highway 410", "the United States", "Bobb McKittrick", "2017", "Donald Fauntleroy Duck", "three", "Sylvester Stallone", "every 23 hours", "never made", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time", "1963", "a costume party", "the altitude", "3 points", "sow", "feces", "John Joseph Travolta", "Allies of World War I, or Entente Powers,", "north-south", "having a smile on her face when her kids were around.", "\"Iron Mike\"", "Utah"], "metric_results": {"EM": 0.5, "QA-F1": 0.6768826568168673}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428572, 0.5, 0.0, 0.6666666666666666, 0.5714285714285715, 0.6666666666666666, 0.631578947368421, 1.0, 1.0, 0.8, 0.3076923076923077, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 0.7692307692307693, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5937", "mrqa_squad-validation-805", "mrqa_squad-validation-2717", "mrqa_squad-validation-3922", "mrqa_squad-validation-9641", "mrqa_squad-validation-2404", "mrqa_squad-validation-9452", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-1173", "mrqa_triviaqa-validation-2329", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2729", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-4179", "mrqa_searchqa-validation-10449"], "SR": 0.5, "CSR": 0.6484375, "retrieved_ids": ["mrqa_squad-train-14616", "mrqa_squad-train-27098", "mrqa_squad-train-51016", "mrqa_squad-train-35620", "mrqa_squad-train-27359", "mrqa_squad-train-37382", "mrqa_squad-train-41490", "mrqa_squad-train-16154", "mrqa_squad-train-50332", "mrqa_squad-train-11669", "mrqa_squad-train-27394", "mrqa_squad-train-14244", "mrqa_squad-train-12416", "mrqa_squad-train-82686", "mrqa_squad-train-69469", "mrqa_squad-train-72555", "mrqa_squad-validation-10186", "mrqa_newsqa-validation-1639", "mrqa_squad-validation-8339", "mrqa_squad-validation-7377", "mrqa_squad-validation-5505", "mrqa_newsqa-validation-807", "mrqa_squad-validation-553", "mrqa_searchqa-validation-1341", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-5666", "mrqa_newsqa-validation-2272", "mrqa_squad-validation-8777", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-9859", "mrqa_searchqa-validation-899", "mrqa_hotpotqa-validation-2792"], "EFR": 0.96875, "Overall": 0.80859375}, {"timecode": 10, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1735", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1877", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3994", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6984", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-4000", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-69", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16779", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2470", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-350", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-9187", "mrqa_squad-validation-10015", "mrqa_squad-validation-10052", "mrqa_squad-validation-10068", "mrqa_squad-validation-1008", "mrqa_squad-validation-10083", "mrqa_squad-validation-10103", "mrqa_squad-validation-10107", "mrqa_squad-validation-10116", "mrqa_squad-validation-10125", "mrqa_squad-validation-10186", "mrqa_squad-validation-10210", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-1030", "mrqa_squad-validation-10308", "mrqa_squad-validation-10333", "mrqa_squad-validation-10333", "mrqa_squad-validation-10344", "mrqa_squad-validation-10367", "mrqa_squad-validation-10374", "mrqa_squad-validation-104", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10466", "mrqa_squad-validation-10493", "mrqa_squad-validation-1051", "mrqa_squad-validation-1052", "mrqa_squad-validation-1068", "mrqa_squad-validation-1113", "mrqa_squad-validation-116", "mrqa_squad-validation-1165", "mrqa_squad-validation-1178", "mrqa_squad-validation-1188", "mrqa_squad-validation-1193", "mrqa_squad-validation-1200", "mrqa_squad-validation-1207", "mrqa_squad-validation-1211", "mrqa_squad-validation-1257", "mrqa_squad-validation-1269", "mrqa_squad-validation-1279", "mrqa_squad-validation-131", "mrqa_squad-validation-1330", "mrqa_squad-validation-1348", "mrqa_squad-validation-1368", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1509", "mrqa_squad-validation-1527", "mrqa_squad-validation-1536", "mrqa_squad-validation-1541", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1714", "mrqa_squad-validation-1769", "mrqa_squad-validation-1802", "mrqa_squad-validation-1891", "mrqa_squad-validation-1947", "mrqa_squad-validation-1967", "mrqa_squad-validation-2030", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2166", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-2297", "mrqa_squad-validation-2331", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2405", "mrqa_squad-validation-2409", "mrqa_squad-validation-2438", "mrqa_squad-validation-25", "mrqa_squad-validation-2554", "mrqa_squad-validation-2559", "mrqa_squad-validation-2564", "mrqa_squad-validation-2567", "mrqa_squad-validation-2576", "mrqa_squad-validation-2579", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2717", "mrqa_squad-validation-2778", "mrqa_squad-validation-2822", "mrqa_squad-validation-2827", "mrqa_squad-validation-2870", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-3050", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-313", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3261", "mrqa_squad-validation-3269", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3388", "mrqa_squad-validation-3445", "mrqa_squad-validation-3492", "mrqa_squad-validation-3603", "mrqa_squad-validation-3617", "mrqa_squad-validation-365", "mrqa_squad-validation-3699", "mrqa_squad-validation-3759", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3815", "mrqa_squad-validation-3833", "mrqa_squad-validation-3837", "mrqa_squad-validation-3844", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3922", "mrqa_squad-validation-3938", "mrqa_squad-validation-3958", "mrqa_squad-validation-3976", "mrqa_squad-validation-4030", "mrqa_squad-validation-4086", "mrqa_squad-validation-4191", "mrqa_squad-validation-4231", "mrqa_squad-validation-4232", "mrqa_squad-validation-4248", "mrqa_squad-validation-4269", "mrqa_squad-validation-43", "mrqa_squad-validation-4419", "mrqa_squad-validation-4480", "mrqa_squad-validation-4491", "mrqa_squad-validation-4560", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4746", "mrqa_squad-validation-475", "mrqa_squad-validation-4765", "mrqa_squad-validation-4836", "mrqa_squad-validation-4847", "mrqa_squad-validation-4896", "mrqa_squad-validation-4935", "mrqa_squad-validation-5009", "mrqa_squad-validation-5075", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5164", "mrqa_squad-validation-5180", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5221", "mrqa_squad-validation-5272", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5357", "mrqa_squad-validation-5363", "mrqa_squad-validation-5424", "mrqa_squad-validation-5451", "mrqa_squad-validation-5455", "mrqa_squad-validation-5471", "mrqa_squad-validation-5505", "mrqa_squad-validation-5519", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5541", "mrqa_squad-validation-5616", "mrqa_squad-validation-5651", "mrqa_squad-validation-5670", "mrqa_squad-validation-5774", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-583", "mrqa_squad-validation-5840", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-5877", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5908", "mrqa_squad-validation-5937", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-5971", "mrqa_squad-validation-5976", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6048", "mrqa_squad-validation-6083", "mrqa_squad-validation-6098", "mrqa_squad-validation-6098", "mrqa_squad-validation-6128", "mrqa_squad-validation-6158", "mrqa_squad-validation-618", "mrqa_squad-validation-6238", "mrqa_squad-validation-6294", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6381", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6506", "mrqa_squad-validation-6527", "mrqa_squad-validation-6530", "mrqa_squad-validation-6569", "mrqa_squad-validation-6580", "mrqa_squad-validation-6605", "mrqa_squad-validation-6670", "mrqa_squad-validation-6681", "mrqa_squad-validation-6707", "mrqa_squad-validation-6754", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-69", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-6996", "mrqa_squad-validation-7002", "mrqa_squad-validation-7020", "mrqa_squad-validation-7022", "mrqa_squad-validation-7034", "mrqa_squad-validation-7080", "mrqa_squad-validation-7083", "mrqa_squad-validation-7092", "mrqa_squad-validation-7094", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7303", "mrqa_squad-validation-7304", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7420", "mrqa_squad-validation-7476", "mrqa_squad-validation-7502", "mrqa_squad-validation-7614", "mrqa_squad-validation-7687", "mrqa_squad-validation-7690", "mrqa_squad-validation-7704", "mrqa_squad-validation-775", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-7886", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7981", "mrqa_squad-validation-805", "mrqa_squad-validation-8052", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8197", "mrqa_squad-validation-8247", "mrqa_squad-validation-829", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8364", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8416", "mrqa_squad-validation-8479", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8526", "mrqa_squad-validation-8546", "mrqa_squad-validation-8580", "mrqa_squad-validation-8600", "mrqa_squad-validation-863", "mrqa_squad-validation-8680", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8777", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8953", "mrqa_squad-validation-8957", "mrqa_squad-validation-8965", "mrqa_squad-validation-9002", "mrqa_squad-validation-9012", "mrqa_squad-validation-902", "mrqa_squad-validation-9023", "mrqa_squad-validation-9024", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9136", "mrqa_squad-validation-9141", "mrqa_squad-validation-9208", "mrqa_squad-validation-9254", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9337", "mrqa_squad-validation-9411", "mrqa_squad-validation-9430", "mrqa_squad-validation-9452", "mrqa_squad-validation-9457", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9527", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9614", "mrqa_squad-validation-9615", "mrqa_squad-validation-9624", "mrqa_squad-validation-9635", "mrqa_squad-validation-9641", "mrqa_squad-validation-9665", "mrqa_squad-validation-9718", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_squad-validation-9845", "mrqa_squad-validation-985", "mrqa_squad-validation-9926", "mrqa_squad-validation-9940", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.919921875, "KG": 0.43203125, "before_eval_results": {"predictions": ["Miller\u2013Urey experiment", "five", "Mughal emperors", "a series of strikes by coal miners and railroad workers over the winter of 1973\u201374 became a major factor in the change of government.", "\" Informal\" and \"informal\" imperialism", "HO", "increased settlement and deforestation", "Afranji", "Abercynon in south Wales", "drinking water", "permafrost", "10 to 15 million", "Cabot Science Library, Lamont Library, and Widener Library", "the Henry Cole wing", "the mayor (the President of Warsaw),", "the time and space hierarchy theorems", "The innate immune system", "Cow Counties", "all", "Royal Institute of British Architects", "6th century", "The owner", "United Parcel Serivce", "Ernie", "Hokkoro", "The Rosetta Stone", "m\u00e0hjeung", "The Tonight Ensemble", "the right hand side of the second line of letters", "Japanese", "William Boyd", "Wolf Hall", "that you can lose points during the game as long as you win the game.", "the gums", "the White House", "Fort Sumter", "Wawrinka", "Humphrey Bogart", "George", "scientific magnetism", "uric Goldfinger", "Hell Upside Down: The Making of The Poseidon Adventure", "5", "Mary Poppins", "Neil Armstrong", "the centre bull", "Metropolitan Borough of Oldham, in Greater Manchester, England", "British Defence Secretary", "Old Ironsides", "Bullnose", "brown", "the skull", "\"Kippis\"", "gold", "Brainy", "New Zealand", "Paige O'Hara", "Chris Martin", "the Processional Way", "neo-Nazi", "several weeks", "an animal tranquilizer", "Robin", "the mouth"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6361243671319102}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6206896551724138, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.25, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3722", "mrqa_squad-validation-9808", "mrqa_squad-validation-6223", "mrqa_squad-validation-962", "mrqa_squad-validation-944", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5918", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-865", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-1579", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-515", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-2154"], "SR": 0.578125, "CSR": 0.6420454545454546, "EFR": 0.9629629629629629, "Overall": 0.7468610585016835}, {"timecode": 11, "before_eval_results": {"predictions": ["\"Old Briton\"", "Tiffany & Co", "1985", "entered Europe in two waves", "4k + 1", "Samuel Reshevsky", "mujahideen Muslim Afghanistan", "July 1977", "coal", "programmes to avoid similar disasters in the future", "Stanford Stadium", "gravity", "his work", "monatomic", "MetroCentre", "SAP Center", "Carmichael numbers", "the courts of member states", "German-language publications", "27", "6", "winter", "Durham Cathedral", "Konakuppakatil Gopinathan Balakrishnan", "Burj Khalifa", "March 14, 1942", "Persian Safavid dynasty", "Bart Howard", "on the microscope's stage", "naturally", "Max Martin", "2005", "31 December 1600", "February 29", "transmission", "216", "Dr. Addison Montgomery", "old English pyrige", "food and clothing", "Kansas City Chiefs", "the Indians", "Claudia Grace Wells", "fascia surrounding skeletal muscle", "the gas exchange membrane", "October 2004", "Babe Ruth", "Jim Capaldi", "Jaydev Shah", "Jodie Foster", "13,000 astronomical units", "1961", "7 spiritual gifts", "Thomas Chisholm", "Rocinante", "before the first letter of an interrogative sentence or clause", "Wikia", "The Daily Mirror", "invoice", "41 kilometres north of the Sydney central business district in the state of New South Wales, Australia", "1,500", "the underprivileged", "Alexander Pushkin", "trigger", "The Time Machine"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6875868055555556}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.8333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10142", "mrqa_squad-validation-4963", "mrqa_squad-validation-9082", "mrqa_squad-validation-3450", "mrqa_squad-validation-3947", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-3841", "mrqa_triviaqa-validation-412", "mrqa_hotpotqa-validation-2913", "mrqa_newsqa-validation-3686", "mrqa_searchqa-validation-14088", "mrqa_searchqa-validation-2383"], "SR": 0.5625, "CSR": 0.6354166666666667, "EFR": 0.9642857142857143, "Overall": 0.7457998511904762}, {"timecode": 12, "before_eval_results": {"predictions": ["2014", "Muslim", "Frederick William", "herbal", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "96.26%", "late 19th century", "Raoul Pierre Pictet", "more than $45,000", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "n < p < 2n \u2212 2", "illegal", "A", "The European Court of Justice", "chlorophyll a and phycobilins", "German-language publications", "1992", "25 percent of all money it raises for philanthropic causes in the Bay Area", "in the stems and roots of certain vascular plants", "Valens and Richardson", "1998", "British Army soldiers shot and killed people while under attack by a mob", "Judiththia Aline Keppel", "Ted '' Levine", "gregorito", "a noble gas", "a revolution", "Jaydev Shah", "Polly Walker", "Raya Yarbrough", "gilbert Grape", "2014 Winter Olympics in Sochi, Russia", "the evolution of light hair", "Las Vegas, Nevada", "to feel close to his son", "28 July 1914", "Jean F Kernel", "Haiti", "1948", "Sauron", "Chandan Shetty", "Bill Henderson", "Liam Cunningham", "Nancy Jean Cartwright", "Max", "Jonathan Goldstein", "a premalignant flat", "on the medulla oblongata", "his friends", "Ren\u00e9 Descartes", "Clarence Anglin", "Jason Lee", "1942", "2013", "on the continent of Antarctica", "Wikishire", "John Nash", "Sir Matthew Alistair Grant", "1989", "nearly $2 billion", "Hutus", "gregorava", "Brownsville", "Ralph Lauren"], "metric_results": {"EM": 0.5, "QA-F1": 0.610875496031746}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.11111111111111112, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.2, 0.0, 1.0, 0.1111111111111111, 0.5714285714285715, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.25, 0.6, 0.4, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8206", "mrqa_squad-validation-8412", "mrqa_squad-validation-3296", "mrqa_squad-validation-6655", "mrqa_squad-validation-394", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-10227", "mrqa_triviaqa-validation-6797", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2319", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-3744"], "SR": 0.5, "CSR": 0.625, "retrieved_ids": ["mrqa_squad-train-65920", "mrqa_squad-train-42400", "mrqa_squad-train-2081", "mrqa_squad-train-84545", "mrqa_squad-train-19069", "mrqa_squad-train-17486", "mrqa_squad-train-51261", "mrqa_squad-train-37112", "mrqa_squad-train-13600", "mrqa_squad-train-63936", "mrqa_squad-train-51138", "mrqa_squad-train-7196", "mrqa_squad-train-40318", "mrqa_squad-train-61238", "mrqa_squad-train-64809", "mrqa_squad-train-56677", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-15033", "mrqa_naturalquestions-validation-3093", "mrqa_newsqa-validation-774", "mrqa_squad-validation-5828", "mrqa_squad-validation-9061", "mrqa_squad-validation-7083", "mrqa_searchqa-validation-12796", "mrqa_squad-validation-10445", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8929", "mrqa_squad-validation-4836", "mrqa_newsqa-validation-1376", "mrqa_squad-validation-2717", "mrqa_squad-validation-9452", "mrqa_newsqa-validation-215"], "EFR": 1.0, "Overall": 0.750859375}, {"timecode": 13, "before_eval_results": {"predictions": ["early 1990s", "17th century", "Four thousand", "P", "the seal of the Federal Communications Commission", "gentrification of older neighbourhoods", "a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "1206", "2011", "in his lab and elsewhere", "their bright colors sometimes override the chlorophyll green", "National Broadcasting Company", "up to three-fourths", "The Five Doctors", "Open Door Policy", "1538", "Cricket Izz Izzie", "a poverty measure", "Tony Almeida", "Herbert Hoover", "Saint Wenceslaus", "coal", "Union Pacific & the Central Pacific", "\"The Daily Colonist (1932-11-20)", "the skulls of \"Peking Man,\"", "Latin", "Folkvang", "Luxor", "the grindstone", "the Osmonds", "hot air", "Hairstreaks", "the dancer keeps the fingers of both arms almost touching to form an oval shape,", "Shweta", "Calypso", "King John", "A Million Little pieces", "\"Give Me Liberty Or Give Me Death\"", "\"Imagine\"", "the Billy Goats Gruff", "jedoublen/jeopardy", "\"bacons\" (cut of pork)", "Saturn", "the Urals", "Amsterdam", "Etna", "the Valley Isle", "Richard Nixon", "the Little Mermaid", "a running back", "George Carlin", "Che Guevara", "raven", "Kurmanji", "Nicholas Sparks", "Lizzy Greene", "B\u00e9la Bart\u00f3k", "Ivan Owen", "English Electric Canberra", "Constitution of Mexico", "Karthik Rajaram", "the Lindsey oil refinery in eastern England.", "a month of training", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.617329744397759}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.5, 0.4444444444444444, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-9575", "mrqa_squad-validation-8229", "mrqa_squad-validation-5605", "mrqa_squad-validation-6250", "mrqa_squad-validation-7792", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-16887", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-16654", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-6353", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-2755", "mrqa_triviaqa-validation-5761", "mrqa_hotpotqa-validation-1876", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-909"], "SR": 0.53125, "CSR": 0.6183035714285714, "EFR": 1.0, "Overall": 0.7495200892857142}, {"timecode": 14, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "J. S. Bach", "Broncos", "from January 1964,", "unicellular organisms", "The Quasiturbine", "Dave Logan", "Roger NFL", "a cascade method", "December 2014", "Ladner", "the Ming dynasty", "Broncos", "15,100 kg", "Ronnie Hillman", "Masha Skorobogatov", "Daniel A. Dailey", "on the urinary floor", "31 December 1600", "along the ridge of the mountains", "1945", "Milira", "a hydrolysis reaction", "a comic book series", "Ceramic art", "State Bar of Arizona", "Tbilisi, Georgia", "a candidate state", "Yondu Udonta", "religious Hindu musical theatre styles", "Rocinante", "6 - 6", "John Roberts", "Ray Charles", "British and French Canadian fur traders", "a routing table", "the donor organ", "Bobby Darin", "a 420A motor with an upgrade to a T3 turbo and front mount intercooler", "Lana Del Rey", "Kim Basinger", "photoelectric ( optical ) smoke detector", "5,534", "Valmiki", "September 19, 2017", "President pro tempore", "March 2, 2016", "Wisconsin", "Merry Clayton", "greatly determine the tenderness of meat", "Florida", "a violation of nature", "Nepal", "Austria - Hungary", "Word Options", "Australia", "June 2, 2008", "Lovejoy", "Arthur E. Morgan III,", "a Coptic family", "Australia", "Oxford Committee for Famine Relief", "a Follies", "Oshkosh"], "metric_results": {"EM": 0.578125, "QA-F1": 0.67468998015873}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3854", "mrqa_squad-validation-6453", "mrqa_squad-validation-690", "mrqa_squad-validation-80", "mrqa_squad-validation-8062", "mrqa_squad-validation-362", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-5650", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-11788"], "SR": 0.578125, "CSR": 0.615625, "EFR": 1.0, "Overall": 0.748984375}, {"timecode": 15, "before_eval_results": {"predictions": ["chest pains", "thermodynamic", "The Lone Ranger", "a broken arm", "Arthur Woolf", "double or triple non-French linguistic origins", "Manning", "Warren Buffett", "Malkin Athletic Center", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "planktonic", "Lucas Horenbout", "a deficit", "Spektor", "writ of certiorari", "Armenia", "Joanne Wheatley", "flawed democracy", "Southwest Florida International Airport ( RSW )", "1987", "the temporal lobes", "Missi Hale", "Leslie and Ben", "a Norwegian town circa 1879", "Scarborough near Flamborough Head", "Television demonstrations are held", "22 days", "1937", "KU", "gastrocnemius", "great king", "336", "24th match", "1 atm pressure", "a major earthquake", "Isaiah Amir Mustafa", "late 1930s in southern California", "smoke detector", "New York University", "mascot", "the ball is fed into the gap between the two forward packs", "microscope's stage", "9 February 2018", "31 October 1972", "Jesse McCartney", "4.25 inches ( 108 mm )", "Lady Gaga", "the Director of National Intelligence", "April 1979", "mainland greece", "no license or advanced training beyond just firearm familiarization ( for rentals )", "the Royal Air Force ( RAF )", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene", "a sometimes ambiguous designation of two classes of organic compounds", "7,926 miles", "Alberich", "Katharine Juliet Ross", "Jawbreaker", "about 100", "58", "forearm", "2", "Monday night", "a fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.5, "QA-F1": 0.6216805946246735}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.6, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-845", "mrqa_squad-validation-9399", "mrqa_squad-validation-4636", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-437", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-2131", "mrqa_hotpotqa-validation-4253", "mrqa_newsqa-validation-1789", "mrqa_searchqa-validation-7968", "mrqa_searchqa-validation-10092", "mrqa_newsqa-validation-85"], "SR": 0.5, "CSR": 0.6083984375, "retrieved_ids": ["mrqa_squad-train-52846", "mrqa_squad-train-62624", "mrqa_squad-train-31671", "mrqa_squad-train-3773", "mrqa_squad-train-39563", "mrqa_squad-train-19797", "mrqa_squad-train-57500", "mrqa_squad-train-42593", "mrqa_squad-train-21805", "mrqa_squad-train-50631", "mrqa_squad-train-9196", "mrqa_squad-train-55698", "mrqa_squad-train-47090", "mrqa_squad-train-38551", "mrqa_squad-train-86268", "mrqa_squad-train-50307", "mrqa_squad-validation-3947", "mrqa_squad-validation-9023", "mrqa_squad-validation-3069", "mrqa_squad-validation-7214", "mrqa_naturalquestions-validation-976", "mrqa_squad-validation-2717", "mrqa_squad-validation-2405", "mrqa_searchqa-validation-217", "mrqa_squad-validation-6671", "mrqa_searchqa-validation-2383", "mrqa_squad-validation-2297", "mrqa_naturalquestions-validation-9185", "mrqa_searchqa-validation-2419", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-4364", "mrqa_squad-validation-1714"], "EFR": 0.9375, "Overall": 0.7350390625000001}, {"timecode": 16, "before_eval_results": {"predictions": ["2,000", "Michael Mullett", "high pressure shock waves that are generated during impact events", "since 2001", "double or triple non-French linguistic origins", "German", "a course of study, lesson plan, or a practical skill", "Super Bowl XXXIII", "a protest that blocked heavy traffic passing over the A13, Brenner Autobahn, en route to Italy", "P = PSPACE", "in whole by charging their students tuition fees", "every four years", "Gatsby", "Swallows", "crawdads", "the Rocky Mountains", "parabhu", "The Age of Innocence", "Logan International Airport", "a thirst", "painting", "Canada", "an asylum", "high", "Liza Minnelli", "the names of God", "airplanes", "Kansas", "bay leaf", "John", "the opera", "a parabola", "carbon dioxide", "San Francisco", "a song", "Saturn", "nickel", "a lichen", "Lake Baikal", "Brazil", "a parabola", "Laos", "Chang Apana", "Jeckle", "Erma Bombeck", "Clinton", "Jio's HoF", "John the Baptist", "Iran", "a serve", "Touch of Evil", "Billy Idol", "John Keats", "G.K. Chesterton", "Daryl Sabara", "holiday", "William Shakespeare", "Simon Wicks", "split 7\"", "the area of the reservation", "Turkey", "sumo wrestling", "more than 26,000", "Fat Man"], "metric_results": {"EM": 0.421875, "QA-F1": 0.47347475258659466}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5263157894736842, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4065", "mrqa_squad-validation-1844", "mrqa_squad-validation-4332", "mrqa_squad-validation-1795", "mrqa_squad-validation-6983", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-16782", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-3214", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-9973", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-15522", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-519", "mrqa_naturalquestions-validation-94", "mrqa_triviaqa-validation-234", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3174", "mrqa_newsqa-validation-1120", "mrqa_hotpotqa-validation-5388"], "SR": 0.421875, "CSR": 0.5974264705882353, "EFR": 1.0, "Overall": 0.7453446691176471}, {"timecode": 17, "before_eval_results": {"predictions": ["forceful taking of property", "Robert Iger", "Central business districts", "the variety of occupations necessary to sustain the community as distinct from the indigenous population", "prime number theorem", "a nonphotosynthetic eukaryote engulfed a chloroplast-containing alga but failed to digest it", "San Jose", "Trevathan", "All My Children", "being drafted into the Austro-Hungarian Army", "an innate force of impetus", "a statue", "James Cameron", "Queen Anne", "Brayden Panettiere", "Prince of Denmarke", "a dog eat dog world", "Sir Anthony Eden", "Defending Your Life", "quaerere", "Jalisco state", "a Pueblo", "San Diego Comic-Con", "Muddy Waters", "a Morse Code", "a vote taken does not adopt either the main motion or the substitute motion", "Manfred von Richthofen", "cowboys", "Michael Collins", "John J. Pershing", "a balloon", "La Crosse", "the Jesuit order", "Javier Bardem", "an American Tragedy", "an anatomical animation", "Susan Lahrman", "the University of Massachusetts Amherst", "60", "a bachelor pad", "a coal", "a heart ventricles", "a rabbit", "livestock", "Jack London", "Vladimir Putin", "Hillary Clinton", "Nikola Tesla", "a gingerbread", "Elza", "a transporter", "a light-gathering mirror", "David Tyree", "Quantitative psychological research", "The federal government", "the Great Antilles", "Doncaster Rovers", "the City of Onkaparinga", "a 7th-century Anglo-Saxon tumulus (or \"barrow\")", "Monday night", "Immigration Minister Eric Besson", "needle - like teeth", "3", "relieve families who had difficulty finding jobs during the Great Depression in the United States"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5128141534391534}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.962962962962963]}}, "before_error_ids": ["mrqa_squad-validation-3106", "mrqa_squad-validation-315", "mrqa_squad-validation-5938", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-1664", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-14153", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-7483", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-8777", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-9231", "mrqa_naturalquestions-validation-222", "mrqa_triviaqa-validation-2095", "mrqa_hotpotqa-validation-3804", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-9856"], "SR": 0.40625, "CSR": 0.5868055555555556, "EFR": 1.0, "Overall": 0.7432204861111111}, {"timecode": 18, "before_eval_results": {"predictions": ["308", "Trajan's Column", "the solution", "the Danube", "conduct inquiries and scrutinise legislation", "Ralph Woodward", "major cities", "19th", "high density", "the Ten Commandments", "Richard Kuklinski", "Jefferson Memorial", "Magic Johnson", "between 7,500 and 40,000", "The Soloist", "bronze", "the Anishinaabeg", "feats of exploration", "Abdul Razzak Yaqoob", "Norwegian", "1868", "Arabella Churchill", "Hopeless Records", "Lorne Michaels", "Jacques Dominique Wilkins", "Ang Lee", "near North Chicago", "1971", "Dutch", "Arkansas", "2009", "The God of Small Things", "Sakura Uzumaki", "northeastern", "March 30, 2025", "Miller Brewing", "Michael Lewis Greenwell", "Daimler-Benz", "1994", "John of Gaunt", "classical", "1,521", "capitol building", "one", "role-playing", "Leofric", "The Division of Fawkner", "Argentinian", "the International Hotel", "American black bear", "Mot\u00f6rhead", "Richa Sharma", "An impresario", "Lake Michigan", "Tulsa", "homeless", "Jessica Smith", "Hungary", "45 minutes, five days a week", "Maryland", "OK", "Grant", "The Maracot & Co.", "kidnapping"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6631087662337662}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-525", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4297", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2241", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-4001", "mrqa_newsqa-validation-4077", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-10375"], "SR": 0.578125, "CSR": 0.5863486842105263, "retrieved_ids": ["mrqa_squad-train-24713", "mrqa_squad-train-20663", "mrqa_squad-train-67083", "mrqa_squad-train-18971", "mrqa_squad-train-74393", "mrqa_squad-train-73455", "mrqa_squad-train-84882", "mrqa_squad-train-80783", "mrqa_squad-train-80104", "mrqa_squad-train-50095", "mrqa_squad-train-66429", "mrqa_squad-train-26589", "mrqa_squad-train-16363", "mrqa_squad-train-25239", "mrqa_squad-train-37371", "mrqa_squad-train-2283", "mrqa_naturalquestions-validation-94", "mrqa_newsqa-validation-1789", "mrqa_squad-validation-10142", "mrqa_naturalquestions-validation-1528", "mrqa_triviaqa-validation-3753", "mrqa_naturalquestions-validation-8503", "mrqa_searchqa-validation-2090", "mrqa_triviaqa-validation-865", "mrqa_squad-validation-4692", "mrqa_squad-validation-394", "mrqa_triviaqa-validation-2856", "mrqa_naturalquestions-validation-10509", "mrqa_squad-validation-315", "mrqa_squad-validation-9630", "mrqa_naturalquestions-validation-7009", "mrqa_newsqa-validation-3046"], "EFR": 1.0, "Overall": 0.7431291118421053}, {"timecode": 19, "before_eval_results": {"predictions": ["toward the end of his life", "motivated students", "native tribes", "Cuba", "Dai \u00d6n Yeke Mongghul Ulus", "Parliamentary time", "Private Bill Committees", "photolysis of ozone", "Eliot Ness", "Joseph Stalin", "Bobby Eli", "Eurasian Plate", "Anglican", "10.5 %", "living - donor", "New South Wales", "silk floss", "Tom Waits", "Cherbourg in France and Queenstown ( now Cobh ) in Ireland", "RMS Titanic", "T.J. Miller", "full '' sexual intercourse", "1995", "Paradise, Nevada", "Mike Nesmith", "the country club pool", "Department of Health and Human Services", "Skat", "1878", "Rockwell", "Daryl `` Chill '' Mitchell as Patton P. Plame", "October 2008", "The Parable of the Prodigal Son", "the theory of T\u0101\u1e47\u1e0dava dance ( Shiva )", "Heather Stebbins", "Portuguese version of this surname is Tavares", "supervillains who pose catastrophic challenges to the world", "1999", "Geraldine Margaret Agnew - Somerville", "Jean Anne Worley", "October 2, 2017", "Stephen Curry of Davidson", "December 15, 2016", "a star", "AMX - 30", "the Overlook Hotel", "Marty Robbins", "Albert Einstein", "early 2017", "Ed Sheeran", "state control of investment", "HTTP / 1.1", "A substitute good", "Midsomer Murders", "Honolulu, Hawaii", "Hopeless Records", "death sentence", "Seasons of My Heart", "wife Linda", "Jack London's \"To Build a Fire\"", "Humorous poem about teachers with funny neck", "business", "Vancouver", "Hudson River"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5713694852941177}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.11764705882352941, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8084", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-6696", "mrqa_hotpotqa-validation-4897", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-10347"], "SR": 0.484375, "CSR": 0.58125, "EFR": 1.0, "Overall": 0.742109375}, {"timecode": 20, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2467", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2025", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-3628", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-16723", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6061", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-6969", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_squad-validation-10052", "mrqa_squad-validation-10107", "mrqa_squad-validation-10125", "mrqa_squad-validation-10149", "mrqa_squad-validation-10186", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10333", "mrqa_squad-validation-10341", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10445", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-116", "mrqa_squad-validation-1193", "mrqa_squad-validation-1257", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1684", "mrqa_squad-validation-1754", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2092", "mrqa_squad-validation-2166", "mrqa_squad-validation-2288", "mrqa_squad-validation-2302", "mrqa_squad-validation-232", "mrqa_squad-validation-2322", "mrqa_squad-validation-2324", "mrqa_squad-validation-2344", "mrqa_squad-validation-2406", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2559", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2737", "mrqa_squad-validation-2778", "mrqa_squad-validation-2827", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-32", "mrqa_squad-validation-3217", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3506", "mrqa_squad-validation-3617", "mrqa_squad-validation-362", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3923", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-401", "mrqa_squad-validation-4086", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4248", "mrqa_squad-validation-4287", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4836", "mrqa_squad-validation-4974", "mrqa_squad-validation-5012", "mrqa_squad-validation-5088", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5379", "mrqa_squad-validation-5451", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5950", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6069", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6250", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6605", "mrqa_squad-validation-6671", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6843", "mrqa_squad-validation-6846", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7476", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-789", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-800", "mrqa_squad-validation-805", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8193", "mrqa_squad-validation-8197", "mrqa_squad-validation-8307", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-845", "mrqa_squad-validation-852", "mrqa_squad-validation-8580", "mrqa_squad-validation-8696", "mrqa_squad-validation-8771", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8798", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8935", "mrqa_squad-validation-8953", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9141", "mrqa_squad-validation-9254", "mrqa_squad-validation-9270", "mrqa_squad-validation-929", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9457", "mrqa_squad-validation-9479", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9630", "mrqa_squad-validation-964", "mrqa_squad-validation-9718", "mrqa_squad-validation-9766", "mrqa_squad-validation-9768", "mrqa_squad-validation-985", "mrqa_squad-validation-9968", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1751", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.876953125, "KG": 0.42578125, "before_eval_results": {"predictions": ["Museum of the Moving Image in London", "Jean Fran\u00e7ois de Troy, Jean-Baptiste Pater", "the Ministry of Justice", "During the Second World War", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "an assembly center", "Leonardo da Vinci", "1350", "two", "as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "June 12, 2017", "Prussian statesman", "Gatwick Airport", "27 November 1956", "Dan Conner", "supernatural psychological horror film", "Billy Joel", "The Times Higher Education Guide", "The conversation", "Al D'Amato", "veto power", "Lush Ltd.", "Port Macquarie", "coca wine", "Lonestar", "Brea, California", "World War II", "Brazilian Jiu-Jitsu", "Biola University", "Revolution Studios and Happy Madison Productions", "February 22, 1968", "Erreway", "2013", "Kim Bauer", "the Joint Chiefs of Staff", "The Postal Service", "1955", "1993", "near Philip Billard Municipal Airport", "\"Lawn Dogs\" (1997)", "John Boyd Dunlop", "Iran", "the Chickamauga Wars", "a creek", "Edward Trowbridge Collins Sr.", "Province of New York", "Archie Andrews", "January 28, 2016", "a championship and an NBA Finals Most Valuable Player Award", "Steve Martin", "Israeli Declaration of Independence", "Sleepy Hollow", "Mumbai", "1996", "Greek mythology", "The National Council", "Doubting Castle", "Noida", "genocide, crimes against humanity, and war crimes", "a poem", "Howard Hughes Jr.", "Dairy Queen", "Australian", "the Iberian Peninsula"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7233506944444444}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.2222222222222222, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7674", "mrqa_squad-validation-5489", "mrqa_squad-validation-8189", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5727", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-5418", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-407", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11847", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-12943"], "SR": 0.640625, "CSR": 0.5840773809523809, "EFR": 1.0, "Overall": 0.7199404761904762}, {"timecode": 21, "before_eval_results": {"predictions": ["Several thousand", "Paris", "Islamic Republic", "different subject specialists each session during the week", "Sonia Shankman Orthogenic School", "inverse proportionality of acceleration to mass", "alternating current", "1002", "Guardians of the Galaxy", "1964", "arts manager", "technical director", "UHF channel 44", "their unusual behavior", "UK garage", "Ireland", "Valley Falls", "the U\u00ed \u00cdmair", "Ry\u016bky\u016b", "Dallas", "due to a leg injury", "Hirsch index rating", "neo-Nazi", "Warner Animation Group", "2008", "Michael Crawford", "Isobel", "Ron Swanson", "American", "the Dominican Republic", "29,000", "Wes Unseld", "Mr. Church", "1990", "Ronald Wilson Reagan", "Ray Romano", "\"The Flying Doctors\"", "Kennedy Road", "Israel", "Peter Yarrow", "\"From Here to Eternity\"", "the Kentucky Bats", "\"The Royal Family\"", "Backstreet Boys", "Lynn Minmei", "Sir Matthew Arundell", "Kristin Scott Thomas", "August 14, 1848", "Catwoman", "The Hindu Group", "Detroit Lions and the Los Angeles Rams", "Audi", "1886", "Robin Cousins", "USS Chesapeake", "Jupiter", "Seal", "CNN", "Somali", "\"Do You Want Crying\"", "Joe Jackson", "Michael Arrington", "Krishna Rajaram,", "at least 12 months"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5705729166666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1894", "mrqa_squad-validation-10424", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4843", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-2473", "mrqa_triviaqa-validation-2997", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-6538", "mrqa_searchqa-validation-939", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-274"], "SR": 0.484375, "CSR": 0.5795454545454546, "retrieved_ids": ["mrqa_squad-train-68180", "mrqa_squad-train-71631", "mrqa_squad-train-153", "mrqa_squad-train-80213", "mrqa_squad-train-72376", "mrqa_squad-train-74055", "mrqa_squad-train-28269", "mrqa_squad-train-52178", "mrqa_squad-train-77097", "mrqa_squad-train-3260", "mrqa_squad-train-65674", "mrqa_squad-train-86140", "mrqa_squad-train-79656", "mrqa_squad-train-42717", "mrqa_squad-train-10504", "mrqa_squad-train-20628", "mrqa_searchqa-validation-10375", "mrqa_squad-validation-2166", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-3393", "mrqa_squad-validation-1891", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-4179", "mrqa_searchqa-validation-15522", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-8503", "mrqa_squad-validation-845", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7505", "mrqa_squad-validation-3811"], "EFR": 1.0, "Overall": 0.7190340909090909}, {"timecode": 22, "before_eval_results": {"predictions": ["1,230 kilometres (764 miles)", "anti-colonial movements", "The Middle and Modern Family", "An increase in imported cars", "immunomodulators", "inequality", "Duval County", "Univision", "Pieter van Musschenbroek", "\"Beauty and the Beast\"", "every aspect of public and private life", "Southern Illinois University Carbondale", "Acela Express", "German", "Cartoon Network", "Aamir Khan", "trans-Pacific flight", "Kristina Ceyton and Kristian Moliere", "Ginger Rogers", "Dan Castellaneta", "from 1995 to 2012", "Saint Petersburg Conservatory", "Columbine", "one", "Colonel", "Ars Nova Theater", "Donna Paige Helmintoller", "Detroit, Michigan", "CBS News", "1838", "Assistant Director Neil J. Welch", "near Philip Billard Municipal Airport", "ZZ Top", "2", "The Handmaid's Tale", "Rick and Morty", "second largest", "Bamyan Province", "Melbourne", "1590", "247,597", "jurisdiction", "Yoruba people", "Madonna Louise Ciccone", "Telugu", "1800000 sqft", "Malayalam cinema", "New York State Route 907E", "Man Booker Prize", "child actor", "October 21, 2016", "Taeko Ikeda", "Emma", "An acronym", "Thomas Chisholm", "Brazil", "Superman", "Wales", "since 1983", "three", "threatening messages", "kiwifruit", "a Polaroid", "the Cambodian Navy"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7688988095238095}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9302", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-2446", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-1384", "mrqa_naturalquestions-validation-8493", "mrqa_triviaqa-validation-5792", "mrqa_newsqa-validation-377", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-6136"], "SR": 0.671875, "CSR": 0.5835597826086957, "EFR": 1.0, "Overall": 0.7198369565217392}, {"timecode": 23, "before_eval_results": {"predictions": ["28.5\u00b0E", "the Saracens", "seven", "BBC Dead Ringers", "Mnemiopsis", "Dr. George E. Mueller", "salmon", "chile", "heating", "George Robert Newhart", "constellations", "chile", "Australia", "Crime and Punishment", "The Plaza Hotel", "Hawking", "Rodeo", "Oahu", "The American Heritage Dictionary", "chiselling", "Gerald Sharpe", "M1 Abrams", "Earth", "an egg-in-a-bottle experiment,", "Gerald IV", "Gerald Hooke", "Democrats", "Kirkus Reviews", "College of William & Mary", "Who's Afraid of Virginia Woolf", "chile", "jamais", "Barnard College", "Septimius Severus", "Gerald Goodstein", "chile", "The Delaware state bird", "chile", "George W. Bush", "anemones that can significantly sting.", "Captain Cook", "Bosom Buddies", "Gerald Cantor Foundation", "lindo perdiguero", "Walter Crawford Kelly, Jr.", "Edith Wharton", "Rapa Nui National Park", "Nike+ iPod Sport Kit", "gravity", "an equatorial bulge", "chile", "lapsis lazuli", "Bay of Montevideo", "The U.S. state of Georgia", "in the 1820s", "a palindrome", "QWERTY", "a chine", "Las Vegas Boulevard", "Slaughterhouse-Five", "Food and Agriculture Organization", "participate in Iraq's government.", "an enormous step forward in Dana Gas' strategy across the Middle East, North Africa and South Asia.", "as soon as 2050,"], "metric_results": {"EM": 0.34375, "QA-F1": 0.44843749999999993}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-4758", "mrqa_searchqa-validation-15407", "mrqa_searchqa-validation-12149", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-13231", "mrqa_searchqa-validation-9639", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-14893", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-7080", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-46", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-4374", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-7572", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-15964", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7047", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-3087", "mrqa_newsqa-validation-3008"], "SR": 0.34375, "CSR": 0.5735677083333333, "EFR": 1.0, "Overall": 0.7178385416666666}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 2 million", "Catholic", "The waxy cuticle of many leaves", "theta", "subsequent long-run economic growth", "the Bush administration's controversial system of military trials for some Guant Bay detainees.", "two-state solution", "WFTV", "an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.\"", "tuatara", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "eight", "Haiti", "Lebanese", "Gulf of Aden", "43,000 people have been killed", "18", "Libyan", "a rally at the State House next week because legislators are starting to come out strongly against Sanford.", "Herman Cain", "humans", "Friday", "Malmo", "murder in the beating death of a company boss who fired them.", "Swat Valley", "two years,", "228", "Keating Holland", "serving its fast burgers in the Carrousel du Louvre,", "Australian Open", "Apple made it exciting and simple and effortless and fun.\"", "Nazi Germany", "\"The Ski Train\"", "30 years ago.", "five", "July", "Oprah: A Biography", "100 percent", "African-white", "Abu Sayyaf", "The Palm", "There's no chance of it being open on time", "Egypt", "Jeddah, Saudi Arabia", "Mexicans who are unemployed or underemployed", "Russia", "2011", "use of torture and indefinite detention", "\"significant skeletal remains\" consistent with those of a small child on the outer perimeter of the search area.", "Iran", "a review of state government practices completed in 100 days.\"", "dancing against a stripper's pole.\"", "Michael Schumacher", "The chief executive of West Virginia", "Massachusetts", "khi", "the natural world and mysticism", "The Lion King", "7pm", "1966", "Wendell Erdman Berry", "Twelfth Night", "Shabbat", "Brazilian"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5882339015151514}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.08333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6435", "mrqa_squad-validation-8681", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-466", "mrqa_naturalquestions-validation-4207", "mrqa_triviaqa-validation-802", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-173", "mrqa_searchqa-validation-2076", "mrqa_searchqa-validation-14268"], "SR": 0.484375, "CSR": 0.5700000000000001, "retrieved_ids": ["mrqa_squad-train-13074", "mrqa_squad-train-25422", "mrqa_squad-train-22703", "mrqa_squad-train-1265", "mrqa_squad-train-29404", "mrqa_squad-train-60811", "mrqa_squad-train-49684", "mrqa_squad-train-30613", "mrqa_squad-train-50508", "mrqa_squad-train-56026", "mrqa_squad-train-74647", "mrqa_squad-train-7740", "mrqa_squad-train-36978", "mrqa_squad-train-78807", "mrqa_squad-train-16882", "mrqa_squad-train-10530", "mrqa_searchqa-validation-8777", "mrqa_naturalquestions-validation-10319", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1142", "mrqa_naturalquestions-validation-10073", "mrqa_searchqa-validation-350", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-14141", "mrqa_squad-validation-7792", "mrqa_newsqa-validation-761", "mrqa_naturalquestions-validation-4830", "mrqa_newsqa-validation-415", "mrqa_squad-validation-9023", "mrqa_hotpotqa-validation-1807", "mrqa_newsqa-validation-3799", "mrqa_searchqa-validation-10783"], "EFR": 1.0, "Overall": 0.717125}, {"timecode": 25, "before_eval_results": {"predictions": ["primes", "UNESCO's World Heritage list", "lower incomes", "Wankel", "five", "the Bronx", "northwest Pakistan", "backbreaking labor", "near Grand Ronde, Oregon", "Democrats and Republicans are saying Meehan shouldn't be using a 9/11 image to make a political point.\"", "Fullerton, California", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "The minister later apologized, telling CNN his comments had been taken out of context.", "Carl Froch", "10 below in Chicago, Illlinois.", "\"have no problems about the school, they are happy about everything.\"", "these planning processes are urgently needed", "a rifle and began firing.", "The pilot, whose name has not yet been released,", "children were Sudanese orphans that it was trying to rescue from a war-torn nation.", "The office of Muqtada al-Sadr accused Iraqi and U.S. forces of attacking Sadr City on Friday,", "air support", "Marie-Therese Walter", "three French journalists, a seven-member Spanish flight crew and one Belgian were also arrested.", "free enterprise in history", "recovery from last spring's tornado,", "Swansea", "U.S. District Judge Ricardo Urbina", "Michael Jackson", "Kurdistan Freedom Falcons,", "insurgent small arms fire", "quality of teaching and learning in American schools", "a lump in Henry's nether regions", "Florida", "glamour and hedonism", "smile gently at the", "a peace sign", "the Southeast", "Diego Milito", "a grizzly bear", "The Human Rights Watch organization", "Florida", "his son, Isaac, and daughter, Rebecca.", "rebels", "three Ghanaians, two Liberians and a Togo national were shot dead at a shop where local residents often brought clothes for minor adjustments.", "\"How I Met Your Mother,\"", "the British capital's other two airports,", "Mafia", "hackers", "The BBC", "CBS, CNN, Fox and The Associated Press.", "Ashley \"A.J.\" Jewell", "thick skin", "Gettysburg College", "the English", "Celsius", "75", "supreme religious leader", "Charles Quinton Murphy", "Rio Gavin Ferdinand", "more than 110", "Athens", "Craig", "artesian"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5020537790436688}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.8571428571428571, 0.0, 1.0, 0.4, 0.2666666666666667, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.28571428571428575, 1.0, 0.3333333333333333, 0.8, 0.15384615384615385, 0.0, 1.0, 0.375, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-561", "mrqa_naturalquestions-validation-3670", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-1390", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-4004", "mrqa_searchqa-validation-1916", "mrqa_searchqa-validation-16961"], "SR": 0.390625, "CSR": 0.5631009615384616, "EFR": 1.0, "Overall": 0.7157451923076923}, {"timecode": 26, "before_eval_results": {"predictions": ["enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible.", "Katy\u0144 Museum", "two", "legitimate medical purpose by a licensed practitioner", "Martin O'Neill", "1822", "Stephanie Plum", "Dominican", "Evgeni Arkadievich Platov", "Knoxville, Tennessee", "Enkare Nairobi", "Field Marshal Stapleton Cotton,", "Shari Shattuck", "Si Da Ming Bu", "The Blue Album", "Apsley George Benet Cherry-Garrard", "a basilica", "steamy pictorials of celebrities.", "Odawa", "Columbus Crew SC", "Yoo Seung-ho", "1989 until 1994", "Phil Spector", "Wildhorn, Bricusse and Cuden.", "pornographicstar", "Montreal", "Domingo", "John Nicholas Galleher", "San Francisco 49ers", "Prince of Cambodia Norodom Sihanouk", "Cameron Indoor Stadium", "Double Crossed", "19th", "Donald McNichol Sutherland", "a Computer reservations system", "Security Management", "14,372", "musician", "Cersei", "Fort Worth", "Dutch", "shorthand writing", "North Atlantic Treaty Organisation (NATO)", "\"The Comic Strip Presents...\"", "Tabasco", "Sunday, November 2, 2003", "1812", "1.5 million", "Neymar", "Plymouth Regional High School", "Tainted Love", "Dissection", "Cecil Lockhart", "MercyMe", "Five years later", "a Cymbal", "Al Jazeera", "Anabaptists and the non-sectarians", "38 feet", "identity documents belonging to Miguel Mejia Munera", "Shenzhen", "Brigham Young", "Jeopardy", "GNMA"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6764880952380953}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6426", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-2131", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-5740", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-3954", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-8085", "mrqa_searchqa-validation-10970"], "SR": 0.5625, "CSR": 0.5630787037037037, "EFR": 1.0, "Overall": 0.7157407407407408}, {"timecode": 27, "before_eval_results": {"predictions": ["In low-light conditions", "1550", "41 sentences drawn from his writings, including the 95 Theses, within 60 days", "$60,000 in cash and stock", "four", "Steve Goodman", "Jonathan Goldstein", "Jean Hill", "Kirstjen Nielsen", "1997", "Tristan Rogers", "Mason Alan Dinehart III", "counter clockwise direction", "Paul Hogan", "tennis", "John Barry", "they are easily confused with III and VIII", "Fa Ze Rug", "8ft", "Robert Irsay", "to manage the characteristics of the beer's head", "a moral tale", "2019", "Scott Drever", "Arctic Ocean", "1988", "The albatross is then literally hung around the mariner's neck by the crew to symbolize his guilt in killing the bird", "1", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "December 1, 2017", "James Watson and Francis Crick", "Sara Gilbert", "London", "the courts", "John Travolta", "Djokovic", "Rigg", "Ant & Dec", "Nepal", "Joanne Wheatley", "UNESCO / ILO Recommendation concerning the Status of Teachers", "September 24, 2012", "89.2 \u00b0 C", "winter", "between the Eastern Ghats and the Bay of Bengal", "Daniel Suarez", "1 - 2 spinal nerve segments above the point of entry", "16 August 1975", "useless, time - wasting", "Kyla Pratt", "Lori McKenna", "Beorn", "UN Supreme Commander Gen. Douglas MacArthur", "Daily Mail and General Trust", "Scotland", "Forbes", "2006", "Fife", "April 28,", "Russian Foreign Minister Sergey Lavrov and Secretary of State Hillary Clinton", "can kill us.", "first unassisted triple play", "Ben Kingsley", "Conrad Hilton"], "metric_results": {"EM": 0.5, "QA-F1": 0.639922482031857}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 0.8333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.8, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 0.0, 0.125, 1.0, 0.07999999999999999, 1.0, 0.09090909090909093, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.9, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.4, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2246", "mrqa_squad-validation-1313", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7208", "mrqa_triviaqa-validation-3649", "mrqa_triviaqa-validation-6967", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-506", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-2350", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-11955"], "SR": 0.5, "CSR": 0.5608258928571428, "retrieved_ids": ["mrqa_squad-train-15501", "mrqa_squad-train-12815", "mrqa_squad-train-28560", "mrqa_squad-train-46108", "mrqa_squad-train-45957", "mrqa_squad-train-7571", "mrqa_squad-train-17651", "mrqa_squad-train-78834", "mrqa_squad-train-5682", "mrqa_squad-train-40796", "mrqa_squad-train-74480", "mrqa_squad-train-79816", "mrqa_squad-train-78012", "mrqa_squad-train-26955", "mrqa_squad-train-74543", "mrqa_squad-train-36559", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-950", "mrqa_searchqa-validation-12149", "mrqa_hotpotqa-validation-5336", "mrqa_naturalquestions-validation-6484", "mrqa_squad-validation-9768", "mrqa_hotpotqa-validation-5456", "mrqa_newsqa-validation-1303", "mrqa_searchqa-validation-11488", "mrqa_newsqa-validation-1966", "mrqa_searchqa-validation-217", "mrqa_triviaqa-validation-2095", "mrqa_naturalquestions-validation-6197", "mrqa_hotpotqa-validation-5098", "mrqa_searchqa-validation-596", "mrqa_squad-validation-4287"], "EFR": 0.9375, "Overall": 0.7027901785714286}, {"timecode": 28, "before_eval_results": {"predictions": ["Elisabeth Sladen", "1206", "Genghis Khan", "Shenzhen in southern China.", "24", "Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "a delegation of American Muslim and Christian leaders", "September 21.", "1,500", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "269,000", "Eden Park", "The Ski Train", "a sailboat matching the description of the missing 38-foot boat was found overturned about 5:15 p.m. Saturday,", "photos from the scene show the man facing up, with his arms out to the side. He is wearing socks but no shoes.", "to stop rocket fire on its southern cities and towns.", "dogs who walk on ice in Alaska.", "his father's", "general Alejandro Ordonez Maldonado", "Haitians", "President Obama", "Chevron oil station", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "it is currently home to 15 African and Asian elephants.", "to secure more funds", "short- and medium-range missile tests", "Karl Eikenberry", "defaulted on the mortgage and the house fell into foreclosure.", "A member of the group dubbed the \"Jena 6\"", "state", "(Anil) Kapoor", "10 below in Chicago, Illlinois.", "by raising its alert level,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Nazi Germany", "government-funded treatment comes in the form of 20 pills.", "gun conviction", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "A planned missile defense system in Eastern Europe poses no threat to Russia,", "David McKenzie", "citizenship", "heavy brush", "through a facility in Salt Lake City, Utah,", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis,", "can have on a patient's quality of life,\"", "for retirement long before his career neared its end.\"", "Alwin Landry's supply vessel Damon Bankston", "along the equator", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "a hot tub alongside a man's lifeless, naked body", "in a tenement in the Mumbai suburb of Chembur,", "California, Texas and Florida,", "Rigg", "the efferent nerves that directly innervate muscles", "the port of Nueva Espa\u00f1a", "The Duchess of Devonshire", "Bahrain", "Tokyo Metropolitan Assembly,", "Danny Lebern Glover", "William Shakespeare", "British", "an assault rifle", "ro aaron", "Turtle Wax"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4856420744288391}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.09523809523809523, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.42857142857142855, 0.0, 0.6666666666666666, 0.25, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.4615384615384615, 1.0, 0.0, 0.6666666666666666, 0.0909090909090909, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.9333333333333333, 0.0, 0.0, 0.5, 0.3636363636363636, 0.0, 0.7272727272727273, 0.923076923076923, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-2338", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-7484", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-1993", "mrqa_hotpotqa-validation-1922", "mrqa_searchqa-validation-9212", "mrqa_searchqa-validation-6288"], "SR": 0.3125, "CSR": 0.5522629310344828, "EFR": 1.0, "Overall": 0.7135775862068965}, {"timecode": 29, "before_eval_results": {"predictions": ["Disney\u2013ABC Domestic Television", "2011", "Soviet", "naltrexone", "sense of smell", "Brigit Forsyth", "Florence", "Wrigley's Spearmint", "Oprah Winfrey", "2004", "a Great Dane", "Director General of the Security Service", "Caracas", "Rock Follies", "Do I Hear a Waltz", "Nobel Prize in Literature", "Celtic", "Northwestern University", "the best value diamond for your money", "The Star Spangled Banner", "a concert", "Ibrox Stadium", "New York", "(born 17 September 1929)", "Micael Caine", "Llyn Padarn", "Little Dorrit", "Tacitus", "apples", "Chekhov", "Chris Evans", "John Keats", "Declaration of Independence", "lome", "a condor", "Belgium", "Pilgrim's Progress", "Plato", "Fulham Football Club", "graphite", "Australia", "Alaska", "God", "the Andaluc\u00eda region", "the Michelin Man", "Tesco", "Deep Throat", "a bear suit", "Clio Awards", "Pygmalion", "Watford Football Club", "Trainspotting", "English", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Vicente Fox", "the medieval city of Halych", "1919", "August 24, 1983", "ultimately ensuring that all prescription drugs on the market are FDA approved,", "International Polo Club Palm Beach in Florida.", "63 people and wounded more than 200.", "Sappho", "the White House", "Steely Dan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6828199404761905}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.4, 1.0, 0.4, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.4, 0.9600000000000001, 1.0, 1.0, 1.0, 0.5, 0.07142857142857142, 0.0, 0.6, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7651", "mrqa_triviaqa-validation-2203", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-4186", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-566", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-855", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-4547", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1612"], "SR": 0.578125, "CSR": 0.553125, "EFR": 0.9259259259259259, "Overall": 0.6989351851851853}, {"timecode": 30, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-3249", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3655", "mrqa_hotpotqa-validation-3701", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-505", "mrqa_hotpotqa-validation-5213", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5645", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-988", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4308", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-919", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-14268", "mrqa_searchqa-validation-14735", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-1664", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_squad-validation-10052", "mrqa_squad-validation-10125", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10308", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-1159", "mrqa_squad-validation-1193", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-1368", "mrqa_squad-validation-1503", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2166", "mrqa_squad-validation-2324", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2778", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-3259", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3831", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3916", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-4065", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4191", "mrqa_squad-validation-4248", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4746", "mrqa_squad-validation-4836", "mrqa_squad-validation-5009", "mrqa_squad-validation-5088", "mrqa_squad-validation-5108", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5180", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5521", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5964", "mrqa_squad-validation-6001", "mrqa_squad-validation-6069", "mrqa_squad-validation-6082", "mrqa_squad-validation-6158", "mrqa_squad-validation-6256", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6592", "mrqa_squad-validation-6605", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-709", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7476", "mrqa_squad-validation-7485", "mrqa_squad-validation-7502", "mrqa_squad-validation-7578", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-8159", "mrqa_squad-validation-8213", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8580", "mrqa_squad-validation-8681", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8935", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9141", "mrqa_squad-validation-9270", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9510", "mrqa_squad-validation-9569", "mrqa_squad-validation-964", "mrqa_squad-validation-9759", "mrqa_squad-validation-9766", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1329", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-576", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-996"], "OKR": 0.86328125, "KG": 0.42734375, "before_eval_results": {"predictions": ["1947", "Madame de Pompadour", "$125 per month", "helen shapi", "Newbury", "Ruth Elizabeth", "bores", "Genesis", "Lisieux", "Astor family", "pennsylvania", "canned Heat", "John Huston", "c.I.D.", "hong kong", "cabot", "the Advisory Council of Science and Industry", "Patrick Kielty", "Norfolk Island", "d'Ivoire", "Mexico", "Worcester Cathedral", "duchess", "Albert Finney", "Carrefour", "d. h. Lawrence", "in the garden of Gethsemane", "(John) Galliano", "swallow sidecar", "wrigman", "mickey Mouse", "sun protection factor", "Plato", "Bugsy Malone", "1812", "michael", "basil", "cancun", "cfs", "lyoness e", "raclette", "AllStars", "hong kong", "Copenhagen", "nihon", "japan", "George Walker Bush", "jockey", "michael Mitchell", "Zachary Taylor", "old constitution", "furrow", "Total Drama World Tour", "General George Washington", "Atl\u00e9tico Madrid", "two", "Balvenie Castle", "January 18, 1977", "threatening messages", "London", "taping of \"The Real Housewives of Atlanta\" reunion special,", "a lemur", "Communist Party", "misfortune"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5020833333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 0.9333333333333333, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5491", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2299", "mrqa_triviaqa-validation-5311", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-6074", "mrqa_triviaqa-validation-3052", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4169", "mrqa_triviaqa-validation-1108", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3285", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-1468", "mrqa_triviaqa-validation-3517", "mrqa_naturalquestions-validation-5597", "mrqa_hotpotqa-validation-1351", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-85", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-3674"], "SR": 0.421875, "CSR": 0.548891129032258, "retrieved_ids": ["mrqa_squad-train-28519", "mrqa_squad-train-13704", "mrqa_squad-train-41753", "mrqa_squad-train-70789", "mrqa_squad-train-27872", "mrqa_squad-train-79706", "mrqa_squad-train-28143", "mrqa_squad-train-3924", "mrqa_squad-train-43264", "mrqa_squad-train-62361", "mrqa_squad-train-43993", "mrqa_squad-train-6416", "mrqa_squad-train-27361", "mrqa_squad-train-78659", "mrqa_squad-train-18609", "mrqa_squad-train-46359", "mrqa_squad-validation-9452", "mrqa_newsqa-validation-3404", "mrqa_searchqa-validation-7572", "mrqa_triviaqa-validation-667", "mrqa_newsqa-validation-720", "mrqa_hotpotqa-validation-3507", "mrqa_newsqa-validation-2636", "mrqa_searchqa-validation-14153", "mrqa_squad-validation-80", "mrqa_squad-validation-8229", "mrqa_searchqa-validation-15407", "mrqa_newsqa-validation-1789", "mrqa_naturalquestions-validation-5460", "mrqa_searchqa-validation-12440", "mrqa_squad-validation-6128", "mrqa_squad-validation-9479"], "EFR": 0.972972972972973, "Overall": 0.7101540704010462}, {"timecode": 31, "before_eval_results": {"predictions": ["nine", "Lower Rhine", "Pet Shop Boys", "rugby", "homogenocene", "Budapest", "leona Makarova", "a doodle", "Ned Sherrin", "I'm Sorry, I'll Read That Again", "Jean-Paul Gaultier", "javelin throw", "silvery blue", "heavy birds", "French", "Vatican City", "buttock", "Marc Brunel", "Turandot", "leona b'Stard", "daniel shannon", "dora maar", "apples", "440 hertz", "Spanish", "York Minster", "a karst cave", "Brazil", "Ukraine", "finland", "gethsemia is usually the result of an underlying cause of hyposmia is often sufficient to treat the sense of taste", "chile", "a joabberwocky", "worcester", "finland", "senior", "nick henderson", "little Rich Girl", "Paris", "ailing fish", "Venado Tuerto, Argentina", "Charlie Chaplin", "Perfect Storm", "middies", "pottery", "ryan mithen", "Newbury", "Abraham", "vice-admiral", "1936", "leona", "glenn letton", "Rachel Kelly Tucker", "the BBC", "pilgrimages to Jerusalem", "co-op of grape growers", "tim Allen", "Anatoly Lunacharsky", "an empty water bottle", "Japanese officials", "Rev. Alberto Cutie", "Eurasian Economic Union", "glarlon Brando", "al Saud"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4630208333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4765", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-1633", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-5259", "mrqa_triviaqa-validation-7645", "mrqa_triviaqa-validation-5549", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-6730", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5728", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-12801", "mrqa_searchqa-validation-2441"], "SR": 0.359375, "CSR": 0.54296875, "EFR": 1.0, "Overall": 0.714375}, {"timecode": 32, "before_eval_results": {"predictions": ["teachers are now selling their lesson plans to other teachers through the web in order to earn supplemental income, most notably on TeacherspayTeachers.com.", "provisional elder/deacon", "yarn", "Prince Harry", "Mujib", "b\u00e9la Bart\u00f3k", "Denver", "sailor", "Salt Lake City", "four feet", "secretary", "Brazil", "Canada", "Peter Nichols", "American Family Publishers", "seven", "Microsoft", "Brigit Forsyth", "Celsius", "curvature", "canterbury", "leicestershire", "Boris Johnson", "HMS Conqueror", "Tamar", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "260 yards", "the inner ear", "Gloucestershire", "sweden", "gymnastics", "ginger Rogers", "Ishmael", "bluebells", "sweden", "Prokofiev", "cyclone", "Dan Brown", "horses", "Newcastle United", "welcome", "canterbury", "second-year", "William Neil Connor", "riyadh", "charles chaplin", "horse-racing", "can only find it as a Germanic root,", "spain", "dragon", "peregrines", "1937", "Anirudh Sinha", "Tim Rice", "five", "The Walking Dead", "1979", "1999", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "Apple employees", "mental health treatment", "marston Moor", "how timing shapes and supports brain function", "Athol Fugard"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6411824136008919}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [0.08695652173913045, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2238", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-2608", "mrqa_triviaqa-validation-1113", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-7671", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-4924", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1889", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-6553"], "SR": 0.578125, "CSR": 0.5440340909090908, "EFR": 1.0, "Overall": 0.7145880681818182}, {"timecode": 33, "before_eval_results": {"predictions": ["1,100", "since 2001", "horseshoe", "(Barry) Briggs", "table tennis", "henry", "bart\u00f3k", "14-time major champion", "Harold Shipman", "The Undertones", "michael hordern", "henry", "yeast", "(born Feb. 16, 1954, Half Way Tree, Kingston, Jam.),", "permian", "thank you", "nipples", "Queen Mary", "lolita", "muscle tissue", "surrealism", "shinto", "sewing machines", "Morgan Spurlock", "john Buchan", "algae", "(Steve) McCormack", "governor", "stenographer", "Altamont Speedway", "15", "jack Sprat", "(Zainab) Hashmi", "allStars", "john Brooke", "praseodymium", "pickled", "ananister", "an antipasto, sometimes herbs, and may at times be topped with onion, cheese and meat", "president George W. Bush's Council of Economic Advisers", "Wicked Witch", "henry", "The Times", "(Isa) Newton", "(Henry) Jermyn", "bullfighting", "(Arthur) c. Clarke", "tom shaperson", "entropy", "Chad", "henry johnson", "Taggart", "on the microscope's stage", "Ole Einar Bj\u00f8rndalen", "Norway", "Virgin", "Philadelphia Naval Shipyard", "West African descendants", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "cross-country skiers", "\"a smoking gun of confirmation of Brazil's effort to engage in operations to overthrow the government of Chile and a discussion of collusion with the United States.\"", "a ladder", "Hill Street Blues", "white"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5805273399845768}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5384615384615384, 1.0, 0.2105263157894737, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-7142", "mrqa_triviaqa-validation-368", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-7043", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-5760", "mrqa_triviaqa-validation-3060", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-2220", "mrqa_naturalquestions-validation-182", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-11071"], "SR": 0.53125, "CSR": 0.5436580882352942, "retrieved_ids": ["mrqa_squad-train-9442", "mrqa_squad-train-36066", "mrqa_squad-train-32204", "mrqa_squad-train-11729", "mrqa_squad-train-5490", "mrqa_squad-train-27382", "mrqa_squad-train-55586", "mrqa_squad-train-703", "mrqa_squad-train-69723", "mrqa_squad-train-76067", "mrqa_squad-train-58062", "mrqa_squad-train-47881", "mrqa_squad-train-78992", "mrqa_squad-train-70277", "mrqa_squad-train-35905", "mrqa_squad-train-12074", "mrqa_searchqa-validation-13257", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-1639", "mrqa_searchqa-validation-3677", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-879", "mrqa_hotpotqa-validation-2131", "mrqa_searchqa-validation-662", "mrqa_squad-validation-2564", "mrqa_squad-validation-8206", "mrqa_triviaqa-validation-6429", "mrqa_newsqa-validation-421", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-5890"], "EFR": 0.9, "Overall": 0.6945128676470589}, {"timecode": 34, "before_eval_results": {"predictions": ["second Gleichschaltung", "two of Tesla's uncles", "Renault", "Kinks", "Massachusetts", "lyoness e", "six", "Bono", "an \"ink sac\"", "Toy Story", "sparrow", "Book of Revelation", "sheep", "Independence Day", "Charlie Brooker", "Oxford", "the number thirteen", "Glasgow", "Stephen Hawking", "Florence", "Wat Tyler", "Tony Meo", "Black Wednesday", "vomiting", "Ennio Morricone", "NBA", "blyton", "Benjamin Franklin", "the rhizome", "chile", "1066", "Carl Philipp Emanuel", "silver certificate One Dollar bill", "Glasgow", "Dian Fossey", "Port", "checkers", "Norman Mailer", "a soldier", "jura", "edward carmen cansanza", "chatsworth", "argos", "quant pole", "Scooby-Doo", "Pennine Way", "thomas carmen cansterman", "lorne Greene", "thomas carmen cansterbury", "Frans Hals", "charles", "margarita carmen cansino", "University of Oxford", "Matt Monro", "William Chatterton Dix", "Belarus", "\"Big Fucking German\"", "Welterweight", "Harrison Ford", "EU naval force", "Miami Beach, Florida,", "an ice age", "the 39 Steps", "Tel Megiddo"], "metric_results": {"EM": 0.625, "QA-F1": 0.671109068627451}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-661", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-1891", "mrqa_searchqa-validation-2191"], "SR": 0.625, "CSR": 0.5459821428571429, "EFR": 0.9166666666666666, "Overall": 0.6983110119047619}, {"timecode": 35, "before_eval_results": {"predictions": ["Tracy Wolfson and Evan Washburn", "2013", "Russ Conway", "green", "boston", "in York", "Austria", "johnson kiam", "jerry boyd", "jons jons arthur", "cuthbert", "legion", "beans", "Annie leibovitz", "Pinot Noir", "ric Pipino", "london", "king jerry vii", "Dick Whittington", "hen", "Pisces", "Ishmael", "salamander", "Brad Pitt", "Eleanor Rigby", "The Simpsons", "One Direction", "yellows", "Cornell University", "kray", "vinegar Joe", "Portugal", "Follicle-stimulating hormone", "paramita", "j johnson", "london", "Thursday", "london", "barbara taco cat", "Costa Concordia", "the Dominican Republic", "Gary Gibbon", "london", "true or false", "horseshoes", "George III", "wales", "the 'Flower of Scotland'", "oranges", "jays", "yasser Arafat", "the Black Sea", "Gary Grimes", "49 cents", "1", "Juan Francisco Antonio Hilari\u00f3n Zea D\u00edaz", "Wings of Desire", "actor and former fashion model", "Airbus A330-200", "The account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Roland Garros", "Cyrano de Bergerac", "global village"], "metric_results": {"EM": 0.40625, "QA-F1": 0.46510416666666665}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-584", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-168", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-6664", "mrqa_triviaqa-validation-3593", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-2706", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-5128", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-276", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-11091"], "SR": 0.40625, "CSR": 0.5421006944444444, "EFR": 1.0, "Overall": 0.714201388888889}, {"timecode": 36, "before_eval_results": {"predictions": ["independent schools", "cymbal", "Djibouti and Yemen", "the Nile", "Barcelona", "Salma Hayek", "porthmadog", "a peasant's wife", "mountain view", "Delaware", "rodents", "crow", "Flintstones", "the Great Chicago Fire", "gelatine", "Ecuador", "spain", "a charity-boy", "homelessness", "Dublin", "Sin City", "Karen Green-Geller", "doesn't include additional costs such as insurance or business rates", "Google", "joan noakes", "Pembrokeshire Coast National Park", "Tripoli", "jimmy Burns", "dreamgirls", "Opus Dei", "Belize", "law", "Monkey Business", "Liberator", "Zephyr", "a goat", "Dubai", "Sydney", "orange citrus liqueur", "Lehman Bros International", "their discord", "Ordovician", "davy", "Dombey and Son", "mexico", "pascal", "John Galsworthy", "Boris Becker", "Julius", "Amsterdam", "michael descart", "24", "Ford", "21 December 2017", "94", "Kang and Kodos", "La Liga", "environmental", "five minutes before commandos descended from ropes that dangled from helicopters,", "Columbia, Illinois,", "cheese", "Jezebel", "goodson", "Volendam"], "metric_results": {"EM": 0.546875, "QA-F1": 0.580078125}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.625, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-275", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-6619", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2281", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-4868", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-7594", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-562", "mrqa_naturalquestions-validation-5647", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-621", "mrqa_searchqa-validation-12212"], "SR": 0.546875, "CSR": 0.5422297297297297, "retrieved_ids": ["mrqa_squad-train-55607", "mrqa_squad-train-16605", "mrqa_squad-train-11980", "mrqa_squad-train-40854", "mrqa_squad-train-73402", "mrqa_squad-train-75299", "mrqa_squad-train-13412", "mrqa_squad-train-31254", "mrqa_squad-train-69158", "mrqa_squad-train-18041", "mrqa_squad-train-46870", "mrqa_squad-train-8409", "mrqa_squad-train-42214", "mrqa_squad-train-22017", "mrqa_squad-train-5961", "mrqa_squad-train-21143", "mrqa_searchqa-validation-4443", "mrqa_triviaqa-validation-226", "mrqa_searchqa-validation-217", "mrqa_squad-validation-6670", "mrqa_triviaqa-validation-7669", "mrqa_naturalquestions-validation-3", "mrqa_triviaqa-validation-1", "mrqa_newsqa-validation-4179", "mrqa_naturalquestions-validation-4751", "mrqa_newsqa-validation-2658", "mrqa_triviaqa-validation-5792", "mrqa_naturalquestions-validation-9560", "mrqa_triviaqa-validation-5418", "mrqa_naturalquestions-validation-7261", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-3563"], "EFR": 1.0, "Overall": 0.714227195945946}, {"timecode": 37, "before_eval_results": {"predictions": ["uncivilized", "robert de Niro", "7", "m\u00e1laga", "hugh Hefner", "Tiananmen Square", "noises Off", "david Nixon", "Till Death Us Do Part", "Javier Bardem", "1720", "Australia", "Period cramps", "The Hague", "red", "circular line", "mary Alexandrovna", "mary ride", "g\u00e9rard Depardieu", "johnson", "lily", "herpes zoster", "small faces", "zoom", "Angela dothea Kasner", "mary cuthbert", "long-term exposure", "henry henry johnson asquith", "siltshire", "two 1/2 Balls", "Aslan", "zipporah", "Vancouver Island", "ken purdy", "The Blues Brothers", "six", "sash", "mary Trepanier", "Stockholm", "st Helens", "alpha", "robert robert", "Doctor at Large", "blue", "robert west", "Boreas", "The Rumble in the Jungle", "stringed instruments", "Lady Gaga", "chardonnay", "robert robert", "retina", "2026", "Don McMillan", "50th anniversary of the founding of the National Basketball Association", "The Cherokee Nation", "Roman Republic", "the 1800s and the era of Mark Twain,", "mary lohan and the band Jonas Brothers", "the man facing up, with his arms out to the side.", "raccoon", "Santo Versace", "trisha Yearwood", "Hal David and Burt Bacharach"], "metric_results": {"EM": 0.421875, "QA-F1": 0.49574032738095236}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.375, 1.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6964", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4963", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-5616", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-4129", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-856", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-3899", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-5797", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-6638", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-1958", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-12111", "mrqa_searchqa-validation-15107", "mrqa_naturalquestions-validation-6125"], "SR": 0.421875, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.71359375}, {"timecode": 38, "before_eval_results": {"predictions": ["71", "(Cecil) Rhodes", "constant", "religion", "a triangle", "longlegs", "constantine carter", "Anne", "root beer", "honey Nut Cheerios", "Venus", "Berry", "jimmy carter", "secretary of state", "density", "Berlusconi", "ocean's Eleven", "Barack Obama", "Stockholm", "little toaster", "jimmy jubneh", "macau", "light", "jaffa", "Dan Marino", "Munich", "viola", "duchy of Luxembourg", "butterflies", "japan", "jimmy pole", "jimmy carterle", "deuteronomy", "jimmy carter", "jubsy Spanier", "yellow", "Crimean War", "an obelisk", "jimmy carter", "concave", "Caspian Sea", "a barbie doll", "Scooby-Doo", "dolphincry", "hypnotist", "South Dakota", "jimmy sennett", "stems", "hautboy", "orson Welles", "jimmy roxy Rothafel", "subduction zone", "Erica Rivera", "jimmy jublin", "wuthering Heights", "Craggy Island", "greece", "argentine, Idaho", "Belgian", "Esteban Ocon", "Roberto Micheletti", "greece", "19", "Oahu"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4921875}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-3544", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-15961", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-13762", "mrqa_searchqa-validation-16868", "mrqa_searchqa-validation-8389", "mrqa_searchqa-validation-7647", "mrqa_searchqa-validation-10062", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-844", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-11967", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-5544", "mrqa_searchqa-validation-9662", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-11364", "mrqa_naturalquestions-validation-6285", "mrqa_hotpotqa-validation-4023", "mrqa_newsqa-validation-267", "mrqa_hotpotqa-validation-4625"], "SR": 0.4375, "CSR": 0.5364583333333333, "EFR": 0.9722222222222222, "Overall": 0.7075173611111111}, {"timecode": 39, "before_eval_results": {"predictions": ["inequality in wealth and income", "Princess Muna al-Hussein", "House of Fraser", "Westchester County", "City Mazda Stadium", "841", "American", "Big Kenny", "Kaep", "The WB supernatural drama series \"Charmed\"", "Southern Rhodesia", "Ricardo \u201cEl Finito\u201d L\u00f3pez", "made into a TV series for the BBC channel CBeebies", "Voni Morrison", "Galleria Vittorio Emanuele II", "October Sky", "An all-female a cappella singing group", "neuro-orthopaedic", "London", "British", "Albert", "6,241", "Dan Bilzerian", "the Space Shuttle \"Discovery\"", "an American business magnate, investor, and philanthropist", "Crackle", "Kristy Lee Cook", "Perth", "Love Streams", "1935", "5.3 million", "New York City", "Dara Grace Torres", "The dyers of Lincoln", "an American actor", "a fictional world which serves as a setting for a role-playing game or wargame campaign", "May 5, 2015", "Red and Assiniboine Rivers", "Ephedrine", "Neymar", "The Jefferson Memorial", "Strange Interlude", "Bothtec", "2,099", "a body of water", "35,672", "2004 Paris Motor Show", "1996", "33", "1999", "Guns N' Roses", "5", "John McConnell", "George Harrison", "Bruno Mars", "Wyre", "Wikis", "Heshmat Tehran Attarzadeh", "a one-shot victory in the Bob Hope Classic", "700", "The Untouchables", "Monopoly", "a cookie jar", "June 2002"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6551091269841269}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.28571428571428575, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.888888888888889, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-3495", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2707", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-5074", "mrqa_triviaqa-validation-1097", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1230"], "SR": 0.546875, "CSR": 0.53671875, "retrieved_ids": ["mrqa_squad-train-9071", "mrqa_squad-train-69357", "mrqa_squad-train-71247", "mrqa_squad-train-56194", "mrqa_squad-train-67724", "mrqa_squad-train-53483", "mrqa_squad-train-3619", "mrqa_squad-train-45778", "mrqa_squad-train-69867", "mrqa_squad-train-66348", "mrqa_squad-train-76301", "mrqa_squad-train-31466", "mrqa_squad-train-86439", "mrqa_squad-train-46281", "mrqa_squad-train-33327", "mrqa_squad-train-63969", "mrqa_triviaqa-validation-802", "mrqa_newsqa-validation-1483", "mrqa_triviaqa-validation-6300", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-1077", "mrqa_triviaqa-validation-2091", "mrqa_hotpotqa-validation-2465", "mrqa_naturalquestions-validation-4192", "mrqa_hotpotqa-validation-1204", "mrqa_squad-validation-6426", "mrqa_newsqa-validation-2636", "mrqa_naturalquestions-validation-410", "mrqa_searchqa-validation-3262", "mrqa_hotpotqa-validation-4625", "mrqa_squad-validation-597", "mrqa_naturalquestions-validation-950"], "EFR": 1.0, "Overall": 0.713125}, {"timecode": 40, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1616", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-612", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12704", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-2178", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7865", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9870", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10149", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1844", "mrqa_squad-validation-1967", "mrqa_squad-validation-2049", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3428", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3815", "mrqa_squad-validation-3836", "mrqa_squad-validation-3837", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4135", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-503", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5338", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-5859", "mrqa_squad-validation-5893", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6435", "mrqa_squad-validation-6506", "mrqa_squad-validation-6671", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7002", "mrqa_squad-validation-7193", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7704", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8084", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8935", "mrqa_squad-validation-902", "mrqa_squad-validation-9254", "mrqa_squad-validation-9300", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9479", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3640", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3874", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4527", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-6762", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7400", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-996"], "OKR": 0.8515625, "KG": 0.48046875, "before_eval_results": {"predictions": ["2006", "Ghana's Asamoah Gyan", "Cliftonville", "Ribhu Dasgupta", "George Gordon Byron, 6th Baron Byron,", "BAFTA and Screen Actors Guild Awards", "Irish", "sulfur mustard H or HD blister gas", "12\u201318", "Mike Pence", "Jeffrey William Van Gundy", "People!", "Ballarat Bitter", "Bank of China Building", "1919", "26,000", "Comeng and Clyde Engineering", "Gillian Leigh Anderson", "Steven John Carell", "The Second City", "Lauren Alaina", "the National Football League (NFL)", "1943", "Fountains of Wayne", "Minnesota", "organist Cristoforo Benvenuti", "WVNH", "October 20, 2017", "Zimbabwe", "Larry Eustachy", "Douglas Jackson", "Friedrich Nietzsche", "Rabat", "Straits of Gibraltar", "American burlesque", "Channel 4", "Jay Schottenstein", "600", "April 30, 1982", "Martin \"Marty\" McCann", "Nine Inch Nails", "the Labour Party", "Orlando\u2013Kissimmee\u2013Sanford, Florida Metropolitan Statistical Area", "Prussia", "Boston, Massachusetts", "Bambi, a Life in the Woods", "from 1993 to 1996", "Watertown, New York", "green and yellow", "ice hockey", "a 2001 American musical comedy film released by Universal Pictures, and Metro-Goldwyn-Mayer", "3 September", "the Gaget, Gauthier & Co. workshop", "200 to 500 mg up to 7 mg", "Paul Gauguin", "Ynys M\u00f4n", "Saturday Night Live", "an antihistamine and an epinephrine auto-injector", "the Big Three", "outside his house in Najaf's Adala neighborhood", "a panic's in thy breastie", "Shauna", "Momentum", "hemoglobin"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5385540674603175}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.5, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.6666666666666666, 0.2, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-507", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-4639", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-2482", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-8555", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-5417", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-1606", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-3357"], "SR": 0.421875, "CSR": 0.5339176829268293, "EFR": 1.0, "Overall": 0.7235804115853659}, {"timecode": 41, "before_eval_results": {"predictions": ["his sons and grandsons", "memory", "a Sump Pump", "NASA", "Rabbly Bears", "Parris Island", "a Guinness Book of", "Eddie Fisher", "Molly Ringwald", "Rolling Stones", "White blood cells", "by the National Crime Syndicate", "Matsu", "Stardust", "a book of Operas", "Who's Afraid of the Big Bad Wolf", "Rabb the Eskimo", "medical malpractice", "Rabbly bear", "Kareem Abdul-Jabbar", "the Police", "Bravo", "Henry Clay", "Soviet Leader Mikhail Gorbachev", "a ghost", "New York", "Jeopardy", "the holy of", "Rabbis Polk", "the Titanic", "Franklin D. Roosevelt", "Pisces", "the Golden Fleece", "Alzheimer's disease", "Chuck Yeager", "Rabbesen", "\"Pail\"s", "ajax", "Vermont", "vilebrequin", "a lightweight infantry anti-tank weapon", "zenith", "the Whig", "Vietnam", "Ectoplasm", "italy", "Old North", "binocular", "Baby I Need", "Legally Blonde", "Scorpio", "Sunni Islam", "Kaley Christine Cuoco", "H ions", "Salix", "Pearl Slaghoople", "Rabbis", "canning", "New Orleans Saints", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "if a majority signed, the company involved would have 90 days to negotiate union representation.", "Aldgate East.", "it is for all Colombians.", "crossword puzzle"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4243438852813853}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1818181818181818, 0.14285714285714288, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-13266", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-9985", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-2998", "mrqa_searchqa-validation-6579", "mrqa_searchqa-validation-9793", "mrqa_searchqa-validation-1329", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-1899", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-12679", "mrqa_searchqa-validation-16343", "mrqa_searchqa-validation-591", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-12899", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-1471", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-1035", "mrqa_triviaqa-validation-987"], "SR": 0.359375, "CSR": 0.5297619047619048, "EFR": 1.0, "Overall": 0.7227492559523809}, {"timecode": 42, "before_eval_results": {"predictions": ["Westminster", "Ratatouille", "Ecclesiastes", "Catherine de' Medici", "Bohemia", "Ecuador", "Microsoft", "Katharine Hepburn", "binocular", "a forest", "London", "Little Boy Blue", "cotton", "(Alexander) the Great", "Seinfeld", "John Paul Jones", "the Bell X-1", "Spider-Man", "Le Morte d'Arthur", "Hudson Bay", "Hamlet", "axios", "Patrick\\'s Cathedral", "Wheat has been cracked, flaked,", "King George III", "Solomon", "Illinois", "(Joseph) Conrad", "Rastafarianism", "Beverly Cleary", "Neapolitan", "Heartbreak Hotel", "Spain", "angels", "Pisa", "V for Victory in World War II", "Bangkok", "Cuba Gooding", "Russia", "Burt Lancaster", "a diagonal", "the Chinese Communist Party", "a sacristy", "Israel", "Othello", "Alabama", "Making the Band 3", "Martinique", "a deodorant", "the Deerslayer", "the Lion King", "Florida", "Norman", "After tentatively courting each other in `` Entropy ''", "a dragonfly", "is our children learning?", "Firethorn", "alt-right", "Australian", "Robert Bunda", "Negotiators for Zelaya and Roberto Micheletti, the politician who was appointed president hours after Zelaya's June 28 removal, reached an agreement late Thursday", "People around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "Daytime Emmy Lifetime Achievement Award", "Australia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6436155913978494}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.25806451612903225, 0.1, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-1830", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8318", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-5271", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-8416", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10320", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-755", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1351"], "SR": 0.546875, "CSR": 0.5301598837209303, "retrieved_ids": ["mrqa_squad-train-63289", "mrqa_squad-train-24483", "mrqa_squad-train-19544", "mrqa_squad-train-49178", "mrqa_squad-train-2966", "mrqa_squad-train-7727", "mrqa_squad-train-15332", "mrqa_squad-train-9756", "mrqa_squad-train-74444", "mrqa_squad-train-82040", "mrqa_squad-train-63091", "mrqa_squad-train-57699", "mrqa_squad-train-44702", "mrqa_squad-train-13049", "mrqa_squad-train-59143", "mrqa_squad-train-67894", "mrqa_searchqa-validation-11779", "mrqa_triviaqa-validation-661", "mrqa_newsqa-validation-1250", "mrqa_squad-validation-5521", "mrqa_triviaqa-validation-7458", "mrqa_searchqa-validation-8029", "mrqa_naturalquestions-validation-4999", "mrqa_triviaqa-validation-3161", "mrqa_searchqa-validation-8085", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-137", "mrqa_naturalquestions-validation-1139", "mrqa_newsqa-validation-407", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-4182"], "EFR": 0.9655172413793104, "Overall": 0.7159323000200482}, {"timecode": 43, "before_eval_results": {"predictions": ["jellyfish", "to solve South Africa's `` ethnic problems", "The standing rib roast", "late January or early February", "Monk's Caf\u00e9", "Christopher Lloyd", "Morgan Freeman", "1912", "over 74 languages", "John Vincent Calipari", "Floyd", "2019", "James Madison", "provided majority of members present at that time approved the bill either by voting or voice vote", "1917", "Icarus", "Mike Alstott", "A. planci", "1982", "1956", "Harry", "Monk's", "Atchafalaya River", "Pete Seeger", "depression", "Simmons", "in sequence with each heartbeat", "Kevin Sumlin", "Bonnie Lipton", "American drama film", "two parallel planes", "gastrocnemius", "usernames, passwords, commands and data", "Roy Eberhardt", "chief petty officer", "February 26, 2018", "937 total weeks", "Johannes Gutenberg", "cadmium", "1961", "1990", "white oak", "Cephalopoda", "1939", "March 26, 1973", "Stephen Foster", "Charles Carson", "Bee Gees", "Brazil", "The White House Executive Chef", "3.9 and 5.5 mmol / L", "Mike Lancer", "conductor", "a game of bridge", "Manchester United", "Skipton", "Duncan Kenworthy", "Elena Kagan", "an eight-week plan for low-calorie meals that he could prepare.", "political dead-end", "Ernesto Che Guevara", "pinnipeds", "Heather Locklear", "The Arizona Health Care Cost Containment System"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4836822710803689}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.4, 0.5000000000000001, 0.4, 1.0, 1.0, 1.0, 0.8, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6086956521739131, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.7499999999999999, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.5, 0.3636363636363636, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-9076", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-946", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-1221", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-912", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-8799", "mrqa_hotpotqa-validation-1803"], "SR": 0.3125, "CSR": 0.5252130681818181, "EFR": 0.9545454545454546, "Overall": 0.7127485795454545}, {"timecode": 44, "before_eval_results": {"predictions": ["Confucianism", "April 2011", "James Intveld", "frontal lobe", "Haliaeetus ( sea eagles )", "black and yellow", "a low concentration in pigmentation", "Carol Ann Susi", "osseo - cartilaginous", "a premalignant flat ( or sessile ) lesion of the colon", "Arnold Schoenberg", "The `` main line '' or `` first line ''", "asexually", "mid November", "Deposition", "Andy Cole and Shearer", "George Strait", "boiling water reactor", "to solve its problem of lack of food self - sufficiency", "October 2012", "201", "Rafael Nadal", "The Bangladesh -- India border", "Melissa Disney", "Hebrew Bible", "Gene MacLellan", "the fingers on either side of the mouth ( usually with the knuckles facing the observer )", "William Shakespeare's As You Like It", "water ice", "Michael Schumacher", "on the microscope's stage", "living - donor", "North Atlantic Ocean", "John Hancock", "silk floss tree", "100,000", "Triple threat", "Clarence Darrow", "alpha efferent neurons", "in teaching elocution", "the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Butter Island off North Haven, Maine in the Penobscot Bay", "a combination of genetics and the male hormone dihydrotestosterone", "British Columbia, Canada", "the remaining surface of the enamel", "Halliwell, French, Timomatic and Sandilands", "Frankie Valli", "899 mbar", "1939", "Pyeongchang County, Gangwon Province, South Korea", "Utah, Arizona, Wyoming, and Oroville, California", "Quito", "Lidice", "Julius Caesar Octavianus", "Ars Nova Theater", "French mathematician and physicist", "1902", "Najaf.", "a monthly allowance,", "Illlinois.", "a cutlery", "Hail Caesar", "Possession", "last summer."], "metric_results": {"EM": 0.5, "QA-F1": 0.6117382097069597}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.25, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 1.0, 0.0, 0.3333333333333333, 0.4615384615384615, 0.1904761904761905, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-9196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-7131", "mrqa_hotpotqa-validation-2863", "mrqa_newsqa-validation-1076", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-9372"], "SR": 0.5, "CSR": 0.5246527777777779, "EFR": 0.9375, "Overall": 0.7092274305555556}, {"timecode": 45, "before_eval_results": {"predictions": ["adaptive immune system", "Andaman and Nicobar Islands", "Jacques Cousteau", "a mid-size four - wheel drive luxury Mercedes -Benz GL - Class", "Wilhelm Groener", "compasses", "Lake Wales, Florida", "an unmasked and redeemed Anakin Skywalker", "Megan Park", "Antan O'Shea", "Tulsa, Oklahoma", "Broken Hill and Sydney", "John Goodman", "the right side of the heart to the lungs", "at 11 p.m. to 3 a. m.", "Oklahoma", "11 January 1923", "a high and rugged mountain range in the Rocky Mountains in southwestern Colorado and northwestern New Mexico", "Ann Gillespie", "the Pontic Mountains in Turkey", "Master Christopher Jones", "832 BCE", "Claudia Grace Wells", "Jerry Leiber", "2002 Mitsubishi Lancer OZ Rally", "Natural - language processing", "Sir Alex Ferguson", "around 1872", "William Jennings Bryan", "2011", "the dollar was left to float", "Cairo, Illinois", "comic", "Abanindranath Tagore CIE", "Coldplay", "in the fovea centralis", "Empiricism", "1,149 feet ( 350 m )", "Lana Del Rey", "The Jewel of the Nile", "a total of six degrees of freedom", "December 1886", "Ludacris", "A costume", "950 pesos", "Frankie Muniz", "Freddie Highmore", "the somatic nervous system and the autonomic nervous system", "Andy Cole", "1966", "henchmen", "a visual defect in which colored objects appear to be tinged with color", "Perth Racecourse", "bramble", "England", "two", "6,241", "Roy Foster", "share personal information.", "Stephen Tyrone Johns", "\"Austen\"tatious novel", "flatware", "Sir Francis Drake", "charles swinburne"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6446891754677314}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.8, 0.4444444444444445, 0.3636363636363636, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.7272727272727272, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.4, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 1.0, 1.0, 0.20689655172413793, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 0.375, 0.2857142857142857, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-3018", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5970", "mrqa_triviaqa-validation-5041", "mrqa_hotpotqa-validation-5438", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2549", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-2528"], "SR": 0.46875, "CSR": 0.5234375, "retrieved_ids": ["mrqa_squad-train-45464", "mrqa_squad-train-84651", "mrqa_squad-train-69610", "mrqa_squad-train-75270", "mrqa_squad-train-26014", "mrqa_squad-train-18243", "mrqa_squad-train-4441", "mrqa_squad-train-45824", "mrqa_squad-train-70318", "mrqa_squad-train-75803", "mrqa_squad-train-79720", "mrqa_squad-train-62550", "mrqa_squad-train-34288", "mrqa_squad-train-16286", "mrqa_squad-train-54082", "mrqa_squad-train-5941", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-946", "mrqa_squad-validation-1894", "mrqa_triviaqa-validation-1", "mrqa_squad-validation-2404", "mrqa_naturalquestions-validation-5562", "mrqa_hotpotqa-validation-3944", "mrqa_searchqa-validation-6920", "mrqa_triviaqa-validation-657", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-3495", "mrqa_newsqa-validation-1894", "mrqa_searchqa-validation-2154", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-3181", "mrqa_triviaqa-validation-3434"], "EFR": 0.9705882352941176, "Overall": 0.7156020220588235}, {"timecode": 46, "before_eval_results": {"predictions": ["New England Patriots", "2.5", "When used alone, ex as a noun is assumed to refer to a former sexual or romantic partner, especially a former spouse", "Cliff Richard", "McKim Marriott", "The British Indian Association", "foreign investors", "Glen W. Dickson", "French Canadian", "committed and effective Sultans", "Jules Shear", "13 to 22 June 2012", "Tandi", "H.L.A. Hart", "Janie Crawford", "West Norse sailors", "2005", "2012", "the region of the thorax between the neck and diaphragm in the front of the body", "Randy Goodrum", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "because in Christianity, it is a symbol of the resurrection of Christ, which is celebrated during Eastertide", "its vast territory was divided into several successor polities", "Buffalo Lookout", "Thespis", "December 1800", "John Donne", "the late 1980s", "Cristeta Comerford", "104 colonists and Discovery", "123rd in per capita GDP ( PPP ) with $6,616 as of 2016", "Arnold Schoenberg", "Definition of the problems and / or goals", "The Outback", "quartz", "Wisconsin", "85 %", "Long Island", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "seven", "NFL owners", "Stan and Cartman accidentally destroy a dam, causing the town of Beaverton to be destroyed", "Juliet", "a writ of certiorari", "gathering money from the public", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "2018", "the contestant", "San Jose, California", "Phil Mickelson", "Indo - Pacific", "Shaft", "Denise van Outen", "West Virginia", "Syracuse", "Girls' Generation", "Manchester", "Authorities in Fayetteville, North Carolina,", "21 percent suggesting that things are going well.", "If a security officer were to pull a gun on an armed individual in a mall, it could result in \"the gunfight at the 'OK corral,'", "Jericho", "James Garfield Davis", "Catherine", "Ashley \"A.J.\" Jewell"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6384279392147971}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.4166666666666667, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5333333333333333, 0.0, 0.721311475409836, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.11764705882352942, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.47058823529411764, 0.4615384615384615, 1.0, 0.06451612903225806, 0.0, 0.0, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.45454545454545453, 0.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-259", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-6448", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-6804", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-2578", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-16615"], "SR": 0.515625, "CSR": 0.5232712765957447, "EFR": 0.9354838709677419, "Overall": 0.7085479045126973}, {"timecode": 47, "before_eval_results": {"predictions": ["mumps", "The Last King of Scotland", "Kazakhstan", "Dickens", "the pleura passes lateralward, lines the inner surfaces of the costal cartilages, ribs, and Intercostales, and is reflected upon the sides of the bodies of the vertebr\u00e6", "Knutsford", "Burma", "Ewan McGregor", "a falcon", "South Park", "Shylock", "Canada", "Phil Spector", "the Kamikaze", "tiptoe Through the Tulips", "Dada", "bemidji", "Roddy Doyle", "geography", "Operation Frequent Wind", "Berlin", "Charlie Chan", "Wanderers", "Pinwright's Progress", "a winter fur hat", "Lady Gaga", "a waterfowl", "Christian Wulff", "the Kinks", "Lucille Desiree Ball", "Debbie Rowe", "Sir Herbert Kitchener", "a centaur", "iodine deficiency", "36", "mike Foot", "James Hogg", "Shropshire", "George Bernard Shaw,", "table tennis", "Woolton Pie", "Agulhas current", "Boston Legal is an American legal drama-comedy (dramedy) created by David E. Kelley, which originally ran on ABC from October 3, 2004 to December 8, 2008", "Brighton", "pippin", "1939", "Motown", "luven", "Benedict XVI", "a dove", "John T. Cable", "a rearrangement of chromosomal material between chromosome 21 and another chromosome", "the American Civil War", "$2 million in 2011, with a winner's share of $315,600", "Amy Poehler", "Eric Allan Kramer", "Koninklijke Ahold N.V.", "via YouTube days after 35 bodies were found in two trucks during rush hour in the city of Boca del Rio.", "Karl Kr\u00f8yer", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "sweden", "Nancy Drew", "schizophrenia", "release heat"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6210887492137491}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14814814814814814, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 0.09523809523809523, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-2077", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-6547", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-961", "mrqa_triviaqa-validation-304", "mrqa_naturalquestions-validation-10537", "mrqa_hotpotqa-validation-4091", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-3960", "mrqa_searchqa-validation-14439"], "SR": 0.578125, "CSR": 0.5244140625, "EFR": 1.0, "Overall": 0.7216796875}, {"timecode": 48, "before_eval_results": {"predictions": ["taxonomy", "eight", "April 1st", "June 1992", "won", "Katharine Hepburn -- Ethel Thayer", "Kimberlin Brown", "March 31, 2017", "vincent van gogh ( 1853 -- 1890 )", "New York City", "George Strait", "John Adams", "a major fall in stock prices", "to Lands End", "Charles Path\u00e9", "Phillip Paley", "from statute or the Constitution itself", "18", "Game 1", "those colonists of the Thirteen Colonies who rebelled against British control during the American Revolution and in July 1776 declared the United States of America an independent nation", "Abraham Gottlob Werner", "IETF", "18th century", "Lesley Gore", "sometime between 124 and 800 CE", "Plank", "mongrel female", "Mel Jones", "John Quincy Adams", "August 1991", "Uralic languages", "bactrian", "Bhupendranath Dutt", "2011", "behaves as an antagonist ( a substance that binds to a receptor but does not activate and can block the activity of other agonists", "Bill Russell", "Battle of Antietam", "pickup trucks", "Hunter Tylo", "Buffalo Bill", "the early 3rd century", "James Rodr\u00edguez", "around 10 : 30am", "Jack Barry", "the 1989 World Series championship", "approximately 11 %", "to condense the steam coming out of the cylinders or turbines", "Bill Russell", "1984", "Identification of alternative plans / policies", "the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "goodge Street", "cilla black", "Champagne house in Epernay", "her grandmother", "Taoiseach", "Los Angeles Dance Theater", "Kurdistan Workers' Party", "AbdulMutallab", "don Draper", "a typewriter", "calico cat", "lute", "Gary Player"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6311044597763348}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.12500000000000003, 0.10714285714285715, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.16, 1.0, 0.4799999999999999, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.9142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0606060606060606, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-9749", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-183", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-4655", "mrqa_newsqa-validation-1506", "mrqa_searchqa-validation-9898", "mrqa_searchqa-validation-8622"], "SR": 0.546875, "CSR": 0.5248724489795918, "retrieved_ids": ["mrqa_squad-train-36399", "mrqa_squad-train-8172", "mrqa_squad-train-74320", "mrqa_squad-train-26635", "mrqa_squad-train-19775", "mrqa_squad-train-40100", "mrqa_squad-train-37950", "mrqa_squad-train-76167", "mrqa_squad-train-11071", "mrqa_squad-train-77178", "mrqa_squad-train-38804", "mrqa_squad-train-66795", "mrqa_squad-train-31883", "mrqa_squad-train-17573", "mrqa_squad-train-74002", "mrqa_squad-train-84751", "mrqa_newsqa-validation-3423", "mrqa_naturalquestions-validation-5769", "mrqa_hotpotqa-validation-1506", "mrqa_squad-validation-2576", "mrqa_hotpotqa-validation-1016", "mrqa_squad-validation-2405", "mrqa_naturalquestions-validation-1282", "mrqa_searchqa-validation-16343", "mrqa_squad-validation-2288", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-7009", "mrqa_searchqa-validation-844", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-15407", "mrqa_triviaqa-validation-6813"], "EFR": 1.0, "Overall": 0.7217713647959184}, {"timecode": 49, "before_eval_results": {"predictions": ["Louis XVIII", "iron", "Patrick Warburton", "a trustee", "pneumonoultramicroscopicsilicovolcanoconiosis", "Charles Crozat Converse", "Andrew Garfield", "July 4, 1776", "Keith Thibodeaux", "Jesus Christ", "Charles Path\u00e9", "eleven", "President alone, and the latter grants judicial power solely to the federal judiciary", "Johannes Gutenberg", "O'Meara", "published in the United States by Melvil Dewey in 1876", "Marley & Me", "fourth season", "four", "Confederate", "5 %", "slavery", "Teddy Randazzo", "the Internal Revenue Service", "The Outback", "its vast territory was divided into several successor polities", "Madame du Barry", "2017", "genome", "Beorn", "legitimacy", "Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated", "in response to the Weimar Republic's failure to continue its reparation payments in the aftermath of World War I", "Indirect rule", "Zachary John Quinto", "the governor of West Virginia", "Wednesday, September 21, 2016", "ninth", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "milling process", "Crunchyroll", "1939", "1992", "Miller Lite", "Felix Baumgartner", "Donald Gets Drafted", "c. 3000 BC", "Bart Howard", "Milan", "1966", "the vascular cambium", "jocky Wilson", "Leeds", "the Netherlands", "Coleman Hawkins", "Samuel Joel \" Zero\" Mostel", "Ellesmere Port, United Kingdom", "the Genocide Prevention Task Force", "Boundary County, Idaho,", "the FBI.", "a Titan", "Treasure Island", "Sergei Diaghilev", "kite surfers"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6486037683127106}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9473684210526316, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.07142857142857144, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8837209302325582, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-1290", "mrqa_naturalquestions-validation-6888", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6021", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3679", "mrqa_searchqa-validation-9956", "mrqa_searchqa-validation-15480", "mrqa_newsqa-validation-1446"], "SR": 0.515625, "CSR": 0.5246875, "EFR": 0.967741935483871, "Overall": 0.7152827620967741}, {"timecode": 50, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1373", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10692", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5387", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8709", "mrqa_naturalquestions-validation-8819", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5744", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99"], "OKR": 0.87109375, "KG": 0.48671875, "before_eval_results": {"predictions": ["Jason", "Trainspotting", "Rockin' at the Hops", "oakum", "spark-ignition", "Concorde", "Al Jazeera", "French", "herd", "squid", "1952", "Goldfinger", "Midway", "Flower", "Nixon", "Dengue fever", "Japan", "Hanoi Jane and Captain Outrageous", "having or seeing nosebleeds or bleeding to death", "primula veris", "Mount Everest", "Strangeways", "Carthage", "Wensum", "Robben", "United Kingdom", "Taekwondo", "toe", "Apple", "Wimbledon", "Mandela", "George Orwell", "Andrew Jackson", "Muriel Spark", "table tennis", "Reservoir", "DeLorean", "six", "Perseus", "Yakutat", "United Nations of Football", "specialist insurance", "muscle", "transuranic elements", "John Buchan", "Tesco", "Lolita", "Barbara Eden", "the Indus valley", "duck", "Pickwick", "Nancy Jean Cartwright", "Watson and Crick", "Authority", "his superhero roles", "German", "Che Guevara", "Robert Barnett", "five", "Jet Republic", "a 4.6 liter V8", "John Lennon", "Murder by Death", "Republicans"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7310763888888889}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-139", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-204", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-4108", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-7267", "mrqa_triviaqa-validation-3982", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3249", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-881", "mrqa_searchqa-validation-5409"], "SR": 0.6875, "CSR": 0.5278799019607843, "EFR": 0.95, "Overall": 0.7112791053921568}, {"timecode": 51, "before_eval_results": {"predictions": ["Flatbush section of Brooklyn, New York City", "Australian", "Traumnovelle\" (\"Dream Story\")", "Denmark", "Bad Meets Evil", "Bellagio and The Mirage", "Lawrence Mikan, Jr.", "more than 20", "Guthred", "The New Yorker", "Jeffrey Jones", "St. Louis Cardinals", "NXT Tag Team Championship", "Lee Byung-hun", "24", "February 1", "capital crimes or capital offences", "Let Me Be the One (The Carpenters song)", "London", "Chuck Noll", "cate Blanchett", "California", "Atlas ICBM", "Democratic", "Kim So-hyun", "Beatles", "22,500 acres", "Trey Parker", "Kew", "Albany", "twelfth", "Wembley Stadium", "Shameless", "Raden Panji Nugroho Notosusanto", "skiing and mountaineering", "Gujarat", "Comedy Film Nerds", "cruiserweight", "five books", "Leofric", "Bigfoot", "March 17, 2015", "Yubin, Yeeun", "5249", "fourth", "seven", "28 November 1973", "\"O\", \"La Nouba\", \"Myst\u00e8re\", \"Alegr\u00eda\", and \"Quidam\"", "Londonderry", "a boy's life", "jewelry designer", "Steve Russell", "Wat '', an old pronunciation of Gaultier or Walter, and similarly derived from the surname Watson ( `` Wat's son '' )", "while studying All My Sons by Arthur Miller, a play about a man whose choice to send out faulty airplane parts for the good of his business and family", "Pegasus", "sunday", "Basildon-born Perry", "15-year-old's", "July 23.", "Baghdad", "berry", "circumcision", "T.S. Eliot", "vincent wales"], "metric_results": {"EM": 0.4375, "QA-F1": 0.54545866673172}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.8571428571428571, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 0.819672131147541, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-3270", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-3712", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-248", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-1886", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-7514", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-4557", "mrqa_searchqa-validation-6192", "mrqa_searchqa-validation-834", "mrqa_triviaqa-validation-4216"], "SR": 0.4375, "CSR": 0.5261418269230769, "retrieved_ids": ["mrqa_squad-train-53594", "mrqa_squad-train-28889", "mrqa_squad-train-22546", "mrqa_squad-train-58675", "mrqa_squad-train-17498", "mrqa_squad-train-84712", "mrqa_squad-train-79013", "mrqa_squad-train-6077", "mrqa_squad-train-59769", "mrqa_squad-train-28506", "mrqa_squad-train-59324", "mrqa_squad-train-24431", "mrqa_squad-train-77699", "mrqa_squad-train-43699", "mrqa_squad-train-71135", "mrqa_squad-train-57053", "mrqa_squad-validation-2322", "mrqa_triviaqa-validation-6146", "mrqa_squad-validation-5605", "mrqa_triviaqa-validation-3473", "mrqa_squad-validation-8681", "mrqa_hotpotqa-validation-5737", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-2540", "mrqa_triviaqa-validation-919", "mrqa_searchqa-validation-591", "mrqa_newsqa-validation-4170", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-3357", "mrqa_hotpotqa-validation-3069", "mrqa_naturalquestions-validation-1284", "mrqa_triviaqa-validation-6882"], "EFR": 1.0, "Overall": 0.7209314903846153}, {"timecode": 52, "before_eval_results": {"predictions": ["Frank Ocean", "Brookhaven", "2010", "Ryukyuan people", "Robert L. Stone", "Mexican", "The King of Hollywood", "five times", "1968", "Charles Eug\u00e8ne Jules Marie Nungesser, MC", "Kim Yoon-seok and Ha Jung-woo", "Jennifer Grey", "1978", "M2M", "Mark Neveldine and Brian Taylor", "Starship Planet", "Beauty and the Beast", "\"Odorama\"", "The Seven Habits of Highly Effective Teens", "Larnelle Harris", "Total Nonstop Action Wrestling", "Lambic", "Bit Instant", "Tom Jones", "Leon Czolgosz", "Secrets and Lies", "Hard rock", "Ludwig van Beethoven", "Peter Kay's Car Share", "Orfeo ed Euridice", "Dirt track racing", "Frederick Barbarossa", "Karakalpak", "Walldorf", "Harrison Ford", "Campbellsville University", "Shinjuku", "1933", "Delphi Lawrence", "Philadelphia", "December 13, 2015", "a governor of the New York Stock Exchange", "Las Vegas", "Russell T Davies", "four studio albums", "Marlborough, New Hampshire", "Mickey\\'s Christmas Carol", "2018\u201319 UEFA Europa League group stage", "Argentinian", "76,416", "Burning Man", "the President of the United States", "Ra\u00fal Eduardo Esparza", "October 30, 2017", "Wikia", "Carrie", "Mars", "Reggae legend Lucky Dube, one of South Africa's most famous musicians,", "quarter-mile pier crumbling into the sea along with two of his trucks.", "the Internet", "a cursor amok", "Yes", "East Germany", "heart"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7556261446886448}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-847", "mrqa_hotpotqa-validation-1263", "mrqa_triviaqa-validation-2638", "mrqa_newsqa-validation-594", "mrqa_searchqa-validation-15932", "mrqa_triviaqa-validation-3362"], "SR": 0.671875, "CSR": 0.5288915094339622, "EFR": 1.0, "Overall": 0.7214814268867925}, {"timecode": 53, "before_eval_results": {"predictions": ["the 2012 London Summer Games", "New York", "on the island", "muezzin", "nippon Sangyo", "binder", "James Hogg", "Sarajevo", "Darby and Joan", "the 2010 BAFTA Awards", "Stanley Kubrick's Full Metal jacket", "Blur", "chicken Marengo", "Nelson Mandela", "general Sir Herbert Kitchener", "green", "a bodice", "grizzly", "bukwus", "northern Italian", "rowing", "his death in 1975", "nowhere Boy", "Donald Trump", "Gorbachev", "Popeye", "John Key", "Charlie Brooker", "pennsylvania state university", "gulf of Mexico", "delilah", "Aegle", "dynamite", "take that", "Jean Alexander", "Norman Brookes", "David Hockney", "La Toya Jackson", "(Harry) Truman", "boston", "chile", "Edinburgh", "Today", "bellingham", "Bolton", "norway", "Super Bowl", "\"Stutter Rap (No Sleep til Bedtime)\"", "Vladimir Putin", "Cameron Dokey", "the US recession", "Augustus Waters", "1967", "silk, hair / fur ( including wool ) and feathers", "Ghana", "Peter Kay's Car Share", "Miller Brewing", "the country's longest-serving ruler.", "more than two years,", "auction off one of the earliest versions of the Magna Carta later this year,", "the tartan of the Argyle clan", "a judgment,", "michael perlin", "Unseeded Frenchwoman Aravane Rezai"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5324675324675325}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.4, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5884", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-2385", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5531", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-5344", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-3499", "mrqa_newsqa-validation-3285"], "SR": 0.453125, "CSR": 0.5274884259259259, "EFR": 0.9714285714285714, "Overall": 0.7154865244708994}, {"timecode": 54, "before_eval_results": {"predictions": ["Tony Curtis", "The Great Gatsby", "ford", "japan", "the First World War", "germany", "prince andrew", "duchess", "japan", "ford administration", "E", "Skylab", "John Poulson", "her husband's", "shoes", "black Tuesday", "wheels", "queen Anne", "atlantic", "dicken\\'s Dream", "Swansea City", "xenon", "silurian", "meatloaf", "jesuits", "honolulu turner", "The Lone Gunmen", "gold", "Duncan Jones", "the north-west corner of the central business district", "wonderwall", "basketball", "carburetor", "norway", "corsets", "germany", "Jean- Martin Charcot", "winged horse", "lillita McMurray", "bathe", "dora peggotty", "Scotland", "germany", "Arthur C. Clarke", "Neil Armstrong", "power", "the French", "index fingers", "jukes of Marlborough", "rihanna", "three Mile island", "28 July 1914 to 11 November 1918", "Fix You", "spectroscopic notation", "Copa Airlines", "my Beautiful Dark Twisted Fantasy", "Sharman Joshi", "for vitamin injections that promise to improve health and beauty.", "debris", "digging ditches.", "Typhoid Mary", "France", "the Edict of Nantes", "Austin and Pflugerville"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5416666666666666}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2312", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5896", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-4795", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-2332", "mrqa_triviaqa-validation-2879", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-1910", "mrqa_naturalquestions-validation-585", "mrqa_hotpotqa-validation-303", "mrqa_newsqa-validation-3330", "mrqa_searchqa-validation-5331", "mrqa_naturalquestions-validation-3995"], "SR": 0.484375, "CSR": 0.5267045454545455, "retrieved_ids": ["mrqa_squad-train-16805", "mrqa_squad-train-26407", "mrqa_squad-train-1540", "mrqa_squad-train-57560", "mrqa_squad-train-33181", "mrqa_squad-train-63450", "mrqa_squad-train-74811", "mrqa_squad-train-80246", "mrqa_squad-train-57951", "mrqa_squad-train-1458", "mrqa_squad-train-36699", "mrqa_squad-train-61573", "mrqa_squad-train-26182", "mrqa_squad-train-45793", "mrqa_squad-train-42221", "mrqa_squad-train-65500", "mrqa_searchqa-validation-12679", "mrqa_squad-validation-7476", "mrqa_searchqa-validation-9147", "mrqa_naturalquestions-validation-4983", "mrqa_triviaqa-validation-6120", "mrqa_naturalquestions-validation-10113", "mrqa_squad-validation-6034", "mrqa_triviaqa-validation-2559", "mrqa_newsqa-validation-853", "mrqa_hotpotqa-validation-1820", "mrqa_squad-validation-10186", "mrqa_searchqa-validation-9297", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-473", "mrqa_triviaqa-validation-3060", "mrqa_triviaqa-validation-7618"], "EFR": 1.0, "Overall": 0.7210440340909091}, {"timecode": 55, "before_eval_results": {"predictions": ["aviva plc", "Venezuela", "Mozart", "anacapri", "astral hartman", "Catherine Cookson", "almonds", "sarah Palin", "estonia", "vince shatner", "peter Paul Rubens", "Arabian Gulf", "durlach", "ascot", "Seine", "12 Yard", "shirley Bassey", "winnie Mae", "spain", "quietly", "the Amorites", "graphite", "Narragansett Bay", "Moby Dick", "The Scream", "gingerbread", "Boddington", "Bad King John", "symbol of auspiciousness", "raspberries", "USVI", "surfing", "oakum", "blancmange", "rochdale", "Penhaligon", "Black September", "510", "Germany", "shoe", "hydrogen", "bTMarch 6, 1987", "Professor Brian Cox", "france", "8", "joints", "Bolivia", "jewish community", "Jordan", "Hans Lippershey", "alfa", "Marlborough Churchill Blenheim Charlton", "statistical advantage for the casino that is built into the game", "the top of the cab can be crushed or sliced off as it swings round violently and tries to fold under the trailer", "Verve", "Japan", "National Aviation Hall of Fame", "Megan Lynn Touma,", "Madeleine K. Albright", "three empty vodka bottles,", "walrus", "grotesque", "Liam Neeson", "a centaur"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5333928571428571}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.08000000000000002, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4616", "mrqa_triviaqa-validation-5454", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-1324", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-3231", "mrqa_triviaqa-validation-6866", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3275", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-3897", "mrqa_triviaqa-validation-2442", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-4164", "mrqa_triviaqa-validation-768", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5510", "mrqa_hotpotqa-validation-752", "mrqa_hotpotqa-validation-2358", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3067", "mrqa_searchqa-validation-2027"], "SR": 0.484375, "CSR": 0.5259486607142857, "EFR": 0.9696969696969697, "Overall": 0.7148322510822511}, {"timecode": 56, "before_eval_results": {"predictions": ["christian michael zager", "Judy Garland", "william hartnell", "friedrich Nietzsche", "Ben Affleck", "jamaican", "magical Mystery Tour", "rio", "titan", "purple", "1929", "norway", "florence", "Steve Davis", "arun kallarackal", "antoine Lavoisier", "30th anniversary", "meerkat", "tara", "henri Rousseau", "albania", "John Lennon", "macau", "robbie coltrane", "phylum", "sweeny tood", "tarn", "roader's Digest", "balthazar king", "robbie coltrane", "Alastair Cook", "petronas Towers", "cribbage", "1960s", "north york", "LMFAO", "Emma Chambers", "Kinks Are the Village Green Preservation Society", "Tony Blackburn", "mexico", "rebecca", "united states", "pink Floyd", "bobby brown", "michael michael", "margaret atlantic richey", "united states", "touto", "hyphenated", "mono", "augustus descartian", "the east coast of Queensland", "Lady Gaga", "revenge and karma", "\"Secrets and Lies\"", "October 3, 2017", "Morris Barney Dalitz", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Unseeded", "Kingman Regional Medical Center", "Walter Payton", "the fairway", "the Wikimedia Foundation", "2018"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5185267857142857}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-59", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-188", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-4757", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1527", "mrqa_triviaqa-validation-2901", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-2743", "mrqa_triviaqa-validation-4550", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-7131", "mrqa_naturalquestions-validation-4719", "mrqa_hotpotqa-validation-4161", "mrqa_newsqa-validation-3287", "mrqa_searchqa-validation-11582"], "SR": 0.484375, "CSR": 0.5252192982456141, "EFR": 0.9393939393939394, "Overall": 0.7086257725279107}, {"timecode": 57, "before_eval_results": {"predictions": ["Wild Bunch", "Illinois", "Edward Hopper", "robocop", "morocco", "Quentin Blake", "bazaar", "sunday", "new york", "Hamlet", "Jose Antonio Reyes", "James Bond", "north atlantic", "hobbits", "Jordan", "Tangled", "So Far Away", "United States", "a crossword puzzle", "sheree mary mary Murphy", "morocco", "Robin Ellis", "sea shells", "davy crockett", "war and peace", "pygmalion", "three", "east of Eden", "de quincey", "zaragoza", "Debbie Abrahams", "argentina", "prince eddy", "british", "a Christmas Carol", "bridge", "elliptical", "trier", "sun jovi", "blood", "zips", "isar river", "Roman history", "mj\u00f6llnir", "Admiral morocco", "florence", "woodstock", "birds", "nijinsky", "wittingham", "Sven Goran Eriksson", "Chairman of the Monetary Policy Committee", "B.J. Thomas", "65,535 bytes", "Prince Amedeo, Duke of Aosta", "San Antonio", "hulder", "the Obama administration", "Republicans", "some of the Awa", "Manhattan Island", "Patrick Henry", "sleep apnea", "Eagles"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6391369047619047}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3686", "mrqa_triviaqa-validation-4243", "mrqa_triviaqa-validation-910", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-1563", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7378", "mrqa_triviaqa-validation-6841", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-6071", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-4973", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-1787", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-2399", "mrqa_searchqa-validation-10934"], "SR": 0.59375, "CSR": 0.5264008620689655, "retrieved_ids": ["mrqa_squad-train-70272", "mrqa_squad-train-60122", "mrqa_squad-train-76635", "mrqa_squad-train-12805", "mrqa_squad-train-86161", "mrqa_squad-train-56594", "mrqa_squad-train-33835", "mrqa_squad-train-64236", "mrqa_squad-train-33287", "mrqa_squad-train-66011", "mrqa_squad-train-3809", "mrqa_squad-train-82583", "mrqa_squad-train-85608", "mrqa_squad-train-30923", "mrqa_squad-train-65137", "mrqa_squad-train-25640", "mrqa_newsqa-validation-377", "mrqa_searchqa-validation-3499", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-8503", "mrqa_searchqa-validation-6579", "mrqa_searchqa-validation-8011", "mrqa_triviaqa-validation-7131", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-1073", "mrqa_triviaqa-validation-346", "mrqa_newsqa-validation-1279", "mrqa_hotpotqa-validation-4560", "mrqa_squad-validation-3958", "mrqa_triviaqa-validation-2802", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-3677"], "EFR": 1.0, "Overall": 0.7209832974137931}, {"timecode": 58, "before_eval_results": {"predictions": ["the re-emerging sovereign Republic following the century of partitions", "Buffalo Soldiers", "Hong Kong Mak\u00e9l\u00e9l\u00e9", "John Robert Cocker", "Taylor Swift", "mountaineer", "\"Lonely\"", "Garrett Morris", "October 5, 1937", "1692", "Dizzy Dean", "Target Corporation", "British Labour Party", "Bandai", "Bill Ponsford", "Patricia Neal", "Code 02PrettyPretty", "Every Rose Has Its Thorn", "Cleveland Browns", "Jacking", "1901", "My Beautiful Dark Twisted Fantasy", "Broadcasting House in London", "20", "Amway", "Congo River", "Minneapolis", "Alemannic", "illnesses", "pornographic website", "1967", "1967", "fictional character", "Lawrence of Arabia", "The Fault in Our Stars", "Gareth Jones", "head of the Cabinet of Bluhme I", "J35-A-23", "Scotty Grainger", "balloon Street, Manchester", "Somerset County, Pennsylvania", "Italy", "Psych", "Gateways", "Iran", "Veneto", "Empire Falls", "Fitzroya", "Vernon L. Smith", "Daniel Hale \"Dan\" Rowan", "Oakdale", "March 18, 2005", "1978", "Austria", "Switzerland", "treaty of Waitangi", "1930-1939", "a U.S. military helicopter", "African National Congress Deputy President Kgalema Motlanthe,", "new DNA evidence", "a blue whale", "Marcia Clark", "books that are no longer being published", "bullfight"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6101516812865497}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false], "QA-F1": [0.10526315789473685, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.5, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4599", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2924", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-10135", "mrqa_triviaqa-validation-5517", "mrqa_newsqa-validation-1382", "mrqa_searchqa-validation-1481", "mrqa_triviaqa-validation-6175"], "SR": 0.515625, "CSR": 0.5262182203389831, "EFR": 1.0, "Overall": 0.7209467690677966}, {"timecode": 59, "before_eval_results": {"predictions": ["the first integrated circuit", "Oracle Corporation", "Levittown", "Mako", "seven", "Ashanti", "1934", "Cheshire County", "1980", "blood sport", "Dachshunds", "red, fallow and roe deer", "Duncan Kenworthy", "Oderturm", "the Netherlands", "Continental Army", "Poetic Edda", "Henry Lau", "1", "Russian Empire", "The Catholic Church in Ireland", "people working in film and the performing arts", "Middle East", "1989", "Gareth Barry", "1999", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "Margarine Unie", "David Naughton", "A123 Systems, LLC", "Ian Fleming", "Minnesota", "14", "an anvil", "50 Greatest Players in National Basketball Association History", "James G. Kiernan", "Dizzy Dean", "Magnus Carlsen", "BBC Focus", "Towards the Sun", "1958", "World War II", "Jenn Brown", "glee", "Pennsylvania University", "Indianapolis", "a heavy metal band", "Greater Manchester, England", "Raabta", "Marxist and a Leninist", "Timothy Clooney", "Laura Jane Haddock", "During metaphase of cell division", "2011", "a camelopardalis", "ghee", "wagner", "us to step up.\"", "former Pakistani Prime Minister Benazir Bhutto in 2007.", "Christina Romete,", "\"H\"", "a meter", "Donnie Wahlberg", "1918"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6585069444444445}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714285, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-1609", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-4817", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1006", "mrqa_hotpotqa-validation-2810", "mrqa_hotpotqa-validation-1055", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-1134", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-3068", "mrqa_searchqa-validation-11781", "mrqa_newsqa-validation-2789"], "SR": 0.578125, "CSR": 0.5270833333333333, "EFR": 1.0, "Overall": 0.7211197916666667}, {"timecode": 60, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4534", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4728", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-637", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-768", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-921", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-10651", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4312", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2415", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1403", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1768", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3432", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5228", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6536", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7606", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-997"], "OKR": 0.830078125, "KG": 0.49453125, "before_eval_results": {"predictions": ["September,", "five minutes before commandos descended", "Arsene Wenger", "carving in the middle of our Mountain View, California, campus.", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement.", "They are co-chair of the Genocide Prevention Task Force.", "allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring,", "4.6 million", "exotic sports cars", "Vicente Carrillo Leyva,", "\"Zed,\"", "Communist Party of Nepal (Unified Marxist-Leninist)", "581 points", "Molotov cocktails, rocks and glass.", "1994,", "10 years", "Gulf", "an average of 25 percent of U.S. consumers who get recall notices don't follow through and fix their vehicles.", "Orbiting Carbon Observatory,", "then-Sen. Obama", "Claude Monet", "4,000", "apartment building", "Pittsburgh", "Former Mobile County Circuit Judge Herman Thomas", "Daytime Emmy Lifetime Achievement Award", "South Africa", "Barack Obama's", "dual nationality", "\"They are, of course, shattered. They are not doing great,\"", "Cash for Clunkers", "It will be the golfer's first public appearance since his November 27 car crash", "Texas Children's Hospital,", "prostate cancer,", "South Africa", "Britain.", "If you have a massive electrical problem", "10 percent", "eight or nine", "Jaipur", "cancer", "Mrs. Graham,", "\"People of Palestine\" and calls on them to fight back against Israel in Gaza.", "forgery and flying without a valid license,", "\"He's tall! He's strong!\"", "Lavau's son, Sean,", "three out of four questioned say that things are going well for them personally.", "poems", "Fourth time lucky in Atlanta in 1996.", "environmental efforts", "Graeme Smith", "2000", "William Jennings Bryan", "The neck", "Nicholas Garland", "Mozambique Channel", "stupid Fashion Questions", "Adelaide", "punk rock", "Great Northern Railway", "Otis Elevator", "Iberian Peninsula", "Balanchine", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.6300628702437913}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.1818181818181818, 0.3, 0.0, 0.16, 1.0, 0.8, 1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15789473684210525, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-4092", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-56", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-2468", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-15121", "mrqa_searchqa-validation-5481"], "SR": 0.53125, "CSR": 0.5271516393442623, "retrieved_ids": ["mrqa_squad-train-10614", "mrqa_squad-train-61111", "mrqa_squad-train-43239", "mrqa_squad-train-49675", "mrqa_squad-train-12880", "mrqa_squad-train-26717", "mrqa_squad-train-78494", "mrqa_squad-train-70083", "mrqa_squad-train-59260", "mrqa_squad-train-25886", "mrqa_squad-train-40254", "mrqa_squad-train-16024", "mrqa_squad-train-76664", "mrqa_squad-train-15096", "mrqa_squad-train-60651", "mrqa_squad-train-18969", "mrqa_naturalquestions-validation-4505", "mrqa_hotpotqa-validation-3219", "mrqa_naturalquestions-validation-7352", "mrqa_searchqa-validation-4118", "mrqa_naturalquestions-validation-8998", "mrqa_searchqa-validation-4158", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-3368", "mrqa_squad-validation-7674", "mrqa_hotpotqa-validation-2906", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-2997", "mrqa_squad-validation-2975", "mrqa_triviaqa-validation-5578", "mrqa_hotpotqa-validation-1434", "mrqa_triviaqa-validation-4962"], "EFR": 1.0, "Overall": 0.7207428278688525}, {"timecode": 61, "before_eval_results": {"predictions": ["Donald Duck", "Iran's parliament speaker", "Department of Homeland Security Secretary Janet Napolitano", "18", "India", "military veterans", "Casalesi Camorra clan", "managing his time.", "club -- which he called \"very diverse\"", "\"We are a nation of Christians and Muslims, Jews and Hindus -- and nonbelievers.\"", "$50 less,", "collaborating with the Colombian government,", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "1,500", "Jada,", "200", "Karen Floyd", "space shuttle Discovery,", "Brazil", "EU naval force", "helped nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "Harrison Ford", "Sunday", "28", "Falklands, known as Las Malvinas", "New York City Mayor Michael Bloomberg", "Department of Homeland Security Secretary Janet Napolitano", "two", "Too many glass shards left by beer drinkers in the city center,", "30-minute", "338", "UNICEF", "eight", "Daniel Radcliffe", "Department of Homeland Security Secretary Janet Napolitano", "Obama girls from Sen. Ted Kennedy.", "a tanker that sailed under a Saudi flag,", "lightning strikes", "if he did cheat on you", "Afghan lawmakers", "New York Philharmonic Orchestra", "Colombia.", "nine-wicket", "nearly 100", "1616.", "to sniff out cell phones.", "Casey Anthony,", "people look at the content of the speech, not just the delivery.", "2005", "take responsibility for terrorists operating within its borders.", "a point for Bayern Munich as the German Bundesliga leaders were held to a 1-1 draw by Cologne on Saturday.", "March 31 to April 8, 2018", "players from a separate era of baseball", "Bobby Beathard", "St Paul's Cathedral", "15", "Latium", "University of Vienna", "Dutch", "Naomi Campbell", "Hawaii", "cricket", "Saturday Night Live", "the courts"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6230111315359477}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.5, 0.2222222222222222, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 1.0, 0.0, 1.0, 0.33333333333333337, 0.2857142857142857, 0.4444444444444445, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.1111111111111111, 0.5, 0.23529411764705885, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-2145", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-4318", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-880", "mrqa_triviaqa-validation-3747", "mrqa_hotpotqa-validation-3500"], "SR": 0.515625, "CSR": 0.5269657258064516, "EFR": 0.9354838709677419, "Overall": 0.7078024193548387}, {"timecode": 62, "before_eval_results": {"predictions": ["Golden Valley, Minnesota,", "an Emmy and four", "small forward", "Polly Hatchet", "Araminta Ross", "the Mach number", "eight", "August 17, 2017", "Al Capone", "atomic", "St Augustine's Abbey", "Vilyam \" willie\" Genrikhovich Fisher", "minister", "Carl Michael Edwards II", "Henry Luce", "The Chicken", "Standard Oil", "over 1.6 million", "British Labour Party", "September 8, 2017", "Obafemi Akinwunmi Martins", "Charles Edward Stuart", "HackThis Site", "Steve Carell", "Saint Motel", "Melissa Ivy Rauch", "Flyweight", "Levon Helm", "Jean Acker", "the attack on Pearl Harbor", "Fountains of Wayne", "Nick Offerman", "Sam Raimi", "SAS Fr\u00f6sundavik", "Double Crossed", "Edmonton, Alberta", "8,211", "KXII", "Wikimedia Foundation", "Greek-American", "Mika H\u00e4kkinen", "Debbie Isitt", "Los Angeles", "1999", "Outside", "United Nations Global Ambassador for the Food and Agriculture Organization", "Wojtek", "King Edward I", "Los Angeles", "West Point", "New York City", "Speaker of the House of Representatives", "13", "Hans Christian Andersen", "an arrowhead", "rome", "Aaron", "UNICEF", "Bob Bogle,", "she was humiliated by last month's incident, in which she was forced to painful remove the piercings behind a curtain as she heard snickers from male TSA officers nearby.", "They Call Me Mr. Tibbs", "a mead", "David", "four"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7087064793866265}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-5444", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-821", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-2552", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-2993", "mrqa_hotpotqa-validation-298", "mrqa_naturalquestions-validation-839", "mrqa_triviaqa-validation-6039", "mrqa_newsqa-validation-390", "mrqa_searchqa-validation-12442", "mrqa_naturalquestions-validation-5292"], "SR": 0.59375, "CSR": 0.5280257936507937, "EFR": 1.0, "Overall": 0.7209176587301587}, {"timecode": 63, "before_eval_results": {"predictions": ["African National Congress", "Ronald Cummings", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "Bob Bogle,", "Bob Bogle,", "\"outlaws\"", "suppress the memories and to live as normal a life as possible;", "Caster Semenya", "her fianc\u00e9,", "the BBC's central London offices", "Kgalema Motlanthe,", "as he tried to throw a petrol bomb at the officers,", "Eikenberry", "left Brooklyn, New York, for Miami Beach, Florida,", "South Korea", "Elena Kagan", "Harrison Ford", "Christmas parade in Soddy-Daisy, Tennessee,", "through a facility in Salt Lake City, Utah,", "Nearly eight in 10", "your ex's loved ones ask why", "2-1", "racial intolerance.", "an acid attack by a spurned suitor.", "23 million square meters (248 million square feet)", "part", "bicycles", "has been hit harder by AIDS and HIV than any other region of the world,", "Akshay Kumar", "August 19, 2007.", "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.", "starting a dialogue while maintaining sanctions,", "bars the federal government from recognizing same-sex unions.", "the test results by the medical examiner's office,", "Green no matter its color", "dance with the Stars.", "Brown-Waite", "we had no real procedure for sectioning off the rear-frame rails,\"", "40", "strife in Somalia,", "protest child trafficking and shout anti-French slogans", "a colonel in the Rwandan army,", "1918-1919.", "in the shooting death of B-movie queen Lana Clarkson after a night out in the clubs of Hollywood.", "cancer", "Susan Atkins,", "137", "the shoreline of the city of Quebradillas.", "for using recreational drugs in September,", "the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "secretary of state.", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "the temperature at which it becomes semi solid and loses its flow characteristics", "a permanent, fast - drying painting medium", "touchstone", "Ennio Morricone", "auric goldfinger", "Thomas Mawson", "Al D'Amato", "Peel Holdings", "Java", "a contrite", "ecliptic", "Harriet the Spy"], "metric_results": {"EM": 0.46875, "QA-F1": 0.577906524643657}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.28571428571428575, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.15384615384615385, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.7499999999999999, 0.8205128205128205, 0.0, 0.1, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2698", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2724", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-10403", "mrqa_triviaqa-validation-2958", "mrqa_searchqa-validation-15537"], "SR": 0.46875, "CSR": 0.527099609375, "retrieved_ids": ["mrqa_squad-train-48867", "mrqa_squad-train-7017", "mrqa_squad-train-39427", "mrqa_squad-train-75411", "mrqa_squad-train-22284", "mrqa_squad-train-11007", "mrqa_squad-train-12127", "mrqa_squad-train-827", "mrqa_squad-train-1453", "mrqa_squad-train-68311", "mrqa_squad-train-59269", "mrqa_squad-train-76185", "mrqa_squad-train-79381", "mrqa_squad-train-33882", "mrqa_squad-train-22108", "mrqa_squad-train-48726", "mrqa_hotpotqa-validation-5520", "mrqa_squad-validation-7614", "mrqa_triviaqa-validation-1517", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-14141", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-446", "mrqa_hotpotqa-validation-230", "mrqa_searchqa-validation-8515", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-646", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-6865", "mrqa_searchqa-validation-0", "mrqa_hotpotqa-validation-1920"], "EFR": 1.0, "Overall": 0.720732421875}, {"timecode": 64, "before_eval_results": {"predictions": ["alan", "Great British Bake Off", "Gary Havelock", "actor", "Fiji", "Natty Bumppo", "Ben Whishaw", "guinea", "Henry Jermyn", "Jon Stewart", "\"Barefoot Bandit\"", "ytterby", "ox", "lithium", "Phillies", "yokai", "gildashedstecker", "1825", "andesitic", "Lisieux", "ascot", "mark Twain", "Charlie Chaplin", "Sunil Gavaskar", "Zeitgeister", "florida", "William Caxton", "Buzz Aldrin", "highball", "a brownish-black fossil fuel", "Dutch", "around the World in Eighty Days", "unite", "because she sang to God, when she prayed", "the Netherlands", "Robin", "stamens", "Arthur Schopenhauer", "Thomas Cranmer", "the Cheshire Cat", "Nick Clegg", "Norfolk, Virginia", "Mr. Spock", "gluteal region", "Nikola Tesla", "Adrian Edmondson", "persian", "the midmost digit of the forelimb", "b Boyle", "Antonio Vivaldi", "Bachelor of Science", "lamina dura", "July 2014", "The Internet protocol suite", "Massachusetts", "Princess Jessica", "supply chain", "Dr. Jennifer Arnold and husband Bill Klein,", "\"I've got a good group of Marines that are behind me, so I'm real excited about the deployment,\"", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "Scandinavia", "Private Benjamin", "the Rhine & the Main", "Amber Heard"], "metric_results": {"EM": 0.359375, "QA-F1": 0.40880208333333334}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.0, 0.08, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-1020", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-6566", "mrqa_triviaqa-validation-5536", "mrqa_triviaqa-validation-105", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-6178", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-6372", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-1949", "mrqa_triviaqa-validation-31", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-5009", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-1171", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-1001", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-2668", "mrqa_searchqa-validation-7466", "mrqa_hotpotqa-validation-652"], "SR": 0.359375, "CSR": 0.5245192307692308, "EFR": 1.0, "Overall": 0.7202163461538462}, {"timecode": 65, "before_eval_results": {"predictions": ["hemlock", "stanley mancher", "robert", "eyes", "sierra leone", "john boynton Priestley", "sierra leone", "p Preston", "sandown castle", "Scorpio", "USS Maine", "Coalbrookdale", "borgia family", "the Periodic Table", "john connors", "bread", "japan", "j Jakarta", "spike milligan", "mitsubishi", "the Isthmian Canal Commission", "1960", "fusion of two or more digits of the feet", "apple", "lug", "john leone", "Hamelin", "Harold Godwinson", "Kuwait", "leicestershire", "sprint", "green", "The Grapes of Wrath", "Coldplay", "pamphlets, posters, ballads", "beautiful little girl,", "Chancellor of the Exchequer", "bottle ring Toss", "fat", "Austria", "stanley boyleby", "a fool", "tom and Jerry", "Markus Aemilius Lepidus", "business", "shed some light", "stanley da ponte", "nikita krushchev", "crawford", "High-F fructose Corn Syrup (HFCS)", "molecular structure", "Schwarzenegger", "Wakanda", "lamina dura", "Bill Cosby", "Robert L. Stone", "Haitian Revolution", "South Korea", "to launch a group that will serve as an alternative to the Organization of American States.", "64", "Nagpur", "tuna tune-up Casserole", "Anaheim", "Old English pyrige ( pear tree )"], "metric_results": {"EM": 0.40625, "QA-F1": 0.48541990165631466}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 0.34782608695652173, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-1899", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-5504", "mrqa_triviaqa-validation-2057", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-1526", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3962", "mrqa_triviaqa-validation-4255", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6015", "mrqa_hotpotqa-validation-4757", "mrqa_newsqa-validation-3226", "mrqa_newsqa-validation-2224", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-16126", "mrqa_searchqa-validation-884"], "SR": 0.40625, "CSR": 0.5227272727272727, "EFR": 1.0, "Overall": 0.7198579545454545}, {"timecode": 66, "before_eval_results": {"predictions": ["air NEXUS card", "hillsborough", "Buddhism", "Andrew Jackson", "A Series of Unfortunate Events", "dennis crawford", "Red sea", "red", "sesame", "grizzly bear", "arthur thatcher", "Swiss", "The Pilgrim's Progress", "Wars of the Roses", "Terry Hall", "acetone", "San Francisco", "Paris", "sewing machine", "arthur", "bud Flanagan", "eye", "stien crawford", "nymphet", "peter Principle", "Video", "Frank McCourt", "Little Jack Horner", "mark-girl", "blancmange", "mark", "Louis- Stanislas-Xavier", "stand-up", "Happy birthday to You", "1948", "Pride & Prejudice", "william golding", "mark of arms", "narnia", "Mr. Brainwash", "calypso", "two eye in the middle of his forehead", "phrenology", "dauphin", "phooey", "apple", "driver", "Joan Rivers", "Mr. Humphries", "katherine mansfield", "heineken", "1987", "1996", "Ella Mitchell", "Wicked Twister", "Lerotholi Polytechnic Football Club", "3730 km", "if they can demonstrate they have been satisfactorily treated for at least 12 months.", "Saturday", "\"green-card warriors\"", "Bering Sea", "Charlottetown", "Agatha Christie", "Former Beatles"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6432291666666667}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6164", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-5391", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-1067", "mrqa_hotpotqa-validation-758", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-140"], "SR": 0.609375, "CSR": 0.5240205223880596, "retrieved_ids": ["mrqa_squad-train-19781", "mrqa_squad-train-52223", "mrqa_squad-train-10794", "mrqa_squad-train-4012", "mrqa_squad-train-67076", "mrqa_squad-train-32644", "mrqa_squad-train-66036", "mrqa_squad-train-31050", "mrqa_squad-train-4119", "mrqa_squad-train-43784", "mrqa_squad-train-76433", "mrqa_squad-train-31183", "mrqa_squad-train-58308", "mrqa_squad-train-28822", "mrqa_squad-train-31960", "mrqa_squad-train-72326", "mrqa_squad-validation-8526", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-1097", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5696", "mrqa_triviaqa-validation-6050", "mrqa_searchqa-validation-16948", "mrqa_newsqa-validation-3046", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-6730", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-16343", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-2385", "mrqa_newsqa-validation-2244", "mrqa_hotpotqa-validation-5128"], "EFR": 1.0, "Overall": 0.7201166044776119}, {"timecode": 67, "before_eval_results": {"predictions": ["Yuri andropov", "Kate Winslet", "John Mortimer", "Camino Franc\u00e9s", "fox", "Victoria Coren", "\"sound and light\"", "coffee", "tomatoes", "fred west", "steel", "saint Columba", "paul", "1215", "1937", "12", "Harriet Harman", "queen of wales", "british", "Venice", "ADNAMS", "Massachusetts", "nikkei", "Nutbush, TN", "robert schumann", "jape", "NASCAR", "Jordan", "chubby Checker", "llanfairfechan", "chicken Marengo", "darshaan", "asthma", "Nicobar", "Blake Griffin", "par-5", "state of oklahoma", "Jason Bourne", "one of my favorite Goddesses", "aluminium", "carbohydrates", "antelopes", "Nevada", "SW20", "joseph", "Eva Marie", "linda simon", "myanmar", "wales", "london", "Aquaman", "Justin Bieber", "2017 season", "seven", "CD Castell\u00f3n", "East Kn Doyle", "Jan Kazimierz", "2.5 million copies", "Vivek Wadhwa,", "Wednesday", "a parodiable", "a 1960s college fraternity house", "Lake Victoria", "air support."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6244791666666666}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-565", "mrqa_triviaqa-validation-1929", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-4669", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-2021", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-1943", "mrqa_triviaqa-validation-506", "mrqa_naturalquestions-validation-344", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3917", "mrqa_searchqa-validation-15285", "mrqa_searchqa-validation-11044"], "SR": 0.578125, "CSR": 0.5248161764705883, "EFR": 0.9629629629629629, "Overall": 0.7128683278867103}, {"timecode": 68, "before_eval_results": {"predictions": ["fort boyard", "Richard Seddon", "5", "son", "stanolas cephalonia", "top cat", "Fotheringhay", "tungsten", "New Zealand", "Fenn Street School", "Kristiania", "South Pacific", "klaine", "mozart", "thalia", "Paddy McGuinness", "woodstock", "stanley", "Chicago", "jack", "dog", "alfresco", "Sarajevo", "hokkaido", "Norman Mailer", "david boyard", "florence", "crabapple", "braille", "a personal computer", "st james", "Washington", "Switzerland", "fort boyard", "pressure", "the Tower of London", "daniel ostroff", "peter", "dr ichak adizes", "1897", "honda", "stan coltrane", "Dunfermline", "cribbage", "midtown", "University of Texas", "stave", "osmium", "pear", "White Star Line", "stanley", "peptide bond", "William the Conqueror", "Aslan", "Gregory Carlton \" Greg\" Anthony", "a 10-year-old Chinese orphan", "Lowe's Companies, Inc.", "India", "Cash for Clunkers", "after Michael Jackson's death", "Nassau", "Maurice Jarre", "Degaussing", "10 Years"], "metric_results": {"EM": 0.5625, "QA-F1": 0.625}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-4762", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-3851", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-3414", "mrqa_triviaqa-validation-1862", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-7144", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-5332", "mrqa_naturalquestions-validation-3016", "mrqa_hotpotqa-validation-3119", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-3611"], "SR": 0.5625, "CSR": 0.5253623188405797, "EFR": 1.0, "Overall": 0.7203849637681159}, {"timecode": 69, "before_eval_results": {"predictions": ["v\u012dk-t\u00f4r\u2032\u0113-\u0259", "Ronald Searle", "dennis taylor", "Loki Laufey", "Avengers", "snakes", "insulin", "lilac", "Sandra Bullock", "a protrusion", "andes", "banshee", "hawaii", "high-elevation", "heraldry", "good life", "japan", "Ireland", "Sherlock Holmes", "Ida Noddack", "Dudley do- right", "vindaloo", "botswana", "bror", "mark Twain", "Holly Johnson", "beef", "khaki uniforms", "spain", "joseph w", "dunfermline athletic", "four", "joseph caiaphas", "penrhyn", "Australia", "African violet", "ourselves alone", "James Dean", "Eva Herzigov\u00e1", "drizzle", "chiropractic", "wicker man", "stieg Larsson", "james Woods", "orecchiette", "Harvard", "hypertext", "Croatia", "cete", "greyfriars", "Mr. chips", "John Locke", "1998", "Matt Monro", "comic", "Disha Patani", "USS \"Enterprise\"", "Ben Freeth", "Kenneth Cole", "Donald Duck", "a total eclipse", "Cher", "the Black Sea", "Crank Yankers"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6432291666666667}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2531", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-4485", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1322", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-7614", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-110", "mrqa_searchqa-validation-9522", "mrqa_searchqa-validation-14235"], "SR": 0.59375, "CSR": 0.5263392857142857, "retrieved_ids": ["mrqa_squad-train-13684", "mrqa_squad-train-80505", "mrqa_squad-train-80114", "mrqa_squad-train-3791", "mrqa_squad-train-22039", "mrqa_squad-train-23637", "mrqa_squad-train-59771", "mrqa_squad-train-56948", "mrqa_squad-train-10461", "mrqa_squad-train-31914", "mrqa_squad-train-52864", "mrqa_squad-train-74865", "mrqa_squad-train-22544", "mrqa_squad-train-79565", "mrqa_squad-train-72918", "mrqa_squad-train-17853", "mrqa_naturalquestions-validation-3373", "mrqa_triviaqa-validation-4313", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-5305", "mrqa_triviaqa-validation-6525", "mrqa_newsqa-validation-2668", "mrqa_triviaqa-validation-2608", "mrqa_naturalquestions-validation-9992", "mrqa_hotpotqa-validation-982", "mrqa_squad-validation-3296", "mrqa_triviaqa-validation-3807", "mrqa_hotpotqa-validation-2707", "mrqa_naturalquestions-validation-4326", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-4758"], "EFR": 0.9615384615384616, "Overall": 0.7128880494505495}, {"timecode": 70, "UKR": 0.7578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3054", "mrqa_hotpotqa-validation-306", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-975", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1461", "mrqa_squad-validation-147", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2564", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3473", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3923", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5884", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6670", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-6981", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7083", "mrqa_squad-validation-7094", "mrqa_squad-validation-7339", "mrqa_squad-validation-78", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-9002", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9344", "mrqa_squad-validation-9411", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1514", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5117", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6285", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.861328125, "KG": 0.50546875, "before_eval_results": {"predictions": ["the Philippines", "silurian", "Warwick Davis", "big eggo", "quila", "french", "perry", "lyon", "gold", "Tina Turner", "Sparks", "nissan", "wash", "mexico", "arvo p\u00e4rt", "Eric Coates", "st Pancras", "beer", "yonge", "cevennes", "Lady Gaga", "phil Glenister", "bcc", "1979", "Donald Trump", "pints", "Tomorrow Never Dies", "tea", "toptenz", "michael housewife", "bullfighting", "Autobahn", "Kiss Me, Kate", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Hindenburg", "Andre Agassi", "four Feather Falls", "Tangled", "spain", "Morrissey", "stockings", "cooperative", "smallpox", "her mother", "Leicester City", "violin", "nipples", "ernest hemingston", "Temple of Artemis", "abietic acid", "Achille Lauro", "Frank Langella", "anion", "a woman who had a sexual relationship with Paul", "Hawaii County, Hawaii", "Objectivism", "16,116", "president", "Daniel Radcliffe", "Eleven", "Anna Eleanor Roosevelt", "bone marrow", "Smilla", "France's top court"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6482843137254902}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8235294117647058, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4689", "mrqa_triviaqa-validation-5348", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-6736", "mrqa_triviaqa-validation-6183", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-5594", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-5521", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-2633", "mrqa_triviaqa-validation-2648", "mrqa_triviaqa-validation-1591", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-2372", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2351", "mrqa_hotpotqa-validation-4382", "mrqa_newsqa-validation-3688", "mrqa_searchqa-validation-15744", "mrqa_newsqa-validation-299"], "SR": 0.578125, "CSR": 0.527068661971831, "EFR": 1.0, "Overall": 0.7303356073943662}, {"timecode": 71, "before_eval_results": {"predictions": ["86400", "suez canal", "robert boyle", "spain", "Paris", "john poulson", "breadfruit", "1963", "r Richard Strauss", "lellard nimoy", "Sandi Toksvig", "louis le Vau", "robert david", "bette Davis", "australian", "bavaria", "the eastern flank of Arabah", "sandie shaw", "Ut\u00f8ya island", "lesley germany", "Ty Hardin", "b\u00e4umer", "42", "limestone", "Bristol Aeroplane", "charlie heen", "anita Brookner", "salty", "endometriosis", "James Stewart", "charlie glidden", "eight", "Pizza Express", "litehead", "Hugh Quarshie", "Billie Holiday", "Hindi", "st leger", "Eric Morley", "sandstone trail", "Assault on Precinct 13", "jennifer leep", "burthen", "mocha", "antelope", "relativistic mass", "no", "muskets", "bajan", "malted barley", "hoagland", "Jonny Buckland", "Stanley Tucci", "Janie Crawford", "teen volleyball", "Milk Barn Animation", "nursery rhyme", "Russian air force,", "President Nicolas Sarkozy and Canada's Prime Minister Stephen Harper", "100% of its byproducts", "Captains Courageous", "one", "kayak", "Wisconsin"], "metric_results": {"EM": 0.390625, "QA-F1": 0.49982638888888886}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.5555555555555556, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-64", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-2746", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-7290", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-4979", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-2128", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7629", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5018", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-564", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5346", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-3830"], "SR": 0.390625, "CSR": 0.5251736111111112, "EFR": 0.9743589743589743, "Overall": 0.7248283920940171}, {"timecode": 72, "before_eval_results": {"predictions": ["Cambridge", "contact lenses", "Poland", "Washington", "apple", "high jump", "desire", "germany", "port Talbot", "gargantua", "akis", "halloween", "Sydney", "Charlie Chaplin", "smell", "russell", "judy holliday", "cyprus", "mary connelly", "Yellowstone", "blue ivy", "christian and Patricia Ramsay", "Israel", "blackburn rovers", "1943", "Elizabeth Taylor", "daimler", "Dublin", "germany", "jenny", "antonia fraser", "peter stuyvesant", "south africa", "leonard armani", "bermany", "mark Twain", "surfer", "ever decreasing circles", "quito", "downward firing woofers", "garrett", "sandown", "a goat", "lady", "germany", "bb", "treacherous", "eleventh", "Kajagoogoo", "carol simon", "Robin Hood Airport", "March 12, 2013", "a song, written solely by Gaye", "into the intermembrane space", "Ronnie Schell", "La Familia Michoacana", "Starlite", "in the neighboring country of Djibouti,", "cervical cancer", "south America and Africa.", "Warsaw", "City Slickers", "out of", "Richa Sharma"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5984375}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1196", "mrqa_triviaqa-validation-3196", "mrqa_triviaqa-validation-4914", "mrqa_triviaqa-validation-7446", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-5510", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4215", "mrqa_triviaqa-validation-2410", "mrqa_triviaqa-validation-249", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-1625", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-180", "mrqa_hotpotqa-validation-1782", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-1370", "mrqa_newsqa-validation-2784", "mrqa_searchqa-validation-1212"], "SR": 0.546875, "CSR": 0.5254708904109588, "retrieved_ids": ["mrqa_squad-train-12994", "mrqa_squad-train-17394", "mrqa_squad-train-25126", "mrqa_squad-train-82161", "mrqa_squad-train-27892", "mrqa_squad-train-10269", "mrqa_squad-train-3544", "mrqa_squad-train-38908", "mrqa_squad-train-32648", "mrqa_squad-train-71281", "mrqa_squad-train-64112", "mrqa_squad-train-238", "mrqa_squad-train-9621", "mrqa_squad-train-85975", "mrqa_squad-train-66162", "mrqa_squad-train-47495", "mrqa_searchqa-validation-7494", "mrqa_triviaqa-validation-5848", "mrqa_newsqa-validation-1820", "mrqa_hotpotqa-validation-2446", "mrqa_newsqa-validation-1899", "mrqa_hotpotqa-validation-3335", "mrqa_newsqa-validation-2272", "mrqa_hotpotqa-validation-3141", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1889", "mrqa_triviaqa-validation-279", "mrqa_naturalquestions-validation-3505", "mrqa_triviaqa-validation-4750", "mrqa_naturalquestions-validation-2777", "mrqa_hotpotqa-validation-3392", "mrqa_naturalquestions-validation-3404"], "EFR": 0.9655172413793104, "Overall": 0.7231195013580538}, {"timecode": 73, "before_eval_results": {"predictions": ["greta garbo", "c\u00e9vennes", "lilo and stitch", "Tacitus", "George Best", "loki", "bagram", "pink", "charlie chaplin", "ostrich", "Ireland", "Louren\u00e7o Marques", "silk", "swaziland", "cartoons", "Dracula", "jaws", "dodo", "Imola", "albus white", "Brazil", "Thailand", "United States", "worcester", "poincar\u00e9 conjecture", "Superman", "wales", "rudolph", "michael mccullen", "The Equals", "baffin island", "woodstock", "molybdaena galena", "permian", "hungary", "apollon", "Matterhorn", "hallmark", "tide-wise", "Genesis", "trumpet", "South Carolina", "ourselves alone", "james chadwick", "coffee house", "Apocalypse Now", "pilgrimage", "boris becker", "althorp", "Pyrenees mountains", "noah", "Steveston Outdoor pool in Richmond, BC", "October 1, 2015", "near Flamborough Head", "25 November 2015", "Jesper Myrfors", "Vancouver", "Rod Blagojevich,", "Mary Procidano,", "opium", "King Charles III", "Minnesota", "quid", "Ugly Betty"], "metric_results": {"EM": 0.71875, "QA-F1": 0.734375}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-80", "mrqa_triviaqa-validation-1967", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-606", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-1499", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-4092", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-5018", "mrqa_newsqa-validation-3632", "mrqa_searchqa-validation-7502"], "SR": 0.71875, "CSR": 0.5280827702702703, "EFR": 1.0, "Overall": 0.730538429054054}, {"timecode": 74, "before_eval_results": {"predictions": ["jamaica", "septoe and son", "turkish republic", "london", "Halloween", "Compundyne", "russell", "phoibos", "selenium", "jamaica", "bathtub curve", "james napier", "porcupine grass", "macbeth", "Eton College", "geomagnetism", "Diego Garc\u00eda", "keeper of the Longstone (Fame Islands) lighthouse", "Waylon Jennings", "joseph bzelius", "phobos", "the Sphinx", "sensibly, maybe", "william mccartney", "pennsylvania state university", "Father Brown", "Henry Ford", "jet", "dihydrogen monoxide", "dennis", "russet", "alison krauss", "raspberries", "four", "neurons", "Poland", "banjo", "cricket", "time bandits", "the Hague", "One Foot in the Grave", "madison", "copper", "joseph tchaikovsky", "speed camera", "food, water, sleep, and warmth", "blue", "passport", "florence", "fancy dress shop", "jabba the Hutt", "the 15th century", "1937", "turkey", "business", "boxer", "The Handmaid's Tale", "Stanford University", "her family", "because the Indians were gathering information about the rebels to give to the Colombian military.", "George Babbitt", "Emanuel Swedenborg", "Nod", "Thorleif Haug"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5348958333333332}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-4919", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-1912", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-4433", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-3070", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-5126", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-4603", "mrqa_triviaqa-validation-53", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-957", "mrqa_naturalquestions-validation-2830", "mrqa_hotpotqa-validation-2388", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3332", "mrqa_naturalquestions-validation-2509"], "SR": 0.421875, "CSR": 0.5266666666666666, "EFR": 0.972972972972973, "Overall": 0.7248498029279279}, {"timecode": 75, "before_eval_results": {"predictions": ["rugby", "hyperbole", "North by Northwest", "danelaw", "mahatma Gandhi", "For Gallantry", "senators", "colette", "willow", "european", "Separate Tables", "perry Spencer", "Ulysses S. Grant", "1929", "carlle", "Antarctica", "hurt locker", "general douglas mac MacArthur", "carl", "zager & Evans", "c\u00e9vennes", "genesis", "Sirhan Sirhan", "tennis", "JeSuisCharlie", "judy gar Garland", "dark blood", "lowestoft", "Washington", "teaching evolution in violation of a Tennessee state law", "lulu", "erinyes", "faggots", "k2", "Angus Deayton", "david bowie", "Chester", "tchaikovsky", "faversham", "Jimmy Knapp", "uranus", "norway", "william wilson", "Corporal", "edward Woodward", "priests", "violins", "charlie taylor", "eucalyptus", "1883", "Herald of Free Enterprise", "3000 BC", "lacteal", "Renishaw Hall, Derbyshire, England", "Jefferson Memorial", "at least 96", "New Orleans, Louisiana", "JBS Swift Beef Company, of Greeley, Colorado,", "Silicon Valley.", "10", "Vlaimir Ilyich Lenin", "beta blockers", "ono", "anacle"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7168177308802308}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5856", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-7341", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-3649", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-7208", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-1020", "mrqa_triviaqa-validation-2328", "mrqa_naturalquestions-validation-10408", "mrqa_hotpotqa-validation-4154", "mrqa_hotpotqa-validation-1123", "mrqa_newsqa-validation-4133", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-8292"], "SR": 0.609375, "CSR": 0.5277549342105263, "retrieved_ids": ["mrqa_squad-train-83137", "mrqa_squad-train-58106", "mrqa_squad-train-45857", "mrqa_squad-train-7370", "mrqa_squad-train-85870", "mrqa_squad-train-16371", "mrqa_squad-train-52254", "mrqa_squad-train-73960", "mrqa_squad-train-63619", "mrqa_squad-train-47509", "mrqa_squad-train-3903", "mrqa_squad-train-64119", "mrqa_squad-train-73921", "mrqa_squad-train-55990", "mrqa_squad-train-65729", "mrqa_squad-train-32608", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-1290", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-6460", "mrqa_naturalquestions-validation-5304", "mrqa_hotpotqa-validation-4297", "mrqa_searchqa-validation-8085", "mrqa_hotpotqa-validation-5305", "mrqa_searchqa-validation-1664", "mrqa_triviaqa-validation-7120", "mrqa_naturalquestions-validation-7134", "mrqa_hotpotqa-validation-5791", "mrqa_newsqa-validation-4170", "mrqa_squad-validation-1028", "mrqa_newsqa-validation-2668", "mrqa_naturalquestions-validation-8355"], "EFR": 1.0, "Overall": 0.7304728618421052}, {"timecode": 76, "before_eval_results": {"predictions": ["Ginsburg", "propeller", "Joe Louis", "George Clooney", "Wyeth", "Louvre", "feminism", "potatoes", "Wallace and Gromit", "arc", "Mozambique", "the Blue Nile", "troy", "\"Timber!\"", "teeth", "TMV", "coconut", "Mattel", "the Tsardom of Russia", "Profumo", "Finland", "Making the Band", "pennies", "Colorado", "a stereoscopic form", "vowels", "the Library of Congress", "New Guinea", "Olav Trygvason", "Georgetown University", "kidney", "difference", "the Super Bowl XLVII", "Madison County", "Kennebunkport", "A Room with a View", "an eye", "Africa", "Ingenue", "Notre-Dame de Paris", "peer review", "Peppermint", "paul mcc McCartney", "Iberian", "bionic", "trip", "baccarat", "Drums", "Wallis Warfield Simpson", "grapevine", "Coco Chanel", "December 14, 2017", "Virginia Dare", "man", "puputan", "panama canal", "will Smith", "1950", "The Bonnie Banks o' Loch Lomond", "An aircraft", "Russia and China", "Alwin Landry's supply vessel", "Obama administration", "funchal"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7344494047619047}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-5372", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-737", "mrqa_searchqa-validation-9899", "mrqa_searchqa-validation-4056", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10756", "mrqa_searchqa-validation-15174", "mrqa_naturalquestions-validation-9781", "mrqa_triviaqa-validation-5219", "mrqa_hotpotqa-validation-2065", "mrqa_hotpotqa-validation-2730", "mrqa_newsqa-validation-2206"], "SR": 0.609375, "CSR": 0.528814935064935, "EFR": 0.96, "Overall": 0.722684862012987}, {"timecode": 77, "before_eval_results": {"predictions": ["the Graceland mansion", "Bob Fosse", "\"S\"", "Mexico", "Miles Davis", "Volleyball", "Havana", "Edwin Hubble", "Einstein", "Lhasa", "the U.S. Census Bureau", "New Kids on the Block", "Manila Bay", "Lady Chatterley's lover", "molasses", "Hard Knock Life", "a crumpet", "Douglas MacArthur", "Fred Thompson", "Sappho", "the Netherlands", "Texas", "Donald Trump", "the Hippocratic Oath", "the Taliban", "Solidarity", "Kookaburra", "the Battle of Hastings", "Tom", "the Arts & Crafts movement", "the Shift Key", "W. H. Auden", "Chuck Berry", "Cal Ripken", "diaphragm", "the Marquis de Sade", "Louis C. Tiffany", "a tornado", "the joker", "New Zealand", "a glove", "Jutland", "the Kindergarten Cop", "feet", "Titanic", "San Francisco", "Gulliver's Travels", "a carriage", "Billy Bathgate", "Richmond, Va.", "steel", "1988", "Cheryl Campbell", "Ohio newspaper on 8 February 1925", "Joe Brown", "funchal", "Virgil", "Timothy Dowling", "WANH", "southwest Denver, Colorado near Bear Creek.", "FBI Special Agent Daniel Cain,", "staff sergeant", "\"procedure on her heart,\"", "eastern part of Dekalb County"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7411458333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-1687", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-1963", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14160", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-11728", "mrqa_searchqa-validation-3821", "mrqa_searchqa-validation-388", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-12470", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-182", "mrqa_naturalquestions-validation-6665", "mrqa_hotpotqa-validation-5500", "mrqa_newsqa-validation-2547", "mrqa_hotpotqa-validation-5006"], "SR": 0.640625, "CSR": 0.5302483974358974, "EFR": 0.9565217391304348, "Overall": 0.7222759023132664}, {"timecode": 78, "before_eval_results": {"predictions": ["Michigan.", "a president who understands the world today, the future we seek and the change we need.", "5:20 p.m.", "Mobile County Circuit Judge Herman Thomas", "Top Gun", "Daniel Radcliffe", "Graham's wife", "Tennessee.", "Intensifying violence, food shortages and widespread drought", "asphyxiation and had two broken bones in his neck,", "Dubai", "gun", "Barack Obama", "Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "to hold on to your money,", "repression and dire economic circumstances.", "African National Congress", "more than 1.2 million people.", "Haiti.", "police", "Zuma", "TV", "US Airways Flight 1549", "estate with its 18th-century sights, sounds, and scents.", "drugging", "auxiliary lock", "The Rosie Show", "Between 1,000 and 2,000", "Tuesday", "English and Russian", "6-2", "fear of discomfort", "Diego Milito's", "\"Three Little Beers,\"", "romantic e-mails between her husband and his mistress", "Sen. Barack Obama", "military commissions are inherently illegitimate, unconstitutional and incapable of delivering outcomes we can trust,\"", "Friday.", "a judge to order the pop star's estate", "consumer confidence", "Afghanistan and India", "Hugo Chavez", "Expedia", "oceans are growing crowded,", "bronze", "two", "JBS Swift Beef Company, of Greeley, Colorado,", "250,000", "state senators who will decide whether to remove him from office", "Friday", "Araceli Valencia,", "veil", "the head of the United States Department of Justice", "China in American colonies without paying any taxes", "Arkansas", "Adam Smith", "ozone", "Manchester Victoria station", "communist", "1896", "the Salone", "Swift", "John Molson", "rachmaninov"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5393449567617367}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.9565217391304348, 0.0, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.7407407407407407, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.17391304347826086, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.5, 0.3076923076923077, 1.0, 0.0, 0.0, 0.43750000000000006, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-4205", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-2026", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-642", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-7615", "mrqa_hotpotqa-validation-375", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-3681", "mrqa_searchqa-validation-15735", "mrqa_triviaqa-validation-5099"], "SR": 0.390625, "CSR": 0.5284810126582278, "retrieved_ids": ["mrqa_squad-train-14774", "mrqa_squad-train-51263", "mrqa_squad-train-35948", "mrqa_squad-train-30664", "mrqa_squad-train-12876", "mrqa_squad-train-72974", "mrqa_squad-train-38415", "mrqa_squad-train-70205", "mrqa_squad-train-5831", "mrqa_squad-train-54637", "mrqa_squad-train-42939", "mrqa_squad-train-42613", "mrqa_squad-train-64609", "mrqa_squad-train-29669", "mrqa_squad-train-35669", "mrqa_squad-train-8617", "mrqa_triviaqa-validation-2332", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-7772", "mrqa_naturalquestions-validation-3658", "mrqa_triviaqa-validation-1261", "mrqa_hotpotqa-validation-5184", "mrqa_naturalquestions-validation-9076", "mrqa_hotpotqa-validation-3288", "mrqa_squad-validation-553", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-10934", "mrqa_squad-validation-7317", "mrqa_searchqa-validation-2337", "mrqa_triviaqa-validation-2787", "mrqa_hotpotqa-validation-4231"], "EFR": 0.9743589743589743, "Overall": 0.7254898724034404}, {"timecode": 79, "before_eval_results": {"predictions": ["an Italian and six Africans", "Daniel Radcliffe", "the remaining rebel strongholds in the north of Sri Lanka,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Samoa", "BET", "Iraq's autonomous region of Kurdistan.", "Adam Yahiye Gadahn,", "mental health and recovery.", "75.", "co-wrote", "2005.", "12", "Phil Spector", "Iran", "attempted robbery", "70,000", "severe flooding", "56", "frozen world located in the Gaslight Theater.", "AbdulMutallab,", "anesthetic and sedative.", "Saturday's Hungarian Grand Prix.", "Sub-Saharan Africa", "Manny Pacquiao", "Aung San Suu Kyi", "Jennifer Aniston, Rosario Dawson, Ginnifer Goodwin, Josh Holloway, Tracee Ellis Ross, Tony Shalhoub and Jeffrey Tambor.", "modern and classic designs", "seven-time Formula One world champion Michael Schumacher", "Austin Wuennenberg", "1983", "the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "Monterrey,", "Elena Kagan", "U.S. program to assassinate terrorists in Iraq.", "misdemeanor", "Thursday", "the man facing up, with his arms out to the side.", "golf", "Israel handed the United Nations Friday a report justifying its actions", "prostate cancer,", "Iraqi and U.S. soldiers were attacked by small-arms, machine-gun and RPG fire", "$1.45 billion", "people look at the content of the speech, not just the delivery.", "an eco videos", "strategy, plans and policy on the Army staff.", "walk on ice in Alaska.", "a city of romance, of incredible architecture and history.", "regulators in the agency's Colorado office", "Angela Merkel", "\"Nothing But Love\"", "number of times a pitcher pitches in a season", "Audrey II", "the benefits of the US privacy Act to Europeans and gives them access to US courts", "troposphere", "arthur ashe", "Hippo", "2004", "Tim \"Ripper\" Owens", "Brad Silberling", "Mother Vineyard", "Mars", "the Capitol", "Out - With"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6839530916494926}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 0.9523809523809523, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.4, 0.15384615384615383, 0.33333333333333337, 0.9411764705882353, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5263157894736842, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2835", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2938", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-1706", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-4441", "mrqa_searchqa-validation-15009", "mrqa_naturalquestions-validation-582"], "SR": 0.578125, "CSR": 0.5291015625, "EFR": 1.0, "Overall": 0.7307421875}, {"timecode": 80, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-512", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7035", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-984"], "OKR": 0.857421875, "KG": 0.525, "before_eval_results": {"predictions": ["Tyler \"Ty\" Mendoza", "\"Sara Crewe: or, What Happened at Miss Minchin's\"", "four", "professional footballer", "five", "Julie Taymor", "Greg Anthony", "Drifting", "Puli Alam", "Rebirth", "President's Volunteer Service Award", "Harpe brothers", "Bedknobs and Broomsticks", "Martin \"Marty\" McCann", "Yubin, Yeeun, Sunmi", "Herbert Ross", "Steve Kiley, M.D,", "melodic hard rock", "9 February 1971", "Christian Kern", "Taylor Swift", "SARS", "son of writer William F. Buckley Jr.", "1345 to 1377", "\"Cs\u00e1sz\u00e1ri \u00e9s Kir\u00e1lyi Hadsereg\"", "Noel", "India Today", "North Dakota", "2006", "\"Histoires ou contes du temps pass\u00e9\"", "Charles Reed Bishop", "MMA", "Yarrow and Stookey", "Mathieu Kassovitz", "E Elaine Atkins", "Summerlin, Nevada", "Jean-Marc Vall\u00e9e", "Klasky Csupo", "1950s", "Prussia", "Newfoundland and Labrador", "Tom Kartsotis", "Knowlton Hall", "shock", "Manhattan", "Professor Frederick Lindemann, Baron Cherwell", "dementia", "Sandy Bentley", "seven", "July 11, 2016", "\"Nina\"", "5 - 7", "40 %", "March 1, 2018", "trumpet", "Pink Panther", "G\u00e9rard Depardieu", "is a businessman, team owner, radio-show host and author.", "dozens", "14", "Rhizo", "Emperor Maximillian", "Lake Michigan", "\"Lust for Life\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.6517485119047619}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.28571428571428575, 1.0, 0.5, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-1975", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-399", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-4131", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-190", "mrqa_triviaqa-validation-1295", "mrqa_newsqa-validation-781", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-6921", "mrqa_triviaqa-validation-85"], "SR": 0.53125, "CSR": 0.5291280864197531, "EFR": 1.0, "Overall": 0.7327006172839506}, {"timecode": 81, "before_eval_results": {"predictions": ["the cornet", "The White Shadow", "Hungary", "HIV", "Nepal", "the second person", "Sanjaya", "Fauvism", "Dresden", "Turkish", "the Shirley Temple Story", "flavor Flav", "Mike Nichols", "backcountry", "blue blood", "acetylene", "the temperature", "Harriet the Spy", "Red Grange", "Cosmopolitan", "Amsterdam", "Cleveland", "Clyde", "James Naismith", "Harold Godwinson", "North Carolina", "Job", "the Visitor Globe", "Take Me", "pickles", "Stand by Me", "lead", "Nokia", "Bernard Malamud", "Cyprus", "the downhill race", "Neil Diamond", "Salt Lake City", "Babe Ruth", "wildebeest", "Sicilian", "Pirates of the Caribbean", "tuna", "Arts and Crafts", "Loam", "Subclue 2", "Uvula", "Biloxi", "Treasure Island", "Robots", "the trousseau", "language", "Robber baron", "NFL owners", "Jane Seymour", "pipaluk", "george bernard shaw", "Thomas Robert \"Tom\" Kitt", "Yewell Tompkins", "Wolfgang Amadeus Mozart", "House and Senate Republicans", "seven", "16", "Mildred,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6552083333333334}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-6682", "mrqa_searchqa-validation-6307", "mrqa_searchqa-validation-7539", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-203", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-14093", "mrqa_searchqa-validation-13991", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-1314", "mrqa_searchqa-validation-7545", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-15450", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-7174", "mrqa_searchqa-validation-2290", "mrqa_naturalquestions-validation-3840", "mrqa_triviaqa-validation-6075", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-4597", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4028"], "SR": 0.609375, "CSR": 0.5301067073170731, "retrieved_ids": ["mrqa_squad-train-52012", "mrqa_squad-train-58726", "mrqa_squad-train-62203", "mrqa_squad-train-38793", "mrqa_squad-train-61073", "mrqa_squad-train-67314", "mrqa_squad-train-40833", "mrqa_squad-train-78543", "mrqa_squad-train-73455", "mrqa_squad-train-10247", "mrqa_squad-train-60503", "mrqa_squad-train-78224", "mrqa_squad-train-32281", "mrqa_squad-train-53387", "mrqa_squad-train-22193", "mrqa_squad-train-12626", "mrqa_hotpotqa-validation-2465", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-2364", "mrqa_naturalquestions-validation-2241", "mrqa_squad-validation-6034", "mrqa_triviaqa-validation-1758", "mrqa_naturalquestions-validation-10319", "mrqa_searchqa-validation-9793", "mrqa_triviaqa-validation-3539", "mrqa_squad-validation-5491", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1120", "mrqa_squad-validation-8062", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-5877", "mrqa_searchqa-validation-14198"], "EFR": 1.0, "Overall": 0.7328963414634146}, {"timecode": 82, "before_eval_results": {"predictions": ["Jaws 2", "the leg", "Ovid", "rock", "a young squire", "Tudor", "Australia", "sugar", "sheep", "Washington, D.C.", "lily", "Hammurabi", "Isle of Wight", "gung ho", "Dale Earnhardt", "Johns Hopkins", "Jordan", "Lindsay Davenport", "North Africa", "a timpani", "Stephen Hawking", "the House and Senate", "an X-Ray", "Disturbia", "Michael Moore", "the Indianapolis 500", "I, Daniel Blake", "tapping", "an anchor", "Johannesburg", "carbon", "Philistines", "tremor", "Chevy", "Morocco", "Green M&M\\'s", "Den Bosch", "Neil Diamond", "Cardinal Richelieu", "Malaysia", "bionic", "Hamlet", "Lance Armstrong", "a steak", "Edith Wharton", "the Berlin Wall", "Uranus", "Jay Buhner", "telephone", "a bonnet", "Henry Moore", "wintertime", "the Great Crash", "Isabel Maru", "Brisbane Road", "vinegar Joe", "austria", "Girl Meets World", "three", "\"Pour le M\u00e9rite\"", "The meter reader", "Human Rights Watch", "used", "Don Draper"], "metric_results": {"EM": 0.625, "QA-F1": 0.6744791666666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-734", "mrqa_searchqa-validation-802", "mrqa_searchqa-validation-5353", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-15533", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-14165", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-1283", "mrqa_searchqa-validation-13445", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12259", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8837", "mrqa_naturalquestions-validation-2495", "mrqa_triviaqa-validation-5807", "mrqa_newsqa-validation-2964"], "SR": 0.625, "CSR": 0.53125, "EFR": 0.875, "Overall": 0.708125}, {"timecode": 83, "before_eval_results": {"predictions": ["Winnipeg Jets", "Dmitri Mendeleev", "calligraphy", "Duke Ellington", "Maria Sharapova", "Chile", "glow", "John Waters", "Aristophanes", "the Clean Air Act", "freelance", "Yahoo Answers", "Thurman Munson", "a barrel", "Chippewa", "Rooster Cogburn", "15", "Richard Burton", "gears", "a meringue", "The Dying Swan", "the Big Bang", "winter", "Alyssa Milano", "Tahiti", "Herbert Hoover", "Keith Urban", "an isosceles triangle", "Nick Naylor", "John Herschel Glenn Jr.", "the Netherlands", "Kelly Clarkson", "Michael Douglas", "aquiline", "Troy weight", "Neil Simon", "Richard Condon", "a trespasser", "Ronald Reagan", "Patrick Henry", "the light bulb", "the Peace Conference", "the viola", "Ostriches", "Joan Jett", "the American Mind", "America", "Ziploc", "Hannibal", "Anne Wiggins Brown", "Beethoven", "Gene Barry", "a heart rate that exceeds the normal resting rate", "American comedy - drama film directed by Fred Schepisi", "American Civil War", "chilpancingo", "Denise Richards", "India Today", "the Distinguished Service Cross", "non-binary", "raping and murdering a woman in Missouri.", "Jeanne Tripplehorn", "Zelaya and Roberto Micheletti,", "New York Islanders"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7260416666666667}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-4451", "mrqa_searchqa-validation-9367", "mrqa_searchqa-validation-7651", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-9041", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16490", "mrqa_searchqa-validation-1448", "mrqa_searchqa-validation-3952", "mrqa_searchqa-validation-3237", "mrqa_searchqa-validation-6308", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-3593", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-435"], "SR": 0.671875, "CSR": 0.5329241071428572, "EFR": 1.0, "Overall": 0.7334598214285715}, {"timecode": 84, "before_eval_results": {"predictions": ["Hoffmann", "Ford Fairlane", "an enzyme", "Grover\\'s Corner", "Lincoln\\'s", "topaz", "Universal City", "surrender", "Kathleen Winsor", "subtraction", "Harpy", "New York", "the capital is Annapolis", "fur", "Titan", "a crossword clue", "quick picks", "Fidel Castro", "the south side of the Kenai Peninsula", "a makrama", "Toy Story", "a fight", "the Tabernacle", "the Grand Cross", "Granite", "emperor", "Klondike", "a dove", "the Jet Propulsion Laboratory", "William Beanes", "Eminem", "Tarzan", "Diebold", "cheese", "New Guinea", "Queen Latifah", "the Liberty Bell", "anchovy", "a saint", "Clarence Thomas", "the seven day week", "nacreous", "a whimper", "Prison Break", "Iberian Peninsula", "the ceiling", "a kart", "Kilimanjaro", "Koala", "the Circus World Museum", "Extradition", "1979", "Lou LaRue", "Van Halen", "pasta", "his finger", "robert shumann", "Pacific Place", "1945", "the Walter Reed Army Medical Center (WRAMC)", "help the convicts find calmness in a prison culture fertile with violence and chaos.", "Polo", "Friday,", "2009"], "metric_results": {"EM": 0.421875, "QA-F1": 0.49062500000000003}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-3217", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-14803", "mrqa_searchqa-validation-12869", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-15747", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-12518", "mrqa_searchqa-validation-12852", "mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-9830", "mrqa_searchqa-validation-4630", "mrqa_searchqa-validation-8767", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-12943", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-7275", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-10541", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-5264", "mrqa_triviaqa-validation-7214", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-6024", "mrqa_hotpotqa-validation-3224", "mrqa_hotpotqa-validation-3149", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-1008", "mrqa_naturalquestions-validation-1856"], "SR": 0.421875, "CSR": 0.5316176470588235, "retrieved_ids": ["mrqa_squad-train-25551", "mrqa_squad-train-46030", "mrqa_squad-train-12115", "mrqa_squad-train-71741", "mrqa_squad-train-40589", "mrqa_squad-train-58848", "mrqa_squad-train-56751", "mrqa_squad-train-81674", "mrqa_squad-train-55319", "mrqa_squad-train-34759", "mrqa_squad-train-11063", "mrqa_squad-train-24587", "mrqa_squad-train-71297", "mrqa_squad-train-53263", "mrqa_squad-train-40721", "mrqa_squad-train-81731", "mrqa_newsqa-validation-1303", "mrqa_searchqa-validation-13154", "mrqa_hotpotqa-validation-303", "mrqa_triviaqa-validation-5918", "mrqa_triviaqa-validation-221", "mrqa_searchqa-validation-10881", "mrqa_hotpotqa-validation-723", "mrqa_triviaqa-validation-7594", "mrqa_triviaqa-validation-6092", "mrqa_searchqa-validation-13762", "mrqa_newsqa-validation-809", "mrqa_triviaqa-validation-4924", "mrqa_naturalquestions-validation-7967", "mrqa_hotpotqa-validation-732", "mrqa_searchqa-validation-15283", "mrqa_newsqa-validation-2296"], "EFR": 0.972972972972973, "Overall": 0.7277931240063593}, {"timecode": 85, "before_eval_results": {"predictions": ["11", "cancer", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "Picasso's muse and mistress, Marie-Therese Walter.", "a motor scooter", "Steve Jobs", "supermodel", "South Africa", "Missouri", "AbdulMutallab", "to kill members of the Zetas cartel whose area of influence includes the eastern state of Veracruz.", "between 5 and 10 knots an hour.", "three", "two immigrants, Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "8 p.m.", "acid attack", "Bowie", "\"The Kirchners have been weakened by this latest economic crisis,\"", "a number of calls,", "summer", "Tuesday afternoon.", "Peppermint oil, soluble fiber, and antispasmodic drugs", "Molotov cocktails, rocks and glass.", "Evan Bayh of Indiana and Virginia Gov. Tim Kaine", "April.", "2008", "\"fusion teams,\"", "three", "the L'Aquila earthquake,", "88", "Cash for Clunkers", "$199", "cancerous tumor.", "American", "CNN's \"Piers Morgan Tonight\"", "several weeks,", "$1.4 million,", "next year", "death", "the 11th year in a row.", "smashing particles into each other by sending two beams of protons around the tunnel in opposite directions.", "Saturn owners", "three empty vodka bottles,", "11 healthy eggs", "Alfredo Astiz,", "drug cartels", "some of the Awa", "two", "Sabina Guzzanti", "more than 4,000", "Robert Gates", "the 50 states of the United States of America", "Hanna Alstr\u00f6m", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "161-D-1", "Gaston Leroux", "Volkswagen", "\u00c6thelwald Moll", "Robert \"Bobby\" Germaine, Sr.", "white and orange", "an epiphyte", "Re- Animator", "a stride", "Girls' Generation"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6857992742551566}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3809523809523809, 0.11764705882352942, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.09523809523809525, 1.0, 1.0, 1.0, 0.6363636363636364, 1.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3186", "mrqa_naturalquestions-validation-3918", "mrqa_triviaqa-validation-164", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3537", "mrqa_searchqa-validation-9547"], "SR": 0.5625, "CSR": 0.5319767441860466, "EFR": 0.9285714285714286, "Overall": 0.718984634551495}, {"timecode": 86, "before_eval_results": {"predictions": ["cement", "the Cayman Islands", "Ferrari", "coax", "a sonnet", "waive", "China", "loverly", "economics", "Graceland", "funnel", "Beverly Hills", "Irish Coffee", "a live young chicken", "gasoline", "Newton", "Billy Budd", "John Brown", "the Communists", "Gene Krupa", "Skulls", "Cain", "Smashing Pumpkins", "a cruller", "I", "Kate \" Doc\" Barker", "Northanger Abbey", "Wyatt Earp", "Star Trek", "Mensa", "febreze", "a portrait", "a mutton", "Philip Seymour Hoffman", "a belief", "Wayne Gretzky", "amu", "Michael Irvin", "Gap", "salt", "Tower of London", "Arbor Day", "Westinghouse Electric Corporation", "a salad Dressing bottle", "The Fugitive", "Sisyphus", "Java", "Ponce de Len", "the Doppler shift", "Baroque", "the central lowlands of Holland and Belgium", "Pakistan", "961", "Reba McEntire and Linda Davis", "dodo", "northumberland", "Louis XVI", "July 16, 1971", "sitters", "9", "monarchy's", "Africa", "Trump", "The Expendables 2"], "metric_results": {"EM": 0.625, "QA-F1": 0.6892237103174603}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false], "QA-F1": [0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-8424", "mrqa_searchqa-validation-13637", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-5358", "mrqa_searchqa-validation-6460", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-10722", "mrqa_searchqa-validation-14681", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-6245", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-13204", "mrqa_searchqa-validation-2161", "mrqa_searchqa-validation-5579", "mrqa_searchqa-validation-15543", "mrqa_searchqa-validation-9723", "mrqa_searchqa-validation-5867", "mrqa_naturalquestions-validation-3672", "mrqa_hotpotqa-validation-680", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1586", "mrqa_hotpotqa-validation-4514"], "SR": 0.625, "CSR": 0.5330459770114943, "EFR": 1.0, "Overall": 0.7334841954022988}, {"timecode": 87, "before_eval_results": {"predictions": ["Washington, Jay and Franklin", "no more than 4.25 inches ( 108 mm )", "Y chromosome", "the Mandate of Heaven", "eleven", "the fourth ventricle", "Cody Fern", "five", "2007 and 2008", "New York City", "Schadenfreude", "Indian Standard Time", "a Nativity scene", "Johannes Gutenberg", "campus", "Rocky Dzidzornu", "Jennifer Grey", "Experimental neuropsychology", "Yosemite National Park", "April 3, 1973", "Sarah Silverman", "administrative supervision", "1997", "Emma Watson", "near Flamborough Head", "Big Boi and Sleepy Brown", "the President of India", "John J. Flanagan", "the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "the Senate and House each drafts and considers its own appropriation bill", "Session Initiation Protocol", "bowel obstruction, short bowel syndrome, gastroschisis, prolonged diarrhea regardless of its cause, high - output fistula, very severe Crohn's disease or ulcerative colitis, and certain pediatric GI disorders", "to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority '' ( Titus 2 : 15 )", "Tessa Peake - Jones", "Jennifer O'Neill as Hermie's mysterious love interest, and Katherine Allentuck and Christopher Norris as a pair of girls whom Hermie and Oscy attempt to seduce", "September 19 - 22, 2017", "the 2001 -- 2002 season", "1773", "Randy VanWarmer", "senators", "Teri Garr", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "13", "10.5 %", "Anne Murray", "longer", "Brad Dourif", "during a game in 1988", "Sanchez Navarro", "1985", "23 September 1889", "My Fair Lady", "Armageddon", "Thailand", "\u00c6thelstan", "Oregon", "\"Highwayman\"", "Israel", "Former Mobile County Circuit Judge Herman Thomas", "abduction of minors.", "Antnio Guterres", "Erin Go Bragh", "Art Garfunkel", "three"], "metric_results": {"EM": 0.625, "QA-F1": 0.7222273336347584}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true], "QA-F1": [0.4210526315789474, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.9473684210526316, 0.0, 1.0, 0.2631578947368421, 0.5925925925925926, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.24489795918367346, 1.0, 1.0, 0.0, 0.0, 1.0, 0.46153846153846156, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8439", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3080", "mrqa_searchqa-validation-3012"], "SR": 0.625, "CSR": 0.5340909090909092, "retrieved_ids": ["mrqa_squad-train-19510", "mrqa_squad-train-67906", "mrqa_squad-train-7898", "mrqa_squad-train-57102", "mrqa_squad-train-28053", "mrqa_squad-train-38194", "mrqa_squad-train-69413", "mrqa_squad-train-327", "mrqa_squad-train-73581", "mrqa_squad-train-54163", "mrqa_squad-train-44703", "mrqa_squad-train-66631", "mrqa_squad-train-62733", "mrqa_squad-train-34775", "mrqa_squad-train-13089", "mrqa_squad-train-68390", "mrqa_hotpotqa-validation-2222", "mrqa_newsqa-validation-2556", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-5514", "mrqa_naturalquestions-validation-10707", "mrqa_newsqa-validation-120", "mrqa_hotpotqa-validation-3393", "mrqa_naturalquestions-validation-6610", "mrqa_searchqa-validation-3830", "mrqa_searchqa-validation-15000", "mrqa_triviaqa-validation-5010", "mrqa_newsqa-validation-2272", "mrqa_triviaqa-validation-2823", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-7760", "mrqa_triviaqa-validation-493"], "EFR": 0.9583333333333334, "Overall": 0.7253598484848485}, {"timecode": 88, "before_eval_results": {"predictions": ["2013", "Triple Alliance of Germany, Austria - Hungary, and Italy", "John von Neumann", "Joanne Wheatley", "Alicia Vikander", "Some states, such as Taiwan, officially claim to hold continental territories but are de facto limited to control over islands", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Mickey Rourke", "4.37 light - years ( 1.34 pc )", "Coordinated Universal Time", "John Joseph Patrick Ryan", "12 to 36 months old", "1988", "Malayalam", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "the Old Testament", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "Kryptonite", "Shawn Wayans", "a crust of mash potato", "the New Testament", "digestive systems", "the employer", "an end - user", "1973", "the reactor core", "Nancy Jean Cartwright", "Germany", "total cost", "Justin Timberlake", "1978", "William Whewell", "1956", "G minor", "Thomas Middleditch", "V\u1e5bksayurveda", "Fix You", "2016", "during initial entry training", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Andy Kim", "2003", "Kirsten Simone Vangsness", "Ludacris", "vehicle", "tissues in the vicinity of the nose", "Yuzuru Hanyu", "Dan Aykroyd", "the RAF, Fighter Command", "1974", "National Industrial Recovery Act ( NIRA )", "Venus", "Bob Marley & the Wailers", "a ronseal phrase", "actress and model", "between the 8th and 16th centuries", "Blue Origin", "2,700-acre", "to provide security as needed.", "forcibly injecting them with psychotropic drugs", "Cleveland", "time", "Atchison", "The elections are slated for Saturday."], "metric_results": {"EM": 0.46875, "QA-F1": 0.6086364246876013}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.2, 1.0, 1.0, 0.5714285714285715, 0.0, 0.48275862068965514, 1.0, 0.7499999999999999, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5245901639344263, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2272", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-2908", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-5026", "mrqa_hotpotqa-validation-4797", "mrqa_newsqa-validation-2756", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-12140", "mrqa_searchqa-validation-11445", "mrqa_newsqa-validation-1133"], "SR": 0.46875, "CSR": 0.5333567415730337, "EFR": 0.9705882352941176, "Overall": 0.7276639953734303}, {"timecode": 89, "before_eval_results": {"predictions": ["Paul Lynde", "the naos", "in England", "420", "the fourth ventricle", "Emmett Lathrop `` Doc '' Brown", "Steve Russell", "if the occurrence of one does not affect the probability of occurrence of the other", "full '' sexual intercourse", "the Archies", "the government - owned Panama Canal Authority", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "a Native American nation from the Great Plains whose historic territory, known as Comancheria", "Jay Baruchel", "thirteen if Plank, a board of wood who acts as one character's imaginary friend", "In the early 20th century", "Thomas Hobbes in his Leviathan", "in the red bone marrow of large bones", "Sarah Silverman", "Janie Crawford", "New England", "3", "supervillains who pose catastrophic challenges to the world", "in 1932", "After World War II", "reproductive", "the NFL", "Biotic -- Biotic resources are obtained from the biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "1975", "the British colonists", "March 1995", "Austin, Texas", "Muhammad", "July 21, 1861", "Part 2", "1980 Summer Olympics boycott was one part of a number of actions initiated by the United States to protest the Soviet invasion of Afghanistan", "9 or 10 national ( significant ) numbers after the `` 0 '' trunk code", "David Joseph Madden", "December 12, 2017", "8 December 1985", "The British Indian Association", "in the barrier function of the skin and similar external epithelia", "on the microscope's stage", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "uvea", "he hosted a short - lived talk show in WCW called A Flair for the Gold", "1971", "Moton Field, the Tuskegee Army Air Field", "1995 Mitsubishi Eclipse", "Beijing", "sport utility vehicles", "Nowhere Boy", "madison", "glomerulonephilage", "Luigi Segre", "Ronald Lyle \" Ron Goldman", "Adrian Lyne", "543", "eight", "stay on track and get me through prison,\"", "a tank", "the Army of the Potomac", "Horn", "Basketball"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7389458978521479}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 0.6666666666666666, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07999999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-699", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-4133", "mrqa_hotpotqa-validation-2410", "mrqa_newsqa-validation-2027"], "SR": 0.671875, "CSR": 0.5348958333333333, "EFR": 0.9523809523809523, "Overall": 0.7243303571428571}, {"timecode": 90, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3812", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8093", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.8359375, "KG": 0.490625, "before_eval_results": {"predictions": ["leif Eriksson", "Tiananmen", "charlie", "December 7", "frauds", "Chrysler", "shakyamuni", "Real Madrid", "york", "Norman Hartnell", "A Beautiful Mind", "Red Admiral", "meadows", "equator", "Verona", "Ishmael", "Let It Snow!", "macbeth", "throw", "physics", "Poland", "Rapa Nui", "Peter Sellers", "febrile", "Milton Keynes", "Kuiper", "1954", "germania", "\u00c6thelthryth", "fishes", "Independence Day", "French", "Keane", "Sarkozy", "Inigo Montoya", "matt", "jack", "de canard", "website", "Helen Gurley Brown", "australia", "Brainy", "Exile", "1664", "Shanghai", "Stieg Larsson", "five", "pra", "Priam", "Denise van Outen", "argument", "775", "nasal septum", "Puerto Rico", "Dallas", "four months in jail", "December 25, 2009", "L'Aquila", "Janet and La Toya", "that he knew the owner of the home,", "nucleus", "rain", "I Will Remember You", "Bavarian"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6458333333333334}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-4163", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-7317", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-7092", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-4760", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-3004", "mrqa_naturalquestions-validation-9574", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-3057"], "SR": 0.59375, "CSR": 0.5355425824175823, "retrieved_ids": ["mrqa_squad-train-76874", "mrqa_squad-train-71692", "mrqa_squad-train-56734", "mrqa_squad-train-11425", "mrqa_squad-train-64750", "mrqa_squad-train-16548", "mrqa_squad-train-63247", "mrqa_squad-train-25629", "mrqa_squad-train-2483", "mrqa_squad-train-77450", "mrqa_squad-train-50137", "mrqa_squad-train-68516", "mrqa_squad-train-63915", "mrqa_squad-train-4566", "mrqa_squad-train-80188", "mrqa_squad-train-38480", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-720", "mrqa_triviaqa-validation-1931", "mrqa_hotpotqa-validation-3554", "mrqa_newsqa-validation-4161", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-204", "mrqa_triviaqa-validation-6164", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-2392", "mrqa_naturalquestions-validation-2969", "mrqa_searchqa-validation-9110", "mrqa_newsqa-validation-3940", "mrqa_triviaqa-validation-6532", "mrqa_searchqa-validation-3910", "mrqa_newsqa-validation-1"], "EFR": 0.9615384615384616, "Overall": 0.7155099587912088}, {"timecode": 91, "before_eval_results": {"predictions": ["the cat", "lusitania", "Bild", "japan", "blind side", "germany", "Imola", "two", "Herald of Free Enterprise", "bridge", "norway", "le Leicester", "Graeme Colquhoun", "ormolu", "yellow", "Burkina Faso", "mortadella", "Wembley", "farmer Phil Archer", "Telegraph", "palladium", "leander", "national militia", "aluminium", "dorset", "1698", "moretti", "Paris", "december", "eye", "Donald Trump", "ruritania", "a scarlet tanager", "lord nelson", "lily alan", "germany", "blofeld", "mozart", "head", "Cardiff", "one", "alberta", "de goya", "tla\u010denica", "jethro", "carousel", "Michael Hordern", "Mary Poppins", "quatermass experiment", "Benjamin Disraeli", "alla capella", "radioisotope thermoelectric generator", "disputes between two or more states", "2014", "Austral L\u00edneas A\u00e9reas", "a tragedy", "Eastern College Athletic Conference", "in the emergency room at LakeWest Hospital in Cleveland,", "The Cycle of Life", "July 23.", "a tartar sauce", "Enron", "selenium", "2010"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5987847222222222}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-684", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-4762", "mrqa_triviaqa-validation-3194", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-409", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-7708", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8092", "mrqa_hotpotqa-validation-2585", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2013", "mrqa_searchqa-validation-6143"], "SR": 0.53125, "CSR": 0.5354959239130435, "EFR": 0.9666666666666667, "Overall": 0.7165262681159421}, {"timecode": 92, "before_eval_results": {"predictions": ["maarten tromp", "indonesia", "boston", "Arizona Diamondbacks", "Dan Dare", "rudolph", "fat", "Singapore", "bird", "stanie musser", "Hebrew", "heisenberg", "carlsberg", "cumberland", "Billy Connolly", "spanish", "kiel Canal", "australia", "faggots", "Madison Square Garden", "jacob deaver", "jacob wren", "woven silk pyjamas", "croquet", "kinks", "spearchucker", "Cheetham Close", "stanley", "giraffe", "king arthur", "rinascimento", "south west africa", "st paul\\'s", "jacob moran", "sandals", "stanley", "haute", "archer", "maussa", "pashana bedhi", "kiki", "stanley taylor", "sheep", "st. Petersburg", "astronaut", "pirot", "three", "winds", "stanley", "Barbra Streisand", "kipps: The Story of a Simple Soul", "in the 2001 -- 2002 season", "the NFL", "a framework", "Melbourne Storm", "A Rush of Blood to the Head", "1993", "1994", "Derek Mears", "11", "a cable bridge", "Lake Titicaca", "apron", "Stratfor, a global intelligence company, has been hacking but it was unclear Monday whether the breach and apparent release of credit card information was the work of the activist hacking group anonymous."], "metric_results": {"EM": 0.5, "QA-F1": 0.5602678571428572}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-4506", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-4337", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6806", "mrqa_triviaqa-validation-3842", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-5230", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-525", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-1601", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-1507", "mrqa_newsqa-validation-406", "mrqa_searchqa-validation-4717", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-6744", "mrqa_newsqa-validation-3021"], "SR": 0.5, "CSR": 0.535114247311828, "EFR": 0.90625, "Overall": 0.7043665994623656}, {"timecode": 93, "before_eval_results": {"predictions": ["A", "lyon", "spain", "japan", "james gernner", "Elektra King", "flower", "kerry kitten", "lisping violet- Elizabeth Bott", "sinus node", "red", "darts checkout table", "Dutch", "bulgaria", "spain", "giambologna", "james bernushi", "gluteal region", "majorca", "12", "carry on leo", "Alexander Borodin", "hector blioz", "king urien", "spain", "st aidan", "jimmy davies", "richard seddon", "moles", "stiefbeen", "Prince Edward", "parma", "cryonic suspension", "sphinx", "takifugu rubripes", "sodor", "waverley", "william nelson", "giambologna", "wales", "Sicilia", "mexico", "travolta", "germany", "eleanora Olive", "Austria", "essex county cricket club", "k Kaiser Chiefs", "philistine", "nicolas cage", "philip catelinet", "an active supporter of the League of Nations", "April 1917", "Saint Alphonsa", "Takura Tendayi", "Whitesnake", "310", "Robert Barnett,", "artificial intelligence.", "The switch had been scheduled for February 17,", "an orchid", "Abu Simbel", "lira", "Joanna Moskawa"], "metric_results": {"EM": 0.375, "QA-F1": 0.4636295995670996}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2634", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-3910", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-4429", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-3070", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-1570", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6771", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-389", "mrqa_triviaqa-validation-4211", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-3276", "mrqa_newsqa-validation-1424", "mrqa_searchqa-validation-14194"], "SR": 0.375, "CSR": 0.5334109042553192, "retrieved_ids": ["mrqa_squad-train-43939", "mrqa_squad-train-79919", "mrqa_squad-train-86124", "mrqa_squad-train-37785", "mrqa_squad-train-16358", "mrqa_squad-train-78944", "mrqa_squad-train-27911", "mrqa_squad-train-27378", "mrqa_squad-train-15690", "mrqa_squad-train-16407", "mrqa_squad-train-17928", "mrqa_squad-train-14093", "mrqa_squad-train-84071", "mrqa_squad-train-12692", "mrqa_squad-train-51405", "mrqa_squad-train-46426", "mrqa_triviaqa-validation-1120", "mrqa_searchqa-validation-4905", "mrqa_triviaqa-validation-1236", "mrqa_newsqa-validation-4077", "mrqa_triviaqa-validation-2785", "mrqa_naturalquestions-validation-5133", "mrqa_searchqa-validation-5353", "mrqa_searchqa-validation-5289", "mrqa_squad-validation-5877", "mrqa_naturalquestions-validation-5656", "mrqa_triviaqa-validation-3362", "mrqa_naturalquestions-validation-4505", "mrqa_newsqa-validation-2327", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-816", "mrqa_squad-validation-9479"], "EFR": 0.95, "Overall": 0.7127759308510638}, {"timecode": 94, "before_eval_results": {"predictions": ["a quarter tone", "Frida Khalo", "the Kite Runner", "Pope John Paul II", "Louisa May Alcott", "Rock Island", "Turandot", "the Bolsheviks", "cloning", "Signs", "Edward", "forgery", "the Police", "a carrots", "Manhattan", "Rehab", "pen", "tap", "Ernie Banks", "Christopher Columbus", "Olivia Newton-John", "the Great White Way", "shrewd", "V.C. Andrews", "Peter Shaffer", "the Ubangi River", "(William) Harvey", "Reptilia", "a gizzard", "Bangkok", "Reform", "Catwoman", "bats", "Puccini", "Omaha", "the Monitor", "magnesium oxide", "silver", "saved the original Star Trek", "Takana", "the Silk Road", "dreams", "Braigo", "Jack Ruby", "Hairspray", "Duncan", "a palace", "type O", "Italy", "green", "Engelbert Humperdinck", "1940", "three", "January 11, 2014", "Velvet Revolution", "taka", "Hampton Court Palace", "Marvel Comics", "three", "Donald Wayne Johnson", "Bastian Schweinsteiger", "At least 88", "African-Americans", "rocket"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7317708333333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-10265", "mrqa_searchqa-validation-5390", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-14368", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-14428", "mrqa_searchqa-validation-3284", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-3999", "mrqa_naturalquestions-validation-4594", "mrqa_hotpotqa-validation-4578", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2068"], "SR": 0.6875, "CSR": 0.5350328947368421, "EFR": 1.0, "Overall": 0.7231003289473684}, {"timecode": 95, "before_eval_results": {"predictions": ["Clifford Roberts", "a bird", "Brooklyn", "mexico", "Peter Paul Rubens", "Tom (Tom) Parker", "Flanders", "the mu-koan", "cholesterol", "his only begotten Son", "the Miami Dolphins", "a saddlebags", "a sharpness", "Marcia Cross", "Northern Exposure", "Pocahontas", "Easy Rider", "the East River", "a blessing", "Ned Kelly", "Jakarta", "the Cherokee Nation", "Jim Bunning", "brood", "\"Jack\" Kennedy", "Arby\\'s", "Einstein", "a virus", "VICTor HUGO", "fudge", "a syllable", "\"Cattle prod\"", "\"The Gilder Lehrman", "stimulation", "an egg", "Ken Russell", "\"The Crucible\"", "the United Healthcare Workers East", "the zenith", "apogee", "Calais", "semaphore", "a reverse", "the Coors Field", "(Edgar) Rice", "the Union", "foragers", "a philosopher", "3.14159", "a monkey", "Kansas City", "1979", "Mockingjay -- Part 1 ( 2014 )", "Massachusetts", "Illinois", "South Africa", "Darth vader", "George Adamski", "five", "Marine", "two remaining crew members", "to add a \"black box\" label warning", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "black"], "metric_results": {"EM": 0.5, "QA-F1": 0.5916666666666667}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3709", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-4090", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-7622", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-4716", "mrqa_searchqa-validation-8919", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9198", "mrqa_searchqa-validation-3964", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-7194", "mrqa_naturalquestions-validation-1427", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1982", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3198"], "SR": 0.5, "CSR": 0.53466796875, "EFR": 0.96875, "Overall": 0.71677734375}, {"timecode": 96, "before_eval_results": {"predictions": ["Flickr", "Eric Angat", "air", "Leontyne Price", "a dragonfly", "Charles I", "Casey Kasem", "Puerto Rico", "a dog", "The Witch of Eastwick", "\"Buffalo Bill\" Cody", "The Name of the Rose", "May", "a cookie", "Cyrano de Guiche", "Alaska", "birds", "the European Union", "Giuseppe Verdi", "Matt Lauer", "the Kremlin", "\"How the firebrand Shi'ite cleric became a major power broker in the new Iraq\"", "frogs", "heracles", "a clerk", "Wales", "Austin City limits", "the Sacred Cod", "Agatha Christie", "get your house back", "Queen Esther", "a scratch", "New Kids on the Block", "Iraq", "country", "The Crucible", "Abraham Lincoln", "center of gravity", "Sherlock Holmes", "Simon Cowell", "potassium", "Lenin", "a fruitcake", "nests", "the Firebird", "Kansas", "a calculator", "Air France", "Louis Brandeis", "a tooth", "David", "the 1979 -- 80 season", "Dollree Mapp", "a quarterback", "Dick Whittington", "l Leeds", "gloster", "1828", "Elijah Wood", "Dizzy Dean", "The Ski Train", "Israel", "a rapist", "skirts"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6817708333333334}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-14494", "mrqa_searchqa-validation-13498", "mrqa_searchqa-validation-3088", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-1921", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-991", "mrqa_searchqa-validation-11974", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-5250", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-6912", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-3093", "mrqa_triviaqa-validation-6319", "mrqa_hotpotqa-validation-4621", "mrqa_newsqa-validation-1176"], "SR": 0.578125, "CSR": 0.5351159793814433, "retrieved_ids": ["mrqa_squad-train-7362", "mrqa_squad-train-23766", "mrqa_squad-train-7448", "mrqa_squad-train-43110", "mrqa_squad-train-67220", "mrqa_squad-train-73251", "mrqa_squad-train-19903", "mrqa_squad-train-9696", "mrqa_squad-train-79981", "mrqa_squad-train-8926", "mrqa_squad-train-3478", "mrqa_squad-train-52525", "mrqa_squad-train-16734", "mrqa_squad-train-55828", "mrqa_squad-train-31074", "mrqa_squad-train-58875", "mrqa_triviaqa-validation-3302", "mrqa_searchqa-validation-5730", "mrqa_hotpotqa-validation-3314", "mrqa_newsqa-validation-1424", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-3223", "mrqa_newsqa-validation-2817", "mrqa_squad-validation-1714", "mrqa_squad-validation-10445", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-1006", "mrqa_triviaqa-validation-3807", "mrqa_hotpotqa-validation-5184", "mrqa_triviaqa-validation-5650"], "EFR": 0.9629629629629629, "Overall": 0.7157095384688812}, {"timecode": 97, "before_eval_results": {"predictions": ["The All-New Blue Ribbon Cookbook", "a pig", "Fear of Flying", "War Admiral", "Abraham Lincoln", "a soot", "Duck", "Czech Republic", "David Bowie", "Maria Callas", "Buddhism", "Theodore Roosevelt", "a deluge", "Cold Mountain", "horror", "boxing", "A Night at the Roxbury", "King Henry II", "the Claddagh Ring", "Keith Richards", "the Lernaean Hydra", "(Albion) Davis", "the Bronx Zoo", "Christopher Darden", "the Lincoln Tunnel", "the albatross", "Bob Fosse", "a Dictum", "Georgia", "(Richard) Nixon", "Madame Tussaud", "Monsters", "Shakespeare", "Mother Jones", "King", "Hatfield and McCoy", "Walter Scott", "Pig Latin", "the Nile", "air traffic control", "a lamb", "Latin", "the New Wave", "Patrick Ewing", "Vienna", "Popular Science", "New Orleans", "a parody", "vegetables", "arteries", "Carol Burnett", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "expressing disagreement with the majority opinion of the court which gives rise to its judgment", "9 February 2018", "18", "Jeremy Thorpe", "blue", "A Song of Ice and Fire", "British Labour Party", "Balvenie Castle", "Animal Planet", "St. Louis, Missouri.", "The strawberry Family,\"", "ITV"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6538807189542484}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.47058823529411764, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-13412", "mrqa_searchqa-validation-15943", "mrqa_searchqa-validation-16528", "mrqa_searchqa-validation-11879", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-4255", "mrqa_searchqa-validation-16590", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-13547", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-2490", "mrqa_newsqa-validation-2474"], "SR": 0.546875, "CSR": 0.5352359693877551, "EFR": 1.0, "Overall": 0.723140943877551}, {"timecode": 98, "before_eval_results": {"predictions": ["Franklin, Indiana", "Nelson County", "47", "Sir Hiram Stevens Maxim", "15 October 1988", "5 February 1976", "Atlanta Hawks of the National Basketball Association (NBA)", "Hermione Youlanda Ruby Clinton-Baddeley", "45,698", "Andries Jonker", "Ashanti Region of Ghana", "Groupe PSA", "Texas Tech", "brigadier general", "Omega SA", "South Australia", "Apatosaurus", "Resorts World Genting", "Beatles", "British", "1950", "six", "Nayvadius DeMun Wilburn", "London", "Marcus T. Reynolds", "New York City", "The Toxics Release Inventory", "Figaro", "South African", "\"Apprendi v. New Jersey\" (2000)", "Bambi: Eine Lebensgeschichte aus dem Walde", "close to 50 million", "Whoopi Goldberg", "Transporter 3", "Disco", "Frederick I", "Afghanistan", "Thriller", "Antonio Lippi", "March 30, 2025", "English", "Azeroth", "Isabella II", "McG", "Vitor Vieira Belfort", "11 November 1821", "villanelle", "Emilia-Romagna Region in Northern Italy", "Who's That Girl", "Alan Young", "Cristiano Ronaldo", "Virginia Dare", "David Tennant", "produced with currently available resources", "Simeon Williamson", "Baffin", "boris", "Alwin Landry", "World leaders", "Sri Lankan", "hoist", "Banana", "Tallahassee", "Brooke Wexler"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6634424603174603}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.5, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-773", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-4088", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-3420", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-1102", "mrqa_searchqa-validation-3489"], "SR": 0.53125, "CSR": 0.5351957070707071, "EFR": 0.9666666666666667, "Overall": 0.7164662247474747}, {"timecode": 99, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5687", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-1037", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-1440", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15285", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15680", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3057", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1734", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2791", "mrqa_triviaqa-validation-2844", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.849609375, "KG": 0.51640625, "before_eval_results": {"predictions": ["Bob Mould", "1926", "Antonio Lippi", "1993", "The Allies of World War I", "Logan International Airport", "1979", "Academy Award in the category Best Sound", "The Suite Life of Zack & Cody", "Switzerland", "2017", "Stern-Plaza", "Pakistan", "jazz homeland section of New Orleans", "Marigold Newey", "Darkroom", "1972", "Royce da 5'9\" (Bad) and Eminem (Evil)", "evangelical Christian periodical", "Lionel Eugene Hollins", "water", "Harlem neighborhood", "Bardot", "Love Actually", "the Commanding General", "George Orwell", "five", "20 March to 1 May 2003", "1993", "imp My Ride", "1886", "The relations between Switzerland and the European Union (EU)", "Syracuse", "Godspell", "1755", "The Big Bang Theory", "1966", "John Paul \"Johnny\" Herbert", "Annales de chimie et de physique", "British", "punk rock", "Atlantic Ocean", "Adelaide Laetitia \" Addie\" Miethke", "Red", "Theodore Robert Cowell", "Matthew Ryan Kemp", "Epic Records", "Roslyn Castle", "2.1 million", "Rothschild banking dynasty", "Corendon Airlines", "Claudia Grace Wells", "Hugo Weaving", "Freedom Day", "disasters", "Henry Hudson", "geometry", "Adidas", "whether to close some entrances, bring in additional officers, and make security more visible,", "3rd Platoon, A Company, 2nd Light Reconnaissance Battalion", "Syria", "cherries", "Halloween", "bullnose"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7467147435897437}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-5238", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-1711", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-1822", "mrqa_naturalquestions-validation-9150", "mrqa_triviaqa-validation-1641", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1859", "mrqa_searchqa-validation-14664"], "SR": 0.65625, "CSR": 0.53640625, "retrieved_ids": ["mrqa_squad-train-44289", "mrqa_squad-train-21553", "mrqa_squad-train-14268", "mrqa_squad-train-37875", "mrqa_squad-train-52668", "mrqa_squad-train-53859", "mrqa_squad-train-72810", "mrqa_squad-train-19459", "mrqa_squad-train-19564", "mrqa_squad-train-23505", "mrqa_squad-train-40066", "mrqa_squad-train-85085", "mrqa_squad-train-2210", "mrqa_squad-train-76346", "mrqa_squad-train-60768", "mrqa_squad-train-5239", "mrqa_hotpotqa-validation-1508", "mrqa_naturalquestions-validation-922", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-3355", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-10131", "mrqa_triviaqa-validation-581", "mrqa_hotpotqa-validation-2300", "mrqa_searchqa-validation-15000", "mrqa_naturalquestions-validation-4792", "mrqa_searchqa-validation-10541", "mrqa_naturalquestions-validation-10225", "mrqa_newsqa-validation-1925", "mrqa_hotpotqa-validation-2358", "mrqa_naturalquestions-validation-7138", "mrqa_squad-validation-8526"], "EFR": 1.0, "Overall": 0.737125}]}