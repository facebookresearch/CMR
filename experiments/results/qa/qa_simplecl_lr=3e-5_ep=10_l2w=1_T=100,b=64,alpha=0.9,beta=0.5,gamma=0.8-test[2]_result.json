{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4030, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Jelme and Bo'orchu", "gauge bosons", "consumer prices", "Albert C. Outler", "a computational problem", "1521", "River Tyne", "Boston", "San Jose Marriott", "illegal boycotts", "Mitochondria", "bilaterians", "Alexandre Yersin", "Methodists today", "Beyonc\u00e9", "the Rhine and its downstream extension", "7\u20134\u20132\u20133", "Horniman Museum", "400 AD to 1914", "early 1526", "The individual is the final judge of right and wrong", "five", "Battle of B\u1ea1ch \u0110\u1eb1ng", "Time Lady", "oxygen-16", "The Day of the Doctor", "Sierra Sky Park", "James Clerk Maxwell", "Bill Clinton", "in areas its forces occupied in Eastern Europe", "20,000", "Queen Elizabeth II", "The Daleks", "gas turbines", "Newton", "Miasma theory", "Ealy", "several medals", "remaining in black and white", "computability theory", "autoimmune", "American Sweetgum", "Pleistocene epoch", "Feynman diagrams", "orange", "oxygen compounds", "four", "Fort Caroline", "counties or powiats", "chemical bonds", "2015", "France's claim to the region was superior to that of the British", "double", "helps many proteins bind the polypeptide", "Islamism", "lines or a punishment essay", "mercuric oxide", "released Islamists from prison and welcomed home exiles", "Washington and Thomas Gage", "The individual", "Bruno Mars", "the public", "Thomas Edison and George Westinghouse", "a freshwater lake"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8040364583333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6848", "mrqa_squad-validation-9923", "mrqa_squad-validation-100", "mrqa_squad-validation-9434", "mrqa_squad-validation-6966", "mrqa_squad-validation-7725", "mrqa_squad-validation-8538", "mrqa_squad-validation-1703", "mrqa_squad-validation-3511", "mrqa_squad-validation-3597", "mrqa_squad-validation-9567", "mrqa_squad-validation-6973", "mrqa_squad-validation-2025"], "SR": 0.796875, "CSR": 0.796875, "EFR": 1.0, "Overall": 0.8984375}, {"timecode": 1, "before_eval_results": {"predictions": ["the General Sejm", "232", "New Holland", "the \"Rhine knee\"", "the U.S. South", "the Schmalkaldic League", "January 1985", "an Executive Committee", "King Sancho VI of Navarre", "the Arizona Cardinals", "36", "Chloroplasts", "Sydney", "the Panic of 1901", "Muslim medicine", "the Silk Road", "silicon dioxide", "statocyst", "Several thousand", "the Fourth Intercolonial War and the Great War for the Empire", "medieval", "30\u201360% of Europe's total population", "the laws of physics", "the Ten Commandments", "the San Fernando Valley", "Roger NFL", "Hugh L. Dryden", "metals", "1.5 gigatons", "Denver Broncos", "\u00a31 of capital", "the 2010 series", "megaprojects", "1024-bit primes", "the portrait of Fran\u00e7ois, Duc d'Alen\u00e7on by Fran\u00e7ois Clouet, Gaspard Dughet", "wheat", "the Electorate of Saxony", "ice-sheets", "the lion, leopard, buffalo, rhinoceros, and elephant", "A plea of no contest is sometimes regarded as a compromise between the two", "seven", "Demaryius Thomas", "Napoleon's", "the Santa Clara Marriott", "kinematic measurements", "shipping", "12th", "helical thylakoid model", "the A69", "14%", "Thomas Edison", "Toshiba", "detention", "antigen presentation", "hunter's garb", "The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League", "British Gas plc", "Demaryius Thomas", "Homebrewing", "Joe Scarborough", "became a politician", "Gareth Jones", "Sivakumar, S. V. Subbaiah, Jayachitra, Srividya, Shubha, Kamal Haasan and Jayasudha", "Teresa Hairston"], "metric_results": {"EM": 0.75, "QA-F1": 0.7830815018315018}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.25, 1.0, 1.0, 0.0, 0.25000000000000006, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-1117", "mrqa_squad-validation-3664", "mrqa_squad-validation-85", "mrqa_squad-validation-4482", "mrqa_squad-validation-8978", "mrqa_squad-validation-5490", "mrqa_squad-validation-8446", "mrqa_squad-validation-8278", "mrqa_squad-validation-6914", "mrqa_squad-validation-7155", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-2275"], "SR": 0.75, "CSR": 0.7734375, "EFR": 0.9375, "Overall": 0.85546875}, {"timecode": 2, "before_eval_results": {"predictions": ["Ugali with vegetables, sour milk, meat, fish or any other stew", "TFEU article 294", "over $20 billion", "was a major source of water pollution", "unity of God", "all war", "1000 and 1900", "Gamal Abdul Nasser", "viniculture and tourism", "minor", "1162", "was lost in the 5th Avenue laboratory fire of March 1895", "ABC Cable News", "22 May 2006", "Germany and Austria", "Golden Gate Bridge", "the Welsh", "acquiring nutrients", "Muslims in the semu class", "the Chinese", "temperature and light", "12 May 1999", "1852", "the development of safety lamps", "stabilize the rest of the chloroplast genome", "The Mongols' extensive West Asian and European contacts", "24%", "Milton Friedman Institute", "Donald Davies", "three", "student motivation and attitudes towards school", "1560", "1891", "Lutheran views", "electron", "fear of their lives", "Science", "John Pell, Lord of Pelham Manor", "Cam Newton", "Osama bin Laden", "international drug suppliers", "President", "expelled Jews", "arid and semi-arid", "Yosemite Freeway", "Annan and his UN-backed panel and African Union chairman Jakaya", "The Warsaw Stock Exchange", "it becomes an Act of the Scottish Parliament", "certification by a recognized body", "a chain or screw stoking mechanism", "Battle of B\u1ea1ch \u0110\u1eb1ng (1288)", "silver", "1947", "1860", "The club will participate in the Premier League, FA Cup, EFL Cup (as holders), UEFA Champions League and UEFA Super Cup.", "Detroit, Michigan", "Emancipation Proclamation", "26,000", "Pakistan A", "Ricky Skaggs", "Saturday Night Live", "the last living pilot of the X-15 program", "Two new U.S. representatives are teaming up with CNN.com to report their \"Freshman Year\" experience through videos and commentaries", "250,000"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8260551948051948}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.9523809523809523, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.1818181818181818, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9343", "mrqa_squad-validation-9093", "mrqa_squad-validation-9388", "mrqa_squad-validation-5157", "mrqa_squad-validation-8399", "mrqa_squad-validation-4562", "mrqa_squad-validation-8383", "mrqa_squad-validation-9499", "mrqa_squad-validation-8222", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-368"], "SR": 0.765625, "CSR": 0.7708333333333334, "EFR": 1.0, "Overall": 0.8854166666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["10 February 1763", "good, clear laws, fairly and democratically", "shaping ideas about the free market", "SAP Center in San Jose", "older", "the university and military academy", "Foreign Protestants Naturalization Act", "inequality", "jigg TV", "three", "permafrost", "Silas B. Cobb", "the traditional salute of a knight winning a bout", "Jane Kim", "the Presiding Officer", "lipophilic alkaloid toxins", "one", "William Rainey Harper", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "June", "the Protestant cause with politics unpopular in France", "the 2007 election aftermath", "Ralph Woodward", "Susan Foreman", "clinical pharmacists", "teleforce", "British failures in North America, combined with other failures in the European theater", "300", "66 million years ago", "in southern China in 1865", "in commerce, schooling and government", "Krak\u00f3w", "France", "three", "power outage", "the preservation of public order is easier and more efficient than anywhere else", "Muslim and Chinese", "free trade", "15,100", "Cuba", "high pressure shock waves", "28,000", "21 to 11", "Cam Newton", "128,843", "Van Gend en Loos v Nederlandse Administratie der Belastingen", "four years", "Howard Keel", "half of the Pangaea supercontinent", "Michael Irwin\u2019s daughter, 60-year-old Christie Brinkley in a one-piece, and a woman raised by murderers.", "George Washington", "John Uhler Lemmon III", "six", "cabbage", "Charles, Eric Clapton, Bob Dylan and Johnny Cash", "Henry Kelly", "hedgehogs", "George IV", "The Time Machine", "the Granite City", "the natural world and mysticism", "more funds", "Brad Blauser", "$1.45 billion"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7076388888888889}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1831", "mrqa_squad-validation-4971", "mrqa_squad-validation-5975", "mrqa_squad-validation-6223", "mrqa_squad-validation-9570", "mrqa_squad-validation-3044", "mrqa_squad-validation-8326", "mrqa_squad-validation-4975", "mrqa_squad-validation-1830", "mrqa_squad-validation-4065", "mrqa_squad-validation-973", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-1148"], "SR": 0.65625, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 4, "before_eval_results": {"predictions": ["vote clerk", "estimated $200,000", "carbohydrates", "redistributive taxation", "the League of Nations", "two", "\"The Time of the Doctor\"", "Nairobi.", "Missy", "free", "7:00 to 9:00 a.m.", "Professor Richard ( Dick) Geary", "lipid monolayer", "2009", "whether a state or threat of war existed", "courts of member states", "Jin", "greater equality but not per capita income", "John Houghton", "carbohydrates", "immediately", "America's Funniest Home Videos", "42%", "19", "specialised education and training", "layered basaltic lava flows.", "October 2007", "Robert Maynard Hutchins", "Shoushi Li", "clerical marriage", "40%", "Kevin Harlan", "200 Troupes de la marine and 30 Indians.", "Half", "Independence Day: Resurgence", "duty", "complex silicates (in silicate minerals)", "Worldvision Enterprises", "hunter's garb", "Channel Islands", "\"Blue Harvest\" and \"420\"", "1225", "joseph", "gibuchadnezzar's dream for him.", "jackson", "\"Hey there Delilah, I know... God speed your love to me\"", "Octavian", "Cuba", "Warren E. Burger", "nouns in the first declension", "melodica", "melae Lander", "giant cephalopod", "billy jackson", "a pigeon named 'Narco Paloma' traffics drugs at a Costa Rica prison", "billy jackson", "rew jackson", "\"If people don't want to come out to the ballpark, nobody's going to stop them\"", "New York City", "Britomart", "Casey Beane", "Islamabad", "The Jefferson Memorial", "several days before Brisman's killing, Markoff allegedly robbed a 29-year-old woman at gunpoint at a Westin Hotel"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6705357142857142}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.05714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-2491", "mrqa_squad-validation-8234", "mrqa_squad-validation-3507", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-2118", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-14197", "mrqa_naturalquestions-validation-5040", "mrqa_newsqa-validation-839"], "SR": 0.65625, "CSR": 0.725, "EFR": 0.9545454545454546, "Overall": 0.8397727272727273}, {"timecode": 5, "before_eval_results": {"predictions": ["Justifying Grace", "neuronal dendrites", "an electrical generator", "Doctor Who", "coronary thrombosis", "his grandson", "San Francisco", "consumer prices", "2016", "as soon as they enter into force", "colonizing empires", "\"hockey stick graph\"", "OpenTV", "intuition", "1720", "around 300", "2001", "The Chase", "cortisol and catecholamines", "Economist Intelligence Unit", "the Decalogue (the Ten Commandments) and the Lord's Prayer", "Paramount Pictures", "Thomas Coke", "The Neighbors", "waldzither", "the United States", "\u20ac53,423", "the Helicosproidia", "to build their own dedicated networks", "2001", "the High Rhine", "Justin Tucker", "Colorado Springs", "1994", "Newton", "26", "University College London", "Jerricho Cotchery", "\"Bean farm\"", "the cube root of a negative number", "Willa Cather", "the nernstSimon statement", "Lewis and Clark", "Truman", "pope", "\"Nittany\"", "a Fokker", "Richter", "(2008)", "Ian Fleming", "the Titan", "The Thing", "manganese", "Ely", "the Bolshevik party", "the duke's honor", "Schiller", "The Tale of Genji", "\"The Daily Show\"", "Ant & Dec", "manganese", "Cherokee Nation", "manganese manigault-Stallworth", "UFC 50: The War of '04"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6311028079710145}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1739130434782609, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999]}}, "before_error_ids": ["mrqa_squad-validation-6267", "mrqa_squad-validation-9865", "mrqa_squad-validation-7827", "mrqa_squad-validation-2391", "mrqa_squad-validation-4874", "mrqa_squad-validation-5499", "mrqa_squad-validation-5214", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-8713", "mrqa_triviaqa-validation-3042", "mrqa_newsqa-validation-1583", "mrqa_hotpotqa-validation-1190"], "SR": 0.59375, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 6, "before_eval_results": {"predictions": ["ten", "", "Von Miller", "a downward pressure on wages", "Catholic", "nine", "11:28", "chest pains", "March 1896", "T cells", "economically", "reach locations not on the private network", "the college", "Yes\u00fcgei", "research", "toward the end of his life", "Bill Clinton", "Ollie Treiz", "esoteric", "San Andreas Fault", "30%\u201350%", "a double coronation", "ESPN", "p", "plantar fasciitis", "Peter Capaldi", "6000 Da", "Queen Victoria and Prince Albert", "cortisol and catecholamines", "Manakintown", "1985", "a stronger, tech-oriented economy", "stream capture", "the general number field sieve", "identity documents", "The Bronx County District Attorneys Office", "a woman", "Nothing But Love", "a man's lifeless, naked body", "the Sri Lankan cricket team", "a comprehensive detainees policy", "Cpl. Richard Findley", "8 p.m. local time Thursday", "the \"Jersey Shore\"", "diabetes and hypertension", "seven", "\"wow.\"", "Israel", "2009", "\"It has never been the policy of this president or this administration to torture.\"", "Himalayan", "Wednesday", "2,000 euros", "Siri", "88", "\"Sesame Street\"", "two", "NATO", "Representatives", "Bangladesh", "Argentinian", "the lion", "Department of Homeland Security", "Bessarabia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6165246212121213}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10413", "mrqa_squad-validation-6008", "mrqa_squad-validation-2122", "mrqa_squad-validation-9213", "mrqa_squad-validation-6696", "mrqa_squad-validation-3193", "mrqa_squad-validation-2835", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3491", "mrqa_naturalquestions-validation-3569", "mrqa_triviaqa-validation-2240", "mrqa_hotpotqa-validation-985", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-9754", "mrqa_searchqa-validation-8198"], "SR": 0.546875, "CSR": 0.6808035714285714, "EFR": 0.9655172413793104, "Overall": 0.8231604064039408}, {"timecode": 7, "before_eval_results": {"predictions": ["Blaydon Race", "The Central Region", "ten", "viral", "100\u2013150", "late 1886", "from \u00a318m to \u00a334m per year", "Alvaro Martin", "more efficient solutions", "Schedule 5", "BBC 1", "victory at Fort Niagara successfully cut off the French frontier forts further to the west and south.", "cantatas", "January 30", "seven", "\u20ac25,000 per year.", "St. Bartholomew's Day massacre", "9th", "principle of equivalence", "The Entertainment Channel", "when they improve society as a whole, including the poorest members", "1735", "the incentive for the democratic changes.", "The St. Johns River", "Life", "priest", "Jan Andrzej Menich", "Jane Kim", "around 70,000 BP", "biomass", "1562", "9 a.m.-1 p.m.", "Empire of the Sun", "22", "four", "Ross Perot", "Baghdad", "$1.45 billion", "in her home", "more than 2,800", "in a tenement in the Mumbai suburb of Chembur", "slip phones into the prison and hide them", "sexual assault on a toddler", "July 4.", "Evan Bayh", "Hu Jintao", "April 24", "the children were Sudanese orphans that it was trying to rescue from a war-torn nation.", "scientific reasons", "September 28, 1918", "the first or second week in April", "Pakistan", "Wajid Shamsul Hasan", "El Zocalo", "appealed against the punishment", "the new-car market", "Steve Williams", "Gil", "February", "Mickey's PhilharMagic", "(Republic/Universal)", "Oedipus Rex", "guitar feedback", "(Cd, Cs-137, Co, Pu-238, Ra, Sr, U-234, 235, 238)"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6948824612887113}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.846153846153846, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.18181818181818182, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2923", "mrqa_squad-validation-10269", "mrqa_squad-validation-2419", "mrqa_squad-validation-7564", "mrqa_squad-validation-9144", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2809", "mrqa_naturalquestions-validation-9707", "mrqa_triviaqa-validation-955", "mrqa_searchqa-validation-6233", "mrqa_hotpotqa-validation-3972"], "SR": 0.59375, "CSR": 0.669921875, "EFR": 0.9615384615384616, "Overall": 0.8157301682692308}, {"timecode": 8, "before_eval_results": {"predictions": ["\"nolo contendere\"", "5,984", "Jacksonville", "fish stocks to collapse by eating both fish larvae and organisms that would otherwise have fed the fish", "the applied force", "DuMont Television Network", "Amtrak San Joaquins", "teaching", "the traditional salute of a knight winning a bout.\"", "a ribosome in the cytosol", "British", "1887", "February 7, 2016", "Cargill Meat Solutions and Foster Farms", "Henry Plitt", "1978", "Von Miller", "about 0.7% of the human population's wealth", "water", "The Electronic Frontier Foundation", "the Tyndale Bible", "specialised education and training", "Il milione", "Guo Shoujing", "1908", "1560", "reason", "only \"essentials\"", "30,000", "a Taliban member", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "CNN/Opinion Research Corporation", "Al Gore.", "Rima Fakih", "his business dealings for possible securities violations", "summer", "the Dalai Lama's current \"middle way approach,\"", "the U.S. Holocaust Memorial Museum,", "a body", "Brian Mabry", "completely changed the business of music,", "more than 4,000", "consumer confidence", "autonomy", "1996", "1831", "Russia", "a vigilante group", "consistent and accessible.", "Arizona", "Muslim festival of Eid al-Adha.", "BET", "Daniel Cain,", "the use of torture and indefinite detention", "Mugabe's opponents", "Italian and six Africans", "tribute to pop legend Michael Jackson,", "China", "a locomotive", "Prime Minister Margaret Thatcher", "october", "1898", "about 375 miles ( 600 km ) south of Newfoundland", "18th century"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7216084956709956}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1212121212121212, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.16]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8960", "mrqa_squad-validation-7552", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-776", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-1352", "mrqa_naturalquestions-validation-8279", "mrqa_triviaqa-validation-2510", "mrqa_searchqa-validation-12759", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-3505"], "SR": 0.65625, "CSR": 0.6684027777777778, "EFR": 1.0, "Overall": 0.8342013888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["the Yassa", "An attorney", "The Quasiturbine", "s = \u22122, \u22124,...", "creates immunological memory", "three", "coal from Tyneside to Newcastle Quayside", "pseudorandom", "average teacher salaries", "the same message routing methodology as developed by Baran.", "The reigns of the later Yuan emperors were short and marked by intrigues and rivalries.", "Temecula and Murrieta", "antibodies", "Korean", "the revolution could only succeed in Russia as part of a world revolution.", "William of Orange", "the Horn of Africa", "lipid monolayer", "Hymn for the Weekend", "Colonel Monckton", "Gymnosperms don't require light to form chloroplasts.", "7 January 1943", "1.7 million", "ABC News Now", "Boomer Esiason and Dan Fouts", "Eintracht Frankfurt", "former U.S. secretary of state", "The allegations of ICE forcibly drugging deportees were raised last month by Sen. Joe Lieberman,", "10", "nearly 28 years of rule.", "Islamabad", "Haleigh Cummings,", "90", "the U.S. Ambassador to Zimbabwe James McGee", "Doral", "terminal brain cancer.", "accused of crushing a vicious Maoist insurgency but, in the process of restoring order, he corrupted and weakened Peru's most vital government institutions", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Leaders of more than 30 Latin American and Caribbean nations", "the Beatles", "Citizens are picking members of the lower house of parliament,", "The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "(l-r)", "The pop star has scheduled a news conference Thursday at London's O2 arena,", "led the weekend box office,", "Some truly mind-blowing structures are being planned for the Middle East.", "Daryeel Bulasho Guud", "United Arab Emirates", "Republican Gov. Bobby Jindal", "Polo because \"it was the sport of kings.", "The military fired warning shots into the air and sprayed water cannons to disperse the crowd.", "made out of either heavy flannel or wool", "The ensemble cast includes Patricia Clarkson, Rosario Dawson, Ginnifer Goodwin, Josh Holloway, Tracee Ellis Ross, Tony Shalhoub and Jeffrey Tambor.", "The National Infrastructure Program", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "Pakistan", "William Strauss and Neil Howe", "Transvaginal ultrasonography", "The Man with the Golden Gun (1974), The Spy Who Loved Me (1977), Moonraker (1979), For Your Eyes Only (1981), Octopussy (1983) and A View to a Kill (1985)", "Ben Hogan", "400", "\"Nebo Zovyot\"", "the Chesapeake Bay", "the list of dos & don'ts,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5480685027658279}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.06896551724137931, 1.0, 0.125, 1.0, 0.6153846153846153, 0.0, 0.0, 0.23529411764705885, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.1714285714285714, 0.6666666666666666, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5113", "mrqa_squad-validation-8132", "mrqa_squad-validation-10128", "mrqa_squad-validation-3304", "mrqa_squad-validation-9912", "mrqa_squad-validation-8880", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2879", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2016", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-2219"], "SR": 0.46875, "CSR": 0.6484375, "EFR": 1.0, "Overall": 0.82421875}, {"timecode": 10, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3412", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4048", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-100", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1886", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-8001", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9754", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10128", "mrqa_squad-validation-10155", "mrqa_squad-validation-10162", "mrqa_squad-validation-10167", "mrqa_squad-validation-1018", "mrqa_squad-validation-10198", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10269", "mrqa_squad-validation-10272", "mrqa_squad-validation-1029", "mrqa_squad-validation-103", "mrqa_squad-validation-10310", "mrqa_squad-validation-10315", "mrqa_squad-validation-10326", "mrqa_squad-validation-10345", "mrqa_squad-validation-1036", "mrqa_squad-validation-10380", "mrqa_squad-validation-10413", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10476", "mrqa_squad-validation-1048", "mrqa_squad-validation-1053", "mrqa_squad-validation-1088", "mrqa_squad-validation-1097", "mrqa_squad-validation-1119", "mrqa_squad-validation-1131", "mrqa_squad-validation-1197", "mrqa_squad-validation-1222", "mrqa_squad-validation-1231", "mrqa_squad-validation-1255", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-139", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1521", "mrqa_squad-validation-1537", "mrqa_squad-validation-1546", "mrqa_squad-validation-1561", "mrqa_squad-validation-1592", "mrqa_squad-validation-1611", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1831", "mrqa_squad-validation-1834", "mrqa_squad-validation-1876", "mrqa_squad-validation-1940", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-20", "mrqa_squad-validation-2048", "mrqa_squad-validation-2048", "mrqa_squad-validation-2087", "mrqa_squad-validation-2116", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2188", "mrqa_squad-validation-2235", "mrqa_squad-validation-2250", "mrqa_squad-validation-2374", "mrqa_squad-validation-239", "mrqa_squad-validation-2391", "mrqa_squad-validation-2403", "mrqa_squad-validation-2419", "mrqa_squad-validation-2422", "mrqa_squad-validation-2447", "mrqa_squad-validation-2462", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2580", "mrqa_squad-validation-2640", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2723", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-2797", "mrqa_squad-validation-282", "mrqa_squad-validation-2835", "mrqa_squad-validation-2848", "mrqa_squad-validation-2870", "mrqa_squad-validation-2873", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-30", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3048", "mrqa_squad-validation-3084", "mrqa_squad-validation-3086", "mrqa_squad-validation-3141", "mrqa_squad-validation-316", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3299", "mrqa_squad-validation-3304", "mrqa_squad-validation-3309", "mrqa_squad-validation-3319", "mrqa_squad-validation-3358", "mrqa_squad-validation-3368", "mrqa_squad-validation-3390", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3511", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3849", "mrqa_squad-validation-3932", "mrqa_squad-validation-3948", "mrqa_squad-validation-4032", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4165", "mrqa_squad-validation-4176", "mrqa_squad-validation-4186", "mrqa_squad-validation-4248", "mrqa_squad-validation-4265", "mrqa_squad-validation-4274", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4413", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4482", "mrqa_squad-validation-4488", "mrqa_squad-validation-4493", "mrqa_squad-validation-4562", "mrqa_squad-validation-4611", "mrqa_squad-validation-4623", "mrqa_squad-validation-4627", "mrqa_squad-validation-465", "mrqa_squad-validation-4698", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-4971", "mrqa_squad-validation-4976", "mrqa_squad-validation-501", "mrqa_squad-validation-506", "mrqa_squad-validation-5079", "mrqa_squad-validation-5113", "mrqa_squad-validation-5133", "mrqa_squad-validation-5150", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5214", "mrqa_squad-validation-5230", "mrqa_squad-validation-5295", "mrqa_squad-validation-5343", "mrqa_squad-validation-5355", "mrqa_squad-validation-5457", "mrqa_squad-validation-5478", "mrqa_squad-validation-5490", "mrqa_squad-validation-5499", "mrqa_squad-validation-55", "mrqa_squad-validation-5544", "mrqa_squad-validation-5563", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5642", "mrqa_squad-validation-5664", "mrqa_squad-validation-567", "mrqa_squad-validation-5698", "mrqa_squad-validation-5708", "mrqa_squad-validation-5762", "mrqa_squad-validation-5820", "mrqa_squad-validation-5835", "mrqa_squad-validation-586", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5978", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6008", "mrqa_squad-validation-6011", "mrqa_squad-validation-6079", "mrqa_squad-validation-6109", "mrqa_squad-validation-6124", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-616", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6223", "mrqa_squad-validation-6247", "mrqa_squad-validation-6267", "mrqa_squad-validation-6273", "mrqa_squad-validation-6284", "mrqa_squad-validation-6350", "mrqa_squad-validation-6362", "mrqa_squad-validation-6382", "mrqa_squad-validation-6421", "mrqa_squad-validation-6452", "mrqa_squad-validation-6475", "mrqa_squad-validation-6509", "mrqa_squad-validation-6535", "mrqa_squad-validation-6561", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6643", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6869", "mrqa_squad-validation-6879", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-7021", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7062", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7250", "mrqa_squad-validation-7306", "mrqa_squad-validation-7474", "mrqa_squad-validation-7521", "mrqa_squad-validation-7540", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7591", "mrqa_squad-validation-7592", "mrqa_squad-validation-7598", "mrqa_squad-validation-7653", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7733", "mrqa_squad-validation-7738", "mrqa_squad-validation-7751", "mrqa_squad-validation-7758", "mrqa_squad-validation-7775", "mrqa_squad-validation-778", "mrqa_squad-validation-7827", "mrqa_squad-validation-7842", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7937", "mrqa_squad-validation-7941", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8023", "mrqa_squad-validation-8028", "mrqa_squad-validation-8066", "mrqa_squad-validation-813", "mrqa_squad-validation-8132", "mrqa_squad-validation-8174", "mrqa_squad-validation-8213", "mrqa_squad-validation-8221", "mrqa_squad-validation-8222", "mrqa_squad-validation-824", "mrqa_squad-validation-8298", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8436", "mrqa_squad-validation-8446", "mrqa_squad-validation-8458", "mrqa_squad-validation-8466", "mrqa_squad-validation-8475", "mrqa_squad-validation-85", "mrqa_squad-validation-8505", "mrqa_squad-validation-8507", "mrqa_squad-validation-8533", "mrqa_squad-validation-8538", "mrqa_squad-validation-855", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8606", "mrqa_squad-validation-8636", "mrqa_squad-validation-8656", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8790", "mrqa_squad-validation-8790", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8836", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8880", "mrqa_squad-validation-890", "mrqa_squad-validation-8941", "mrqa_squad-validation-8960", "mrqa_squad-validation-8962", "mrqa_squad-validation-8978", "mrqa_squad-validation-9008", "mrqa_squad-validation-9101", "mrqa_squad-validation-9144", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9297", "mrqa_squad-validation-9308", "mrqa_squad-validation-9343", "mrqa_squad-validation-9388", "mrqa_squad-validation-9431", "mrqa_squad-validation-9470", "mrqa_squad-validation-9499", "mrqa_squad-validation-9567", "mrqa_squad-validation-9638", "mrqa_squad-validation-9661", "mrqa_squad-validation-9692", "mrqa_squad-validation-973", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9865", "mrqa_squad-validation-9912", "mrqa_squad-validation-9923", "mrqa_squad-validation-9935", "mrqa_squad-validation-9975", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-955"], "OKR": 0.927734375, "KG": 0.41484375, "before_eval_results": {"predictions": ["France", "Turkey", "the League of Augsburg", "the Marburg Colloquy", "1951", "nearly three hundred years", "enter the priesthood", "32%", "Southwest Fresno", "receptions, gatherings or exhibition purposes", "marry one of his wife's ladies-in-waiting", "Prime numbers", "a six membraned chloroplast", "Timucua", "capturing prey", "the Presiding Officer", "1521", "around 5 million,", "a supervisory church body", "the member state cannot enforce conflicting laws,", "Ed Asner", "an alphabet", "\"Quiet Nights,\"", "Friday,", "\"momentous discovery\"", "We Found Love", "five", "\"It was a comment that shouldn't have been made and certainly one that he wished he didn't make.\"", "Karl Kr\u00f8yer", "Austin Wuennenberg,", "133", "\"Barbarian Queen\" and \" Amazon Women on the Moon.\"", "\"Mad Men\"", "the area where the single-engine Cessna 206 went down,", "water continues flow through the river channel and not spread out over land.", "\"Doogie Howser, M.D.\"", "Mutassim, and his former defense minister, Abu Baker Yunis.", "\"I deal with families who lose their babies and I will cry with them,", "The Palestinian Islamic Army, which has links to al Qaeda, claimed responsibility for what was the deadliest attack on Christians in Egypt in some time -- but far from the only one.", "ambassadors", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.\"", "the picturesque Gamla Vaster neighborhood", "17", "said she also told FBI agents Lisa's parents never mentioned anyone wanting to harm them.\"", "\"Five\" tells stories of different women coping with breast cancer in five vignettes.", "make an emotional connection to their lost loved ones.", "\"novel\"", "the Frank case seemed to press every hot-button issue of the time: North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "Fullerton, California,", "the BBC's central London offices", "45 minutes, five days a week.", "his past and his future", "Arab Emirates", "Lashkar-e-Tayyiba", "July", "a U.S. military helicopter", "San Antonio", "February 6, 2005", "Falkland Islands,", "the concentration of H+ is 1.00 \u00d7 10-7,", "\"Lions for Lambs\"", "Dana Andrews", "tests", "a Rock and Roll Band"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6188586528361345}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 0.0, 0.19999999999999998, 0.09523809523809523, 0.0625, 1.0, 0.08333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7058823529411764, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9047", "mrqa_squad-validation-2468", "mrqa_squad-validation-4272", "mrqa_squad-validation-1676", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-2195", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-976", "mrqa_newsqa-validation-1170", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-5158", "mrqa_hotpotqa-validation-1398", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-15716"], "SR": 0.5625, "CSR": 0.640625, "EFR": 0.9642857142857143, "Overall": 0.7465290178571429}, {"timecode": 11, "before_eval_results": {"predictions": ["manned lunar landings", "The Better Jacksonville Plan", "the perceived difficulty of its tune.", "zero", "Hymn for the Weekend", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "\"Blue Harvest\" and \"420\",", "$105 billion", "Samarkand", "27", "an occupancy permit", "1774", "circuit switching", "TEU articles 4 and 5", "plastoglobulus", "Pakistan", "San Jose State", "the mouth and pharynx", "automatically assigned addresses, updated the distributed namespace, and configured any required inter-network routing.", "erosion", "Jada,", "African National Congress Deputy President", "12-hour-plus shifts", "Kit of Elsinore", "28", "the chest cavity.", "Kim Clijsters", "not feel Misty Cummings has told them everything she knows.", "Sub-Saharan Africa", "the North Korean regime intends to fire a missile toward Hawaii", "the man was dead,", "Dr. Jennifer Arnold and husband Bill Klein,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "North Korean Foreign Ministry spokesman described U.S. Vice President Dick Cheney as a \"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "his parents", "International Polo Club Palm Beach", "an antihistamine", "UNICEF", "Casey Anthony,", "a skilled hacker", "Leo Frank,", "the west African nation", "Cash for Clunkers", "Turkey,", "teary Native Americans", "everyone can use solar and renewable energy at home everyday,\"", "41,280 pounds", "Natalie Cole", "not", "Fayetteville, North Carolina.", "25 dead", "Rolling Stone", "shark River Park", "Kitima Canutt", "Arizona", "The Man", "to ensure party discipline in a legislature", "The wood", "La Toya", "Lincoln Memorial University", "Ant Timpson, Ted Geoghegan and Tim League.", "Tawny Frogmouth", "sack- cloth", "over a 20 - year period"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6842413662726163}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.4, 0.4444444444444445, 0.0, 0.23076923076923078, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.9090909090909091, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-6248", "mrqa_squad-validation-4789", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3035", "mrqa_naturalquestions-validation-8434", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-319", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-2726", "mrqa_naturalquestions-validation-7253"], "SR": 0.546875, "CSR": 0.6328125, "EFR": 1.0, "Overall": 0.7521093750000001}, {"timecode": 12, "before_eval_results": {"predictions": ["green spaces", "to make the hosts responsible for reliable delivery of data, rather than the network itself,", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "15 June 1899", "Ismail El Gizouli,", "by department", "the Rhine-Ruhr region", "the trans-Atlantic wireless telecommunications facility", "a bachelor's degree", "two", "Death wish Coffee", "The French Protestant Church of London", "Edward Burne-Jones", "Francisco de Orellana", "\"missing self.\"", "Bruno Mars", "Sybilla of Normandy,", "420,000", "plate tectonics", "the liver and kidneys", "InterContinental Hotels Group", "Walter Brennan", "Warren Hastings", "Poems : Series 1", "April 1917", "the New Testament", "Aristotle", "Five years later", "a No. 16 seed", "2015", "April 2, 2018", "that men use violence within relationships to exercise power and control", "Zoe", "16 June", "Lituya Bay in Alaska", "Thomas Jefferson", "1932 Games", "HTTP / 1.1", "Labour Party", "Roger Dean Stadium", "May 2002", "light skin", "a ranking used in combat sports", "the portal tomb", "the team", "the Isthmus of Corinth", "159", "Missouri River", "Brazil", "318", "Kim Basinger", "Domhnall Gleeson", "the chest, back, shoulders, torso and / or legs", "the skin", "religious Hindu musical theatre styles", "jackson", "duck", "Super Junior's", "Nanyue", "18", "Iran", "power play", "the New York City Russian-Jewish community", "drew their"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6261658653846154}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.24000000000000002, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1040", "mrqa_squad-validation-4786", "mrqa_squad-validation-1384", "mrqa_squad-validation-5422", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-976", "mrqa_triviaqa-validation-6508", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-4596", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-6195", "mrqa_searchqa-validation-3679", "mrqa_searchqa-validation-9108"], "SR": 0.5625, "CSR": 0.6274038461538461, "EFR": 0.9642857142857143, "Overall": 0.7438847870879121}, {"timecode": 13, "before_eval_results": {"predictions": ["the Ominde Commission", "2,000", "2005", "the university's off-campus rental policies", "seven", "Thomas Edison and George Westinghouse", "southern Suriname", "a doctor or to obtain medications which their doctors were unwilling to prescribe", "automobiles", "the first NASA scientist astronaut to fly in space,", "Cam Newton", "KGPE", "the difference in potential energy", "prime elements and prime ideals", "Julia Butterfly Hill", "five", "a coomb or combe", "ViennaVienna", "40", "Sir John Nott", "Spice Girls", "the hose", "Sandi Toksvig, 57,", "Salvador Allende", "Paris", "Arkansas", "\"The Blind Side\"", "a negative effect on your quality of life", "Dennis Potter", "Burma", "peregrines", "pungency", "leucistic female", "MauritaniaMauritania", "the piano", "corey", "James Carville", "Charlie Sheen", "pungent", "sheep", "Santiago", "the Republic of Upper Volta", "Laurie Lee", "Karl Marx and Friedrich Engels", "John Mortimer", "Beaujolais", "Humphrey Bogart", "Bon Jovi,", "Kansas", "Amy", "Carl Sagan and his wife and co-writer, Ann Druyan", "Alex Turner", "a St. Tropez drag-show nightclub", "commitment", "his brother", "2001 -- 2002 season", "\"Back to December\"", "Mike Greenwell", "an independent homeland since 1983.", "a man had been stoned to death by an angry mob.", "\"brindled\" (14... a fabric with a watered pattern, especially silk or taffeta", "a doctor that specializes in", "a 1992 American action-thriller film directed by Andrew Davis and written by J.F. Lawton", "International Boxing Federation"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6536728896103896}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-6342", "mrqa_squad-validation-9087", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-4630", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-530", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-14224", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-47"], "SR": 0.609375, "CSR": 0.6261160714285714, "EFR": 0.96, "Overall": 0.7427700892857143}, {"timecode": 14, "before_eval_results": {"predictions": ["Iberia", "that contemporary accounts were exaggerations", "lectures", "Victoria", "erosion", "Cricket", "2", "Gabriel Zwilling", "\"The Day of the Doctor\"", "confirmation", "in the city of Deabolis", "Wijk bij Duurstede", "June 6, 1951", "24 September 2007", "18", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "Teri Garr", "Malayalam", "lamb", "the red - bed country of its watershed", "DNA was the genetic material", "Steve Russell", "Africa", "Lord's", "Dalveer Bhandari", "boy", "Humphrey Bogart", "Michael Jackson and Lionel Richie", "6 March 1983", "\" Complicating matters, her husband Henry ( Lancaster ) is overdue and their servants have the night off, leaving her all alone in a Manhattan apartment", "Glenn Close", "Andreas Vesalius", "1959", "in a forest", "in the year 2026", "1901", "Gorakhpur", "electors", "1834", "Indian Standard Time", "New York University", "song rose to number 4 on Billboard's Pop Singles chart and number one for two weeks on the R&B Singles charts on August 14 through to August 27, 1971", "Sophia Akuffo", "Death Eaters", "2", "April 29, 2009", "regulatory", "restarting play after a minor infringement", "that country's surprise attack on Pearl Harbor the prior day", "Wembley Stadium", "1992", "Jonathan Cheban", "green", "4", "lile- choruses", "in Arabah", "May 27, 2016", "frigate", "an assortment of human papillomavirus", "Lonnie", "a muskrat", "carlicity", "Lake Placid, New York", "Russell T Davies"], "metric_results": {"EM": 0.484375, "QA-F1": 0.583693043068043}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0909090909090909, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4921", "mrqa_squad-validation-1188", "mrqa_squad-validation-1076", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-4998", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-3977", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-11598", "mrqa_hotpotqa-validation-2357"], "SR": 0.484375, "CSR": 0.6166666666666667, "EFR": 0.9393939393939394, "Overall": 0.7367589962121213}, {"timecode": 15, "before_eval_results": {"predictions": ["The Writers Guild of America", "every five years", "NFIL3", "826", "Treaty of Logstown", "mid-Cambrian", "Cabot Science Library, Lamont Library, and Widener Library", "Palestine", "PNU and ODM camps", "The waxy cuticle of many leaves", "2,200", "WWSB and WOTV", "whether he stood by their contents", "cloven hoof", "the \"Hudson River Bridge\"", "two dissimilar organisms", "a doll", "The Physical Basis of Long-Range Weather Forecasts", "Southern elephant", "Sally Margaret Field", "1908-1993", "Captain Nemo", "David Elkouby", "an air guitar", "Agamemnon", "jedoublen/jeopardy", "to raise money for the Muscular Dystrophy Association", "rice", "Boeing", "Saskatchewan", "keem Abdul-Jabbar", "900 rebels", "undercard", "Willa Cather", "a piece of jewellery inherited from our grandmother,", "Keith Urban", "French", "deer", "mine resistant", "loss of sleep", "change to a boat", "parrots, gorillas, and tarantulas", "Rick Springfield", "Pocono Mountains", "70% isopropyl alcohol in water", "sweet, musical, or pleasant to hear", "George Custer", "Pulex irritans", "kabbalah", "palomino", "piedmont", "French Lgion d' Honneur", "Elisabeth", "Rent", "Camping World Stadium in Orlando, Florida", "John Cooper Clarke", "President Woodrow Wilson", "Nicola Adams", "1943", "Pearl Jam", "\"It is not acceptable,\"", "there is not a process to ensure that auto owners comply with recalls", "two", "16\u201321"], "metric_results": {"EM": 0.46875, "QA-F1": 0.539093660968661}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14814814814814817, 0.15384615384615385, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4802", "mrqa_squad-validation-6435", "mrqa_squad-validation-6091", "mrqa_searchqa-validation-12940", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-12272", "mrqa_searchqa-validation-9570", "mrqa_searchqa-validation-8796", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-4536", "mrqa_searchqa-validation-1446", "mrqa_searchqa-validation-16715", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-13578", "mrqa_searchqa-validation-1624", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-2200", "mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-14789", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2358", "mrqa_hotpotqa-validation-5438"], "SR": 0.46875, "CSR": 0.607421875, "EFR": 1.0, "Overall": 0.74703125}, {"timecode": 16, "before_eval_results": {"predictions": ["within the Church of England", "Lenin", "a qualified majority vote, if not consensus", "36", "Brough Park in Byker", "2012", "Stress", "quantum", "Jonathan Stewart", "George Westinghouse", "human", "significantly increased British military resources in the colonies", "Aquitaine", "( Bilbo) Baggins", "St. Augustine", "forests", "the Netherlands", "the Ritz-Carlton", "kiss a fool", "masks", "(Moze) Bigby", "the Sons of Liberty", "movie house", "National Security Agency", "Ugly Betty", "Ambrose Bierce", "the Key deer", "the All-New Blue Ribbon Cookbook", "flowers", "Pheonix", "Mary Jo Buttafuoco", "A Portrait of the Artist as a Young Man", "an acrobat", "guttural", "polio", "Jennifer Tilly", "the Mausoleum of Halicarnassus", "King George III", "the tyke", "(Bistro.com", "the Firmament", "dark places", "(Jerry Maguire)", "Wendy Beckett", "Ferris B Mueller", "the catcher", "Richard Rodgers", "Bill Hemmer", "Samuel Goldwyn", "Annika Sorenstam", "(Asparagus", "Thurman Munson", "Washington, D.C.", "flip-flops", "the International Border", "1783", "chilevos", "(Bo' Olsen", "McComb, Mississippi", "Dorothy Zbornak", "Tutsi and Hutu rivalry", "at the University of Alabama in Huntsville", "18", "\"Twilight\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5656622023809523}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.08333333333333333, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4216", "mrqa_squad-validation-5456", "mrqa_squad-validation-10388", "mrqa_squad-validation-10158", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-11531", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-16832", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10708", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9957", "mrqa_searchqa-validation-4484", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-2627", "mrqa_hotpotqa-validation-959", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-608"], "SR": 0.453125, "CSR": 0.5983455882352942, "EFR": 0.9714285714285714, "Overall": 0.7395017069327732}, {"timecode": 17, "before_eval_results": {"predictions": ["49\u201315", "113", "protein structure prediction", "Deformational events", "August 2004", "Department of State Affairs", "Prague", "attempted to enter the test site knowing that they faced arrest", "governments", "M\u00f6ngke Khan", "by using net wealth (adding up assets and subtracting debts), the Oxfam report, for instance, finds that there are more poor people in the United States and Western Europe than in China", "Danny Lee", "snow", "Cyril J. O'Brien", "Romeo and Juliet", "Jane Addams", "Rand McNally & Company", "Dean Acheson", "the pound sterling", "Auguste Rodin", "the Andes", "Sherlock Holmes", "the Taj Mittal", "the trampoline", "a axe", "a cereal", "Constantine", "Daniel Inouye", "argon", "a Zen monastery", "Kung Fu", "the Star-Crossed Stars of Showgirls, Crossroads, and Glitter", "a brandy", "the Jungle of Cities", "Milwaukee", "silver", "Bangkok", "the Soviet Union", "a scotch", "the young Belfast-born actor and director", "a doses", "Frank Sinatra", "Christopher Columbus", "the King of the Hill", "Schtze Benjamin", "Donald and Nellie Ruth Pillsbury", "George Gordon", "Japan", "Joan of Arc", "Jaguar", "the Oompa-Loompas", "a hound", "R. Stanton Avery", "at U.S. Bank Stadium", "the one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "Jim hacker", "Jim Branning", "May 10, 1976", "(IATA: VNO, ICAO: EYVI)", "Cipro, Levaquin, Avelox, Noroxin and Floxin", "murder", "Parlophone Records", "five times", "Nazi concentration camps"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6601376488095239}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.05714285714285715, 0.0, 1.0, 0.0, 0.8, 1.0, 0.8, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.375, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6702", "mrqa_squad-validation-7554", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-7825", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-2546", "mrqa_searchqa-validation-4402", "mrqa_naturalquestions-validation-5674", "mrqa_naturalquestions-validation-3108", "mrqa_hotpotqa-validation-3728", "mrqa_newsqa-validation-57", "mrqa_hotpotqa-validation-5499"], "SR": 0.5625, "CSR": 0.5963541666666667, "EFR": 1.0, "Overall": 0.7448177083333334}, {"timecode": 18, "before_eval_results": {"predictions": ["hunter's garb", "from the official declaration of war in 1756 to the signing of the peace treaty in 1763", "two forces", "a decision problem", "10 to 15 million", "about 17", "Establishing \"natural borders\"", "at his Houston Street lab", "vary by geographic area and subject taught", "\"Turks\" (Muslims) and Catholics", "provides the public with financial information about a nonprofit organization", "the Northeast Monsoon", "April 3, 1973", "Fa Ze YouTubers", "July 14, 1969", "Krypton", "Mandarin", "in the bone marrow", "Ukraine", "Coldplay", "Yugoslavia was set up as a federation of six republics, with borders drawn along ethnic and historical lines", "head coach", "May 19, 2017", "T - Bone Walker", "April 2, 2018", "Rose Stagg", "Doug Diemoz", "Iran", "southern Anatolia", "classical architecture", "the pyloric valve", "1546", "100,000 writes", "the cella", "16 seasons", "Long Island", "lacteal", "the Beldam / Other Mother", "1987", "Timothy B. Schmit", "in Kent", "Jikji", "Panning", "the RAF", "Pepsi", "Detective Superintendent Dave Kelly", "Isabela Moner", "Ethel Merman", "provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "New Orleans", "the Outback", "Anakin Skywalker", "Russell Huxtable", "Namibia", "to the tooth", "all-time leader in total passing yards, touchdowns, and completions", "the managers and coaches", "eight", "Los Angeles", "North by Northwest", "Naples", "Etna", "Edward VI", "Lutherhaven"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6047440984940984}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.962962962962963, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10121", "mrqa_squad-validation-10395", "mrqa_squad-validation-1600", "mrqa_squad-validation-3300", "mrqa_squad-validation-1482", "mrqa_squad-validation-2054", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-3727", "mrqa_hotpotqa-validation-466", "mrqa_hotpotqa-validation-5293", "mrqa_searchqa-validation-9438"], "SR": 0.484375, "CSR": 0.5904605263157895, "EFR": 0.9696969696969697, "Overall": 0.7375783742025519}, {"timecode": 19, "before_eval_results": {"predictions": ["Thomas Vasey and Richard Whatcoat", "reminding their countrymen of injustice", "sex offenders register", "Kenya", "the violence that subsequently engulfed the country", "aristocracy", "1905", "Saul Alinsky", "specific devolved matters are all subjects which are not explicitly stated in Schedule 5 to the Scotland Act as reserved matters", "in the absence of the \u00d7 character, U + 002A * Asterisk became the de facto standard notation of the multiplication operator in computing", "February 1834", "Jonathan Goldstein", "President Yahya Khan", "milk", "BC Jean and Toby Gad", "Debbie Reynolds, Gene Kelly and Donald O'Connor", "Anatomy", "in Pyeongchang County, Gangwon Province, South Korea", "Jacqueline MacInnes Wood", "Kristy Swanson", "2017", "a child with Treacher Collins syndrome trying to fit in", "Andaman and Nicobar Islands", "interstitial fluid in the ` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals ), blood plasma and lymph in the `` intravascular compartment ''", "by member states on a voluntary basis", "in the United Kingdom", "Rocinante", "1526", "May 1, 2018", "Hans Christian Andersen", "Leon Battista Alberti", "Cheitharol Kummaba", "1", "alveolar process", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "October 14, 2017", "16", "July 21, 1861", "Italy", "45 %", "in the bible", "bypasses", "Sally Dworsky", "Robert E. Lee", "Clarence L. Tinker", "Soviet Russia defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "the church", "a federal republic", "Thomas Edison", "mitosis", "light in the form of either glowing or a flame", "pit road speed", "in Pyeongchang County, Gangwon Province, South Korea", "the Loop", "Gianni Versace", "David Simon", "1999", "Madrid's Barajas International Airport", "in Seoul", "a chalk", "pours tea", "three", "the Dalai Lama", "Christopher Savoie"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6553311871441467}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4848484848484849, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.923076923076923, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.43750000000000006, 0.923076923076923, 0.8, 1.0, 0.0, 1.0, 0.35294117647058826, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.4615384615384615, 0.09523809523809525, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8370", "mrqa_squad-validation-9640", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9760", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-5069", "mrqa_triviaqa-validation-2196", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-84", "mrqa_searchqa-validation-10274", "mrqa_searchqa-validation-2516"], "SR": 0.484375, "CSR": 0.58515625, "EFR": 0.9696969696969697, "Overall": 0.736517518939394}, {"timecode": 20, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1705", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4329", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-466", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-12", "mrqa_naturalquestions-validation-1436", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3376", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9809", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3578", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10708", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-11360", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11598", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16636", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1924", "mrqa_searchqa-validation-1928", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2887", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-3899", "mrqa_searchqa-validation-4133", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4750", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6233", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9561", "mrqa_searchqa-validation-9570", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10158", "mrqa_squad-validation-10162", "mrqa_squad-validation-10198", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10471", "mrqa_squad-validation-1076", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1188", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1330", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1424", "mrqa_squad-validation-1506", "mrqa_squad-validation-1540", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1611", "mrqa_squad-validation-1703", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1834", "mrqa_squad-validation-1908", "mrqa_squad-validation-1976", "mrqa_squad-validation-2015", "mrqa_squad-validation-2025", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2111", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2250", "mrqa_squad-validation-2395", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2532", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-3001", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3193", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-331", "mrqa_squad-validation-3368", "mrqa_squad-validation-3449", "mrqa_squad-validation-3493", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3626", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3948", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4159", "mrqa_squad-validation-4176", "mrqa_squad-validation-4248", "mrqa_squad-validation-4248", "mrqa_squad-validation-4272", "mrqa_squad-validation-4274", "mrqa_squad-validation-4301", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4686", "mrqa_squad-validation-4698", "mrqa_squad-validation-4765", "mrqa_squad-validation-4789", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-501", "mrqa_squad-validation-5133", "mrqa_squad-validation-5157", "mrqa_squad-validation-5214", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-55", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5664", "mrqa_squad-validation-5715", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5897", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6251", "mrqa_squad-validation-6253", "mrqa_squad-validation-6264", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6435", "mrqa_squad-validation-6452", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7191", "mrqa_squad-validation-7226", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7592", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7751", "mrqa_squad-validation-7775", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7889", "mrqa_squad-validation-7932", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8010", "mrqa_squad-validation-8019", "mrqa_squad-validation-8199", "mrqa_squad-validation-8213", "mrqa_squad-validation-826", "mrqa_squad-validation-8278", "mrqa_squad-validation-8298", "mrqa_squad-validation-830", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8383", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-861", "mrqa_squad-validation-8612", "mrqa_squad-validation-8636", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8786", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9308", "mrqa_squad-validation-9315", "mrqa_squad-validation-9322", "mrqa_squad-validation-9388", "mrqa_squad-validation-9405", "mrqa_squad-validation-9431", "mrqa_squad-validation-9495", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9640", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9865", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1646", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5140", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6531", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-88"], "OKR": 0.869140625, "KG": 0.459375, "before_eval_results": {"predictions": ["the dukes", "seeing the Muslim faith as a tool of the devil", "Ed Lee", "40%", "post-World War I", "10,000", "can produce both eggs and sperm at the same time", "\u00d6gedei Khan", "1971", "Kohlberg K Travis Roberts", "John McClane", "Selected Writings by Steve Biko", "1910s", "between 11 or 13 and 18", "Song Il-gon.", "\"Let's Make Sure We Kiss Goodbye,\" \"Feels Like Love\"", "Martin \"Marty\" McCann", "Moon Shot", "Paul Donald Trump's presidential campaign team", "December 17, 1974", "\"The Royal Family\"", "Sir Seretse Khama", "Mike Holmgren", "the Galaxy S6, S6 Edge and S6Edge+", "a British archaeologist, military officer, diplomat, and writer", "Donald Richard \"Don\" DeLillo", "the University of Nevada, Las Vegas", "a co-op of grape growers,", "South America", "five", "Kramer", "the Bank of China Building", "Shane Meadows.", "Bisexuality", "\"Winnie the Pooh\"", "Excalibur Hotel and Casino", "Rigoletto", "Knoxville, Tennessee", "Prada", "Omega SA", "1978", "Pim Fortuyn", "Firth of Clyde, Scotland,", "Todd McFarlane", "M. Night Shyamalan", "Magdalen College", "Eric Allan Kramer", "1894", "Province of New York", "a royal residence", "Ghana", "Michael Seater", "1933", "a English expression meaning `` mind your manners ''", "Charles Darwin", "Osborne Road", "King Ethelred the Unready", "his past and his future", "Buenos Aires", "Engelbert Humperdinck", "the Colorado", "John Denver", "The Princess bride", "a republic in W Africa"], "metric_results": {"EM": 0.5, "QA-F1": 0.5902020676691729}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.3157894736842105, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2293", "mrqa_squad-validation-5236", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-891", "mrqa_naturalquestions-validation-4109", "mrqa_triviaqa-validation-7452", "mrqa_triviaqa-validation-5748", "mrqa_newsqa-validation-3889", "mrqa_searchqa-validation-6205", "mrqa_triviaqa-validation-6564"], "SR": 0.5, "CSR": 0.5811011904761905, "EFR": 1.0, "Overall": 0.7307514880952382}, {"timecode": 21, "before_eval_results": {"predictions": ["a interception", "end the true Islamic system, something for which it blames \"the disbelieving (Kafir) colonial powers\" working through Turkish modernist Mustafa Kemal Atat\u00fcrk.", "pharmacy practice science and applied information science", "thought it may have been a combination of anthrax and other pandemics.", "1912", "rubisco", "the Huguenot rebellions", "Psych", "a Romance language consisting of a group of dialects (which some consider part of a unitary Rhaeto-Romance language)", "August 11, 1946", "Protestant Christian", "Queens, New York", "Erreway", "Oakland County", "Madeleine L' Engle", "FAI Junior Cup", "1966 US tour", "February 18, 1965", "Sydney", "Cuban descent", "1951", "Alberta", "Mickey's PhilharMagic", "Eielson Air Force Base", "Taylor Swift", "Italy", "Tel Aviv University", "the Donny & Marie Showroom, at the Flamingo Las Vegas", "John of Gaunt", "William Clark Gable", "Stage Stores", "C. J. Cherryh", "Spanish", "channel 33", "Axl Rose", "Daniil Shafran", "Soha Ali Khan", "1945", "four", "13 October 1958", "70", "3 May 1958", "the Women's World Curling Classic", "Vancouver", "Marlborough", "Fountains of Wayne", "one of the commanders of the Great Army", "sulfur mustard H or HD blister gas", "The Saturdays", "the Chick tract of the same name", "southwestern", "The Campbell Soup Company", "The Gang", "2010", "The vascular cambium is the main growth layer in the stems and roots of many plants, specifically in dicots such as buttercups and oak trees, and gymnosperms such as pine trees", "A", "a Rh\u00f4ne Grape Varietal", "the Illinois Reform Commission", "Iraq", "fas", "a motorcycle", "last home series defeat on Australia in almost 16 years as they wrapped up a nine-wicket win over the world's number one ranked Test nation in Melbourne on Tuesday.", "$60 billion", "can also taste a hamburger and pizza, and drink coffee from a cup, the \"things we take for granted every day,\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.6071660450247974}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5185185185185185, 1.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.33333333333333337, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.28571428571428575, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8, 0.4444444444444445, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1935483870967742]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_squad-validation-9918", "mrqa_squad-validation-5029", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-1287", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-51", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-8220", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2301", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1784", "mrqa_searchqa-validation-7453", "mrqa_searchqa-validation-2193", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-1093"], "SR": 0.484375, "CSR": 0.5767045454545454, "EFR": 0.9696969696969697, "Overall": 0.7238115530303031}, {"timecode": 22, "before_eval_results": {"predictions": ["one of the daughters of former King of Thebes, Oedipus,", "environmental determinism", "1110 AM", "Articles 106 and 107", "corpses", "Cobb Lecture Hall", "Cortina d'Ampezzo", "Eric Edward Whitacre", "1983", "EQT Plaza in Pittsburgh, Pennsylvania", "Ruth Westheimer", "Giotto", "Nickelodeon", "Rage Against the Machine", "Bobby Hurley", "11", "PEN America: A Journal for Writers and Readers", "March 19, 2017", "Disney California Adventure", "Anah\u00ed Giovanna Puente Portilla de Velasco", "Nicholas John \"Nic\" Cester", "Anne, Princess Royal", "Harry Robbins \"Bob\" Haldeman", "264,152", "tales of various deities, beings, and heroes", "the title character", "more than 40 million", "Seattle, Washington", "sub-Saharan Africa", "William Cavendish", "20 March to 1 May 2003", "23 July 1989", "Kinnairdy Castle", "Javed Miandad", "\"The Catcher in the Rye\"", "Indianapolis Motor Speedway", "Marjorie Jacqueline \"Marge\" Simpson (n\u00e9e Bouvier)", "May 4, 1924", "Transporter 3", "Richard Street", "\"Queen In-hyun's Man\"", "Steve Carell", "Green Chair", "The Princess and the Frog", "15 February 1970", "Germany", "September 3, 2017", "CTV Television Network", "Australian", "Rafael Palmeiro", "Eric Allan Kramer", "\"Orchard County\"", "Orographic lift", "Reproductive system", "Coroebus of Elis", "\u201cMy Favorite Martian,\u201d", "Disraeli", "Jennifer Ellison", "Sunday,", "5 1/2-year-old", "one day,", "the orchestra", "the Supreme Court", "the Pledge of Allegiance"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6321676587301588}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [0.42857142857142855, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.4, 1.0, 0.4444444444444445, 1.0, 1.0, 0.2857142857142857, 1.0, 0.4, 0.0, 0.0, 0.8333333333333333, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.4, 1.0, 0.4, 0.0, 0.0, 0.3333333333333333, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_squad-validation-4772", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5636", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-4204", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-5166", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3629", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-4189", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-13691"], "SR": 0.46875, "CSR": 0.5720108695652174, "EFR": 1.0, "Overall": 0.7289334239130435}, {"timecode": 23, "before_eval_results": {"predictions": ["Article 17(3)", "G\u00fcy\u00fck", "Department of Justice", "tidal currents", "intercepted it", "The Eleventh Doctor", "Malware", "Claims adjuster", "1995", "Caleb", "Arunachal Pradesh", "1996", "Richard of Shrewsbury", "The geopolitical divisions in Europe", "2009", "Stevie Wonder", "18", "to form a higher alkane", "The Lightning thief", "Ali", "After Kelly and Dylan last broke up", "UMBC", "the customer's account", "two reservoirs in the eastern Catskill Mountains", "Elena Anaya", "January 2018", "a prison", "biscuit - sized", "Woodrow Wilson", "Commander in Chief of the United States Armed Forces", "Waylon Jennings", "the Italian / Venetian John Cabot", "the Boston Red Sox", "protect the genome", "a deboned duck", "a balance sheet", "Angel Island", "Mickey Mantle", "the ACU", "1956", "2011", "Toot - Toot", "April 2011", "David Gahan", "the Italian pignatta", "South Asia", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "stuffing", "note number 60", "New England Patriots", "Tristan Rogers", "novella", "The Brady Bunch", "Ron Kovic", "Mexico", "Apsley George Benet Cherry-Garrard", "the Ecumenical Award", "Brig Gen Augustine Warner Robins", "Florida", "Department of Homeland Security Secretary Janet Napolitano", "Kerstin", "Colorado", "gusto", "Ulysses"], "metric_results": {"EM": 0.5, "QA-F1": 0.577210461131246}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6976744186046512, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8230", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-10586", "mrqa_triviaqa-validation-4545", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2908"], "SR": 0.5, "CSR": 0.5690104166666667, "EFR": 0.96875, "Overall": 0.7220833333333334}, {"timecode": 24, "before_eval_results": {"predictions": ["the third and fourth series", "Doctor in Bible", "c1750", "60%", "a deterministic Turing machine", "Henry III of England", "Henkel", "Bowie", "Handel", "Pol Pot", "the Pyrenees Mountains", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Motel 6", "a restaurant in New York\u2019s Greenwich Village", "the Old Dominion", "Isaac", "a crystal ball", "horses", "the gallbladder", "his deputy Thabo Mbeki", "1921", "a dog", "Robert Schumann", "The Benedictine Order", "Corey Pavin", "translations", "King County Executive", "Scotland", "the Penguin", "The Sahara Desert", "Mata Hari", "Saturn\u2019s rings", "the Brisbane River", "The Aidensfield Arms", "armada", "Liechtenstein", "the Rolling Stones", "Rodney", "the Simpson trial, \"If it doesn't fit, you must acquit,\"", "the euro", "Prokofiev", "horses", "\"Stutter Rap (No Sleep til Bedtime)\"", "Kansas", "Australia", "EGBDF", "\"Little Red Rented Rowboat\"", "smell", "David David (; ; ISO 259-3 Dawid; ; )", "driving Miss Daisy", "the Isles of the Blessed", "the Anthropocene", "Edward Kenway ( Matt Ryan )", "Darren McGavin", "Sir Henry Cole", "Scottish", "Sarah Hurst", "1916", "Lyon", "Jason Chaffetz", "if Gadhafi suffered the wound in crossfire or at close-range", "an analog watch", "oliver", "Alexander Haig"], "metric_results": {"EM": 0.5, "QA-F1": 0.5747407106782108}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true], "QA-F1": [0.28571428571428575, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1818181818181818, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.3, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7762", "mrqa_squad-validation-1819", "mrqa_triviaqa-validation-5569", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-5245", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-1702", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-3189", "mrqa_hotpotqa-validation-5274", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-2385", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-11092"], "SR": 0.5, "CSR": 0.5662499999999999, "EFR": 0.9375, "Overall": 0.7152812500000001}, {"timecode": 25, "before_eval_results": {"predictions": ["The next architect to work at the museum was Colonel (later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "floor function", "QuickBooks", "Westinghouse Electric", "The Bachelor", "statue", "2018", "2017 Georgia Bulldogs", "Honor\u00e9 Mirabeau", "England and Wales", "Eleanor Roosevelt, United States ( Chair )", "radioisotope thermoelectric generator", "1908", "honey bees", "Samantha Jo `` Mandy '' Moore ( born March 28, 1976 in St. Louis, Missouri and raised in Breckenridge, Colorado )", "Edward Douglass White, Charles Evans Hughes, Harlan Fiske Stone, and William Rehnquist", "Mangal Pandey of the 34th BNI", "Woodrow Strode", "Woodrow Wilson", "four", "tissues in the vicinity of the nose", "season two", "January 17, 1899", "Ptolemy's geocentric model", "16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins, dealing with Christian dispensationalist End Times", "British Ultra code - breaking intelligence", "623", "a mirror", "5.7 million", "Vincent Price", "Steve Russell, in collaboration with Martin Graetz and Wayne Wiitanen, and programmed by Russell with assistance from others including Bob Saunders and Steve Piner", "Janis Joplin", "1871", "December 8, 2013", "Ahmad ( Real ) selected Doll, while Kamal ( Chance ) selected Hot Wings", "Haiti", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "since its premiere on October 14, 2017", "2026", "2018", "The Outback", "the President", "2004", "the South Pacific Ocean", "eleven", "Master Christopher Jones", "Around 1200", "12.65 m ( 41.50 ft )", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "2017", "in the North Cascades range of, Washington", "1986", "Italy", "music (to be performed) in a fiery manner", "Culture Club", "12 Play", "Indian", "Sofia the First", "software magnate", "Hurricane Gustav", "21", "Joseph Heller", "austria", "uranium"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5510632885632885}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [0.5925925925925926, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.0, 0.38095238095238093, 0.0, 0.7692307692307693, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.33333333333333337, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.8, 0.0, 0.6, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-8004", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-6935", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7128", "mrqa_hotpotqa-validation-3423", "mrqa_hotpotqa-validation-3842", "mrqa_newsqa-validation-1234", "mrqa_searchqa-validation-3483", "mrqa_searchqa-validation-9088"], "SR": 0.40625, "CSR": 0.5600961538461539, "EFR": 1.0, "Overall": 0.7265504807692308}, {"timecode": 26, "before_eval_results": {"predictions": ["Soviet", "Thomas Commerford Martin", "seven", "labor inputs (workers)", "1996", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Indraprastha", "John Quincy Adams", "third season", "Christopher Columbus", "\" Wonder is a 2017 American drama film directed by Stephen Chbosky and written by Jack Thorne, Steve Conrad, and Stephen chbosky, based on the 2012 novel of the same name by R.J. Palacio", "an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "the President of the United States", "administrative supervision over all courts and the personnel thereof ''", "while in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "Mandy", "king Harold Godwinson", "during meiosis", "the Miracles", "Spanish / Basque origin", "in spring and summer", "a relationship", "January 2, 1971", "Hirschman", "in the books of Exodus and Deuteronomy", "Lucknow", "Neuropsychology", "The User State Migration Tool ( USMT )", "Effy", "May 1, 2018", "291 episodes in Japan", "Naturalization Act of 1790", "flawed democracy", "in Pashto and Persian as \u0647\u0646\u062f\u0648\u06a9\u0634 \u202c", "the last Ice Age", "Ren\u00e9 Descartes", "in people and animals that collects and stores urine from the kidneys before disposal by urination", "The First Battle of Bull Run ( the name used by Union forces )", "yishai", "in 2017", "Germany", "London, United Kingdom", "Jeff East", "President Yahya Khan", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "6th century AD", "Arnold Schoenberg", "~ 3.5 million years old", "111", "when a population temporarily exceeds the long term carrying capacity of its environment", "Mike Leeson and Peter Vale", "amount to a crime and deserve punishment", "Joan Crawford", "Principality of Andorra", "Ned Sherrin", "Band-e Amir National Park", "Cartoon Network", "Chrysler", "in Afghanistan", "more than 100 Native American-owned businesses.", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Roanoke Colony", "ballet master", "apples"], "metric_results": {"EM": 0.5, "QA-F1": 0.6363066499413857}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.28571428571428575, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.3870967741935484, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.4210526315789474, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3771", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-9812", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-1694", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-7787"], "SR": 0.5, "CSR": 0.5578703703703703, "EFR": 0.96875, "Overall": 0.719855324074074}, {"timecode": 27, "before_eval_results": {"predictions": ["eight", "after sustaining an injury which would be fatal to most other species", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "chloroplast", "Naples", "black, red or white,", "2009", "the Cowardly Lion", "Diego Maradona", "Harkat-ul-Jihad al-Islami ( HuJi)", "\"I'm certainly not nearly as good of a speaker as he is.\"", "\"I don't watch TV,\"", "golf", "Used Acura", "Floxin", "Copts", "February 12", "Roberto Micheletti,", "when times get tough,", "Shiza Shahid, 20, from California's Stanford University,", "Akio Toyoda", "was arrested in a federal sting after his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "African-Americans", "environmental and political events", "trying to detonate an explosive device in his underwear aboard a Christmas 2009 flight to Detroit,", "three empty vodka bottles,", "Euna Lee,", "Angela Merkel", "the Ku Klux Klan", "some dental work done, including removal of his diamond-studded teeth.", "Manuel Mejia Munera", "requires police to question people if there's reason to suspect they're in the United States illegally.", "UNICEF", "club managers,", "used-luxury market", "Marines that are behind me,", "\"I'm certainly not nearly as good of a speaker as he is.\"", "suicides", "$15 billion", "Karen Floyd", "Mary Phagan,", "a skilled hacker could disrupt the system and cause a blackout.", "Cogentin and Haldol,", "$2 billion", "Dan Parris, 25, and Rob Lehr, 26,", "Krishna Rajaram,", "flooding", "Chievo", "Dubai", "up three of the last four months.", "Sunday,", "12-hour-plus", "three times", "Canada", "South Dakota", "Carolus a Linn\u00c3\u00a6us", "potash", "Something In The Air", "16 March 1987", "Peter Kay's Car Share", "Cyclic Defrost", "pfeffernuesse", "Skip school", "14"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5682583768521268}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.4444444444444445, 1.0, 0.09523809523809525, 1.0, 1.0, 0.2162162162162162, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.42857142857142855, 0.07692307692307691, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4648", "mrqa_squad-validation-8704", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2325", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-2456", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7659", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-5378", "mrqa_searchqa-validation-2197"], "SR": 0.515625, "CSR": 0.5563616071428572, "EFR": 0.967741935483871, "Overall": 0.7193519585253456}, {"timecode": 28, "before_eval_results": {"predictions": ["deserts", "political parties", "discipline problems with the Flight Director's orders during their flight", "Amos McCracken,", "Cherokee Nation", "Flamingo Las Vegas", "Galt\u00fcr avalanche", "Bantu", "2013,", "Luis Edgardo Resto", "the performance of Hofmannsthal's \"Jedermann\"", "Jay Park", "Blackpool Football Club", "New Orleans Saints", "2012", "\"Charmed\"", "Dundalk", "Ashley Jensen", "Syracuse University", "Dame Eileen June Atkins, DBE", "Mollie Elizabeth King", "Esteban Ocon", "the flags of dependent territories", "Ouse and Foss", "Emilia-Romagna", "\"Casablanca\"", "The Go-Go's", "1943", "Sleepy Hollow", "Ronnie Schell", "Wandsworth, London", "Christopher Lloyd Smalling", "Chevron Corporation", "World Music Awards award as the \"World's Best Selling Russian Artist\"", "La Liga", "Australian", "Floyd Nathaniel \"Nate\" Hills", "Fort Hood, Texas", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay, and Keller", "Lauren Alaina", "1961", "London", "Preston, Lancashire, UK", "Prudential Center in Newark, New Jersey", "\"The Clash of Triton\"", "Mach number (M or Ma)", "1950", "Mexico", "Chevy", "Jack Rabbit (Seabreeze)", "Disco", "Theodor W. Adorno", "re-education", "Sir Rowland Hill", "compound sentence", "President Jimmy Carter", "John Logie Baird", "1066", "Joan Rivers", "\"She was briefly hospitalized this summer for \"scheduled testing,\"", "auction off one of the earliest versions of the Magna Carta later this year,", "the Wall of the Moon", "Windows 7,", "kids"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6461045898545899}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2791", "mrqa_squad-validation-4060", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-2699", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-1926", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-1667", "mrqa_naturalquestions-validation-8329", "mrqa_triviaqa-validation-4927", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-9671", "mrqa_searchqa-validation-14497"], "SR": 0.53125, "CSR": 0.5554956896551724, "EFR": 0.9666666666666667, "Overall": 0.7189637212643678}, {"timecode": 29, "before_eval_results": {"predictions": ["some teachers and parents", "July 1969", "glaucophyte", "God and the just cause", "Mercury Records", "Evgeni Platov", "Nye County", "Karolina Dean", "\"Supergirl\"", "ten", "White Knights of the Ku Klux Klan", "the Chechen Republic", "Green Lantern", "Kansas City, Missouri", "March 16, 1927", "English", "Food and Agriculture Organization", "Albertsons", "Jeff Meldrum", "Crossed", "Romance language", "Philip K. Dick", "exercise power directly or elect representatives from among themselves to form a governing body, such as a parliament", "English", "Cartoon Network Too", "David Starkey", "Cherokee River", "pop music and popular culture", "Field Marshal Lord Gort", "Hopeless Records", "Razor Ramon", "Godspell", "8 August 1907", "Boeing 757", "7.63\u00d725mm Mauser (.30 Mauser Automatic)", "Bangkok", "51st Disney animated feature film.", "his exploration and settlement", "August 28, 1774", "Afghanistan", "British", "Potomac River", "the Netherlands", "Love the Way You Lie", "Rio Gavin Ferdinand", "Boston", "Las Vegas", "actor", "Rockbridge County", "St. Louis, Missouri", "Stephen Lee", "Bay Ridge, Brooklyn", "Anatomy", "Tyler", "the intersection of Del Monte Blvd and Esplanade Street", "AFC Wimbledon", "The Gondoliers", "1", "Meira Kumar", "The U.S. Food and Drug Administration", "barter -- trading goods and services without exchanging money", "spoiled", "malted", "Iceland"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6669202327670198}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.9411764705882353, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-1941", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-3362", "mrqa_triviaqa-validation-4462", "mrqa_newsqa-validation-719", "mrqa_searchqa-validation-11933"], "SR": 0.59375, "CSR": 0.5567708333333333, "EFR": 0.9615384615384616, "Overall": 0.718193108974359}, {"timecode": 30, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1079", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1524", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2823", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3382", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3950", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4058", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4334", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4953", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-5313", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-932", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-999", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9867", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1582", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-719", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-958", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-1228", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1828", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9613", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1379", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1546", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1600", "mrqa_squad-validation-1751", "mrqa_squad-validation-1819", "mrqa_squad-validation-1908", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-2025", "mrqa_squad-validation-2106", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-2848", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-3001", "mrqa_squad-validation-3103", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3449", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-4065", "mrqa_squad-validation-4132", "mrqa_squad-validation-4159", "mrqa_squad-validation-4216", "mrqa_squad-validation-4248", "mrqa_squad-validation-4274", "mrqa_squad-validation-4472", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4698", "mrqa_squad-validation-4736", "mrqa_squad-validation-4765", "mrqa_squad-validation-4772", "mrqa_squad-validation-4789", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5270", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5908", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6382", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7043", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7217", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7564", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7775", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7937", "mrqa_squad-validation-8010", "mrqa_squad-validation-8023", "mrqa_squad-validation-826", "mrqa_squad-validation-8298", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8466", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-8612", "mrqa_squad-validation-8665", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9308", "mrqa_squad-validation-9499", "mrqa_squad-validation-9594", "mrqa_squad-validation-9638", "mrqa_squad-validation-9918", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-4504", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6173", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7452", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-88"], "OKR": 0.869140625, "KG": 0.48046875, "before_eval_results": {"predictions": ["the State Department", "immediately", "a second Gleichschaltung", "the International Hotel", "the Recording Industry Association of America", "between 7,500 and 40,000", "mountaineer", "Belgian", "Jaeden Lieberher", "\"Slaughterhouse-Five\"", "Allan McNish (born 29 December 1969) is a British former racing driver, commentator, and journalist from Scotland", "William Jefferson Clinton (born William Jefferson Blythe III; August 19, 1946) is an American politician who served as the 42nd President of the United States from 1993 to 2001", "Oldham County", "sandstone", "Channel 4", "Christmas Day, December 25, 2009", "Jeff Brannigan", "punk rock", "Robert \"Bobby\" Germaine, Sr.", "Lord Byron", "Laura Elizabeth Dern", "Carrefour", "burlesque", "Argentina", "Forever Living Products", "the FBI", "The Saturdays", "Indianapolis", "French", "1968", "Edinburgh", "Charles Bronson", "Oklahoma Sooners", "Orson Welles", "Sharyn McCrumb", "The National Archives and Records Administration (NARA)", "1.23 million", "Ford Motor Company", "J. K. Rowling", "Sullivan University", "Zola", "New Zealand", "January 28, 2016", "The Wolf of Wall Street", "1979", "Being John Malkovich", "Merrimack County", "RAF Tangmere, West Sussex", "\"Brotherly Leader\"", "Suicide Kings", "Digby, Lincolnshire", "A stolperstein", "Wyatt `` Dusty '' Chandler ( George Strait )", "Montgomery", "Bart Howard", "South Park", "radio", "Andre Agassi", "10 below", "Asashoryu", "heavy turbulence", "a guitar", "Engelbert", "The Secret"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6393833268025079}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.20689655172413793, 0.8, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.8571428571428571, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 0.3333333333333333, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-3846", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3367", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4470", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-5278", "mrqa_hotpotqa-validation-528", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-7912", "mrqa_triviaqa-validation-7167", "mrqa_newsqa-validation-1077", "mrqa_searchqa-validation-8942"], "SR": 0.53125, "CSR": 0.5559475806451613, "EFR": 1.0, "Overall": 0.7272051411290322}, {"timecode": 31, "before_eval_results": {"predictions": ["NCAA Division I", "1985", "mistress of the Robes", "2006", "25 million", "age 15", "Tom Kitt", "The Bye Bye Man", "Hermione Youlanda Ruby Clinton-Baddeley", "Game Informer", "Fort Frederick", "Thorgan Ganael Francis Hazard", "Robert Marvin \"Bobby\" Hull, OC", "Love Actually", "Larnelle Steward Harris", "Brisbane", "Southbank", "the Commanding General", "1976", "Sean Yseult", "1998", "Benjamin Andrew \" Ben\" Stokes", "newspapers, television, radio, cable television, and other businesses", "Francis Keogh Gleason", "Royal Navy", "The Land of Enchantment", "$10\u201320 million", "on the Cumberland Plain", "Formula E", "most artists' lofts and art galleries", "Province of Canterbury", "the Anhaltisches Theater", "Alemannic", "1932", "128", "Telugu", "1937", "Windermere Hotel", "Curtis James Martin Jr.", "Marco Fu", "a 2005 South Korean horror film", "Isabella (Belle) Baumfree", "Kate Millett", "the American comedy-drama series \"Gilmore Girls\"", "Premier League", "Aqua", "St. Louis, Missouri", "Dan Castellaneta", "Bury St Edmunds, Suffolk, England", "Philip K. Dick", "Labour Party", "Bury Football Club", "Toby Keith", "Lady Olenna Tyrell", "August 2, 1990", "Kenya", "King", "Sir William Hamilton", "Liverpool", "\"Britain's Got Talent\"", "\"unnamed international terror group\"", "The Church of Christ, Scientist", "Ronald Reagan Presidential Library", "East Germany"], "metric_results": {"EM": 0.53125, "QA-F1": 0.713765834859585}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.15384615384615385, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.7499999999999999, 1.0, 0.6666666666666666, 0.28571428571428575, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2793", "mrqa_hotpotqa-validation-4223", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-3488", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-3892", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-3798", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5482", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-7447", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-2828", "mrqa_searchqa-validation-1275", "mrqa_searchqa-validation-1396"], "SR": 0.53125, "CSR": 0.55517578125, "EFR": 1.0, "Overall": 0.72705078125}, {"timecode": 32, "before_eval_results": {"predictions": ["Licensed Local Pastor", "a power outage", "13", "Hebrew", "Blenheim Palace", "five", "Edith Cavell", "Cotentin", "De Lorean DMC-12", "Cold War", "Action Comics", "Queen Elizabeth II", "Merchant of Venice", "Northwestern University", "curling", "Cole Porter", "Colorado", "Google", "Aviva plc", "oil", "Project Gutenberg", "surf", "Dr John Sentamu", "Kiel Canal", "British", "Cevennes", "Eggs Benedict", "Luigi Pirandello", "Sheffield United", "Ross MacManus", "Midtown", "Eddy Shah", "Hugh Laurie", "A cappella", "Dutch", "Take That", "Red squirrel", "Sindh", "Francois Quesnay", "Model T", "Spice Girls", "Brian Blessed", "Michael Caine", "Sebastian Beach", "pig", "Bank of England", "William Gilbert", "Brian Wilson's", "the monarch", "Bangladesh", "St Martin Orgar church", "Castor", "Nalini Negi", "his last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "Ole Einar Bj\u00f8rndalen", "Kansas Cityizards", "University of Mississippi", "The Rebirth", "\"Bagosora, 67, a colonel in the Rwandan army,", "Apple", "Akio Toyoda", "Hamlet", "a reddish-orange nose", "chicken Kiev"], "metric_results": {"EM": 0.546875, "QA-F1": 0.58515625}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-6454", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-2721", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-373", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-907", "mrqa_naturalquestions-validation-6853", "mrqa_hotpotqa-validation-217", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-250", "mrqa_searchqa-validation-16717", "mrqa_searchqa-validation-10619"], "SR": 0.546875, "CSR": 0.5549242424242424, "EFR": 1.0, "Overall": 0.7270004734848484}, {"timecode": 33, "before_eval_results": {"predictions": ["Administrator Webb", "Duval County", "Atlantic", "Richard Branson", "ohm centimeter", "tibet", "yorkshire", "1709", "Tutankhamun", "Morgan Spurlock", "iris", "Massachusetts", "steff Graff", "Jane Austen", "Dutch", "Bruce", "nacre", "yellow", "Tbilisi", "Mrs Merton", "R.A.P. of Amsterdam", "Wyoming", "Catherine Cookson", "Hugh Quarshie", "Flanagan", "The Shoop Shoop Song", "Alan Sugar", "man", "Dodi Fayed", "Red Sea", "Helen Gurley Brown", "Wash", "wool", "Benfica", "Mark Carney", "Eva Cassidy", "dijon", "Utah", "Toy Story", "David Lloyd George", "Italy", "lord Nelson", "George Osborne", "August 10, 1960", "Apollo", "Gentlemen Prefer Blondes", "Waddington Games", "Harry Shearer", "Paul Gauguin", "Tanzania", "proton", "Demi Moore", "28 July 1914", "Spike", "between 2004 and 2007", "Ellie Kemper", "7 June 1954", "Oryzomyini", "Ryder Russell,", "NATO", "The Washington Post", "Plymouth", "1492", "Pearl Jam"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6942708333333334}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3929", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-3327", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1375", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-5950", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-8908", "mrqa_hotpotqa-validation-5041"], "SR": 0.65625, "CSR": 0.5579044117647058, "EFR": 1.0, "Overall": 0.7275965073529411}, {"timecode": 34, "before_eval_results": {"predictions": ["international metropolitan region", "petroleum", "Pepsi", "new Orleans", "Dan", "if", "GIGO", "pawn", "house", "bead", "bamboo", "andre c. Clarke", "rice", "fox", "the Wise", "aston villa", "scoop", "Led Zeppelin", "Alderney", "Charles Lindbergh", "river Phoenix", "elphaba", "heroes", "the Krntnertor Theatre", "Jason", "The Curse of the play", "the marine band", "AbeBooks.com", "the Lady", "Kennedy", "aston villa", "Robbie Turner", "Bea Arthur", "Pop Warner", "paulaepolis", "borat", "coal", "Jean Foucault", "Hanna Glawari", "humerus", "Harriet Tubman", "a dog", "Mark Adamo", "scoop", "Hugh Williams", "Margaret Atwood", "cheese", "Khartoum", "Joaquin Phoenix", "Winslow Homer", "Moby Dick", "The Hot Chick", "iOS", "Mars Hill", "Aaron Harrison", "Gwyneth Paltrow", "slug", "aromatherapy", "Keith Crofford", "140 million", "1923", "Saturday", "7-1", "Karen Floyd"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48020833333333335}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.8, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-1570", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4738", "mrqa_searchqa-validation-7166", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-12487", "mrqa_searchqa-validation-15135", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-12354", "mrqa_searchqa-validation-9156", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-15325", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-8940", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-1882", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-6035", "mrqa_searchqa-validation-2976", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7832", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4753", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-30", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2996"], "SR": 0.421875, "CSR": 0.5540178571428571, "EFR": 1.0, "Overall": 0.7268191964285714}, {"timecode": 35, "before_eval_results": {"predictions": ["arrow", "the Chicago Bears", "Floridians", "green and yellow", "Galicia (Eastern Europe)", "Regional Rural Bank", "M2M", "3D computer-animated comedy film", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies", "Division of Cook", "July 16, 1971", "13 May 2018", "Kentucky River", "Barbara Niven", "the Ferengi Cockark", "Firth of Forth", "Santiago del Estero Province", "a super-regional shopping mall", "the Passion", "Abbey Road", "Mel Blanc", "Czech Kingdom (Czech: \"\u010cesk\u00e9 kr\u00e1lovstv\u00ed\"", "2000", "Lamar Wyatt", "Alfred Preis", "Terry Malloy", "Countess of Lovelace", "The interview", "various registries", "20th episode in the first season", "John Bingham, 7th Earl of Lucan", "January 15, 1975", "Chiwetel Ejiofor", "Appleby-in-Westmorland", "27 November 1956", "Paris", "The St Andrews Agreement", "the Seasiders", "Victorian College of the Arts and Melbourne Conservatorium of Music", "north", "Nick Cassavetes", "Cate Blanchett", "Linda Ronstadt", "January 28, 2016", "Hopi", "John Meston", "Juliet", "\u00c6thelwald Moll", "political party", "Keele University of Keele", "Battle of the Rosebud", "Jaffrey, New Hampshire", "1998", "Red Sea", "Sara Gilbert", "Elkie Brooks", "acrostic poem", "blue", "Asashoryu", "processing data, requiring that all flight-plan information be processed", "December 7, 1941", "(J.D) Salinger", "emerald", "Luxor Las Vegas"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6702651515151515}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.4444444444444445, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-3644", "mrqa_hotpotqa-validation-3867", "mrqa_naturalquestions-validation-6452", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-1457", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-15743"], "SR": 0.578125, "CSR": 0.5546875, "EFR": 1.0, "Overall": 0.726953125}, {"timecode": 36, "before_eval_results": {"predictions": ["the thylakoid space", "the Caesars Palace Grand Prix", "A compact car (North America), or small family car", "Benjam\u00edn Arellano F\u00e9lix", "Koch Industries", "Enigma", "Yellow fever", "Julia Compton Moore", "Lord's Resistance Army", "Workers' Party", "Yasiin Bey", "1763", "Bulgaria", "the Swiss national team", "the remake", "Oldham County, Kentucky", "The Captain Matchbox Whoopee Band", "Alec Berg", "the United States, the Falkland Islands, and Peru", "Ghana", "Nikolai Alexandrovich Morozov", "Rabies", "Switzerland", "Tennessee", "Godiva", "October 21, 2016", "August 1973", "Lawrence of Arabia", "The Ansonia", "1937", "Government of Ireland", "Leona Lewis", "John Robert Cocker", "$7.3 billion.", "Angus Brayshaw", "Michael Chekhov", "the \"Black Abbots\"", "Sarah Kerrigan, the Queen of Blades", "German", "Katy Perry", "Bharat Ratna", "his exploration and settlement of what is now Kentucky,", "Bodie", "\"Nebo Zovyot\"", "\"Orchard County\"", "The Kree", "110 films", "The shortest player ever to play in the National Basketball Association", "The authorship of Titus Andronicus", "Denmark", "Patricia Arquette", "James Corden", "Audrey II ''", "the end of the 2015 season", "red", "Shakyamuni", "Jim Braddock", "His son should be devoting his energy to fighting hunger and poverty in Africa, not fighting in Iraq.", "co-wrote", "Brazilian supreme court judge", "the Mets", "the Washington Redskins", "William Wordsworth", "(A - Abel)"], "metric_results": {"EM": 0.5, "QA-F1": 0.5927624458874459}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5454545454545454, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-667", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-1872", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-4569", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-4184", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-2512", "mrqa_triviaqa-validation-768"], "SR": 0.5, "CSR": 0.5532094594594594, "EFR": 1.0, "Overall": 0.726657516891892}, {"timecode": 37, "before_eval_results": {"predictions": ["the plain moraine plateau", "the original name in Latin", "leaves", "\"Nizhny Novgorod\"", "James Bond", "a unit of money", "keeper of the Longstone (Fame Islands) lighthouse", "\"Carlos the Jackal\"", "Australia", "Margot Frank", "Belgium", "Sufjan Stevens", "Spain", "Benny Hill", "Roddy Doyle", "Kevin Spacey", "Athens", "the Republic of Chad", "2014", "neck", "David Hockney", "Rudyard Kipling", "lactic acid", "Fleet Street", "The Netherlands", "fractal geometry", "Cosmos: A Spacetime Odyssey", "fish, birds, reptiles and insects", "Aquaman", "(Jean-Paul) Sartre", "novel", "india", "\"The Hat\" McVitie", "whisky", "Switzerland", "sheep", "trumpet", "Babe Ruth", "(Loutton) Kant", "lamas", "Heston Blumenthal", "Tony Curtis", "U2", "a peasant's wife", "was not only the greatest threat to Nazi", "Charlie Sheen", "Shirley Caesar", "Canada", "Buster Edwards", "Chief Inspector of Prisons", "Henley Royal Regatta", "lamas Moorehead", "Mayor Hudnut", "Cairo, Illinois", "Lazio region", "Chuck Schumer", "\" SKUM\"", "TNT", "HSH Nordbank Arena", "October 2007", "\"Dora\" Salter", "Romanian Communist leader", "the Alaskan Malamute", "pronghorn"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48697916666666663}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-953", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5911", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-665", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-1647", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-2340", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6211", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-7575", "mrqa_triviaqa-validation-4500", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-7109", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-8239", "mrqa_hotpotqa-validation-3529", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-731", "mrqa_searchqa-validation-10306", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-10797", "mrqa_hotpotqa-validation-2366"], "SR": 0.421875, "CSR": 0.5497532894736843, "EFR": 1.0, "Overall": 0.7259662828947369}, {"timecode": 38, "before_eval_results": {"predictions": ["telecommunications, and computers.", "Dutch", "tenor Peter Yarrow, baritone Noel Paul Stookey and alto Mary Travers.", "Washington", "Zack Snyder", "Mondays", "The Cosmopolitan of Las Vegas", "Anna Clyne", "Meghan Markle", "exploitation of natural resources particularly North Sea oil", "Commissioner", "August 1973", "Burnley", "Teenitans Go!", "Evey's mother", "Love and Theft", "1978", "SKUM", "Edmonton, Alberta.", "Seattle", "\"The School Boys\"", "Orchard Central", "The Kennedy Center", "commanders", "Environmental Protection Agency", "Humberside", "Diamond Rio", "The Tempest", "Northampton, England,", "Mike Greenwell", "2017", "Scandinavian Airlines System Aktiebolag", "Polka", "Irish Chekhov", "1860", "2006", "Ghanaian national team", "Coronation Street", "The Dragon", "Cold Spring Historic District", "Arctic fox", "Sophie Monk", "The Supremes", "Sunset Publishing Corporation", "Melbourne Storm", "twenty", "9 November 1967", "Retina display", "technical director", "Cincinnati metropolitan area", "Captain B.J. Hunnicutt", "19 June 2018", "the naos", "31 - member", "kendo", "Jeffrey Archer", "annie", "little blue booties.", "Majid Movahedi,", "Espinoza Patron's", "Groundhog Day", "ninjas", "witch", "Mozart"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7110615079365079}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.8571428571428571, 1.0, 0.42857142857142855, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.22222222222222224, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3812", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-2868", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-1300", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-5545", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-16163", "mrqa_searchqa-validation-3163"], "SR": 0.609375, "CSR": 0.5512820512820513, "EFR": 1.0, "Overall": 0.7262720352564103}, {"timecode": 39, "before_eval_results": {"predictions": ["funding education, sanitation, and traffic control within the city limits", "Aly Raisman", "40,400 members", "Seoul, South Korea", "English", "banjo", "William Powell \"Bill\"Leary", "Distinguished Service Cross", "Revolver", "Sam Raimi", "\"Martian Manhunter\"", "Appleby-in-Westmorland", "Wolfgang Amadeus Mozart", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Mercer Bears", "Dame Eileen June Atkins", "June", "\"Little Dixie\"", "1979", "1905", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Loughborough University", "Los Angeles", "2.1 million", "Sierra Nevada mountains", "Alfred Joel Horford Reynoso", "Bad News", "Ghanaian", "Anthony Stephen Burke", "Lorman, Mississippi", "Wilderness Road", "Alfred Edward Housman", "The Killer", "London", "4 km", "25 October 1921", "\"War & Peace\"", "United States Senate election", "Tayeb Salih", "Mickey Gilley", "Kenobi", "Edward Albert Heimberger", "Whoopi Goldberg", "Gian Carlo Menotti", "Indian", "Target Corporation", "Archbishop of Canterbury", "cetaceans", "City of Newcastle", "gamecock", "Aaliyah Dana Haughton", "Mad - Eye Moody", "A vanishing point", "16,801", "Lady Gaga", "March", "Georgetown", "The Palm Jumeirah", "consumer confidence", "Japanese Foreign Ministry spokesman Hidenobu Sobashima", "Saturday Night Live", "Fried Green Tomatoes", "Bon Jovi", "Marcie Blane"], "metric_results": {"EM": 0.625, "QA-F1": 0.734011994949495}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7246", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-4770", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1074", "mrqa_searchqa-validation-2773"], "SR": 0.625, "CSR": 0.553125, "EFR": 1.0, "Overall": 0.726640625}, {"timecode": 40, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1611", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2331", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3188", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5538", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-5705", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-987", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4998", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-809", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12714", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5071", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9480", "mrqa_squad-validation-10044", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-10326", "mrqa_squad-validation-10425", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1231", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1472", "mrqa_squad-validation-1608", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2006", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2819", "mrqa_squad-validation-297", "mrqa_squad-validation-3001", "mrqa_squad-validation-3262", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3812", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4078", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4543", "mrqa_squad-validation-4611", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5079", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5355", "mrqa_squad-validation-5563", "mrqa_squad-validation-5597", "mrqa_squad-validation-5616", "mrqa_squad-validation-5881", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6223", "mrqa_squad-validation-6251", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-7952", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8199", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9768", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-244", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6607", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.833984375, "KG": 0.4703125, "before_eval_results": {"predictions": ["gilt bronze", "These oceans are kind of the last frontier for use and development,", "President Obama's", "they would not be making any further comments,", "Pakistan's North West Frontier Province,", "$250,000", "The Valley swim Club", "the actor who created one of British television's most surreal thrillers", "in a stream in Shark River Park", "helicopters and unmanned aerial vehicles", "\"I would ultimately be a better person and of more service in whatever doors God opened next in life if I stuck around to learn lessons rather than running and hiding down at the farm.\"", "between 1917 and 1924", "Frank Ricci", "Janet Napolitano", "Mahmoud Ahmadinejad", "\"distortion\"", "Jacob", "\"local Anjuna boy\" who was a bartender at Liu's,", "AMD", "Lousiana", "prostate cancer", "Eintracht Frankfurt", "Tsvangirai", "the FBI", "\"GoldenEye\"", "The Georgia Aquarium", "puppy", "Kurdish militant group", "\"It's very new and involves repairing my leaky valve using a clip device, without open heart surgery, so that my heart will function better,\"", "Ralph Cifaretto", "not", "Because the federal government is asleep at the switch", "17", "a rally", "an empty water bottle", "Val d'Isere, France", "a head injury", "News of the World tabloid", "Pakistan's combustible Swat Valley", "200.", "Manny Pacquiao", "1962", "Pixar", "Mandi Hamlin", "environmental", "80,", "The EU naval force", "The meter reader", "BC Place Stadium.", "Gary Player", "200", "Adam", "Erika Mitchell Leonard", "1969", "V\u00e1clav Havel", "right side", "fever", "London", "703", "20 October 1980", "glaciers", "mayor", "Cerberus", "1967"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5118011277937748}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.18181818181818182, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.05714285714285715, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-2237", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1159", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-2427", "mrqa_searchqa-validation-2894"], "SR": 0.453125, "CSR": 0.5506859756097561, "EFR": 0.9714285714285714, "Overall": 0.7098135344076655}, {"timecode": 41, "before_eval_results": {"predictions": ["Business Connect", "\"explosion of violence.\"", "40-year-old", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "six alleged victims,", "Kevin Kuranyi", "Kenyan and Somali governments", "Aung San Suu Kyi", "legitimacy of that race.", "\"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma, by starting a dialogue while maintaining sanctions,", "for the New Yorker, as well as a quarter century as an advocate for social activism.", "Mashhad", "Swat Valley.", "North Korea", "The National Infrastructure Program,", "Steven Green", "Americans", "over 1000 square meters", "sailor", "they'd get to bring a new puppy with them to the White House in January.", "27-year-old", "Friday,", "Los Ticos", "Seasons of My Heart", "helping on the sandbags", "$17,000", "opium", "for not doing more since taking office.", "10 years", "$8.8 million", "in Noida, located in the outskirts of the capital New Delhi.", "Lisa Brown", "at least two and a half hours.", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England this weekend.", "MDC head Morgan Tsvangirai.", "31 meters (102 feet) long and 15 meters (49 feet) wide", "London's Heathrow airport", "165-room", "Transport Workers Union leaders", "state senators", "84-year-old", "(5 hours)", "the Southern Baptist Convention,", "Sen. Barack Obama", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Anil Kapoor", "most devices carry few security risks.", "Alberto Giacometti.", "South African police", "Hurricane Gustav", "Felix Baumgartner", "July 2012", "1260 cubic centimeters ( cm ) for men", "(George Bernard Shaw)", "a Dutch painter", "kievan Rus", "Toronto", "1993", "Robert L. Stone", "Nike", "Barbara Bush", "Qwerty", "corpulent"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6127886002886003}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.13333333333333336, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3430", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-902", "mrqa_naturalquestions-validation-7458", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2480", "mrqa_hotpotqa-validation-582", "mrqa_searchqa-validation-7340"], "SR": 0.546875, "CSR": 0.5505952380952381, "EFR": 1.0, "Overall": 0.7155096726190476}, {"timecode": 42, "before_eval_results": {"predictions": ["Wittenberg", "Prohibition", "Bligh", "Parkinson's", "crime", "crimean", "Moscow", "dutch", "Portugal", "first among equals", "Friedrich Nietzsche", "the moon", "Moldova", "Zak Starkey", "Craggy", "Suez", "otters", "hickory", "Port Talbot", "Rapa Nui", "Democrat", "Charlie Cairoli", "Allende", "Mike Tyson", "Adam Smith", "conductor", "Boyle", "divination", "fur", "Tony Blair", "Adolf Hitler", "Jamaica", "June Brae", "ADHD and hypertension", "1066", "the Three Graces", "Jesse James", "Purple Heart Medal", "Cerebro- Spinal Fluid", "Jessica Simpson", "dutch", "the MacKenzie", "Robert Devereux", "NASCAR", "Canada", "dutch", "Louis Daguerre", "Argentina", "Kwame Nkrumah", "The Color Purple", "terror", "lithium", "can be produced with constant technology and resources per unit of time", "Mariah Carey", "lifetime achievements", "unidentified flying objects", "the Chicago Bears", "bear", "Turkey", "Pew Research Center", "corticosteroids", "Ferrari", "Smoky", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5930148524720893}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.21052631578947367]}}, "before_error_ids": ["mrqa_squad-validation-2165", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-2340", "mrqa_triviaqa-validation-882", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-5069", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-797", "mrqa_newsqa-validation-3680", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-9911", "mrqa_naturalquestions-validation-4915"], "SR": 0.46875, "CSR": 0.5486918604651163, "EFR": 0.9705882352941176, "Overall": 0.7092466441518468}, {"timecode": 43, "before_eval_results": {"predictions": ["John M. Grunsfeld", "dancing with the Stars", "psychotropic drugs", "opium", "1991-1993", "below zero", "Democrat", "high school", "16", "iReporter Rany Freeman", "forgery and flying without a valid license", "President Bush", "15-year-old's", "seven", "upper respiratory infection", "543", "Kevin Kuranyi", "Amy Bishop Anderson", "Susan Atkins", "Ameneh Bahrami", "Virgin America", "$1,500", "Al Nisr Al Saudi", "We Found Love", "his parents", "Ralph Lauren", "team Katarina Smirnoff", "humanitarian mission", "North Korea", "beetles", "Old Trafford", "The Bronx County District Attorneys Office", "Arabic, French and English", "Britain.", "Arsenal manager Arsene Wenger", "The FBI's Baltimore field office", "Michael Jackson", "all day starting at 10 a.m.", "her mom,", "South African police", "an American who entered the country illegally from China", "Palestinian Islamic Army,", "same-sex civil unions", "was killed", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "consumer confidence", "Phil Spector", "the District of Columbia National Guard", "Australia and New Zealand", "Steven Gerrard", "unprecedented wave of buying", "Durham Cathedral", "Hermann Ebbinghaus", "statesmen who led the secession movement", "lion", "myxoma virus", "Wisconsin", "Battle of Britain and the Battle of Malta", "Viacom Media Networks", "five", "carbon", "salvador", "hitler", "100"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6524305555555556}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2065", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9516", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-2853", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-7435"], "SR": 0.59375, "CSR": 0.5497159090909092, "EFR": 1.0, "Overall": 0.7153338068181818}, {"timecode": 44, "before_eval_results": {"predictions": ["the Supreme Court of the United Kingdom", "Seal", "nasal cavity", "coffin-maker", "63 to 144 inches", "river of the Wye", "Zorro", "USS Thresher", "\"tattoo\"", "of a city of a country is also the most populous.", "Volkswagen", "eagle", "Morgan Spurlock", "Jackie Jackson", "helps managers understand employees' needs in order to further employees' motivation.", "the Castle", "Yellowstone", "Hornets", "Nevada", "muezzin", "goat", "Rihanna", "Tintin", "Victoria Alexandrina", "22", "Hector BERLIOZ", "Azerbaijan", "Ireland", "Ash", "Madness", "the Dalton Gang", "Australia", "Debbie Abrahams", "bats", "the United States", "Penelope Keith", "Alexei Kosygin", "John Galsworthy", "Vinegar Joe", "James Van Allen", "pangram", "Tyrrhenian Sea", "Steel Beads", "Nicaragua", "Passepartout", "grosvenor Crescent", "Lancashire", "Manet", "the McDonald brothers", "of Thebes", "onda", "Owned by Renhe Sports Management Ltd", "President of the sitting Governors", "David Ben - Gurion", "Chow Tai Fook Enterprises.", "an organ", "Point of Entry", "1973's", "a violent government crackdown seeped out.", "Morgan Tsvangirai.", "Hungarian", "China", "China", "Isabella (Belle) Baumfree"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-6188", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4657", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-7893", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3391", "mrqa_searchqa-validation-15957"], "SR": 0.5625, "CSR": 0.55, "EFR": 1.0, "Overall": 0.715390625}, {"timecode": 45, "before_eval_results": {"predictions": ["trespassing at a nuclear-missile installation", "alexandria", "1", "skull", "alexandria", "Justin Trudeau,", "Washington", "Nuuk", "Philippines", "pool", "China", "woolen", "wool", "barbara", "alexandrovna", "soybeans", "Leeds", "wood", "Elizabeth II", "dogs", "a llama", "piccadilly Line", "antonyms", "manon Lescaut", "Nepal", "scurvy", "tailors", "Indonesia", "purpurea", "orchestral", "keane", "gauteng", "alexandown", "Pakistan", "Uranus", "fast", "alexandria", "my favorite martin", "niki lauda", "petronas", "The Daily Mirror", "Eric Morley", "radio waves", "yorkshire", "notts County", "football", "Reform Club", "William Shakespeare", "snark", "trimdon", "Tasmania", "26,000 years", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "Orange Juice", "Fifteenth", "Plato", "physicist Richard Feynman", "on the 12th on the Blue Monster course at Doral", "Martin Aloysius Culhane", "400 years", "freelance", "mouth", "newt Gingrich", "sentence set at $500,000 each."], "metric_results": {"EM": 0.40625, "QA-F1": 0.4935763888888889}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.5555555555555556, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-7559", "mrqa_triviaqa-validation-2995", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-4038", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-5221", "mrqa_naturalquestions-validation-4437", "mrqa_naturalquestions-validation-10402", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-5176", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-4100", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-11241", "mrqa_newsqa-validation-3806"], "SR": 0.40625, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.714765625}, {"timecode": 46, "before_eval_results": {"predictions": ["Konwiktorska Street,", "mashed potato", "Lalo Schifrin", "July 2000", "Don McMillan", "7 correct numbers", "Billy Hill", "Paul Lynde", "halogenated paraffin hydrocarbons", "the body - centered cubic ( BCC ) lattice", "May 2002", "2010", "virtual reality simulator", "beneath the liver", "between 1881 and 1885", "prior to European contact", "2014", "Most days are sunny throughout the year", "chlorine and bromine from manmade organohalogens", "Karen Gillan", "1975", "similar product but differs in ingredients", "the homicidal thoughts of a troubled youth", "The Hunger Games : Mockingjay -- Part 2 ( 2015 )", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "October 22, 2017", "Amitabh Bachchan", "1998", "nine", "Jeff Bezos", "adrenal medulla", "in lymph", "electron donors", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "produced with constant technology and resources per unit of time", "Lewis Carroll", "January 2004", "William Paca", "Cetshwayo", "early 1980s", "Asuka", "Erica Rivera", "New York", "September 2001", "in elocution teaching to demonstrate rounded vowel sounds", "Chris Rea", "mashed potato", "cases that have not been considered by a lower court may be heard by the Supreme Court in the first instance", "three", "Zoe Badwi", "The Mongol - led Yuan dynasty", "candy bars", "1948", "Ruth Rendell", "Hidden America with Jonah Ray", "the United Kingdom", "people working in film and the performing arts", "Jean Van de Velde", "between 1917 and 1924", "lawyers trying to save their client from the death penalty", "my Name is Earl", "The Virgin Spring", "cookies", "uncle"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7039649945820022}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307692, 1.0, 0.08695652173913042, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.0, 0.888888888888889, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-6321", "mrqa_triviaqa-validation-7778", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-2156", "mrqa_searchqa-validation-7061"], "SR": 0.59375, "CSR": 0.5478723404255319, "EFR": 0.9615384615384616, "Overall": 0.7072727853927987}, {"timecode": 47, "before_eval_results": {"predictions": ["Acadia National Park", "George S. Long", "Luxembourg", "low caste", "Challenger", "the Rolling Stones", "Thomas G Nazareth", "lapis lazuli", "the Pentagon", "valley", "snails", "bamboo", "the Vietnam War", "Kingston", "Gerard Mercator", "a sailor", "Barnum & Bailey", "Diz 'N Bird", "oh no", "Ernie Els", "FYROM", "gestation", "grime", "an independent oil man", "Herb Alpert", "John Adams", "the Rolling Stones", "Bernard Montgomery", "Zeus", "Fast Food Nation", "the Tasmanian devil", "hatta al-nasr", "a terrarium", "cyclotron", "prithee", "brothel", "Barcelona", "Yellow", "the Indy 500", "The Hills", "lowest", "porter", "Kansas", "bordered by the Atlas Mountains of North Africa", "Baton Rouge", "Kamehameha I", "menudo", "Alan Alda", "dense", "a park", "sirloin", "New England", "Walter Pauk, an education professor at Cornell University", "left atrium of the heart", "the American Civil War", "a group of elders in Brooklyn, New York,", "the United States", "first published in the \"First Folio\" in 1623.", "Obafemi Akinwunmi Martins", "\"Twice in a Lifetime\".", "two-state solution", "Republican", "Air traffic delays began to clear up", "The Tempest"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6069444444444445}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-847", "mrqa_searchqa-validation-11698", "mrqa_searchqa-validation-9392", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-13199", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-15464", "mrqa_searchqa-validation-624", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-16787", "mrqa_searchqa-validation-6431", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4236", "mrqa_searchqa-validation-11015", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-3996", "mrqa_searchqa-validation-1817", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2980", "mrqa_naturalquestions-validation-3174", "mrqa_triviaqa-validation-7062", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-821", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-904"], "SR": 0.546875, "CSR": 0.5478515625, "EFR": 1.0, "Overall": 0.7149609375}, {"timecode": 48, "before_eval_results": {"predictions": ["photoelectric", "31", "It Ain't Over'til It's Over ''", "The Satavahanas", "a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "gamma efferent neurons", "bohrium", "London", "Chernobyl Nuclear Power Plant", "Dalveer Bhandari", "Bush", "the center of the Northern Hemisphere", "the Noahic Covenant", "David Tennant", "a political protest and mercantile protest by the Sons of Liberty in Boston, Massachusetts", "is married to Bobby", "his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "A footling breech", "the speech was not given at the 1964 Republican National Convention in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater", "four", "Jonathon Dutton", "1990", "pickup trucks", "Bob Dylan", "qualitative data, quantitative data", "Johannes Gutenberg", "to collect menstrual flow", "Her Majesty's Ship ''", "William the Conqueror", "Sir Ernest Rutherford", "Nicole Gale Anderson", "William Chatterton Dix", "April 12, 2017", "Gupta Empire", "Freedom Day", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "Gustav Bauer", "the external genitalia", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "in technical journals, the abstract ; and before any lists of tables or figures, the foreword, and the preface", "Antarctica", "Cristeta Comerford", "revenge and karma", "the efferent nerves that directly innervate muscles", "1986", "1546", "a crime or any wrongful act", "1942", "Bachendri Pal", "John Adams", "early 2014", "the Old Vic", "Jessica Smith", "Edward Elgar", "\"Big Mamie\"", "six", "\"First Family of Competitive Eating\"", "every dollar", "he had struggled to adapt to the different culture and religious life in Sudan.", "on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "St. Valentine's Day Massacre", "(Liza) Minnelli", "Gabriel", "Gloria Macapagal Arroyo"], "metric_results": {"EM": 0.5, "QA-F1": 0.5765752376573152}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false], "QA-F1": [0.4, 0.0, 0.0, 0.0, 0.9500000000000001, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.7894736842105263, 0.06451612903225806, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-4001", "mrqa_hotpotqa-validation-1210", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1290", "mrqa_searchqa-validation-16493", "mrqa_newsqa-validation-3113"], "SR": 0.5, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.714765625}, {"timecode": 49, "before_eval_results": {"predictions": ["1912", "Standard Oil", "noh", "( Olivia) Holt", "pachycephalosaurs", "germany", "Nancy Lopez", "the Antarctic ozone hole", "who's the Boss", "Donnie Wahlberg", "Tasmania", "the Baltimore Orioles", "Tunisia", "RMS Queen Mary 2", "Zionism", "prague", "horse-breaking", "(Isaac) Newton", "Toby Keith", "accordion", "Black swan", "Edith Piaf", "the Stratosphere", "parkinsonism", "orchestral works", "(William) Rehnquist", "Guinevere", "Department of Energy", "tangerine", "Belgium", "Dead Ringers", "Johann Strauss II", "Solidarity", "( Amy) Carter", "Nick and Norah", "Charles Lindbergh", "(Nymphaea caerulea)", "Haunted Mansion", "blogging", "(Jack) Nicklaus", "a war between organized groups", "a vacuum flask", "Teen Titans Go", "an owl", "Istanbul", "Mary Poppins", "St. Louis", "the Amish", "the Air Force", "Levi Strauss", "Badminton", "Canada", "Sunday evenings", "autopistas", "Phar Lap", "a dove", "Lewis Carroll", "1983", "Excalibur Hotel and Casino", "Czech Kingdom", "Arsenal", "Abdullah Gul,", "humans", "Action Comics"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7503720238095237}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4796", "mrqa_searchqa-validation-3755", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-14374", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-767", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-8462", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-1337", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-6473", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-8129", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-15313", "mrqa_naturalquestions-validation-8350"], "SR": 0.640625, "CSR": 0.5487500000000001, "EFR": 1.0, "Overall": 0.715140625}, {"timecode": 50, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1338", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1604", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7716", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12265", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3025", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-3764", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8267", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9911", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3635", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.845703125, "KG": 0.49375, "before_eval_results": {"predictions": ["Aerosmith", "Willa Cather", "the General Assembly", "\"The Who\"", "a science fiction novel", "Bismarck", "a analog watch", "a Tricer Programme", "Luisa Tetrazzini", "Renoir", "the polio vaccine", "the deer", "the Rothschilds", "the bar", "Fyodor Dostoevsky", "Smucker\\'s", "Almagro", "Canada", "grease", "Hollandaise", "Esau", "Dry Ice", "Martin Luther King", "a process in which oxygen atoms from the air", "a catalyst", "Kansas City", "a American infantryman of WWI # Quiz # Question", "Uganda", "senators", "Sappho", "the Battle of Thermopylae", "the Maccabees", "John Paul Jones", "Hamlet", "a flounder", "the Ganges", "New Brunswick", "the Copacabana", "I Write the Songs", "We Own the Night", "Shimon Peres", "Mr. & Mrs. Smith", "Triceratops", "a cake", "B.B. King", "Thomas Mann", "Krackel", "a dog eat dog world", "Dmitri Mendeleev", "the Prisoner of Azkaban", "the Aura", "Barry Mann", "a Arabic masculine given name and occasional surname with the meaning `` beloved ''", "the heads of federal executive departments who form the Cabinet of the United States", "Microsoft", "General John J. Pershing", "Rupert Murdoch", "Taylor Swift", "September 23, 1935", "An invoice, bill or tab", "Martin \"Al\" Culhane,", "Harry Potter will be able to gamble in a casino, buy a drink in a pub or see the horror film \" Hostel: Part II,\"", "Hyundai", "2015"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6142734869297368}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 0.0, 0.0, 0.18181818181818182, 0.25000000000000006, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.4, 1.0, 0.07692307692307691, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15858", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-130", "mrqa_searchqa-validation-16767", "mrqa_searchqa-validation-803", "mrqa_searchqa-validation-5612", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-7278", "mrqa_searchqa-validation-14038", "mrqa_searchqa-validation-13163", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-14305", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-16283", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-752", "mrqa_searchqa-validation-159", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-7741", "mrqa_naturalquestions-validation-998", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-6798", "mrqa_hotpotqa-validation-5801", "mrqa_newsqa-validation-762", "mrqa_newsqa-validation-1048"], "SR": 0.515625, "CSR": 0.5481004901960784, "EFR": 1.0, "Overall": 0.7271200980392156}, {"timecode": 51, "before_eval_results": {"predictions": ["peanut butter", "The Moonstone", "Chief of Staff", "the Bible", "Helen of Troy", "the Civil War", "the Nobel Prize", "mime", "Dracula", "Technetium", "Nazareth", "867-5309", "Miss Havisham", "Thailand", "Whitney Houston", "opal", "Taft", "hot air", "air", "echidna", "water", "porcelain", "Synchronicity", "Tim Duncan", "bees", "dark energy", "Reptiles", "Conakry", "William Herschel", "Tiberius", "Barbara Walters", "Jubal Anderson", "Taft", "Pumice", "watermelon", "Cole Porter", "Taft", "ro- mittagong", "Taft", "Cosmopolitan", "Madagascar", "Carl Jung", "Carnarvon", "Ontario", "Olympia", "Nicolaus Copernicus", "ross", "the Palladium", "the Black Death", "Google", "Defense", "they believed that it violated their rights as Englishmen", "16 April 1898", "Brevet Colonel Robert E. Lee", "Don Revie", "Gilda", "the Buddha", "Lancia-Abarth #037", "1963", "Brittany Anne Snow", "Christopher Savoie", "authorizing killings and kidnappings by paramilitary death squads.", "\"Watchmen\"", "Cork"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5316349637681159}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.391304347826087, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-3888", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-1496", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-440", "mrqa_searchqa-validation-5000", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-10284", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-9206", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-4585", "mrqa_searchqa-validation-4971", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-1277", "mrqa_triviaqa-validation-7153", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-5582"], "SR": 0.453125, "CSR": 0.5462740384615384, "EFR": 0.9714285714285714, "Overall": 0.7210405219780219}, {"timecode": 52, "before_eval_results": {"predictions": ["Deere", "(E) POD", "electrons", "the Missouri River", "wine", "George Babbitt", "GIGO", "Rossini", "to go to a prostitutehouse", "Rome", "the Isle of Wight", "Colorado Springs", "hay", "Possession", "(Etonian) Scott", "the contact lens", "kilometers", "surface-to-air missile", "Vibe", "Pulp Fiction", "yelping", "Frederick Forsyth", "August 1947", "Princess Leia", "Vietnam", "Vince Lombardi", "the global village", "Dublin", "Sudan", "kwanzaa", "Warren Buffett", "Charlie\\'s Angels", "Lincoln", "imagism", "whimper", "obsoleteness", "oscar wilde", "Taiwan", "Mickey Spillane", "Buzz Lightyear", "the United States", "Scrabble", "kidney stones", "Necessity", "diamonds", "Clinton", "Atlanta", "Texas", "glucose", "hours", "tigre", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Sir Donald Bradman", "The Caucasus Mountains", "Rambo", "Darby and Joan", "antelope", "Jung Yun-ho", "7.63\u00d725mm Mauser", "a united Ireland", "raping and murdering a woman in Missouri.", "troy Livesay", "a series of wildfires from late summer through autumn in Bastrop County.", "Estadio Victoria"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6301339285714285}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-3292", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-14452", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-14015", "mrqa_searchqa-validation-9935", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-12531", "mrqa_searchqa-validation-13465", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5962", "mrqa_searchqa-validation-9744", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-8817", "mrqa_searchqa-validation-14998", "mrqa_triviaqa-validation-6223", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3364"], "SR": 0.578125, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.7268749999999999}, {"timecode": 53, "before_eval_results": {"predictions": ["to encourage rebellion against the British authorities", "Debbie Gibson", "three", "February 1", "Ireland", "December 2, 1942", "unknown origin", "the heart", "March 26, 1973", "Necator americanus", "April 7, 2016", "Gare du Nord", "Games played", "East Antarctic Plateau", "Frank Langella", "The Maidstone Studios", "Human fertilization", "Father Christmas", "16 seasons", "Bill Russell", "the Washington Redskins", "Donald Trump", "The vascular cambium", "Epidemiology", "1895", "the contestant", "between the Mediterranean Sea to the north and the Red Sea to a south, and is a land bridge between Asia and Africa", "a Border Collie", "Washington metropolitan area", "Julie Adams", "Gatiman", "John Young", "Kevin Spacey", "a novella", "The neck", "Gene MacLellan", "a flash music video featuring an animated dancing banana was created", "2010", "on location", "Frankie Muniz", "before the first letter of an interrogative sentence or clause to indicate that a question follows", "Jack - Jack Parr", "Jenna Boyd", "not about drugs", "2017", "1978", "a loanword of the Visigothic word guma `` man ''", "air moisture", "birch", "First Lieutenant Israel Greene", "brothers Henry, Jojo and Ringo Garza", "Ken Barlow", "tides", "Ancient Rome", "Buddha\\'s delight", "Matt Kemp", "pro-Confederate partisan rangers", "they", "30,000", "Collier County", "(Prince) Picasso", "Conner Peterson", "(Prince) Albert", "a thumb"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6203258903306654}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.06451612903225806, 0.7142857142857143, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.631578947368421, 0.2, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-4240", "mrqa_triviaqa-validation-6845", "mrqa_triviaqa-validation-5886", "mrqa_hotpotqa-validation-3154", "mrqa_hotpotqa-validation-458", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3873", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-16117", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-5691"], "SR": 0.53125, "CSR": 0.5465856481481481, "EFR": 0.9333333333333333, "Overall": 0.7134837962962963}, {"timecode": 54, "before_eval_results": {"predictions": ["Norway", "Ecuador", "Home Improvement", "iron", "a salmon", "Berlin", "Iago", "(Fidel) Castro", "Patrick Floyd Garrett", "Montana", "Annenberg Hall", "(General) Custer", "an arboretum", "Marie Curie", "Abnormal Psychology", "love", "a Representative and a Senator from Maine", "the Italian flag", "Samuel Butler", "Kitty Kelley", "Abraham Lincoln", "teddy bears", "Crouching Tiger, Hidden Dragon", "a baseball diamond in his corn fields", "upsilon", "banknotes", "arizonensis", "Jupiter", "conformation", "Ziegfeld Girl", "David Cassidy", "a mountain", "the Louvre", "Greek", "opera", "Phosphorus", "a den", "the Wessex", "snowmobil", "Aaron Copland", "red", "voltage", "Act I of The Royal Ballet", "Nikola Tesla", "Lil Jon", "the diamond", "a plum", "Lizzie Borden", "hockey", "Pop-Tarts", "bovine spongiform encephalopathy", "Henry Purcell", "Mankombu Sambasivan Swaminathan", "eight", "Ethiopia", "a cactus", "Argentina", "Real Madrid and the Spain national team", "1983", "Richard Allen Street", "Brazil", "mpire of the Sun", "whether he should be charged with a crime,", "four"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6692708333333334}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-8125", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-16878", "mrqa_searchqa-validation-10404", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-9692", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-4573", "mrqa_searchqa-validation-5952", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-13757", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-3273", "mrqa_triviaqa-validation-261"], "SR": 0.59375, "CSR": 0.5474431818181817, "EFR": 1.0, "Overall": 0.7269886363636363}, {"timecode": 55, "before_eval_results": {"predictions": ["Santa Fe", "Beanie Babies", "kick drum", "chess", "cola", "Berlin", "Jon Stewart", "Michael Phelps", "( Modest) Mussorgsky", "Romeo", "Cerberus", "rice", "the Nile", "bullion", "Plutarch", "figure skating", "the submarine", "St. Augustine", "the test", "Fiji", "the burnoose", "Tesla", "the Mekong", "29", "Valentina Tereshkova", "Canada", "acrobat", "Missouri", "ribonucleic acid", "Rubeus Hagrid", "Manitoba", "Death of a Salesman", "Chocolate", "inshallah", "Saudi Arabia", "Pamela Anderson", "Jenny Craig", "Idaho", "tootsie", "Edward VI", "the Empire State Building", "the lines", "Tennessee", "the constitution", "Toronto", "University of Exeter", "Ford", "Lawrence", "Andy Warhol", "baseball games", "Tara Reid", "Michigan State Spartans", "moist temperate climates", "B.F. Skinner", "\"Nokia tune\".", "General Paulus", "Cyprus", "2010", "Tufts", "the Crips", "U.S. Court of Appeals for the District of Columbia", "he fears a desperate country with a potential power vacuum that could lash out.", "Monday.", "Spc. Megan Lynn Touma,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6244791666666667}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-13435", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-3517", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9266", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-10897", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-7386", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-438", "mrqa_hotpotqa-validation-4049", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2772"], "SR": 0.546875, "CSR": 0.5474330357142857, "EFR": 0.9655172413793104, "Overall": 0.7200900554187192}, {"timecode": 56, "before_eval_results": {"predictions": ["the former English county of Humberside", "the Federal Bureau of Prisons", "\"Dumb and Dumber\"", "the first flights between Australia and New Zealand", "John Hunt", "Walt Disney Productions", "Reinhard Heydrich", "a British singer-songwriter and multi-instrumentalist", "\"Sheen Michaels Entertainment\"", "her sixth studio album", "1770", "Indianola", "September 10, 1993", "A Bug's Life", "the U.S. military", "a few", "the Qin dynasty", "Kentucky River", "the fourth", "\"The Bob Edwards Show\"", "The S7 series", "White Knights of the Ku Klux Klan", "Key West, Florida", "Charlie Puth", "Fort Albany", "three Golden Globe Awards", "Lake Placid, New York", "the National Society of Daughters of the American Revolution", "Martin Scorsese", "John Monash", "Protestant Christian", "the Kentucky RiverBats", "Firestorm", "Agra", "50 million", "Henry II", "Scotty Grainger", "the study of movement and dance", "An agricultural cooperative", "Kairi", "Texas", "the Democratic Unionist Party (DUP)", "five", "Candice Swanepoel", "the greater risk-adjusted return of value stocks over growth stocks", "Chris \"Izzy\" Cole", "multiple awards", "McComb, Mississippi", "\"King of Cool\"", "1995", "\"Losing My Religion\"", "The Royalettes", "three", "April 17, 1982", "Jaguar XJS", "'Q'", "Aintree", "\"Britain's Got Talent\"", "Turkey", "Seoul", "Charleston", "emerald", "El burlador de Sevilla", "Pandora"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5753348214285714}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-983", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-314", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1229", "mrqa_newsqa-validation-154", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-7140"], "SR": 0.484375, "CSR": 0.5463267543859649, "EFR": 1.0, "Overall": 0.726765350877193}, {"timecode": 57, "before_eval_results": {"predictions": ["Italian architect and art theorist Leon Battista Alberti", "in a counter clockwise direction", "The Sixth Extinction II : Amor Fati '', Fowley comes in disagreement with him", "December 2, 1942", "Ray Charles", "late November or early December", "The period of being a junior doctor starts when they qualify as a medical practitioner following graduation with a Bachelor of Medicine, Bachelor of surgeons degree and start the UK Foundation Programme", "Daniel Suarez", "its population", "throughout Mexico, in particular the Central and South regions, and by people of Mexican ancestry living in other places, especially the United States", "Matthew Roberts", "Veronica", "Dan Stevens", "9.7 m", "Assam Provincial Congress Committee", "Muno, Foofa, Brobee, and Toodee", "modern state system", "efferent nerves", "William Wyler", "Dragon Ball GT", "American country music group The Nitty Gritty Dirt Band", "Donald Fauntleroy Duck", "2013", "investment bank Friedman Billings Ramsey", "2018", "skeletal muscle and the brain", "Joe Spano", "Spanish moss", "Georges Auguste Escoffier", "Nodar Kumaritashvili", "December 27, 2015", "Madeline Reeves", "New England Patriots", "Ashoka", "Great Britain", "Government House at New Delhi", "an unknown recipient", "Andy Warhol", "Elected Emperor of the Romans", "Dalveer Bhandari", "Middle Eastern alchemy", "Vienna", "Nalini Negi", "The Rashidun Caliphs", "Lituya Bay in Alaska", "The Demon Barber of Fleet Street", "2002", "George Strait", "A vanishing point", "The United States is a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "adversely affect one group of people of a protected characteristic more than another", "Emma Chambers", "Hans Lippershey", "Thermopylae", "October 6, 1931", "an album", "Spain", "Arnold and Klein", "A family friend of a U.S. soldier", "60 euros", "Blue", "lump", "a knish", "CBS"], "metric_results": {"EM": 0.5625, "QA-F1": 0.651690046758479}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.28571428571428575, 1.0, 1.0, 1.0, 0.08163265306122448, 0.0, 0.0, 0.5517241379310345, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.5581395348837209, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-3688", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-31", "mrqa_hotpotqa-validation-5628", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-419", "mrqa_searchqa-validation-416"], "SR": 0.5625, "CSR": 0.5466056034482758, "EFR": 0.9642857142857143, "Overall": 0.719678263546798}, {"timecode": 58, "before_eval_results": {"predictions": ["in the five - year time jump for her brother's wedding to Serena van der Woodsen", "111", "every president since Woodrow Wilson", "Uralic", "22", "IBM", "Thomas Jefferson", "2018", "Jesus Christ", "1999", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "24", "Coriolis force", "Hugh S. Johnson", "Paul Lynde", "Erica Rivera", "Malina Weissman", "the Ming", "Bo\u00f6tes / bo\u028a\u02c8o\u028ati\u02d0z", "1969", "DeWayne Warren", "the nucleus", "1996", "German", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Tom Brady", "pilgrimages to Jerusalem", "1996", "Coconut Cove", "Curtis Armstrong", "Dolby Theatre in Hollywood, Los Angeles, California", "Category 4", "Rust", "Gamora", "$19.8 trillion", "1,228 km / h ( 763 mph )", "Tommy Shaw", "the warships", "Nigel Lythgoe", "Central Germany", "Atlanta", "Ricky Nelson", "James Chadwick", "Welch, West Virginia", "Tristan Rogers", "15 February 1998", "Houston Astros", "Americans who served in the armed forces and as civilians during World War II", "it reaches to the south coast of eastern New Guinea, thereby including the Gulf of Papua", "middle of the 15th century", "Norman Whitfield", "Lake Nicaragua", "priestly", "rue", "Delphi Lawrence", "Juan Manuel Mata Garc\u00eda", "Edward James Olmos", "vitamin injections that promise to improve health and beauty.", "Scotland", "U.S. troops", "a heart", "James Stewart", "Adolf Hitler", "Harare"], "metric_results": {"EM": 0.609375, "QA-F1": 0.705109126984127}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-10452", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2503", "mrqa_searchqa-validation-8636", "mrqa_searchqa-validation-9866", "mrqa_searchqa-validation-14991"], "SR": 0.609375, "CSR": 0.5476694915254237, "EFR": 0.96, "Overall": 0.7190338983050848}, {"timecode": 59, "before_eval_results": {"predictions": ["Three French journalists, a seven-member Spanish flight crew and one Belgian", "at least 300", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "Frank Ricci", "medicine that contained the banned substance cortisone", "Wednesday.", "Linda Hogan", "be silent.", "Crandon, Wisconsin,", "Turkey", "John Demjanjuk", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "eight", "U.S.", "Missouri", "\"perezagruzka,\"", "Haiti.", "9", "many as 250,000", "Maj. Nidal Malik Hasan,", "Robert Clark", "more to stop the Afghan opium trade", "Nick Adenhart", "order", "One of Osama bin Laden's sons", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Operation Crank Call", "tighter restrictions on propofol,", "Susan Boyle", "10-person", "April.", "theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg, a spokesman for the Kunsthaus, a major art museum in Zurich, said.", "promotes fuel economy and safety while boosts the economy.", "gasoline", "to do jobs that Arizonans wouldn't do.", "\"prostitute\"", "digging", "Croatia", "should spur U.S. diplomacy to prevent Iran from developing nuclear weapons", "Joe Pantoliano", "3-2", "\"remained at the bottom of the hill surviving on leaves and water from a nearby creek,\" the report said.", "\"I didn't think I was going to learn so much about myself through the process,\"", "56,", "Paul McCartney and Ringo Starr clowned around and marveled at their band's amazing impact in an interview Tuesday on CNN's \"Larry King Live.\"", "17-month", "intravenously in operating rooms", "Summer", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "heavy turbulence", "Zac Efron", "drivers who were Daytona Pole Award winners, former Clash race winners, former Daytona 500 pole winners who competed full - time in 2017, and drivers who qualified for the 2017 Playoffs", "the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "show", "2", "John Buchan", "NCAA Division I Football Bowl Subdivision", "Kris Kristofferson", "Ben Savage", "The Chronicles of Narnia", "Bering Sea", "Dame Ninette de Valois", "Ayahuasca"], "metric_results": {"EM": 0.375, "QA-F1": 0.4641154268865666}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false], "QA-F1": [0.0, 0.0, 0.08, 0.0, 0.2857142857142857, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.23529411764705885, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 0.0, 0.08, 0.0, 1.0, 0.37037037037037035, 0.0, 0.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.5499999999999999, 0.6, 0.8205128205128205, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-920", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-585", "mrqa_naturalquestions-validation-4387", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-7393", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-3871", "mrqa_searchqa-validation-16736", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-13957"], "SR": 0.375, "CSR": 0.5447916666666667, "EFR": 1.0, "Overall": 0.7264583333333333}, {"timecode": 60, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2665", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10461", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11693", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11890", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-11925", "mrqa_searchqa-validation-12105", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12441", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13915", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14450", "mrqa_searchqa-validation-14481", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16382", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-2634", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5612", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9852", "mrqa_searchqa-validation-9911", "mrqa_searchqa-validation-9935", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1876", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1145", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5886", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937"], "OKR": 0.841796875, "KG": 0.50859375, "before_eval_results": {"predictions": ["$7.8 million", "prostate cancer,", "the Ku Klux Klan", "Whitney Houston", "The attorney general will announce his decision early next week, Abrahamson said.", "South Africa", "consumer confidence", "Saturn", "Prague", "35,000.", "Osama", "The EU naval force", "Police", "threatening messages", "in the west African nation", "misdemeanor", "the lead plaintiff in perhaps the most controversial case involving Judge Sonia Sotomayor,", "$273 million", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "on the blog", "air support.", "20", "$250,000 for Rivers' charity: God's Love We Deliver.", "February 5,", "at least 18 federal agents and two soldiers", "Blacks and Hispanics", "Australian officials", "the leader of a drug cartel that set off two grenades during a public celebration in September, killing eight people and wounding more than 100.", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "Rev. Alberto Cutie", "The meter reader", "the man facing up, with his arms out to the side.", "Garth Brooks", "Monday", "Friday,", "10,000", "Ryan Adams.", "$1.5 million.", "three out of four", "relatives of the five suspects,", "is now a dad.", "581", "his health and about a comeback.", "up", "\"utterly baseless.\"", "Mexicans who are unemployed or underemployed", "Caylee,", "0-0", "her boyfriend,", "Lonnie", "the first", "the words spoken to Adam and Eve after their sin", "Tokyo", "access to US courts", "Doncaster Rovers", "Bolt", "1973", "2,099", "PPG Paints Arena", "early 2017", "lactic acid", "The Greatest Show on Earth", "a dream", "Germany"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7210069444444445}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-56", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-3229", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-3833", "mrqa_hotpotqa-validation-4619", "mrqa_searchqa-validation-12647"], "SR": 0.640625, "CSR": 0.5463627049180328, "EFR": 1.0, "Overall": 0.7207569159836066}, {"timecode": 61, "before_eval_results": {"predictions": ["Windows Easy Transfer", "John Cooper Clarke", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "two", "Judi Dench", "accomplish the objectives of the organization", "Omar Khayyam", "P.V. Sindhu", "1665 to 1666", "Saturday", "1982", "Siddharth Arora / Vibhav Roy", "1948", "mitosis", "Butter Island", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Pat McCormick", "10,605", "U.S. Electoral College", "American country music artists", "August Darnell", "the Yangtze River and in provinces in the south", "New York City", "July 21, 1861", "Nashville, Tennessee", "104 colonists and Discovery", "the opisthodomus", "at 11 : 40 p.m. ship's time", "April 12, 2017", "October 2012", "Tom Burlinson, Red Symons and Dannii Minogue", "John Joseph Patrick Ryan", "49 cents", "counter clockwise", "Kit Harington", "Human anatomy", "above the light source and under the sample in an upright microscope", "divergent tectonic", "prokaryotic", "neither an acid nor a base", "Speaker of the House of Representatives", "1877", "18", "uprooted", "norm that sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "the winter solstice", "powers, duties, and responsibilities are determined by government regulations ( including the jurisdiction's corporations law ) and the organization's own constitution and bylaws", "the 1820s", "The Royalettes", "Katherine Kiernan Maria `` Kate '' Mulgrew", "Fred E. Ahlert", "Felix Leiter", "\"Land of the Rising Sun\".", "James Hogg", "Lawrence", "\"Realty Bites\"", "the \"Fuerza A\u00e9rea Argentina\" (FAA)", "Athens,", "prostate cancer,", "Italy", "chicken Little", "Saturn", "Russia's", "Anna Caterina Antonacci"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6325850627086091}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6428571428571429, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9302325581395349, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-1975", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3093", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-3859", "mrqa_searchqa-validation-4176", "mrqa_triviaqa-validation-6243"], "SR": 0.5625, "CSR": 0.5466229838709677, "EFR": 0.9285714285714286, "Overall": 0.7065232574884792}, {"timecode": 62, "before_eval_results": {"predictions": ["1983", "Bacon", "from 1922 to 1991", "79", "Gibraltar", "1 January 1904", "Athens", "Brooke Wexler", "March 29, 2018", "in the 1980s", "Montgomery", "Evermoist", "in the mid - to late 1920s", "differential erosion", "Kanawha River", "Graham McTavish", "Thomas Alva Edison", "since been adopted by five other countries", "due to a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "Richard Masur", "Frankie Valli", "one", "JackScanlon", "Saturday", "Gametes", "sometime in 2018", "2015", "Sarah Josepha Hale", "Nickelback", "Ledger", "IB", "in a 1945 NCAA game between Columbia and Fordham", "2017", "permanently absorbed the superhuman powers and the psyche of Carol Danvers", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Pink Floyd", "1983", "during Christmas season in the late 1970s", "1986", "1939", "Himadri Station", "on a beach in Malibu, California", "birch", "February 2017 in Japan and in March 2018 in North America and Europe", "FaZe Rug", "at the fictional elite conservative Vermont boarding school Welton Academy", "1973", "9 February 2018", "a two - layer coat", "94 by 50 feet", "Tom\u00e1s de Torquemada", "1981", "Kent", "Nicaraguan Sign Language", "Dusty Dvoracek", "northeastern Argentina, southeastern Bolivia and southwestern Brazil, and is a second official language of the Argentine province of Corrientes", "Los Angeles", "has an inspiration: U.S. President Barack Obama.", "foyer of the BBC building in Glasgow, Scotland", "\"a violent and brutal extremist group with a number of individuals affiliated with al Qaeda.", "Frederic Chopin", "spring", "pesos", "The Rev. Alberto Cutie"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6811197585094644}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.2857142857142857, 0.0, 0.5, 0.8181818181818181, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.2666666666666667, 1.0, 0.9411764705882353, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-10583", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-761", "mrqa_hotpotqa-validation-2170", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-219", "mrqa_newsqa-validation-1330"], "SR": 0.578125, "CSR": 0.5471230158730158, "EFR": 1.0, "Overall": 0.7209089781746032}, {"timecode": 63, "before_eval_results": {"predictions": ["Malay Peninsula", "Billy Martin", "Nova Scotia", "a chainmaille", "Strindberg's", "oarai port", "Morocco", "Opera Flashcards", "Anne Rice", "LaSalle", "Pop art", "embalming", "Portland", "Mariah Carey", "Dionysus", "an eel", "symbiosis", "Planets", "Barack Obama", "When You Look Me In The Eyes", "The Lost World", "Prince Edward Island", "Penn Station", "Bab el Mandeb Strait", "Red Heat", "Atlas Mountains", "kafkaesque", "Heather Mills", "spanish", "Paris", "Mont Blanc On", "Rene Lacoste", "preemption", "the Nobel Prize", "summer", "osca", "Jawaharlal Nehru", "Aug 28, 2014", "osca", "a cat", "congruent", "Spain", "toad", "San Francisco", "A Brief History of Time", "a crossword", "Macy's", "osca", "a psychic midget", "\"Walker, Texas Ranger", "Benazir Bhutto", "CBS", "Gibraltar", "Orographic lift", "Virginia Plain", "strawberry", "Venice", "Chad", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller Foundation", "Stephen King", "Pat Quinn", "murder in the beating death of a company boss who fired them.", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "Comeng and Clyde Engineering"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6856770833333333}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-6802", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-14989", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-16392", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-15031", "mrqa_hotpotqa-validation-5688", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2308"], "SR": 0.640625, "CSR": 0.548583984375, "EFR": 0.9565217391304348, "Overall": 0.712505519701087}, {"timecode": 64, "before_eval_results": {"predictions": ["a Hispanic", "Bonnie Elizabeth Parker", "Forrest Gump", "My Favorite Mistake", "The Crossing Guard", "Martin Van Buren", "I Have No Mouth, and I Must Scream", "Friday Night Lights", "contractions", "skull", "China", "Florida State", "Ukraine", "Boston", "a Tibetan antelope", "the Godfather", "a bolt", "Australia", "Napalm", "Roald Dahl", "Mount Kenya", "John Lennon", "the Stamp Act", "Yale Basmati", "CO", "Greco-Persian Wars", "Buenos Aires Times", "Mulberry Street", "Romeo & Juliet", "Tucson", "Helen Hayes", "Wesley Clark", "cobalt", "sing sing", "salmon", "a falling star", "Herman Melville", "Abercrombie & Fitch", "Beatrix Potter", "le Roman de la Rose", "a kagu", "the Gadsden Purchase", "the umbilical cord", "trees", "Latvia", "the House of Lords", "the International Committee of the Red Cross", "terrorists", "\"I'm Your Boogieman\"", "Ypres", "Graceland", "David Tennant", "southeastern coast of the Commonwealth of Virginia", "pulmonary heart disease ( cor pulmonale )", "10.8 U.S. gallons", "squash", "small web page", "the Gospel Starlighter", "500-room", "Wal-Mart Canada Corp.", "Tuesday,", "Diversity,", "were directly involved in an Internet broadband deal with a Chinese firm.", "gang rape"], "metric_results": {"EM": 0.484375, "QA-F1": 0.619390925229741}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true], "QA-F1": [0.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.2222222222222222, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.7499999999999999, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9473684210526316, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11200", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-6468", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-16325", "mrqa_searchqa-validation-691", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-16100", "mrqa_searchqa-validation-8741", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-11743", "mrqa_searchqa-validation-2681", "mrqa_searchqa-validation-6512", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-12394", "mrqa_searchqa-validation-6678", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-14256", "mrqa_searchqa-validation-15724", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-14787", "mrqa_searchqa-validation-3727", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-1283", "mrqa_hotpotqa-validation-2565", "mrqa_newsqa-validation-3111"], "SR": 0.484375, "CSR": 0.5475961538461538, "EFR": 1.0, "Overall": 0.7210036057692308}, {"timecode": 65, "before_eval_results": {"predictions": ["Blue Ridge Parkway", "3,384,569", "Vishal Bhardwaj", "around 169 CE", "Ed O'Neill", "Milwaukee Bucks", "138,535 people", "Dennis Hull, as well as painter Manley MacDonald.", "Max Martin, Savan Kotecha and Ilya Salmanzadeh", "jerseys", "Love Letter", "Brazil", "1968", "2005", "Stacey Kent", "Shenandoah National Park", "Regionalliga Nord", "Clube Atl\u00e9tico Mineiro", "Samantha Spiro", "William Shakespeare", "over 1.6 million", "Joulupukki", "West Africa", "special economic zone", "Portal A Interactive", "Graduados", "Sada Carolyn Thompson", "World Health Organization", "Chow Tai Fook Enterprises", "Michelle Anne Sinclair", "2011", "2017", "Hawaii", "Lalit", "Kal Ho Naa Ho", "Srinagar", "Ronald Wilson Reagan", "Musicology", "left", "1835", "1926 Paris during the period of the Lost Generation", "Erreway", "Forbes", "January 28, 2016", "69.7 million litres", "500-room", "Russia", "2027 Fairmount Avenue", "Carroll County, the eastern portion of Grafton County, and the northern portions of Strafford County and Merrimack County", "Black Panther Party", "globetrotters", "in the United Kingdom ( with the exception of Scotland since August 1, 2016", "mitochondrial membrane", "Butter Island off North Haven, Maine in the Penobscot Bay", "eye", "Leo Tolstoy", "gizzard", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "2,000", "Takashi Saito,", "the sirloin", "Shirley Jackson", "Sue Miller", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.5, "QA-F1": 0.6008588270579549}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.4444444444444445, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5581395348837209, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-3880", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-6912", "mrqa_triviaqa-validation-3498", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-2296"], "SR": 0.5, "CSR": 0.546875, "EFR": 0.96875, "Overall": 0.714609375}, {"timecode": 66, "before_eval_results": {"predictions": ["musician", "Captain Hans Geering", "50JJB Sports Fitness Clubs", "2015", "American", "Seventeen", "Bhushan Patel", "the widow of veteran film director Yash Chopra", "Mark O'Connor", "Kinnairdy Castle", "South African", "Agent 99", "The 2008\u201309 UEFA Champions League", "National Hockey League", "his advocacy", "Parlophone Records", "a Soldier in Truck", "eight", "Cuban descent", "arts manager", "The Royal Family", "girls aged 11 to 18", "Jackie Harris", "water", "National Basketball Development League", "Operation Overlord", "invoice", "[O.S. 20 October] \u2013 8 March 1723", "1851", "World War I", "12-year", "World War II", "Graham Payn", "Martin Truex Jr.", "twice", "Malayalam cinema", "47,818,", "Every Rose Has Its Thorn", "13", "New England", "1953", "German", "Scapegoat Mountain", "future AC/DC founders", "Boston Celtics", "1912", "Jewish", "Bill Curry", "notable for being one of the youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure.", "2004", "311", "Jason Lee", "Steve Mazzaro & Missi Hale", "secession", "The Agulhas current", "The History Boys", "a tabby", "Mexico", "3,", "'overcharged.'\"", "Dame Melba", "Gary", "Henry Hudson", "the brain"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5758064516129032}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666665, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.45161290322580644, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4061", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-2842", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-2168", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-685", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-7417", "mrqa_newsqa-validation-2935", "mrqa_searchqa-validation-7226", "mrqa_naturalquestions-validation-3368"], "SR": 0.46875, "CSR": 0.5457089552238805, "EFR": 1.0, "Overall": 0.7206261660447761}, {"timecode": 67, "before_eval_results": {"predictions": ["classical", "Dr. Alberto Taquini", "freedom of choice, other social freedoms, and \"laissez-faire\" capitalism", "Rudolf Kehrer", "2015", "FK Austria Wien", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Edward Albert Heimberger", "Squam Lake", "lambics", "Croatian", "the Harpe brothers", "Adam Rex", "Marvel's Agent Carter", "Everton", "shorthand writing", "twelfth", "Coalwood, West Virginia", "31 July 1975", "Dark Heresy", "Theodore Robert Bundy", "1943", "doctorates", "Humvee", "Malta", "East Knoyle", "Philadelphia", "Maria Brink", "Jyothika", "Sippin' on Some Syrup", "24 January 76 \u2013 10 July 138", "Leonard Cohen", "Simon Bolivar Buckner", "BraveStarr", "25 million records", "Paul Avery", "Sunflower County", "848 km", "Ellesmere Port", "Homer Hickam, Jr.", "South America", "Montreal", "Eugene", "the Chief of the Operations Staff of the Armed Forces High Command", "CBS News", "Philadelphia", "Parlophone", "October", "the best known globetrotters", "Henry Lau", "John Schlesinger", "Pasek & Paul", "Diary of a Wimpy Kid", "Ed Sheeran", "Sarah Palin", "Ub Iwerks", "Heartbeat", "onto the college campus.\"", "14", "one American diplomat to a \"prostitute\"", "touchpad", "Beaker", "Damascus", "missing"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6692489801864802}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false], "QA-F1": [0.18181818181818182, 1.0, 0.30769230769230765, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-705", "mrqa_hotpotqa-validation-259", "mrqa_hotpotqa-validation-5082", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-5385", "mrqa_naturalquestions-validation-714", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-5330", "mrqa_newsqa-validation-1398"], "SR": 0.5625, "CSR": 0.5459558823529411, "EFR": 1.0, "Overall": 0.7206755514705883}, {"timecode": 68, "before_eval_results": {"predictions": ["Stephen T. Kay", "Trey Parker", "Jay Gruden", "Wayman Lawrence Tisdale", "New Boston Air Force Station", "the Corps of Discovery", "sarod", "Fleetwood Mac", "County Louth", "Chelmsford", "2009", "Comedy Central", "five", "Lazio region", "Mick Jackson", "The Livingston family", "U.S. saloon-keeper", "Fort Albany", "itty Hawk", "the Qin dynasty", "\"our greatest comedienne - Australia's Lucille Ball\".", "Charles de Gaulle Airport", "Francophone", "Matthew Perry", "Cricket fighting", "Jaguar Land Rover Limited", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "Noel Gallagher", "1966", "Love", "a band director", "Ford Field in Detroit, Michigan", "Conscription", "Florence Nightingale", "Bolton, England", "February 9, 1994", "Las Vegas", "Kongo", "Missouri River", "World War II", "Ector County", "Norse mythology", "Mercedes-Benz Superdome in New Orleans, Louisiana", "1998", "classroom specialist", "October 12, 1962", "rural", "Lombardy region", "August 14, 1848", "Punjabi/Pashtun", "41st", "Shalmaneser V", "He chose to charter a plane to reach their next venue in Moorhead, Minnesota", "1948", "Mallard", "Sherlock Holmes", "Tokyo", "It will join Facebook and Google, which both have their headquarters in the Irish capital.", "near Warsaw, Kentucky,", "Marxist guerrillas", "W. Somerset Maugham", "a spruce", "Typing Test", "Thomas Jefferson"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6313875360750361}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.5714285714285715, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-4935", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-4720", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7939", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-2573", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-16102", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-1104"], "SR": 0.46875, "CSR": 0.5448369565217391, "EFR": 1.0, "Overall": 0.7204517663043479}, {"timecode": 69, "before_eval_results": {"predictions": ["November 1999", "the senior-most judge of the supreme court", "the Norman given name Robert", "March 16, 2018", "Irsay", "Germany", "lift", "The federal government", "Matt Monro", "December 2, 2013", "the 7th century", "it was first published on November 12, 1976 by Ballantine Books", "2018", "Walter Pauk", "Richard Dashut", "1955", "maquila", "1959", "Schwarzenegger", "Salman Khan", "2016", "2003", "Isekai wa Sum\u0101tofon", "two - year terms", "a recognized group of people who jointly oversee the activities of an organization", "New Zealand", "a large roasted turkey", "currently a free agent", "Joe Pizzulo", "Times Square in New York City", "immediately follows the year 1 BC", "Nuevo Reino de Le\u00f3n", "the mid-1970s", "Florida and into the town of Coconut Cove", "state", "tropical desert climate", "1988", "in the 1970s", "anembryonic gestation", "February 27, 2015", "John Hancock", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "lakes or reservoirs", "March 2016", "FIGG Bridge Engineers", "Carol Ann Susi", "in vitro", "crown cutting", "Chris Rea", "2017", "16 August 1975", "the ovule", "Samuel", "Amazon.com", "Dutch", "Winecoff Hotel fire", "India Today", "Jet Republic", "Thousands", "Kabul", "Prison Break", "the Lone Ranger", "Egypt", "President Obama and Britain's Prince Charles"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6284027777777778}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.14285714285714285, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.24000000000000002, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.26666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-9328", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-3386", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-7398", "mrqa_triviaqa-validation-6372", "mrqa_triviaqa-validation-2261", "mrqa_hotpotqa-validation-886", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-12778", "mrqa_newsqa-validation-2497"], "SR": 0.546875, "CSR": 0.5448660714285714, "EFR": 0.9310344827586207, "Overall": 0.7066644858374383}, {"timecode": 70, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4068", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-757", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8993", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10731", "mrqa_searchqa-validation-10872", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16049", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-226", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7278", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1330", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1949", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3687", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4301", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-562", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-6362", "mrqa_squad-validation-66", "mrqa_squad-validation-6962", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7693", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-855", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_squad-validation-9923", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4657", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-7726"], "OKR": 0.81640625, "KG": 0.503125, "before_eval_results": {"predictions": ["jujitsu", "Peter Stuyvesant", "Cornell", "a cactus", "NASCAR", "Vivaldi", "eighth", "Grace Slick", "London", "Sweden", "(Phil) Lynott", "purple", "Zachary Taylor", "Kempton Park", "Leonardo", "John Everett Millais", "Belfast", "coconut shy", "Fulham", "Kent", "bryophyta", "June Brae", "Adonijah", "fondue", "glockenspiel", "Drew Carey", "William Shakespeare", "Mackinac Bridge", "a 965-foot ocean liner", "\"Sunny After noon\"", "tea", "Joan Crawford", "red", "Alexandria", "1969", "the queen", "Queensberry", "Lord Beaconsfield", "Duke", "Babylon", "Nottingham", "George III", "25", "(S. Jimmy) Beck", "Antoine Lavoisier", "Australia", "Agenor", "X-Men Origins: Wolverine", "Jimmy Carter", "David Mitchell", "King William IV", "December 15, 2017", "Laura Bertram", "Fox Ranch in Malibu Creek State Park", "Loretta Lynn", "National Association for the Advancement of Colored People", "gGmbH (\"gemeinn\u00fctzige\" GmbH)", "Second seed Fernando Gonzalez", "Alaska or Hawaii.", "\"falling space debris,\"", "jury", "an egg", "Brooke Shields", "Nepal"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6224506578947369}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.7368421052631579, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-5144", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-3041", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-968", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6959", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6308", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-468", "mrqa_hotpotqa-validation-4252", "mrqa_hotpotqa-validation-1720", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-3347", "mrqa_searchqa-validation-9986"], "SR": 0.53125, "CSR": 0.5446742957746479, "EFR": 0.9333333333333333, "Overall": 0.7048202758215962}, {"timecode": 71, "before_eval_results": {"predictions": ["four", "Wyoming", "addiction and behavior change/issues", "Oprah Winfrey", "Phil Spector", "Margaret Beckett", "Robin Hood Men in Tights", "Easter Island (Rapa Nui)", "Fringillidae", "Greyfriars School", "Peter Ustinov", "fusilli", "baron Ritchie", "Harry Anders", "Augustus", "True History of the Kelly Gang", "cistercian", "Qing Dynasty", "Catherine Cookson", "Stockton-on-Trent", "trumpet", "$100", "king midas", "sheep", "French Open", "a cactus", "a bairn", "nectarines", "greece", "hip joint", "bruise", "barber", "terra di siena", "baron", "Washington, D.C.", "Daedalus", "Tommy Roe", "villa", "a pint", "komando Pasukan Khusus", "Uranus", "leon", "Oceanic Flight 815", "behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing, or engaging in sexual activity", "pascal", "cerebral hemisphere", "ash", "1985", "Sisyphus", "Kathryn C. Taylor", "Hawaii", "usually in May", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "diurnal and insectivorous", "Derry City F.C.", "URO VAMTAC", "Christian", "Stop the War Coalition", "U.S. program to assassinate terrorists in Iraq.", "\"procedure on her heart,\"", "lexicographer", "a great blue heron", "Billy the Kid", "mayor of Seoul"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5424197330447331}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.5, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7878787878787877, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3122", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-4581", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-207", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-3236", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-574", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-55", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-1222", "mrqa_hotpotqa-validation-1630", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2547", "mrqa_searchqa-validation-16275", "mrqa_newsqa-validation-3686"], "SR": 0.4375, "CSR": 0.5431857638888888, "EFR": 0.9722222222222222, "Overall": 0.7123003472222222}, {"timecode": 72, "before_eval_results": {"predictions": ["\"Up\"", "Colombia", "U.S. and NATO forces.", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "they'd get to bring a new puppy with them", "three", "Kurdish militant group", "closing these racial gaps.", "Barbara Streisand", "U.S. President-elect Barack Obama", "phone calls or by text messaging", "people from dying from HIV, preventable causes like heart conditions are causing deaths.", "Friday night", "off the coast", "Pixar", "Friday", "state senators who will decide whether to remove him from office", "The Rosie Show", "President Bush", "South Africa", "Brazil", "Mugabe has sought to keep control of both and make the prime minister position \"ceremonial,\" the official said.", "rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds,", "Clifford Harris,", "Jeffrey Jamaleldine", "to sniff out cell phones.", "21 percent", "five", "At least four county GOP chairmen have also called for Sanford to go.", "around 8 p.m. local time Thursday", "15-year-old", "150", "and was showing several folks his many guns when he snapped his double-barreled 12-gauge shotguns shut.", "Ennis", "haitians", "start a dialogue of peace based on the conversations she had with Americans along the way.", "Daniel Radcliffe", "1 million", "55-year-old", "Southeast,", "Aravane Rezai", "The U.S.", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "Caylee Anthony", "March 22,", "ordered the eventual closure of Guantbs Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "Bill Stanton", "prisoners", "Woosuk Ken Choi,", "Apple employees", "In October 1973", "Norman Greenbaum", "over the next seven years", "kiki", "Leeds Rhinos", "prisca", "Matthew Ward Winer", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP.", "Sun Woong", "darts", "Joe Louis", "16th", "roman civilization"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5586131358560794}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.25, 1.0, 0.761904761904762, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.19354838709677416, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.1, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-4104", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-1114", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-857", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-3654", "mrqa_hotpotqa-validation-3757", "mrqa_searchqa-validation-2301", "mrqa_searchqa-validation-10852", "mrqa_hotpotqa-validation-1958"], "SR": 0.46875, "CSR": 0.542166095890411, "EFR": 1.0, "Overall": 0.7176519691780822}, {"timecode": 73, "before_eval_results": {"predictions": ["12", "Sodra nongovernmental organization,", "the United States", "they don't feel Misty Cummings has told them everything she knows.", "Mogadishu", "\"horrible crime that is designed to sabotage reconciliatory efforts by the Iraqi people, who, I am confident, will continue on the road of dialogue.\"", "more than 100", "apparently died after shooting himself three times in the head with a.40-caliber pistol,", "Scarlett Keeling", "took on water", "two", "Omar", "Dr. Cade", "Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "keystroke", "growing crowded, and governments are increasingly trying to plan their use.", "relatives of the five suspects,", "165", "U.S. Holocaust Memorial Museum,", "the simple puzzle video game,", "curfew in Jaipur", "40-year-old", "stand down.", "U.S. 93 in White Hills, Arizona, near Hoover Dam.", "different women coping with breast cancer in five vignettes.", "two years,", "269,000", "Harare", "UK", "allegedly robbed a 29-year-old woman at gunpoint at a Westin Hotel in Boston, Suffolk County District Attorney Dan Conley said.", "gathering information about the rebels to give to the Colombian military.", "Hundreds of militants, believed to be foreign fighters,", "they'd get to bring a new puppy with them to the White House in January.", "as he tried to throw a petrol bomb at the officers, police said.", "Austin, Texas,", "forgery and flying without a valid license,", "Kurt Cobain", "ALS6,", "he was one of 10 gunmen who attacked several targets in Mumbai", "rabbit hole,", "Alinghi", "anesthetic", "10", "space for aspiring entrepreneurs to brainstorm with like-minded people.", "39,", "has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\" leaving them \"vulnerable to disruption,\"", "on the bench", "12.3 million", "\"private client\" list,", "Seasons of My Heart", "the video-game challenge of continuously trying to best your own fuel economy achievements,\"", "Justice A.K Mathur", "somatic cell nuclear transfer", "1956", "Jack Russell Terrier", "Chile", "Nicolas cage", "Donald Richard \"Don\" DeLillo", "10 Years", "\"Queen City\"", "Carl Sagan", "Israel", "Copacabana", "50\u201340\u201390"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6771358572487202}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8571428571428572, 1.0, 0.0, 1.0, 0.7000000000000001, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.12500000000000003, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8750000000000001, 1.0, 1.0, 0.0, 1.0, 0.15789473684210525, 0.15384615384615383, 0.2222222222222222, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 0.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-534", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2397", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-345", "mrqa_searchqa-validation-8780", "mrqa_hotpotqa-validation-1757"], "SR": 0.59375, "CSR": 0.5428631756756757, "EFR": 0.9230769230769231, "Overall": 0.7024067697505197}, {"timecode": 74, "before_eval_results": {"predictions": ["brazil", "North yorkshire", "Lou Gehrig", "Goat Island", "Loretta Lynn", "a bat", "electronic junk mail or junk newsgroup posting", "Andrew Lloyd Webber", "east of Eden", "Sir Henry Neville", "Mark Hamill", "Aslan", "rugby", "The Merchant of Venice", "kvetch", "colombia", "Harold Wilson", "Bleak House", "HP.52 Hampden", "Tina Turner", "takifugu rubripes", "leeds", "Capricorn", "jack kennedy", "the bluebird", "Toy Story", "Damon Albarn", "White", "Kiel Canal", "leinster", "Avro Lancaster Bomber", "Sarah Vaughan", "Abu Dhabi", "33", "Emily Davison", "Marc Brunel", "Aberystwyth", "Oasis", "Peter Sellers", "the Indus valley", "Ghent\u2013Terneuzen Canal", "someone you can take advantage of (by cheating him or her) a fair chance of winning", "David Bowie", "Lorne Greene", "1655", "colombia", "Thai", "colombia", "\u00e1stron", "Ramadan", "sewing machines", "Montgomery County", "11 January 1923", "the Roman Empire", "1994", "\"Sevens\"", "Tamara Ecclestone Rutland", "Kaka,", "addressed a racially-tinged remark made by his former caddy, telling reporters Steve Williams apologized and is not a racist.", "Three", "noncommissioned officer", "William Henry Harrison", "Shelley", "1982"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6496527777777777}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-1135", "mrqa_triviaqa-validation-1615", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-5470", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-531", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-7746", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5335", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6722", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-3882", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4672", "mrqa_newsqa-validation-2813"], "SR": 0.59375, "CSR": 0.5435416666666666, "EFR": 0.9615384615384616, "Overall": 0.7102347756410257}, {"timecode": 75, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "reached an agreement late Thursday to form a government of national reconciliation.", "Dead Weather's \"Horehound\"", "two", "South African police", "twice.", "Haleigh Cummings,", "eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Hamas,", "a mosque in Rawalpindi frequented by military personnel,", "Miss USA Rima Fakih is a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "not necessarily better...some vitamins and minerals can be toxic in high doses,\" particularly the fat-soluble ones which the body stores like Vitamins A, D, E and K,", "copenhagen", "Arthur E. Morgan III,", "job training", "has to move out of her rental house because it is facing foreclosure", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "22", "1969", "diabetes and hypertension,", "Dilshan scored his sixth Test century of a remarkable year to give Sri Lanka a fine start to the third match of their series against India in Mumbai on Wednesday.", "Tuesday,", "100,000", "The apartment building collapsed together with two other buildings on March 3.", "$17,000", "President Obama", "Caylee Anthony", "Glasgow office", "journalists and the flight crew will be freed,", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "returning combat veterans", "September 21.", "Thursday.", "246", "40", "stole", "fine", "is not a zoo.", "At least 15", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "Arabic, French and English", "$40 and a loaf of bread.", "many as 250,000", "state senators", "Iran", "Mohammed Ali al-Moayad", "in the 1950s,", "Orbiting Carbon Observatory,", "2.5 million", "5,600", "Yemen,", "very long forward pass in American football, made in desperation, with only a small chance of success and time running out on the clock", "amphetamines", "contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "pantomime The Miraculous Mandarin", "William Anthony Perry", "blackcurrant liqueur", "Premier League club Everton", "Magic Band", "Fat Man", "messenger", "the beaver", "Jan Hus", "R2-D2"], "metric_results": {"EM": 0.625, "QA-F1": 0.6896213722470838}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [0.4444444444444445, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.2727272727272727, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.08108108108108107, 1.0, 0.06451612903225806, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-3275", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-2207", "mrqa_triviaqa-validation-1848", "mrqa_triviaqa-validation-7344", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-5388"], "SR": 0.625, "CSR": 0.5446134868421053, "EFR": 0.75, "Overall": 0.6681414473684211}, {"timecode": 76, "before_eval_results": {"predictions": ["Hawaii", "\"The Real Housewives of Atlanta\"", "a review of state government practices completed in 100 days.", "number of deaths linked to cantaloupes contaminated with the Listeria monocytogenes bacteria has risen to 28,", "said he was released Friday and taken to the Australian embassy in Bangkok, where he stayed until leaving for Australia at about midnight.", "Sheikh Sharif Sheikh Ahmed", "two", "golf", "being shot in the head during an armed robbery.", "Nigeria", "Ameneh Bahrami", "the chief executive officer, the one on the very top,", "Daytime Emmy Lifetime Achievement Award.", "test-launched a rocket capable of carrying a satellite,", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "35,000.", "Roger Federer", "March 22,", "Venezuela", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "power-sharing talks", "Operation Crank Call,\"", "the body of the aircraft", "Rima Fakih", "more than 100", "Obama and McCain camps", "Ameneh Bahrami", "about 5:20 p.m.", "President Bill Clinton", "Haleigh Cummings,", "Transportation Security Administration", "allegedly faking a doctor's note", "response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "J. Crew,", "the man was dead,", "Nook", "Islamabad", "three", "Nigeria", "Her husband and attorney, James Whitehouse,", "fill a million sandbags and place 700,000 around our city,\"", "workers of the dependable Camry", "15-year-old's", "normal", "David Bowie", "15,000", "looked depressed", "Franklin, Tennessee,", "such joint exercises between nations are not unusual.", "Authorities in Fayetteville, North Carolina,", "Saturday.", "Lake Powell", "Lew Brown", "Rockwell", "Sarah Churchill", "Neighbours", "Pesach", "Walcha", "Dunlop", "Hern\u00e1n Crespo", "copenhagen", "Verdi", "Serengeti", "greece"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6973035200274165}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.5806451612903226, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.1904761904761905, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-2396", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-2519", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-648", "mrqa_hotpotqa-validation-5413", "mrqa_searchqa-validation-3547", "mrqa_triviaqa-validation-925"], "SR": 0.609375, "CSR": 0.5454545454545454, "EFR": 0.88, "Overall": 0.6943096590909091}, {"timecode": 77, "before_eval_results": {"predictions": ["Thursday,", "(Charlotte Gainsbourg)", "Patrick McGoohan,", "flooding and debris", "Vicente Dale Coutinho, commander of Brazil's 4th Army, reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Woosuk Ken Choi,", "a head injury.", "1994,", "Mawise Gumba", "gun charges,", "at least 25 dead", "At least 15", "at least nine", "shows the world that you love the environment and hate using fuel,\"", "\"falling space debris,\"", "Transportation Security Administration", "$10 billion", "\"Rin Tin Tin: The Life and the Legend\"", "102", "his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "Jaime Andrade", "14 people were dead", "innovative, exciting skyscrapers", "the United States", "financial gain,", "Gary Coleman", "trading goods and services without exchanging money", "Laura Ling and Euna Lee,", "high tide", "Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "16", "the Register -- Iowa's largest newspaper -- backed Romney in his bid for the Republican presidential nomination and just over two weeks before", "100 to 150", "negotiators for Zelaya and Roberto Micheletti, the politician who was appointed president hours after Zelaya's June 28 removal, reached an agreement late Thursday", "Zelaya and Roberto Micheletti,", "one of Africa's most stable nations,", "last week.", "prison inmates.", "Cannes Film Festival,", "Mark Obama Ndesandjo", "UCLA Medical Center,", "two", "Afghan", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "executive director of the Americas Division of Human Rights Watch,", "\"Empire of the Sun,\"", "AS Roma", "\"@\"", "Jeffrey Jamaleldine", "Heshmatollah Attarzadeh", "At least 14", "Daryl Sabara", "during the 2013 -- 14 television season, when Barry Allen was introduced in the eighth episode of Arrow's second season", "Gibraltar", "Barry White", "john Adams", "Anita Brookner", "9", "British comedian", "consulting", "flamboyant", "Orson Welles", "barbed wire", "loyal to a person or an idea"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6827756468674058}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.7894736842105263, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.25806451612903225, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 0.5, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.4166666666666667, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-2803", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-1497", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-2177", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-795", "mrqa_naturalquestions-validation-9330", "mrqa_triviaqa-validation-635", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-12008"], "SR": 0.578125, "CSR": 0.5458733974358974, "EFR": 0.9629629629629629, "Overall": 0.7109860220797721}, {"timecode": 78, "before_eval_results": {"predictions": ["the dress shop", "call option", "British Columbia, Canada", "45 %", "September 14, 2008", "to prevent further offense by convincing the offender that their conduct was wrong", "Brad Johnson", "the closing of the atrioventricular valves and semilunar valves", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "a computer program", "Dalveer Bhandari", "bone marrow", "to collect menstrual flow", "the Kansas City Chiefs", "`` Killer Within ''", "Filipino", "the 10th century", "Wakanda", "the Roman Empire", "Joel", "a young girl ( an illustration by Everest creative Maganlal Daiya back in the 1960s )", "Gertrude Niesen", "Australia", "1983", "Kathleen Erin Walsh", "March 12, 2013", "The photoelectric ( optical ) smoke detector", "the compass", "jazz", "sedimentary", "$2 million", "Michael Buffer", "George Harrison", "115", "c. 3000 BC", "Andrew Garfield", "each team", "1898", "a routing table", "the rez", "Twickenham", "Glenn Close", "Kenneth Cook", "Tim Russert", "the anterolateral corner of the spinal cord", "a large, high - performance luxury coupe", "displacement", "Second Continental Congress", "the commemoration of Jesus'birth", "a mixture of phencyclidine and cocaine", "a proton gradient across a biological membrane", "Lewis Carroll", "lemmium", "The Apprentice", "A123 Systems, LLC", "President of the United States", "thomas swan", "34", "Caylee Anthony,", "Michael Brewers,", "a bugle", "Tennessee Williams", "The Scarlet Letter", "Friday,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6910528273809524}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9333333333333333, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.2857142857142857, 1.0, 1.0, 0.9, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.25000000000000006, 0.33333333333333337, 1.0, 1.0, 0.0, 0.33333333333333337, 0.625, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-5207", "mrqa_triviaqa-validation-5536", "mrqa_hotpotqa-validation-1433", "mrqa_newsqa-validation-3438", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-16420"], "SR": 0.546875, "CSR": 0.5458860759493671, "EFR": 0.9310344827586207, "Overall": 0.7046028617415976}, {"timecode": 79, "before_eval_results": {"predictions": ["Bury, Greater Manchester, England", "the Battle of the Rosebud", "Rabat", "Potomac River", "Hermione Baddeley", "Harmony Korine", "January", "New Jersey", "rock and roll", "Red and Assiniboine Rivers", "King George IV and the Duke of Wellington", "June 24, 1935", "The Washington Post", "odd-eyed", "2001", "Perth's number one rating radio station, MIX 94.5", "1999", "Tempo", "Presbyterian Church (USA),", "2002", "English", "Southaven", "Anheuser-Busch", "University of Missouri-Kansas City in Kansas City, Missouri", "Francis the Talking Mule", "Leslie James \"Les\" Clark", "Giuseppe Verdi,", "County Louth", "Gal Gadot", "Kurt Vonnegut", "top division", "film", "Mississippi Institute of Arts and Letters", "Vince Guaraldi", "2007", "Grave Digger", "Mulberry", "Isabella (Belle) Baumfree", "Jay Park", "The final of 2011 AFC Asian Cup", "Mel Blanc", "Centers for Medicare & Medicaid Services (CMS),", "Afghanistan", "Godspell", "Steven Selling", "Scunthorpe", "Australian", "Argentinian", "Robert Norton Noyce", "1970s and 1980s", "\"Personal History\",", "Elliot Scheiner", "Larry Bird", "the direction from which the wind is blowing", "France", "Shropshire", "\"S. molloyi", "Alfredo Astiz,", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip,", "Kim Clijsters", "katherine of Aragon", "Little Boy Blue", "Cheyenne", "March 27, 2017"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6997395833333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-598", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-5515", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-601", "mrqa_triviaqa-validation-4205", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-809", "mrqa_searchqa-validation-5939", "mrqa_naturalquestions-validation-5649"], "SR": 0.609375, "CSR": 0.5466796875, "EFR": 1.0, "Overall": 0.7185546875}, {"timecode": 80, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1288", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.837890625, "KG": 0.5203125, "before_eval_results": {"predictions": ["John Nash", "Two Greedy Italians,", "Richard Hope-Hawkins", "usually called the x unit (ordinate)", "Mel Brooks", "Agent 007", "Edward Woodward", "Scotland", "Fiat SpA", "three-stringed", "Bob Anderson", "Andre Agassi", "kelly", "India", "Piet\u00e0", "Mark Darcy", "California Chrome", "Jason Bennetto", "Naples", "Tony Meo", "Bash Street", "Leonard Rossiter", "Robin Hood", "Me and My Girl", "Yeshua", "Augustus Caesar", "Shepherd Neame", "Titanic", "Peter Falk", "tax collector", "Robert Maxwell", "Mikhail Gorbachev", "Pocahontas", "Noah Beery, Jr.", "Argentina", "spinal cord", "myxomatosis", "avocadoe", "World War I", "Captain America", "HARIBO", "bolognese", "New Zealand", "Eva Braun", "Sindh\u016b River", "Devon Loch", "SAR", "Bruce Willis", "Kwame Nkrumah", "Fifth Amendment", "cording", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "pneumonoultramicroscopicsilicovolcanoconiosis", "Hercules", "Philip Livingston", "2010", "\"The Process\"", "Ameneh Bahrami", "Nearly eight in 10", "a tenement in the Mumbai suburb of Chembur,", "camels", "Shrek", "Seoul", "Asia"], "metric_results": {"EM": 0.640625, "QA-F1": 0.671875}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7305", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-3677", "mrqa_triviaqa-validation-2223", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-5713", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-1137", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-1693", "mrqa_searchqa-validation-11588", "mrqa_searchqa-validation-457"], "SR": 0.640625, "CSR": 0.5478395061728395, "EFR": 1.0, "Overall": 0.728083526234568}, {"timecode": 81, "before_eval_results": {"predictions": ["London", "his writings about the outdoors, especially mountain-climbing", "50th anniversary of the founding of the National Basketball Association", "Roger Staubach", "World Health Organization", "Pulitzer Prize", "Argentine", "Pittsburgh Steelers", "Kim Jong-hyun", "Las Vegas", "Scandinavian design", "romantic comedy", "dennis Schr\u00f6der", "Tsung-Dao Lee", "December 19, 1967", "South African-born", "dennis ron Howard", "1822", "1926 Paris", "Bulgarian", "fictional city of Quahog, Rhode Island", "Operation Neptune", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Roslyn Castle", "German Campaign", "2015", "Violet", "Free Range Films", "Mondays", "Chrysler K platform", "Edinburgh", "Laurel, Mississippi", "Matt Flynn", "Camber Sands", "Jaguar Land Rover Limited", "Viet Minh's base of support", "Adelaide", "Shepardson Microsystems", "The Fault in Our Stars", "sixteen", "Crips", "Deftones", "Doctor of Philosophy", "Don DeLillo", "Claude Mak\u00e9l\u00e9l\u00e9", "#364", "Magic Band", "Salzburg Festival", "German princely Battenberg", "German World War I fighter ace credited with 35 victories.", "The Nassau Herald", "Malibu Creek State Park, northwest of Los Angeles", "You are a puzzle", "pineapple", "asia", "incompetent", "Poland", "Steven Green", "Steve Williams", "AbdulMutallab", "Dr. No", "the spine", "Caonero II", "was a drug lord with ties to paramilitary groups,"], "metric_results": {"EM": 0.625, "QA-F1": 0.7281333887721755}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.2857142857142857, 0.375, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.588235294117647, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-376", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2925", "mrqa_naturalquestions-validation-468", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-5053", "mrqa_searchqa-validation-11199", "mrqa_searchqa-validation-5081", "mrqa_newsqa-validation-877"], "SR": 0.625, "CSR": 0.5487804878048781, "EFR": 1.0, "Overall": 0.7282717225609756}, {"timecode": 82, "before_eval_results": {"predictions": ["Kenny Young", "40 million", "American Idol", "brother-in-law", "Martin Scorsese", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Daimler-Benz", "Edward M. Kennedy", "private", "Lee Seok-hoon", "Two Pi\u00f1a Coladas", "mythology", "power directly or elect representatives from among themselves to form a governing body, such as a parliament", "Eucritta", "Umberto II", "1866", "1860", "Attorney General and as Lord Chancellor of England", "Sexred", "British", "Tea Tree Plaza", "924", "1951", "Darci Kistler", "1966", "Potomac River", "Europe", "The Kingkiller Chronicle series", "Gateways", "England", "Black Panthers", "Extended play", "\"Sausage Party\" (2016)", "May 5 to July 8, 2014", "The Primettes", "Cesario", "Wolf Creek", "Sky News", "High Court of Admiralty", "Chelsea Does", "bobsledder", "Double Agent", "Oregon Ducks", "Aksel Sandemose", "Theme Park 2", "John Glenn", "Eric Whitacre", "Sullivan University College of Pharmacy", "24 January 76 \u2013 10 July 138", "Marvel Comics", "Zambesi river", "along the Californian coast at The Inn at Newport Ranch", "1961 during the Cold War", "September 14, 2008", "Harry Truman", "The Kentucky Derby", "Gianni Versace", "London.", "his former Boca Juniors teammate and national coach Diego Maradona,", "his club", "pink", "Beaker", "Claddagh", "Emperor Concerto"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7736972402597403}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.09090909090909091, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6399999999999999, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.9523809523809523, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5066", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-4293", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-3769", "mrqa_hotpotqa-validation-4974", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-1828", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1461", "mrqa_searchqa-validation-10992"], "SR": 0.65625, "CSR": 0.5500753012048193, "EFR": 1.0, "Overall": 0.7285306852409639}, {"timecode": 83, "before_eval_results": {"predictions": ["Thunder Road", "from the former Kingdom of Strathclyde who spoke Cumbric, a close relative of the Welsh language, or possibly an incomer from Wales, or the Welsh Marches", "between the Eastern Ghats and the Bay of Bengal", "through 13 states : New York, New Jersey, Pennsylvania, Ohio, Indiana, Illinois, Iowa, Nebraska, Colorado, Wyoming, Utah, Nevada, and California", "New York University", "the oral mucosa ( a mucous membrane ) lining the mouth and also on the tongue and palates and mouth floor", "Eydie Gorm\u00e9", "Werner Ruchti", "Stephen Lang", "used their knowledge of Native American languages as a basis to transmit coded messages", "the English", "Himadri Station", "1959", "Sun Tzu ( `` Master Sun '', also spelled Sunzi )", "their son Jack ( short for Jack - o - Lantern ) is born on Halloween 2023", "alternative realities", "Pope Gregory I the Great", "1966", "December 20, 1951", "1978", "Warren Hastings", "The mixing of sea water and fresh water", "carbon dioxide", "June 1992", "John Vincent Calipari ( born February 10, 1959 )", "1975", "Charles Darwin and Alfred Russel Wallace", "Left Behind", "1936", "Who Wants to Be a Millionaire? in the United Kingdom", "in the 1820s", "May 18, 2018", "Russia", "China ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government", "Fusajiro Yamauchi", "Etienne de Mestre", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "pre-Christian festivals that were celebrated around the winter solstice", "Speaker of the House of Representatives", "by the NFL well in advance, usually three to five years before the game", "Italy", "Detroit Red Wings", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral", "Captain Jones", "Blue", "Florida and into the town of Coconut Cove", "S - shaped", "February 16, 2010", "the shooter must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "Lori Rom", "Jennifer Eccles", "Sinclair Lewis", "Akon", "Lily Hampton", "Hawaii Five-0", "Gregg Harper", "23-year-old", "more than 200.", "Transportation Security Administration", "the Vaio Z Canvas", "Newman", "The Color Purple", "East Knoyle"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6888708600427351}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.16, 1.0, 0.125, 1.0, 1.0, 1.0, 0.06666666666666667, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.8, 1.0, 0.6, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.16, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-6308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8391", "mrqa_naturalquestions-validation-3968", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-3243", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-1447", "mrqa_searchqa-validation-8173"], "SR": 0.59375, "CSR": 0.5505952380952381, "EFR": 1.0, "Overall": 0.7286346726190477}, {"timecode": 84, "before_eval_results": {"predictions": ["to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "the heart", "Grand Inquisition", "Theodore Roosevelt", "the Soviet Union and its satellite states", "Allison Janney", "in the s - block", "late - night", "Emma Watson", "April 10, 2018", "the New York Yankees", "a visible cross", "usually are placed to the left of the dinner plate", "Ireland", "Saphira", "Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluated", "Saint Peter", "the root respiration", "1983", "restricted naturalization to `` free white persons '' of `` good moral character ''", "a heart rate that exceeds the normal resting rate", "Ptolemy", "2011", "Battle of Antietam", "Emma Watson", "1939", "upon braking to a full stop", "Thomas Mundy Peterson", "Filipino Americans", "Judy Collins", "The Continental Congress", "kharis", "Napoleon Bonaparte", "Lagaan", "A blighted ovum", "September 19 - 22, 2017", "the New Testament", "Kirsten Simone Vangsness", "asphyxia", "in 1902", "Camp Green Lake", "limited period of time", "1999", "10,605", "East Coast of the United States", "the Reverse - Flash", "March 1930", "John Donne", "4 hurricane", "season four", "786", "Rugby School", "gold", "Doctor Dolittle", "goalkeeper", "Christian Kern", "50 best cities to live in", "inspectors in the agency's Colorado office", "Miguel Cotto", "Average", "Rocky", "Cumberland Gap", "salinity", "George Fox"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6612713675213675}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.4, 0.5, 0.8, 0.5, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.4444444444444445, 1.0, 0.8, 1.0, 0.0, 0.5, 0.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-1688", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-6118", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-4863", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2653"], "SR": 0.53125, "CSR": 0.5503676470588235, "EFR": 0.9666666666666667, "Overall": 0.721922487745098}, {"timecode": 85, "before_eval_results": {"predictions": ["John Marshall", "Pirates of the Caribbean: At World's End", "Samuel de Champlain", "Louis XIV", "Lady Jane Grey", "the Barbary Coast", "Iceland", "Excalibur", "Richard Cory", "Volkswagen", "baldness", "Athens", "rum", "tea rose", "Aida", "give love a bad name", "the Banni grasslands", "Marie Antoinette", "rotunda", "the magnolia", "haryana", "a bicentennial", "the Gallic War", "PayPal", "the peace sign", "Michael Dell.", "Pizza Hut", "Lusitania", "1972", "a hurricane", "Amish", "the Rocky Mountains", "carbon", "The Amber Spyglass", "Boston", "Wu-Tang Clan", "king", "(Jose de San) Martin", "a whale", "Salt Lake City", "Luxembourg", "Texas", "Dragging", "Python", "chrie", "Las Vegas", "Laura", "The New Yorker", "People of the Book", "a snout beetle", "Tufts University", "The results of the Avery -- MacLeod -- McCarty experiment, published in 1944, suggested that DNA was the genetic material,", "the cast", "from the top of the leg to the foot on the posterior aspect", "f\u00fa Hu\u00ec", "Oxygen", "The Truman Show", "Robert Redford", "Great Lakes and Midwestern", "three people", "five", "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "March 3, 2008,", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6933035714285714}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-12392", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-6995", "mrqa_searchqa-validation-1981", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-186", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-15481", "mrqa_searchqa-validation-4152", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-3852", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-5727", "mrqa_triviaqa-validation-2130", "mrqa_hotpotqa-validation-3415", "mrqa_newsqa-validation-4210", "mrqa_newsqa-validation-2665"], "SR": 0.59375, "CSR": 0.5508720930232558, "EFR": 1.0, "Overall": 0.7286900436046512}, {"timecode": 86, "before_eval_results": {"predictions": ["Granite", "Bull", "Horse Feathers", "Bleak House", "Chaillot", "driving Miss Daisy", "Colouring", "Asteroids", "a bad peace", "Yves Saint Laurent", "Wyoming", "England", "Lend-Lease Act", "Spanglish", "Monica Lewinsky", "Friday Night Lights", "Google", "Medusa", "the vest", "Prince", "a gull", "Hammurabi", "Nixon", "precipitation", "(Erwin) Rommel", "a sudden jerk", "Ned", "the 747", "Terry Bradshaw", "Chris Evert", "Azerbaijan", "Mamma Mia!", "Fallingwater", "Alanis Morissette", "commas", "a barrel", "Etna", "a law clerk", "Faneuil Hall", "Louisiana", "(George) Orwell", "tea", "Toro", "Stalin", "Metallica", "change horses", "Get Smart", "Lafayette", "the Great Gatsby", "Captain Kangaroo", "Kosher", "1996", "The 1972 Dolphins", "Gabrielle - Suzanne Barbot de Villeneuve", "a dragon", "nitrogen", "South Africa", "CN Too", "Via Port Rotterdam", "Bruce Grobbelaar", "Tehran.", "The Palm Jumeirah", "35,000.", "Mashhad"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7130208333333332}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-14053", "mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-7312", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-2504", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-16876", "mrqa_searchqa-validation-4740", "mrqa_searchqa-validation-13220", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-4307", "mrqa_naturalquestions-validation-288", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-1471", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-337", "mrqa_newsqa-validation-3141"], "SR": 0.640625, "CSR": 0.5519037356321839, "EFR": 1.0, "Overall": 0.7288963721264368}, {"timecode": 87, "before_eval_results": {"predictions": ["Oblivion", "chiffon", "Corpus Christi", "Grover Cleveland", "an eye", "Federalist Papers", "Martin Luther King,", "transitive & intransitive", "California", "the Pacific", "\"BOB ATE THE PIE\"", "Tom Cruise", "Sicilian pizza", "a panda", "Risk", "rice", "Kansas State University", "fish", "1945", "Kentucky", "a stork", "Towers", "the Rattus", "anime", "Daisy Miller", "Icelandic", "Robin Hood", "Mercury", "the Skoda Kodiaq", "The Stars and Stripes Forever", "One Hundred Years of Solitude", "fatal", "Henry Cavendish", "vanilla", "terminal", "Italy", "Night of the Iguana", "Rhode Island", "Baseball", "Anne Rice", "the root", "the Book of the Judges", "1066", "Sir John Soane", "Little Yellow jacket", "the hip", "a hearse", "City Slickers", "Ned Kelly", "souci", "the Doge of Venice", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "to the southeastern United States", "5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Italy", "Pakistan International Airlines", "Darisabeth Frink", "1998", "\"Peshwa\"", "Sleeping Beauty", "the Beatles", "ceo Herbert Hainer", "President Bush", "Consumer Reports"], "metric_results": {"EM": 0.578125, "QA-F1": 0.723859126984127}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15068", "mrqa_searchqa-validation-2683", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-4561", "mrqa_searchqa-validation-9515", "mrqa_searchqa-validation-5164", "mrqa_searchqa-validation-11547", "mrqa_searchqa-validation-13444", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-2206", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-8657", "mrqa_searchqa-validation-12017", "mrqa_searchqa-validation-727", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-13093", "mrqa_searchqa-validation-12454", "mrqa_searchqa-validation-5566", "mrqa_naturalquestions-validation-7133", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-4719", "mrqa_hotpotqa-validation-354", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-663"], "SR": 0.578125, "CSR": 0.5522017045454546, "EFR": 1.0, "Overall": 0.7289559659090908}, {"timecode": 88, "before_eval_results": {"predictions": ["Sputnik", "A Good Day to Die Hard", "The Kinks", "Mikhail S. Gorbachev", "Jerez de la Frontera", "Buncefield Depot", "the royal court", "cable", "Westminster Abbey", "Cast", "A-K-Q-J-10", "Hawaii", "World War II", "aromatherapy", "The Bill", "Downton Abbey", "Bobby Darin", "France", "Montmorency", "Kent", "Cliff Thorburn", "St. Moritz", "four", "cymbal", "violin", "Ireland", "Venus", "beetles", "paralysis", "eight", "Japanese silvergrass", "Swindon Town", "Billy Preston", "Happy Birthday to You", "Everton", "Awning window", "Marc", "The Brothers Karamazov", "malaysia trollope", "Mud", "1941", "Jimmy Knapp", "Check Me", "31536000", "11,034", "John Galliano", "Bloodaxe", "Richard Seddon", "Chiricahua", "Albert Reynolds", "Aug. 24, 1572", "in the very late 1980s", "Taylor Michel Momsen", "Malayalam", "2008", "Canada", "Dutch", "the peace with Israel", "Jiverly Wong,", "28 states", "Lili Taylor", "Ager", "a jam", "Mike Mills"], "metric_results": {"EM": 0.625, "QA-F1": 0.6666666666666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 0.4, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-4019", "mrqa_newsqa-validation-3464", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-9544", "mrqa_searchqa-validation-3464"], "SR": 0.625, "CSR": 0.5530196629213483, "EFR": 0.9583333333333334, "Overall": 0.7207862242509364}, {"timecode": 89, "before_eval_results": {"predictions": ["18", "A third beluga whale belonging to the world's largest aquarium has died", "Los Ticos", "reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Aniston, Demi Moore and Alicia Keys", "around Haiti,", "gendarmerie", "dance", "new materials", "gadahn", "10", "\u00a341.1 million", "Hong Kong's Victoria Harbor", "helping to plan the September 11, 2001, terror attacks,", "to provide security as needed.", "Los Ticos", "some dental work done,", "the hiring of hundreds of foreign workers", "fill a million sandbags", "helping on the sandbag lines", "Eric Besson", "since 1983", "133", "promotes fuel economy and safety while boosting the economy.", "planning processes are urgently needed", "Jewish", "12.3 million", "The Ski Train", "two years", "Robert Park", "Piedad Cordoba", "23", "Itawamba County School District", "Frank Ricci,", "1959", "$249", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "1983", "13", "The Valley Swim Club", "\"still trying to absorb the impact of this week's stunning events.\"", "International Polo Club Palm Beach in Florida.", "dennis Mohler", "Sharon Bialek", "not", "John Dillinger,", "five", "New Year's Day", "Iggy Pop", "two", "Opryland", "Garfield Sobers", "R.E.M.", "at the Cow Palace", "John Donne", "127 Hours", "kiel Canal", "\"Slaughterhouse-Five\"", "1885", "Germanic", "hearsay", "buffoon", "17th century", "the Book of Esther"], "metric_results": {"EM": 0.53125, "QA-F1": 0.630275974025974}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-411", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-268", "mrqa_naturalquestions-validation-878", "mrqa_hotpotqa-validation-4986", "mrqa_triviaqa-validation-6731"], "SR": 0.53125, "CSR": 0.5527777777777778, "EFR": 1.0, "Overall": 0.7290711805555556}, {"timecode": 90, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.85546875, "KG": 0.49921875, "before_eval_results": {"predictions": ["More than 15,000", "Jezebel.com's", "Gov. Mark Sanford", "the Russian air force,", "Chevron", "one", "serving its fast burgers in the Carrousel du Louvre,", "Melbourne.", "allergen", "opium", "Tuesday,", "\"mission incapable\" -- the troops were \"hostile, vengeful and needed increased control and command,\"", "the fact that the teens were charged as adults.", "15 years on,", "the fastest circumnavigation of the globe in a powerboat", "order", "General Motors", "St. Louis, Missouri.", "\"Star Wars,\"", "Nearly eight in 10", "66th annual Golden Globe Awards", "no motive has been determined for the killing,", "Opry Mills,", "striker", "Jan Brewer.", "the oceans", "Columbia, Missouri.", "cutesy.", "\"These guys need to take a look around and see that we're facing 10 percent unemployment and an economy on the brink of collapse,\"", "Leo Frank,", "Ralph Lauren,", "the government.", "Islamabad", "evokes childhood memories in this four-line ode to Mom.", "September,", "Olympia", "\"Percy Jackson & The Olympians,\"", "Frank Ricci,", "Coast Guard", "the Russian air force,", "they are co-chair of the Genocide Prevention Task Force.", "she wonders if part of the appeal of plus-sized", "Turkey can play an important role in Afghanistan as a reliable NATO ally.", "the operator or by passengers.", "Jaipur", "Venezuela", "likening one American diplomat to a \"prostitute\"", "meeting with the president to discuss her son.", "Toffelmakaren.", "she also believed police were trying to cover up the truth behind her daughter's murder,", "buckling under pressure from the ruling party.", "16 seasons", "12.9 - kilometre ( 8 mi )", "1939", "Afghanistan", "agnolo", "60", "Nicolas Winding Refn", "Father Dougal McGuire", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "icago sativa", "Jefferson", "The Exorcist", "Adolphe Adam"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6432135596162957}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.26086956521739124, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.06060606060606061, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.17391304347826086, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1368", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2983", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-3262", "mrqa_searchqa-validation-8549"], "SR": 0.578125, "CSR": 0.5530563186813187, "EFR": 0.9629629629629629, "Overall": 0.7214069813288563}, {"timecode": 91, "before_eval_results": {"predictions": ["Coca-Cola.", "Linus", "chestnut", "bitter almond", "Mark Twain", "Oslo", "Humphrey Bogart", "Hawaii", "glockenspiel", "George Orwell", "Goldtrail", "archers", "Ben Franklin", "Jack Nicholson", "photography", "honey and spices", "Taiwan", "Willem de Zwijger", "Oliver Stone", "President Nixon", "Oregon", "your Excellency", "Nikola Tesla", "Thomas De Quincey", "Susie Dent", "Pancho Villa", "Crusades", "Ivan Owen", "1919", "copper", "Pickwick", "Bluebell Girls", "Columbus", "switzerland", "Ann Darrow", "blue", "the Flying Pickets", "St Moritz", "switzerland", "Vietnam", "1985", "Bogota", "the United States", "James Murdoch", "Crystal Palace", "Belfast", "Moulin Rouge", "Thermopylae", "Elton John", "Lake Union", "Marshalsea", "card verification", "Jacques Cousteau", "at Tandi", "94", "August 6, 1845", "Ted", "24", "Tuesday", "Afghan opium trade", "Japheth", "resuscitation", "Hudson", "Subway"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7059850146198831}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473684, 0.4444444444444445, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-4026", "mrqa_triviaqa-validation-6962", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-7195", "mrqa_triviaqa-validation-3121", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-6887", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-2175", "mrqa_searchqa-validation-417", "mrqa_searchqa-validation-13767", "mrqa_searchqa-validation-8329"], "SR": 0.65625, "CSR": 0.5541779891304348, "EFR": 0.9545454545454546, "Overall": 0.7199478137351779}, {"timecode": 92, "before_eval_results": {"predictions": ["orangutans", "lowestoft", "Solomon", "new zealand", "Demi Moore", "Ernest Hemingway", "Charles II", "the \u201cGodfather of Italian cooking\u201d", "NASA\u2019s Hubble Space Telescope", "France", "greece", "a window", "dennis smith", "a coffee house", "the little dog laughed,", "baseball", "bansk\u00e1 \u0160tiavnica", "Hilary Swank", "Neighbours", "kursk", "Jessica Simpson", "6", "the Blind Beggar", "Mark Darcy", "one Thousand and One", "homo sapiens", "british airways", "gorbachev", "surrey", "Theo Walcott", "netherlands", "netherlands", "santa taylor", "netherlands", "petula Clark", "dr tamseel", "a dice game", "Saturn", "Sinclair Lewis", "the Fleet River", "Unseen Academicals", "james chadwick", "sealion", "1879", "the long-tailed weasel", "table tennis", "bison", "alberich", "geomorphology", "mars", "bartertown", "com TLD", "abdicated", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Guangzhou", "29, 1985", "Jane Mayer,", "At least seven deaths were attributed to the storm,", "a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "American", "avanti", "Coors Field", "the Chrysler Building", "the Enterprise"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5498945932539683}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125, 0.22222222222222218, 0.6666666666666666, 0.0, 1.0, 0.2222222222222222, 0.09999999999999999, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-5157", "mrqa_triviaqa-validation-1504", "mrqa_triviaqa-validation-1795", "mrqa_triviaqa-validation-654", "mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-5916", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-904", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2308", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-476", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-2717", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-3990", "mrqa_searchqa-validation-6690", "mrqa_hotpotqa-validation-2871"], "SR": 0.484375, "CSR": 0.5534274193548387, "EFR": 0.9696969696969697, "Overall": 0.7228280028103617}, {"timecode": 93, "before_eval_results": {"predictions": ["Robert Barnett,", "a public housing project,", "a hospital in Amstetten,", "new zealand", "Zimbabwe", "a one-shot victory in the Bob Hope Classic", "\"evolve faster' on supporting full marriage equality,\"", "\"an accomplished pilot\"", "Susan Atkins,", "80,", "she is an angel, she is God-sent,\"", "U.S.", "Passers-by", "subscribers to a daily publication which is the primary service of Stratfor,\"", "talk show queen Oprah Winfrey.", "Former Mobile County Circuit Judge Herman Thomas", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "heavy flannel or wool", "Diego Milito's", "Uma Bazaar (Ostra Forstadsg 13)", "the worst snowstorm to hit Britain in", "potential revenues from oil and gas", "the men and women of the armed forces", "two", "finance", "Indonesian", "The Charlie Daniels Band,", "three out of four", "trading goods and services without exchanging money", "gasoline", "Thai Army Col. Sansern Kaewkumnerd", "South Africa", "Harrison Ford", "Jacob,", "Cash for Clunkers", "100 meter", "Beijing", "fluoroquinolone", "Chinese and international laws", "the player", "\"Great Charter\" in Latin.", "27,", "five", "Secretary of State", "the American Civil Liberties Union.", "it was unjustifiable", "his father, Osama bin Laden,", "\"The Sopranos,\"", "finance", "$60 billion", "managing his time.", "merengue", "exports to other states occurring around 1858", "currently a free agent", "John Sullivan", "bryson", "argument form", "1961", "Ariel Ram\u00edrez", "\"Boston Herald\"", "the Yangtze River", "oui", "the Andes Mountains", "season"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6735696444256227}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 0.8, 0.8333333333333333, 0.0, 1.0, 1.0, 0.8333333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.2857142857142857, 0.1818181818181818, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3233", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-4204", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-4073", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-747", "mrqa_triviaqa-validation-3004", "mrqa_searchqa-validation-13597", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-6906", "mrqa_triviaqa-validation-2535"], "SR": 0.546875, "CSR": 0.5533577127659575, "EFR": 0.9310344827586207, "Overall": 0.7150815641049155}, {"timecode": 94, "before_eval_results": {"predictions": ["Barack Obama:", "Immigration Minister Eric Besson", "$500,000", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "bankruptcy", "At least 38", "hooked up with Mildred, a younger woman of about 80, in March.", "Eleven", "41,", "McDonald's", "new voters became the key to his Iowa win and revealed the outline of a general election plan: Create a wide coalition to bring new voters to the polls in record numbers.", "10.1,\"", "Democrats and Republicans", "Zac Efron", "The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "Sylt", "the other women who couldn't or wouldn't.\"", "Congress", "Cash for Clunkers", "11th year in a row.", "three thousand", "the \"face of the peace initiative has been attacked.\"", "after Wood went missing off Catalina Island, near the California coast,", "grossed $55.7 million during its first frame,", "Tulsa, Oklahoma.", "100", "Janet Napolitano", "56,", "There's no chance", "Barack Obama,", "gas emissions,", "1980,", "30,000", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "$14.1 million.", "Thamer Bin Saeed Ahmed al-Shanfari.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Orbiting Carbon Observatory,", "the IAAF", "al Qaeda, the Taliban, and those linked to the attack on mainland America on 9/11,\"", "Kim Clijsters", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Daryeel Bulasho Guud", "the college campus.", "a body", "Opry", "Caylee Anthony", "in the neighboring country of Djibouti,", "18", "school,", "Hayden", "to collect menstrual flow", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "Massachusetts", "greece", "Muhammad Ali", "Ub Iwerks", "Pisgah", "4,613", "black Sabbath", "Raytheon", "the Lion King", "Radiohead", "Johnny Got His"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6530540341136455}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.35294117647058826, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.10714285714285715, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.24489795918367346, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2341", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-644", "mrqa_triviaqa-validation-2079", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3036", "mrqa_searchqa-validation-6118"], "SR": 0.546875, "CSR": 0.5532894736842106, "EFR": 1.0, "Overall": 0.728861019736842}, {"timecode": 95, "before_eval_results": {"predictions": ["Ford", "organic compounds", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "around 3,000 - 5,000 program - erase cycles, but some flash drives have single - level cell ( SLC ) based memory that is good for around 100,000 writes", "an opinion in a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "October 6, 2017", "autopistas", "Bemis Heights", "Fusajiro Yamauchi", "1854", "Tim McGraw and Kenny Chesney", "Qutab - ud - din Aibak", "the Devastator", "Abraham Gottlob Werner ( 1749 -- 1817 )", "lacteal", "`` Fix You ''", "In the early 20th century", "Britain and France", "the Origination Clause of the United States Constitution", "architecture", "in the fictional town of West Egg on prosperous Long Island in the summer of 1922", "the arms of Ireland", "pop ballad", "2010", "( 2017 - 12 - 10 )", "Annette Strean", "Charles Path\u00e9", "eukaryotic", "during World War II", "Bactrian", "2003", "introduced and elaborated as early as in 1651 by Thomas Hobbes in his Leviathan, though with a somewhat different meaning ( similar to the meaning used by the British associationists )", "fall of 2015", "Rodney Crowell", "Spanish moss", "Guant\u00e1namo or GTMO ( / \u02c8\u0261\u026atmo\u028a / )", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "961", "The Lightning thief", "milling", "The Cornett family", "Anna Faris and Allison Janney in lead roles as dysfunctional daughter / mother duo Christy and Bonnie Plunkett", "around 2011", "Patrick Swayze", "McFerrin, Robin Williams, and Bill Irwin", "Ann Gillespie", "Lula", "2017", "March 15, 1945", "an Aldabra giant tortoise", "Oliver Goldsmith", "Brussels", "indus Valley", "Seventeen", "Cartoon Network", "What You Will", "Kurt Cobain", "Madonna", "a 12-year veteran of the Utah state police,", "Shakespeare", "Bob Hope", "three", "money"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6542119956018674}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.25806451612903225, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.782608695652174, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.32258064516129037, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-1910", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-3309", "mrqa_triviaqa-validation-3684", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4098", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-7262", "mrqa_newsqa-validation-1983"], "SR": 0.5625, "CSR": 0.5533854166666667, "EFR": 0.9642857142857143, "Overall": 0.7217373511904761}, {"timecode": 96, "before_eval_results": {"predictions": ["the game", "shot and killed while on duty Wednesday -- allegedly gunned down by an 88-year-old white supremacists who stepped into the museum with a rifle and began firing.", "Another high tide -- expected to reach about 4 meters (13 feet) high", "April 22,", "\"Hawaii Five-O\"", "a number of calls, and those calls were intriguing, and we're chasing those down now,\"", "30", "U.S. senators", "The son of Gabon's former president", "Congressman", "could be secretly working on a nuclear weapon", "Abu Sayyaf,", "Islamabad", "to travel,", "nine newly-purchased bicycles at the scene,", "hot and humid", "a hospital", "in Fayetteville, North Carolina,", "promised federal help for those affected by the fires.", "Ewan McGregor", "September,", "\"This is not something that anybody can reasonably anticipate,\"", "genocide, crimes against humanity, and war crimes.", "any person who has been abused by any priest of the Diocese of Cloyne during my time as bishop or at any time,\"", "Barney Stinson,", "19-year-old", "Dr. Death in Germany", "collaborating with the Colombian government,", "U.S. Holocaust Memorial Museum,", "sportswear,", "soluble fiber, peppermint oil, and antispasmodic drugs", "Sunday's", "Venus Williams", "researchers", "five of us for the United States and two against us because they were stranded in Japan\" when the war came.", "30-minute", "Grease", "in good spirits, especially comforted to be receiving care from talented doctors in a world-class hospital named in honor of her late husband,\"", "gunned down four Lakewood, Washington, police officers Sunday.", "the estate with its 18th-century sights, sounds, and scents.", "shows that students often know ahead of time when and where violence will flare up on campus.", "between 1917 and 1924", "the strike means all buses, subways and trolleys in Philadelphia and on the Frontier line in Bucks and Montgomery counties stopped running at 3 a.m.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "the children of street cleaners and firefighters.", "\"CNN Heroes: An All-Star Tribute\"", "clean up Washington State's decommissioned Hanford nuclear site,", "over 1,000 pounds", "3,000 kilometers (1,900 miles)", "UH-60 Blackhawk helicopters", "Marc Jacobs", "Lorazepam", "the Coppolas and, technically, the Farrow / Previn / Allens", "To capitalize on her publicity", "ecclesiastical communities", "The Men Behaving Badly", "bunch grasses -- like perennial rye grass and tall fescue.", "\u00c6thelwald Moll", "The Bears", "Nikolai Trubetzkoy", "the Greek alphabet", "Charles Dana Gibson", "the Constitution", "snail"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5324602047258298}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.13333333333333333, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 0.1, 1.0, 1.0, 0.5454545454545455, 1.0, 0.2222222222222222, 0.9696969696969697, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.2857142857142857, 0.22222222222222224, 0.0, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1781", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2130", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-3543", "mrqa_searchqa-validation-114"], "SR": 0.421875, "CSR": 0.5520296391752577, "EFR": 0.972972972972973, "Overall": 0.723203647429646}, {"timecode": 97, "before_eval_results": {"predictions": ["Ferrari", "Fernando Verdasco,", "Six", "girls around 11 or 12.", "David Bowie,", "1957,", "behind the counter.", "\"We want to reset our relationship and so we will do it together.'\"", "heavy brush,", "1959.", "Phoenix, Arizona,", "\"project work\"", "Buddhism", "40", "\"He is telling me to regain the trust of those customers who are driving our vehicles.\"", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus -- and went on a 100-day killing rampage.", "Molotov cocktails, rocks and glass.", "Alfredo Astiz,", "on the 11th anniversary of the September 11, 2001, terror attacks.", "Mashhad, Iran.", "wars in Iraq and Afghanistan", "$106,482,500", "Obama should have met with the Dalai Lama.", "Matthew Fisher,", "autonomy.", "head", "control and censorship remain rife across the Middle East and North Africa,", "Guinea, Myanmar, Sudan and Venezuela.", "Brazil's", "Uzbekistan.", "45 minutes, five days a week.", "urged NATO to take a more active role in countering the spread of the narcotics trade,", "took on water", "ash and rubble in place of their homes.", "July", "Visitors aren't allowed onto the property to view the elephants,", "an auxiliary lock", "Diego Milito's", "a music video on his land.", "Robert Mugabe", "gang rape", "\"wacko.\"", "threatening messages", "Mexico", "more than 1.2 million people.", "Sunday.", "tennis", "Osama bin Laden's sons", "Chesley \"Sully\" Sullenberger", "Gary Player,", "Karen Floyd", "Asuka", "Authority", "Prince William, Duke of Cambridge", "the Astor family", "Gryffendor", "Rowan Atkinson", "841", "Adelaide", "gender queer", "Windsor Castle", "Tad Hamilton", "Israel", "Bill Irwin"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6835231649477973}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.11764705882352941, 0.08, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1362", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-1775", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-2337", "mrqa_triviaqa-validation-5311", "mrqa_triviaqa-validation-5472", "mrqa_hotpotqa-validation-2217"], "SR": 0.578125, "CSR": 0.5522959183673469, "EFR": 0.9629629629629629, "Overall": 0.721254901266062}, {"timecode": 98, "before_eval_results": {"predictions": ["Cambodian officials", "monarchy.", "\"pleased\"", "AbdulMutallab", "partially submerged in a stream in Shark River Park in Monmouth County", "consumer confidence", "anesthetic", "Madonna", "Wigan Athletic", "Iran's parliament speaker", "18", "as soon as 2050,", "U.S. State Department and British Foreign Office", "Janet Napolitano", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "break up ice jams.", "is likely to top $60 million by the time the Presidents Day holiday weekend is over.", "unable to pass", "Islamabad", "3rd District of Utah.", "75.", "Nineteen", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "nearly $2 billion", "Jeddah, Saudi Arabia,", "Al-Shabaab,", "February 12", "Kenneth Cole", "T.I.", "heavy turbulence", "\"We've just lost count of how many demonstrations are taking place now,\"", "to sniff out cell phones.", "California, Texas and Florida,", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "Ozzy Osbourne", "Revolutionary Armed Forces of Colombia,", "suicides", "Jaime Andrade", "Aung San Suu Kyi", "$81,8709", "Patrick McGoohan,", "\"GoldenEye\"", "J.Crew,", "Dublin.", "1995", "Chester Arthur Stiles, 38,", "hopes the journalists and the flight crew will be freed,", "a skilled hacker", "$7.8 million", "AMD, a competitor,", "remains committed to British sovereignty", "Lee Thompson Young", "September 2017", "Norman occupational surname ( meaning tailor ) in France", "Monopoly", "carpe diem", "Cerebro- Spinal Fluid", "India Today", "Eliot Cutler", "7 January 1936", "Bran Mak Morn", "Istanbul", "Washington Bullets", "Mrs. Miniver"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7025878773753971}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.9, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.9565217391304348, 0.0, 0.35294117647058826, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.9333333333333333, 1.0, 1.0, 0.2857142857142857, 0.21052631578947364, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3888", "mrqa_naturalquestions-validation-8858", "mrqa_triviaqa-validation-3058", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-6956"], "SR": 0.609375, "CSR": 0.5528724747474747, "EFR": 0.96, "Overall": 0.720777619949495}, {"timecode": 99, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2871", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1961", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-739", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-2079", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4698", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-5339", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-904"], "OKR": 0.7890625, "KG": 0.4703125, "before_eval_results": {"predictions": ["bacteria", "1648 - 51", "England", "Darren McGavin", "a certified question or proposition of law from one of the United States Courts of Appeals", "Emily Osment", "The pour point of a liquid", "Madison's", "May 29, 2018", "Dan Stevens", "in Ephesus in AD 95 -- 110", "Bulgaria", "zinc", "1986", "2018", "The symbol", "2010", "September 2017", "Clarence Anglin", "Ariana Clarice Richards", "James Intveld", "Darlene Cates", "one", "mashed potato", "2020", "in the dress shop", "George Harrison", "Seton Hall Pirates men's basketball", "second season", "Universal Pictures and Focus Features", "Cyanea capillata", "supported modern programming practices and enabled business applications to be developed with Flash", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "the county seat and commercial center of Lee County, Florida", "digestive systems", "Theodosius I", "Christopher Allen Lloyd", "South Africa", "Mark Jackson", "Fall 1998", "TLC - All That", "Inequality of opportunity", "In 1929", "Abid Ali Neemuchwala", "Bhupendranath Dutt", "May 19, 2017", "people of France to the people of the United States", "an armature of piped masonry often carved in decorative patterns", "about 2,621 kilometres ( 1,629 mi ) from its headwaters in the Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "Gustav Bauer", "either in front or on top of the brainstem", "leicestershire", "\"The curse is come upon me,\"", "15", "Secretary of State Hillary Clinton", "Soviet Union", "Heinkel Flugzeugwerke", "She returned to Pakistan in October after President Pervez Musharraf signed an amnesty lifting corruption charges.", "Matthew Perry and Leslie Mann,", "Aryan Airlines Flight 1625", "Ocean's Twelve", "Trajan's Column", "Anne Rice", "Wilkie Collins"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7257492201426025}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.5000000000000001, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-5094", "mrqa_hotpotqa-validation-5509", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-1524", "mrqa_searchqa-validation-15139"], "SR": 0.609375, "CSR": 0.5534375, "EFR": 0.96, "Overall": 0.6951875000000001}]}