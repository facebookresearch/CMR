{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=3e-5_ep=10_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 7810, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Spain", "too much grief", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "underpinning", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "It says \"Adam Trask was born on a farm on the outskirts of a little town which was not far from a big town in Connecticut", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a", "unemployment benefits"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7578004807692308}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.08, 0.16666666666666666, 0.0, 0.0, 0.33333333333333337, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.703125, "CSR": 0.7734375, "retrieved_ids": ["mrqa_squad-train-41657", "mrqa_squad-train-54323", "mrqa_squad-train-1393", "mrqa_squad-train-2070", "mrqa_squad-train-68222", "mrqa_squad-train-71632", "mrqa_squad-train-1093", "mrqa_squad-train-85699", "mrqa_squad-train-3143", "mrqa_squad-train-24636", "mrqa_squad-train-59966", "mrqa_squad-train-20537", "mrqa_squad-train-43217", "mrqa_squad-train-64685", "mrqa_squad-train-85890", "mrqa_squad-train-78769", "mrqa_squad-validation-7527", "mrqa_squad-validation-3119", "mrqa_squad-validation-2372", "mrqa_squad-validation-1766", "mrqa_squad-validation-2289", "mrqa_squad-validation-9918", "mrqa_squad-validation-3130", "mrqa_squad-validation-4662", "mrqa_squad-validation-1891", "mrqa_squad-validation-7574"], "EFR": 1.0, "Overall": 0.88671875}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating the LM and Saturn V. Apollo 4", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "burning a mixture of acetylene and compressed O2", "war, famine, and weather", "Wesel-Datteln Canal", "TLC", "on the south side of the garden", "novel medications", "friendly and supportive", "Eero Saarinen", "Newton", "41", "he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "Father Cameron Faller and Father Patrick", "Fondue", "the Green Hornet", "the scrum-half", "\"not just for dancing\"", "Kingston", "sanguine", "that he sent him to... John Calvin can't influence certain Baptists because he wasn't one; or that... their famous debate that became the source for The Bloudy Tenent.", "Sequoyah Nuclear Plant", "1 year 6 months", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.75, "QA-F1": 0.775462962962963}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-3456", "mrqa_squad-validation-5525", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.75, "CSR": 0.765625, "retrieved_ids": ["mrqa_squad-train-54711", "mrqa_squad-train-25586", "mrqa_squad-train-171", "mrqa_squad-train-50269", "mrqa_squad-train-51760", "mrqa_squad-train-77238", "mrqa_squad-train-71515", "mrqa_squad-train-35244", "mrqa_squad-train-2029", "mrqa_squad-train-76821", "mrqa_squad-train-27432", "mrqa_squad-train-73968", "mrqa_squad-train-72464", "mrqa_squad-train-44574", "mrqa_squad-train-79130", "mrqa_squad-train-27397", "mrqa_squad-validation-3119", "mrqa_squad-validation-1891", "mrqa_squad-validation-2372", "mrqa_squad-validation-3165", "mrqa_squad-validation-1766", "mrqa_squad-validation-7527", "mrqa_newsqa-validation-160", "mrqa_squad-validation-7307", "mrqa_squad-validation-1500", "mrqa_searchqa-validation-541", "mrqa_squad-validation-3355", "mrqa_squad-validation-1092", "mrqa_squad-validation-4999", "mrqa_squad-validation-5835", "mrqa_squad-validation-4662", "mrqa_searchqa-validation-4674"], "EFR": 0.9375, "Overall": 0.8515625}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "technological superiority", "four classes", "San Joaquin Light & Power Building", "1972", "three", "books, films, radio, TV, music, live theater, comics and video games", "behavioral and demographic data", "David McLetchie", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "issues with technical problems and flight delays", "US Supreme Court", "trust God's word", "zeta function", "those who proceed to secondary school or vocational training", "139th out of 176 total countries", "eight", "kinetic friction force", "early 1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "private citizen", "ten", "1 a.m.", "Department of State Affairs", "occupational stress among teachers", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT (1110 AM)", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service", "BBC HD", "Byker", "Genoa, Italy", "Northwestern State University", "Chickamauga", "a brown one with gold mane is one of the tier 2 horses", "Porous Media", "Gaius Maecenas", "Christopher Tolkien", "Sweden", "the Student loan Scheme Act", "a miserably tedious mess", "the Opera", "Albert Spalding", "John James Osborne", "the BBC Eastern Service", "The Gleaners", "Wikia", "a mansard roof"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7135416666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9533", "mrqa_squad-validation-5824", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-412"], "SR": 0.6875, "CSR": 0.74609375, "retrieved_ids": ["mrqa_squad-train-12495", "mrqa_squad-train-84638", "mrqa_squad-train-3716", "mrqa_squad-train-59878", "mrqa_squad-train-67080", "mrqa_squad-train-70293", "mrqa_squad-train-64886", "mrqa_squad-train-7656", "mrqa_squad-train-80461", "mrqa_squad-train-49306", "mrqa_squad-train-28347", "mrqa_squad-train-24795", "mrqa_squad-train-6724", "mrqa_squad-train-27669", "mrqa_squad-train-16820", "mrqa_squad-train-21434", "mrqa_squad-validation-7574", "mrqa_squad-validation-2920", "mrqa_squad-validation-3165", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_squad-validation-1766", "mrqa_searchqa-validation-9403", "mrqa_squad-validation-3355", "mrqa_squad-validation-3699", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-4266", "mrqa_squad-validation-2372", "mrqa_squad-validation-9918", "mrqa_squad-validation-3130", "mrqa_squad-validation-7687"], "EFR": 0.95, "Overall": 0.848046875}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "a liturgical setting of the Lord's Prayer", "$5 million", "hypersensitive response of plants", "2.666 million residents", "Industry and manufacturing", "violence", "The Parish Church of St Andrew", "1262", "New Orleans", "April 1523", "Dating of lava and volcanic ash layers", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "an imposed selective breeding version of eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Tuesday", "Miami", "plan the physical proceedings, and to integrate those proceedings with the other parts", "Cybermen", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "standard", "Lucas\u2013Lehmer", "Level 3 Communications", "Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "the length of Noah's Ark", "opera seria", "Okinawa", "a chestave", "organ system", "latkes", "Casper", "Tarsus", "Haggerty's", "Woody Allen", "Louisa May Alcott", "Walter Cronkite", "Treasure Island", "Charles R. Tunley", "Kerry Moosman", "Williams Pear Liqueur", "white", "Blended", "1960s", "a chestnut", "Alistair Grant", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7320075757575757}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.1212121212121212]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-6791", "mrqa_squad-validation-117", "mrqa_squad-validation-455", "mrqa_squad-validation-10140", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_naturalquestions-validation-1549", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.671875, "CSR": 0.73125, "retrieved_ids": ["mrqa_squad-train-28236", "mrqa_squad-train-30026", "mrqa_squad-train-30337", "mrqa_squad-train-74749", "mrqa_squad-train-84144", "mrqa_squad-train-36044", "mrqa_squad-train-12720", "mrqa_squad-train-9026", "mrqa_squad-train-66215", "mrqa_squad-train-36006", "mrqa_squad-train-52505", "mrqa_squad-train-31861", "mrqa_squad-train-63124", "mrqa_squad-train-51870", "mrqa_squad-train-74666", "mrqa_squad-train-29828", "mrqa_squad-validation-8558", "mrqa_squad-validation-1766", "mrqa_searchqa-validation-4266", "mrqa_squad-validation-3130", "mrqa_squad-validation-4528", "mrqa_squad-validation-2226", "mrqa_newsqa-validation-160", "mrqa_squad-validation-2289", "mrqa_searchqa-validation-3441", "mrqa_squad-validation-5824", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-12119", "mrqa_squad-validation-7527", "mrqa_searchqa-validation-16816", "mrqa_squad-validation-3165", "mrqa_searchqa-validation-12876"], "EFR": 1.0, "Overall": 0.865625}, {"timecode": 5, "before_eval_results": {"predictions": ["an ash leaf", "75,000 to 100,000", "1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The majority may be powerful but it is not necessarily right", "Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center", "one-eighth", "Video On Demand content", "extended structure", "principle of equivalence", "pump this into the mesoglea", "closed system", "21 to 11", "crustal rock", "formalize a unified front in trade and negotiations with various Indians", "two", "the public PAD service Telepad", "a separate condenser", "to the North Sea", "Cam Newton", "requiring his arrest", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element in the universe", "39", "a human", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the ball toward the wicket", "Donner", "Colonel (Tom) Parker", "New Netherland", "Monrovia", "the umpire is the person charged with officiating the game", "Taiwan", "Omaha", "(Giggly)", "Nez Perce", "George Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "Teri", "Inchon", "February 29", "(GMAIL.COM", "Alabama", "Bennington", "Giorgio Armani", "the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "District of Columbia National Guard"], "metric_results": {"EM": 0.5, "QA-F1": 0.6004521346708847}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5, 1.0, 0.07692307692307693, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.0, 0.9600000000000001, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-9640", "mrqa_squad-validation-2976", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-9320", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.5, "CSR": 0.6927083333333333, "retrieved_ids": ["mrqa_squad-train-51732", "mrqa_squad-train-75132", "mrqa_squad-train-57721", "mrqa_squad-train-39877", "mrqa_squad-train-58766", "mrqa_squad-train-32005", "mrqa_squad-train-73198", "mrqa_squad-train-69293", "mrqa_squad-train-18518", "mrqa_squad-train-47445", "mrqa_squad-train-17483", "mrqa_squad-train-26383", "mrqa_squad-train-38833", "mrqa_squad-train-45776", "mrqa_squad-train-6220", "mrqa_squad-train-68772", "mrqa_searchqa-validation-11770", "mrqa_squad-validation-5824", "mrqa_hotpotqa-validation-1297", "mrqa_squad-validation-4462", "mrqa_squad-validation-7574", "mrqa_naturalquestions-validation-1549", "mrqa_squad-validation-1891", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-2983", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14398", "mrqa_squad-validation-8597", "mrqa_searchqa-validation-5762", "mrqa_triviaqa-validation-6334", "mrqa_squad-validation-9918", "mrqa_squad-validation-7307"], "EFR": 1.0, "Overall": 0.8463541666666666}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views", "Bible", "water pump", "86.66%", "Gender pay gap in favor of males in the labor market", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger NFL", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "British and Europeans", "Judith Merril", "the connection id in a table", "Von Miller", "weekly screenings of all available classic episodes", "type III secretion system", "10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "when the oxygen concentration is too high", "to punish Christians by God", "the global village", "Sun City", "Freeport", "a dolphin", "David Bowie", "Liberty Island", "next of kin", "Matt Lauer", "Lenin", "Abilene", "Amtrak", "the Pioneer Log House", "the Pianist", "Patty Duke", "the king", "a Mackintosh", "Richard Cory", "Homer J. Simpson", "South Africa", "the greyhound", "Beany and Cecil", "the mountains of eastern Nevada", "Trenton", "kupfernickel", "different philosophers and statesmen have designed different lists of what they believe to be natural rights", "flamenco", "Margarita", "prostate cancer", "DNA's structure", "the eastern Pyrenees mountains"], "metric_results": {"EM": 0.625, "QA-F1": 0.7133837197159566}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2105263157894737, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-7473", "mrqa_squad-validation-7449", "mrqa_squad-validation-9334", "mrqa_squad-validation-87", "mrqa_squad-validation-5589", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-6372", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-7474"], "SR": 0.625, "CSR": 0.6830357142857143, "retrieved_ids": ["mrqa_squad-train-62284", "mrqa_squad-train-51350", "mrqa_squad-train-31947", "mrqa_squad-train-75321", "mrqa_squad-train-60008", "mrqa_squad-train-44139", "mrqa_squad-train-40042", "mrqa_squad-train-13788", "mrqa_squad-train-19272", "mrqa_squad-train-80704", "mrqa_squad-train-68705", "mrqa_squad-train-48897", "mrqa_squad-train-73049", "mrqa_squad-train-49506", "mrqa_squad-train-85076", "mrqa_squad-train-63096", "mrqa_searchqa-validation-15847", "mrqa_squad-validation-8597", "mrqa_squad-validation-8551", "mrqa_searchqa-validation-10318", "mrqa_triviaqa-validation-6334", "mrqa_squad-validation-9533", "mrqa_searchqa-validation-2175", "mrqa_squad-validation-973", "mrqa_searchqa-validation-16653", "mrqa_squad-validation-2226", "mrqa_squad-validation-3699", "mrqa_squad-validation-6809", "mrqa_searchqa-validation-11770", "mrqa_squad-validation-3119", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-10274"], "EFR": 1.0, "Overall": 0.8415178571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "1994 Works Council Directive", "the Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "the Pittsburgh Steelers", "McManus", "Gemini", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-rays", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and Semuren", "the highest penalty that can be inflicted upon me", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "the holy catholic (or universal) church", "competition between workers", "1516", "decrease in wages", "Prudhoe Bay", "alexandrite", "cigar", "William Godwin", "Lucy Hayes", "ribonucleic acid", "a Grapes of Wrath", "Eight Is Enough", "Tel Aviv", "Humphrey Bogart", "The Name of the Rose", "Thomas Paine", "a dzawada'enuxw", "the Silver Surfer", "G4", "H.L. Mencken", "Julius Caesar", "malaria", "a blonde", "Hairspray", "Johann Wolfgang von Goethe", "a black mask", "Phi Beta Kappa", "a rainplane", "Sherman Antitrust Act", "Hafnium", "a sheriff's deputy and co-worker of Dan's   Thomas Lennon as Novak", "Harold Bierman, Jr.", "Winnie the Pooh", "Ryder Russell", "economic opportunities", "Officer Joe Harn", "he to step down as majority leader"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6971624440374441}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-1407", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-86", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.65625, "CSR": 0.6796875, "retrieved_ids": ["mrqa_squad-train-23104", "mrqa_squad-train-15749", "mrqa_squad-train-26080", "mrqa_squad-train-5435", "mrqa_squad-train-33296", "mrqa_squad-train-17358", "mrqa_squad-train-24190", "mrqa_squad-train-32691", "mrqa_squad-train-31123", "mrqa_squad-train-47377", "mrqa_squad-train-48960", "mrqa_squad-train-38368", "mrqa_squad-train-64290", "mrqa_squad-train-19982", "mrqa_squad-train-52385", "mrqa_squad-train-8669", "mrqa_searchqa-validation-13077", "mrqa_squad-validation-5835", "mrqa_squad-validation-4452", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-4266", "mrqa_squad-validation-8551", "mrqa_searchqa-validation-9109", "mrqa_squad-validation-2372", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-11770", "mrqa_squad-validation-3543", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-16653", "mrqa_naturalquestions-validation-9273", "mrqa_searchqa-validation-697"], "EFR": 1.0, "Overall": 0.83984375}, {"timecode": 8, "before_eval_results": {"predictions": ["the 1970s", "Madison Square Garden", "Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "its safaris", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped", "his friendship", "Kevin Harlan", "30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat", "a virus", "a little blue engine", "the NanoFrazor", "tango", "Burnet, Texas", "bamboos", "Nevil Shute", "Claudius", "Vlad Tepes", "a thorn hedges", "ginseng", "coffee", "Depeche Mode", "Gatorade", "Deep brain stimulation", "Vanna White", "a hippo", "the West", "the Madding Crowd", "(M Mikhail) Baryshchev.", "Mars", "the Boston Massacre", "a bee", "a Hardmode gun", "Venice", "May", "Mrs. Calabash", "Carl Sagan", "in February 2011, while overseas, she discovered that she was pregnant.", "General Paulus", "John Ford", "Cirque du Soleil", "a donor molecule", "Sylvester Stallone", "The Mongol - led Yuan dynasty"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6370535714285714}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.16666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6402", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6321"], "SR": 0.578125, "CSR": 0.6684027777777778, "retrieved_ids": ["mrqa_squad-train-71175", "mrqa_squad-train-14835", "mrqa_squad-train-19973", "mrqa_squad-train-85493", "mrqa_squad-train-67013", "mrqa_squad-train-4838", "mrqa_squad-train-60241", "mrqa_squad-train-65150", "mrqa_squad-train-11279", "mrqa_squad-train-58213", "mrqa_squad-train-79659", "mrqa_squad-train-76294", "mrqa_squad-train-16152", "mrqa_squad-train-59433", "mrqa_squad-train-29465", "mrqa_squad-train-2448", "mrqa_searchqa-validation-4266", "mrqa_squad-validation-3130", "mrqa_newsqa-validation-1289", "mrqa_searchqa-validation-9109", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-14148", "mrqa_squad-validation-7473", "mrqa_squad-validation-3699", "mrqa_squad-validation-9320", "mrqa_squad-validation-1441", "mrqa_squad-validation-6791", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-4674"], "EFR": 0.9629629629629629, "Overall": 0.8156828703703703}, {"timecode": 9, "before_eval_results": {"predictions": ["Metropolitan Police Authority", "Francis Marion", "parallel importers", "two", "the Tangut relief army", "five", "governmental", "the Great Yuan", "Mario Addison", "immune system adapts its response during an infection to improve its recognition of the pathogen", "70", "movements of nature", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "5,500,000", "an adjustable spring-loaded valve", "classical position variables", "The Left Hand of Darkness", "an alleged robbery", "George Jetson", "deus ex machina", "an arboretum", "pommel horse", "William McKinley", "PSP", "Daphne du Maurier", "Turkey", "a quip", "a saguaro cactuses", "Daughters of the American Revolution", "Morrie Schwartz", "an all-fired all-important", "Mercury and Venus", "Tokyo", "an entry-level restaurant job", "a gorillas", "the Pentagon Building", "oats", "4", "Iran", "Gone With the Wind", "A Delicate Balance", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "a pancreas", "in the mid-1990s", "the Hudson Bay", "NLP", "Melpomen\u0113", "Boston Bruins", "James Lofton", "a digital converter box", "raping and murdering a woman in Missouri"], "metric_results": {"EM": 0.5, "QA-F1": 0.5859375}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-1640", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_naturalquestions-validation-4124", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-5338", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.5, "CSR": 0.6515625, "retrieved_ids": ["mrqa_squad-train-25369", "mrqa_squad-train-17836", "mrqa_squad-train-4796", "mrqa_squad-train-68121", "mrqa_squad-train-38258", "mrqa_squad-train-20857", "mrqa_squad-train-63930", "mrqa_squad-train-62066", "mrqa_squad-train-38805", "mrqa_squad-train-8881", "mrqa_squad-train-76927", "mrqa_squad-train-38240", "mrqa_squad-train-79698", "mrqa_squad-train-24125", "mrqa_squad-train-32970", "mrqa_squad-train-48024", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-8582", "mrqa_squad-validation-3559", "mrqa_searchqa-validation-5539", "mrqa_squad-validation-3699", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-5329", "mrqa_squad-validation-2226", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-2963", "mrqa_squad-validation-9489", "mrqa_squad-validation-4662", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-4266"], "EFR": 1.0, "Overall": 0.82578125}, {"timecode": 10, "UKR": 0.798828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.908203125, "KG": 0.475, "before_eval_results": {"predictions": ["Mike Figgis", "1.7 billion years ago", "Waal", "technical problems and flight delays", "the fact (Fermat's little theorem)", "Virgin Media", "killed through overwork", "Times Square Studios", "Philip Webb and William Morris", "service to the neighbor", "Amtrak San Joaquins", "refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "formal", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "a third group of pigments found in cyanobacteria", "a photoelectric sensor", "Peggy", "the mycelium", "Memoirs of a Geisha", "stability control", "a pistol", "the plague", "iron", "Rhett Akins", "the Cenozoic", "the Maghreb", "Reddi-wip", "Jeopardy", "a brew", "Larry Fortensky", "the oxygen", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time & 1936", "the Jeffersons", "the Sopranos", "The Crucible", "Liston", "Impressionists", "Willa Cather", "Aida", "Walden", "Bergerac", "the rights to Religious Freedom", "(intr) to take rest or recreation", "zero", "Australian & New Zealand", "Maine", "Doug Diemoz", "the sink rim", "Hal Ashby", "John Ford", "119", "CR-X", "a skilled hacker", "Frank Ricci"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6979166666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9023", "mrqa_squad-validation-8839", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-2651", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400"], "SR": 0.609375, "CSR": 0.6477272727272727, "retrieved_ids": ["mrqa_squad-train-28993", "mrqa_squad-train-19750", "mrqa_squad-train-52987", "mrqa_squad-train-29614", "mrqa_squad-train-78161", "mrqa_squad-train-11860", "mrqa_squad-train-84467", "mrqa_squad-train-42091", "mrqa_squad-train-19864", "mrqa_squad-train-54503", "mrqa_squad-train-47261", "mrqa_squad-train-19852", "mrqa_squad-train-41072", "mrqa_squad-train-21488", "mrqa_squad-train-85412", "mrqa_squad-train-15987", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-10624", "mrqa_triviaqa-validation-2735", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-14148", "mrqa_squad-validation-7574", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-11495", "mrqa_squad-validation-1941", "mrqa_searchqa-validation-2617", "mrqa_newsqa-validation-491", "mrqa_squad-validation-6791", "mrqa_squad-validation-2976"], "EFR": 1.0, "Overall": 0.7659517045454545}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "allowing the lander spacecraft to be used as a \"lifeboat\"", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "97", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000", "oppidum Ubiorum", "The entrance to studio 5 at the City Road complex", "1.7 million", "August 4, 2000", "an al Qaeda leader", "free", "Bob Dole", "1959", "the activist hacking group", "three", "137", "\"How the Grinch Stole Christmas!\"", "Opryland", "Asashoryu", "Conway", "How I Met Your Mother", "three", "the insurgency", "Arizona", "he would actively engage Arab media.", "the war funding provides nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "Hearst Castle", "some rumors would insinuate that \"Sex and the City\"", "Rev. Alberto Cutie", "Aeneh Bahrami", "military trials for some Guant Bay detainees.", "opium", "President Obama's race in 2008.", "named his company Polo", "Egypt", "Arabic, French and English", "a minor league baseball team", "seven", "Honduran President Jose Manuel Zelaya", "Abu Sayyaf", "two soldiers and two civilians", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "Mike Meehan", "in Yemen's Sufi monasteries", "1966", "J. S. Bach", "Brainy", "Fitzroya", "Janet Evanovich", "Sweeney Todd", "Andorra", "Uncle Tom's Cabin"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5552803896553896}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.06060606060606061, 0.0, 0.4166666666666667, 1.0, 0.0, 0.10256410256410256, 1.0, 0.10810810810810811, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2666666666666667, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-3070"], "SR": 0.515625, "CSR": 0.63671875, "retrieved_ids": ["mrqa_squad-train-8895", "mrqa_squad-train-25466", "mrqa_squad-train-28403", "mrqa_squad-train-29561", "mrqa_squad-train-86019", "mrqa_squad-train-25831", "mrqa_squad-train-11427", "mrqa_squad-train-9689", "mrqa_squad-train-51933", "mrqa_squad-train-36908", "mrqa_squad-train-68271", "mrqa_squad-train-4785", "mrqa_squad-train-48819", "mrqa_squad-train-55026", "mrqa_squad-train-22251", "mrqa_squad-train-78218", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_squad-validation-6759", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-3127", "mrqa_squad-validation-3559", "mrqa_squad-validation-7449", "mrqa_squad-validation-3119", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-11888", "mrqa_triviaqa-validation-5338", "mrqa_searchqa-validation-16848", "mrqa_squad-validation-9489", "mrqa_newsqa-validation-1425", "mrqa_squad-validation-1766"], "EFR": 1.0, "Overall": 0.76375}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "the main porch", "Warren Buffett", "3.55 inches", "Doctor Who", "Prime ideals", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax in Valencia", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years", "between Pyongyang and Seoul", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi,", "Suwardi", "Maj. Nidal Malik Hasan, MD, a Muslim American military psychiatrist at Fort Hood", "U.S. senators", "a cold shower in his home in New Zealand.", "Muslim", "California, Texas and Florida", "Robert De Niro", "Argentina", "Three searches", "creation of an Islamic emirate in Gaza", "near Garacad, Somalia", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "Azzam the American,", "he was abusive and uncalled for. And coercive. Clearly coercive. It was that medical impact that pushed me over the edge\" to call it torture,", "Apple employees", "a German scout who proudly wears a Stetson hat and spurs on his boots,", "Haiti", "Buster Keaton", "test-launched a rocket capable of carrying a satellite", "Nieb\u00fcll", "Juan Martin Del Potro.", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state,", "Seoul", "John Wayne.", "Pakistan", "seven", "the journalists' Swedish attorney.", "Fix You", "Bill McPherson", "Ytterby", "George III", "Philadelphia", "Alien Resurrection", "Fester", "Moscow", "a dressage"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6395209779255833}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.3157894736842105, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5657", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_naturalquestions-validation-4193", "mrqa_searchqa-validation-266"], "SR": 0.578125, "CSR": 0.6322115384615384, "retrieved_ids": ["mrqa_squad-train-28460", "mrqa_squad-train-81339", "mrqa_squad-train-2646", "mrqa_squad-train-30730", "mrqa_squad-train-75498", "mrqa_squad-train-60039", "mrqa_squad-train-51472", "mrqa_squad-train-28598", "mrqa_squad-train-68278", "mrqa_squad-train-19860", "mrqa_squad-train-59574", "mrqa_squad-train-72311", "mrqa_squad-train-64541", "mrqa_squad-train-14301", "mrqa_squad-train-64681", "mrqa_squad-train-32792", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-2863", "mrqa_squad-validation-3165", "mrqa_searchqa-validation-3497", "mrqa_squad-validation-2976", "mrqa_naturalquestions-validation-7203", "mrqa_searchqa-validation-5298", "mrqa_squad-validation-7473", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-1425", "mrqa_triviaqa-validation-7474", "mrqa_squad-validation-4999", "mrqa_squad-validation-3699", "mrqa_squad-validation-6757", "mrqa_triviaqa-validation-862", "mrqa_squad-validation-10506"], "EFR": 1.0, "Overall": 0.7628485576923076}, {"timecode": 13, "before_eval_results": {"predictions": ["1872\u20131967", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "forgiveness", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "the City of Edinburgh Council", "Osama", "Israel", "Hearst Castle", "CNN's \"Larry King Live.\"", "Al Gore", "the shoreline of the city of Quebradillas", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane", "Adam Yahiye Gadahn,", "the iPods", "in the southern port city of Karachi,", "John McCain", "South Africa", "2006", "Iran's nuclear program.", "North Korea", "December 1", "\"This is not something that anybody can reasonably anticipate,\"", "Haeftling", "i report form", "Kurt Cobain", "Nkepile M abuse", "to get this child home safe and sound.", "San Diego", "Ralph Lauren", "At least 40", "$1,500", "25", "137", "suppress the memories and to live as normal a life as possible", "Copts", "poor", "Tom Hanks", "The Louvre", "27-year-old", "165", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Tyler, Ali, and Lydia", "Kansas", "September", "rhythm, structure, and musical expression", "Rebekah Hinds", "Lusitania", "the Earth", "Casualty", "Turkic"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5827809343434344}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9333333333333333, 0.6666666666666666, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.16666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9886", "mrqa_squad-validation-2009", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-1028", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2251"], "SR": 0.46875, "CSR": 0.6205357142857143, "retrieved_ids": ["mrqa_squad-train-7757", "mrqa_squad-train-303", "mrqa_squad-train-61111", "mrqa_squad-train-30486", "mrqa_squad-train-18621", "mrqa_squad-train-59421", "mrqa_squad-train-72195", "mrqa_squad-train-38525", "mrqa_squad-train-52534", "mrqa_squad-train-74112", "mrqa_squad-train-84523", "mrqa_squad-train-7975", "mrqa_squad-train-48399", "mrqa_squad-train-56049", "mrqa_squad-train-53927", "mrqa_squad-train-44372", "mrqa_squad-validation-7719", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-12117", "mrqa_newsqa-validation-818", "mrqa_searchqa-validation-8368", "mrqa_squad-validation-3118", "mrqa_squad-validation-1849", "mrqa_searchqa-validation-11139", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1011", "mrqa_searchqa-validation-86", "mrqa_squad-validation-1941", "mrqa_squad-validation-1891"], "EFR": 1.0, "Overall": 0.7605133928571429}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart", "between September and November 1946", "$2.50 per AC horsepower royalty", "1990s", "organic solvents", "Stagg Field", "2010", "Reuben Townroe", "the 'Great Pestilence'", "a water pump", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "southern Bhola district", "88 people had been hurt, 28 of them seriously enough to go to a hospital,", "bankruptcies", "Inter Milan", "98", "the European Alps", "race or its understanding of what the law required it to do.", "The Ski Train", "severe", "in their Naples home.", "top designers, such as Stella McCartney", "Col. Elspeth Cameron-Ritchie,", "homicide", "the \"surge\" strategy he implemented last year.", "shut down, and desperately needed aid cannot be unloaded quickly.", "onstage demos.", "Tim O'Connor,", "impeachment charges", "Kearny, New Jersey", "Thessaloniki and Athens,", "in Harare,", "a keystroke", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces by Sunday,", "the genocide", "genocide, crimes against humanity, and war crimes.", "bikinis", "in Fullerton, California,", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"Don't Ask, Don't tell\" policy", "Consumer Reports", "two women", "Sheikh Abu al-Nour al-Maqdessi,", "the remaining rebel strongholds in the north of Sri Lanka,", "Florida's Everglades.", "six-year veteran", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "ninth w\u0101", "Magnavox Odyssey", "William Tell", "the robin", "Kent Hovind", "The Guest", "\"Time of Your Life\"", "a skull", "2020 National Football League ( NFL ) season", "6 January 793"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5286773122710622}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.5, 0.8, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-4863"], "SR": 0.453125, "CSR": 0.609375, "retrieved_ids": ["mrqa_squad-train-9777", "mrqa_squad-train-59339", "mrqa_squad-train-40922", "mrqa_squad-train-7601", "mrqa_squad-train-7594", "mrqa_squad-train-2657", "mrqa_squad-train-43404", "mrqa_squad-train-48551", "mrqa_squad-train-14729", "mrqa_squad-train-34377", "mrqa_squad-train-39005", "mrqa_squad-train-53534", "mrqa_squad-train-33032", "mrqa_squad-train-54121", "mrqa_squad-train-75119", "mrqa_squad-train-62299", "mrqa_squad-validation-9886", "mrqa_squad-validation-6809", "mrqa_searchqa-validation-10297", "mrqa_squad-validation-1891", "mrqa_searchqa-validation-16848", "mrqa_naturalquestions-validation-5297", "mrqa_searchqa-validation-5539", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-983", "mrqa_squad-validation-6614", "mrqa_newsqa-validation-2629", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-2338", "mrqa_newsqa-validation-3817"], "EFR": 1.0, "Overall": 0.75828125}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "late 1870s", "Death wish Coffee", "the quality of a country's institutions and high levels of education", "proportionally", "North", "Mohammed Ali al-Moayad and Mohammed Mohsen Zayed", "\"still trying to absorb the impact of this week's stunning events,\"", "Lisa Polyak,", "Friday,", "CNN affiliate WFTV.", "The cause of the deaths has not been determined,", "the station.", "sculptures", "Atlantic Ocean.", "725-mile Veracruz", "200", "Greek site of Olympia", "Patrick McGoohan", "Michael Partain,", "$627", "27-year-old's", "Virgin America", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "gossip Girl", "Ketchum, Idaho.", "laundromats", "Sporting Lisbon", "tie salesman", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Tiger Woods", "overturned", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Hillary Clinton,", "smashing particles into each other by sending two beams of protons around the tunnel in opposite directions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars.", "two", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "1.2 million", "club managers,", "give more resistance and we will be with you in the field,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "her mother", "pigs", "Matt Flinders", "Isar River", "the Hamiltons", "Sam Bettley", "33", "the Sea of Galilee", "honey", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6784629186602871}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.9333333333333333, 0.9090909090909091, 0.0, 1.0, 0.5, 0.7272727272727273, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.08, 0.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473685, 1.0, 1.0, 0.1142857142857143, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-1945", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-5573"], "SR": 0.5625, "CSR": 0.6064453125, "retrieved_ids": ["mrqa_squad-train-59460", "mrqa_squad-train-38553", "mrqa_squad-train-35838", "mrqa_squad-train-21869", "mrqa_squad-train-45503", "mrqa_squad-train-64886", "mrqa_squad-train-15903", "mrqa_squad-train-69729", "mrqa_squad-train-70773", "mrqa_squad-train-8843", "mrqa_squad-train-24815", "mrqa_squad-train-828", "mrqa_squad-train-38713", "mrqa_squad-train-13299", "mrqa_squad-train-3970", "mrqa_squad-train-24384", "mrqa_triviaqa-validation-6277", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-3660", "mrqa_squad-validation-1456", "mrqa_squad-validation-3119", "mrqa_triviaqa-validation-5492", "mrqa_searchqa-validation-12119", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-502", "mrqa_searchqa-validation-3497", "mrqa_triviaqa-validation-6939", "mrqa_searchqa-validation-2214", "mrqa_squad-validation-490", "mrqa_newsqa-validation-1271", "mrqa_squad-validation-6402", "mrqa_newsqa-validation-407"], "EFR": 1.0, "Overall": 0.7576953125}, {"timecode": 16, "before_eval_results": {"predictions": ["np\u2261n (mod p)", "an adjustable spring-loaded valve", "Grumman", "Synthetic aperture", "A fundamental error", "recant his writings", "diversity", "one can include arbitrarily many instances of 1 in any factorization", "136", "unions", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "her family is \"not defined by religion,\"", "fatally shooting a limo driver", "2nd Lt. Holley Wimunc.", "1918-1919.", "Ben Kingsley", "U.S. Holocaust Memorial Museum,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan", "William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "the first American team to win yachting's most prestigious trophy since 1992.", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "Ninety-two percent", "Larry Ellison,", "Taher Nunu", "President Obama", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "Daniel Nestor,", "Caylee Anthony,", "because its facilities are full.", "25 dead", "more than 200.", "that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "they recently killed eight Indians whom the rebels accused of collaborating with the Colombian government,", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South African President Thabo Mbeki announced his resignation in a televised address Sunday.", "in Seoul,", "Haiti", "The United States", "he didn't know if Woods' wife, Elin Nordegren, would appear with her husband.", "Daytime Emmy Lifetime Achievement Award", "Democrat", "\" Teen Patti\"", "Eleven people died and 36 were wounded in the Monday terror attack,", "Hugo Chavez", "Four bodies", "attached to another chromosome", "starch", "the United Kingdom of Great Britain and Northern Ireland", "Diptera", "100th anniversary of the first \"Tour de France\"", "British acid techno and drum and bass electronic musician", "fibrous", "Johannes Brahms", "the 17th century", "Orson Welles"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6648984338782133}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.6153846153846153, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09090909090909091, 0.47058823529411764, 1.0, 0.14285714285714285, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.6666666666666666, 0.16666666666666669, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.25, 0.7142857142857143, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2788", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_searchqa-validation-2260", "mrqa_hotpotqa-validation-4478"], "SR": 0.578125, "CSR": 0.6047794117647058, "retrieved_ids": ["mrqa_squad-train-43626", "mrqa_squad-train-80756", "mrqa_squad-train-54275", "mrqa_squad-train-79072", "mrqa_squad-train-54546", "mrqa_squad-train-35923", "mrqa_squad-train-72924", "mrqa_squad-train-17853", "mrqa_squad-train-25238", "mrqa_squad-train-9754", "mrqa_squad-train-61772", "mrqa_squad-train-52652", "mrqa_squad-train-72775", "mrqa_squad-train-11060", "mrqa_squad-train-75791", "mrqa_squad-train-35040", "mrqa_squad-validation-10011", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-3319", "mrqa_searchqa-validation-5128", "mrqa_squad-validation-5589", "mrqa_squad-validation-8294", "mrqa_searchqa-validation-3369", "mrqa_triviaqa-validation-6939", "mrqa_newsqa-validation-4011", "mrqa_hotpotqa-validation-4463", "mrqa_naturalquestions-validation-9660", "mrqa_searchqa-validation-6372", "mrqa_squad-validation-8400", "mrqa_newsqa-validation-1101", "mrqa_squad-validation-1500", "mrqa_newsqa-validation-469"], "EFR": 1.0, "Overall": 0.7573621323529411}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "multi-cultural", "the father of the house when in his home", "John Fox", "US$1,000,000", "United Methodist Church", "Colonel Monckton", "thermodynamic", "CNN Moscow Correspondent", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was in good health,", "Saturn owners", "iTunes,", "Seoul", "northwestern Montana", "Iran's President Mahmoud Ahmadinejad", "South Africa", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "Sunday", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Republican", "\"Dr. No\"", "2006", "the FBI.", "250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "allegations that a dorm parent mistreated students at the school.", "Pakistan", "Columbia, Illinois,", "\"I'm just getting started.\"", "his Japanese grandmother", "flooding and debris", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "Alfredo Astiz", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford", "Ginger Rogers", "five", "Marine Corps", "Garfield", "Cutpurse", "seven", "Ash", "a transistor"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6677579365079365}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5714285714285715, 0.0, 0.5, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_squad-validation-10073", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_triviaqa-validation-5425"], "SR": 0.578125, "CSR": 0.6032986111111112, "retrieved_ids": ["mrqa_squad-train-71254", "mrqa_squad-train-31582", "mrqa_squad-train-16138", "mrqa_squad-train-30776", "mrqa_squad-train-39526", "mrqa_squad-train-83577", "mrqa_squad-train-34863", "mrqa_squad-train-6765", "mrqa_squad-train-38158", "mrqa_squad-train-32579", "mrqa_squad-train-68920", "mrqa_squad-train-59202", "mrqa_squad-train-3072", "mrqa_squad-train-32401", "mrqa_squad-train-39226", "mrqa_squad-train-21018", "mrqa_searchqa-validation-12750", "mrqa_newsqa-validation-117", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-11888", "mrqa_newsqa-validation-2608", "mrqa_squad-validation-9533", "mrqa_searchqa-validation-2394", "mrqa_hotpotqa-validation-939", "mrqa_naturalquestions-validation-7203", "mrqa_searchqa-validation-5539", "mrqa_squad-validation-4452", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2068", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-4197"], "EFR": 1.0, "Overall": 0.7570659722222223}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline", "specialty drugs", "Doctor of Theology", "God", "The Prince of P\u0142ock", "multi-stage centrifugal pumps", "Pet Sounds", "40", "Sax Rohmer", "Aug 24,", "algebra", "a real whale", "\u00ef\u00bf\u00bdNastase", "Jezebel", "archer", "General Paulus", "Anne Boleyn", "Golda Meir", "a round, slightly tapered, icebergless fur hat", "Alan Greenspan", "Thai", "a Lion", "Japan", "Runic", "plutonium", "Andy Murray", "blancmange", "baloney", "fraxage", "recorder", "the heptathlon", "Microsoft", "Austria", "Brunel", "Edward Lear", "Jamaica", "Francis Ford", "Petronas", "Beyonce", "Microsoft", "Charlemagne", "Praseodymium", "The Battle of the Three Emperors", "southern Pacific Ocean,", "Trimdon,", "Midnight Cowboy", "Dada", "FIFA World Cup 2010", "Southwest Airlines", "Afghanistan", "Thomas Middleditch", "Rudolf H\u00f6ss", "3 May 1958", "Ewan McGregor", "off Somalia's coast.", "cannibalism", "Braun", "Ford Motor Company", "Banff", "a calves"], "metric_results": {"EM": 0.59375, "QA-F1": 0.670610119047619}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.59375, "CSR": 0.602796052631579, "retrieved_ids": ["mrqa_squad-train-55074", "mrqa_squad-train-2483", "mrqa_squad-train-9904", "mrqa_squad-train-26731", "mrqa_squad-train-31430", "mrqa_squad-train-15382", "mrqa_squad-train-38111", "mrqa_squad-train-44146", "mrqa_squad-train-50203", "mrqa_squad-train-29395", "mrqa_squad-train-9840", "mrqa_squad-train-48630", "mrqa_squad-train-69237", "mrqa_squad-train-54079", "mrqa_squad-train-53765", "mrqa_squad-train-58487", "mrqa_squad-validation-7527", "mrqa_squad-validation-9640", "mrqa_squad-validation-87", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3527", "mrqa_squad-validation-6680", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-5679", "mrqa_squad-validation-8551", "mrqa_searchqa-validation-11495", "mrqa_squad-validation-9145", "mrqa_squad-validation-3130", "mrqa_newsqa-validation-3370", "mrqa_squad-validation-2372", "mrqa_searchqa-validation-3478", "mrqa_newsqa-validation-150"], "EFR": 0.8846153846153846, "Overall": 0.7338885374493928}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Sky Q Silver set top boxes with a Wi-Fi or Power-line connection", "\"ash tree\"", "24 September 2007", "2007", "34\u201319", "1991", "Canada", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands,", "Tony Blair", "The Flintstones", "9-1-1", "Jonathan Swift", "South Sudan", "Maria Esther Andion Bueno", "dill", "Frankie Laine", "July 28, 1948", "Thor", "Austria", "Goosnargh", "a bear", "dna structure", "Montr\u00e9al", "ruda", "\"Maljanne\"", "Rocky and Bullwinkle", "Ray Winstone", "Lackawanna Six", "Poland", "Indiana Jones", "sousa", "duke of Wellington", "Sydney", "Alabama", "Jura", "a caterpillar tractor", "his finger", "a meteoroid", "Lew Hoad", "il' Escargot", "lola", "bodhidharma", "Klaus Barbie", "Albert Reynolds", "a rope", "Baltic Sea port", "Singapore", "cathead", "yellow", "cat food", "Vespa", "Squamish, British Columbia, Canada", "65", "Sim Theme Park", "Cape Cod", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "10 percent", "867-5309", "dill", "a medium", "the small intestine", "Prince Siddhartha"], "metric_results": {"EM": 0.5, "QA-F1": 0.5761335784313726}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.7058823529411764, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-4634", "mrqa_squad-validation-8598", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-556", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-3139"], "SR": 0.5, "CSR": 0.59765625, "retrieved_ids": ["mrqa_squad-train-49323", "mrqa_squad-train-44714", "mrqa_squad-train-40393", "mrqa_squad-train-46897", "mrqa_squad-train-11796", "mrqa_squad-train-22572", "mrqa_squad-train-45424", "mrqa_squad-train-60155", "mrqa_squad-train-16615", "mrqa_squad-train-79242", "mrqa_squad-train-25431", "mrqa_squad-train-18676", "mrqa_squad-train-30481", "mrqa_squad-train-77841", "mrqa_squad-train-51295", "mrqa_squad-train-47995", "mrqa_newsqa-validation-3172", "mrqa_searchqa-validation-4888", "mrqa_squad-validation-10274", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2739", "mrqa_triviaqa-validation-4363", "mrqa_searchqa-validation-13657", "mrqa_newsqa-validation-2785", "mrqa_searchqa-validation-2252", "mrqa_squad-validation-6185", "mrqa_squad-validation-664", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-1104", "mrqa_squad-validation-5835", "mrqa_newsqa-validation-2872", "mrqa_searchqa-validation-3102"], "EFR": 0.96875, "Overall": 0.7496875000000001}, {"timecode": 20, "UKR": 0.818359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.892578125, "KG": 0.42890625, "before_eval_results": {"predictions": ["red algal derived", "pathogens", "1525\u201332", "Only a few", "solution", "2011", "random noise", "Wardenclyffe", "dennis verne", "Ogaden", "Washington Post", "baltic", "Steve Biko", "pottery", "a4202", "acute", "dravidian", "dum vinegar", "McCartney", "dennis humbert", "Oliver!", "Lone Ranger", "Bolton", "humbert", "tsarevitch", "dennis humbert", "dia", "Hartford", "your Excellency", "George IV", "Lincoln", "River Severn", "Canada", "dif-tor heh smusma", "preston", "Stalin", "Jesse Garon Presley", "Kopassus", "lithium", "40", "The Duchess", "Nick Owen", "white", "China", "Salt Lake City,", "Perseus", "Capricorn", "rugby", "sergio Garc\u00eda Fern\u00e1ndez", "a butterfly", "Jason Alexander", "The Savoy", "Steve Jobs", "a habitat", "2 %", "729", "Twitch Interactive", "right-wing extremist groups.", "Rocky Ford brand cantaloupes", "humbert", "a rhinoceros", "Wes Craven", "Australian", "$10,000 Kelly"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5864583333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205"], "SR": 0.53125, "CSR": 0.5944940476190477, "retrieved_ids": ["mrqa_squad-train-77574", "mrqa_squad-train-24834", "mrqa_squad-train-50028", "mrqa_squad-train-60809", "mrqa_squad-train-75559", "mrqa_squad-train-40801", "mrqa_squad-train-84704", "mrqa_squad-train-63873", "mrqa_squad-train-33757", "mrqa_squad-train-73487", "mrqa_squad-train-15051", "mrqa_squad-train-34006", "mrqa_squad-train-65320", "mrqa_squad-train-35103", "mrqa_squad-train-35950", "mrqa_squad-train-63933", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2664", "mrqa_searchqa-validation-3441", "mrqa_naturalquestions-validation-3427", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-150", "mrqa_triviaqa-validation-237", "mrqa_newsqa-validation-3098", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-6638", "mrqa_triviaqa-validation-544", "mrqa_squad-validation-5589", "mrqa_searchqa-validation-5329", "mrqa_hotpotqa-validation-1297", "mrqa_triviaqa-validation-2431", "mrqa_newsqa-validation-1442"], "EFR": 1.0, "Overall": 0.7468675595238095}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois architecture", "confrontational", "San Francisco Bay Area", "gold", "Chinese", "Surrey", "Telstar", "wED", "Buzz Aldrin", "paul mccartney", "Niger", "Backgammon", "Instagram", "Home alone", "Columbus", "t.S. Eliot", "Venus", "Bob Marley & the Wailers", "Crusades", "topham Chase", "a curb-roof", "dennis", "danae", "piu forte", "Socrates", "selenium", "Stephen King", "chestnut", "Catskill Mountains", "a Yorkshire terriers", "a kilovolt-ampere", "Fluids", "Jordan", "dennis j. Cannell", "London Underground", "Husqvarna", "Poland", "EGBDF", "forehead", "dill", "eukharisti\u0101", "100 years", "sugar", "Washington, D.C.", "Piccadilly Circus", "tundra", "Melbourne, Victoria, Australia", "meadowbank thistle", "Tangled", "Vincent Motorcycle Company", "daffy Duck", "inner core", "novella", "The Prodigy", "jerry white", "Michelle Rounds", "21-year-old", "gertrude Stein", "Daytona Beach", "Brent", "Mickey's Twice Upon a Christmas", "hiphop"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5390625}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-927", "mrqa_squad-validation-170", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-13792", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-550"], "SR": 0.46875, "CSR": 0.5887784090909092, "retrieved_ids": ["mrqa_squad-train-29226", "mrqa_squad-train-15986", "mrqa_squad-train-37805", "mrqa_squad-train-52836", "mrqa_squad-train-83045", "mrqa_squad-train-31752", "mrqa_squad-train-33099", "mrqa_squad-train-83295", "mrqa_squad-train-17847", "mrqa_squad-train-77445", "mrqa_squad-train-46646", "mrqa_squad-train-53297", "mrqa_squad-train-64627", "mrqa_squad-train-68333", "mrqa_squad-train-1035", "mrqa_squad-train-9382", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-3015", "mrqa_triviaqa-validation-862", "mrqa_searchqa-validation-10017", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1104", "mrqa_triviaqa-validation-4760", "mrqa_newsqa-validation-1309", "mrqa_squad-validation-6185", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1636", "mrqa_triviaqa-validation-1993", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-2632", "mrqa_squad-validation-4861", "mrqa_triviaqa-validation-6684"], "EFR": 1.0, "Overall": 0.7457244318181818}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "he explored the mountains in hunter's garb", "63,517", "faith alone", "Ticonderoga Point", "a seal", "Season 4", "Tyrion", "the Third Republic", "Randy Goodrum", "October 1980", "james garner", "the Central and South regions", "Muguruza", "Missi Hale", "2018", "Malibu", "variation in plants", "Baltimore, Maryland", "the beginning of the American colonies", "Battle of Antietam", "104 colonists and Discovery", "left atrium and ventricle", "Mayflower", "1560s", "Davos", "Prince James", "New Orleans", "the Yarnell Fire", "U.S. service members who have died without their remains being identified", "March 16, 2018", "Narendra Modi", "Sohrai", "explosion", "the feeling I need to walk with", "Annette", "May 2017", "a rhinoceros", "ABC", "the cell nucleus", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome )", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "9.1 %", "the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "37.7", "Flag Day in 1954", "1922 to 1991", "nee ebert", "Popowo", "Ethiopia", "the Mountain West Conference", "Sydney", "Common", "look at how the universe formed by analyzing particle collisions.", "five female pastors", "Department of Homeland Security Secretary Janet Napolitano", "warren Eliot", "Antarctica", "cherry bombs"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5128716324801309}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.125, 1.0, 0.4, 0.0, 0.0, 0.8, 0.6451612903225806, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.08695652173913043, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1232", "mrqa_squad-validation-2919", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-4157", "mrqa_searchqa-validation-9687"], "SR": 0.421875, "CSR": 0.5815217391304348, "retrieved_ids": ["mrqa_squad-train-3580", "mrqa_squad-train-5572", "mrqa_squad-train-41227", "mrqa_squad-train-82572", "mrqa_squad-train-15587", "mrqa_squad-train-47306", "mrqa_squad-train-15625", "mrqa_squad-train-20545", "mrqa_squad-train-16982", "mrqa_squad-train-8911", "mrqa_squad-train-31847", "mrqa_squad-train-47572", "mrqa_squad-train-29561", "mrqa_squad-train-69042", "mrqa_squad-train-17538", "mrqa_squad-train-65019", "mrqa_newsqa-validation-3876", "mrqa_triviaqa-validation-2735", "mrqa_newsqa-validation-2042", "mrqa_searchqa-validation-6992", "mrqa_triviaqa-validation-5766", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-2651", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-644", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-2871", "mrqa_triviaqa-validation-3168", "mrqa_squad-validation-4662", "mrqa_squad-validation-3699", "mrqa_newsqa-validation-3035", "mrqa_squad-validation-9320"], "EFR": 1.0, "Overall": 0.7442730978260869}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside", "vicious and destructive", "60%", "girls", "Amsterdam Motor Show in April 1948", "picturebook Shiji no yukikai", "As of March 2017, Jimmy John's has almost 3,000 stores", "Chinese flower shop", "T'Pau", "Millerlite", "The Ranch is an American comedy web television series starring Ashton Kutcher, Danny Masterson, Debra Winger, Elisha Cuthbert, and Sam Elliott", "Universal Pictures and Focus Features", "LED illuminated display", "a line of committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "the retina", "IBM", "Felicity Huffman", "Djokovic", "84", "United States economy first went into an economic recession", "Welsh Borders and Shropshire area of the UK", "1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia", "Nalini Negi", "very important", "in the Southern United States", "Jodie Foster", "the head of state", "May 18, 2018", "10 May 1940", "Sally Field", "Queen M\u00e1xima of the Netherlands", "`` It Ain't Over'til It's Over ''", "Massillon, Ohio", "white rapper B - Rabbit", "giant planet", "RAF, Fighter Command", "10,000 BC", "New York City", "Egypt", "1961", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "total mortality", "Pakistan", "Sam Raimi", "7 October 1978", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "1819", "wiki", "gaffer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6224430950993451}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.4615384615384615, 0.0, 0.4, 0.0, 0.05555555555555555, 1.0, 0.2857142857142857, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.2, 0.8, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9047619047619047, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.515625, "CSR": 0.5787760416666667, "retrieved_ids": ["mrqa_squad-train-81881", "mrqa_squad-train-72798", "mrqa_squad-train-48057", "mrqa_squad-train-14073", "mrqa_squad-train-55853", "mrqa_squad-train-71051", "mrqa_squad-train-69070", "mrqa_squad-train-34694", "mrqa_squad-train-59031", "mrqa_squad-train-35134", "mrqa_squad-train-75719", "mrqa_squad-train-63354", "mrqa_squad-train-33523", "mrqa_squad-train-15199", "mrqa_squad-train-10759", "mrqa_squad-train-70040", "mrqa_searchqa-validation-697", "mrqa_naturalquestions-validation-519", "mrqa_searchqa-validation-14398", "mrqa_squad-validation-6809", "mrqa_triviaqa-validation-7233", "mrqa_searchqa-validation-11888", "mrqa_squad-validation-9334", "mrqa_searchqa-validation-10193", "mrqa_newsqa-validation-1271", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-3815", "mrqa_searchqa-validation-3267", "mrqa_squad-validation-8046", "mrqa_triviaqa-validation-3662", "mrqa_searchqa-validation-4266", "mrqa_squad-validation-3456"], "EFR": 0.9354838709677419, "Overall": 0.7308207325268816}, {"timecode": 24, "before_eval_results": {"predictions": ["22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors.", "German creedal hymn", "April 20", "Tanzania", "March 29, 2018", "European powers during the period of New Imperialism, between 1881 and 1914", "1928", "the ruling city of the Northern Kingdom of Israel, Samaria", "northern China", "Missouri River", "Harrys", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1946", "May 3, 2005", "David Hemmings", "Vijaya Mulay", "Mediterranean Shipping Company S.A. ( MSC )", "1977", "Cody Fern", "22 November 1970", "Reveille", "2007", "Camping World Stadium in Orlando, Florida", "Aldis Hodge", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "James", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "a single, very long DNA helix", "Tagalog or English", "American rock band R.E.M.", "differs in ingredients", "Juliet", "colonialism was coming to an end worldwide, France fashioned a semi-independent State of Vietnam, within the French Union", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "rocks and minerals", "various submucosal membrane sites of the body", "Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "its vast territory was divided into several successor polities", "in the North Cascades range of, Washington", "Mandy", "Kingsholm Stadium and Sandy Park", "Ahmad Givens ( Real ) and Kamal Given ( Chance )", "Beorn", "Robert Plant", "beetle", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Elbow River", "41,", "Fareed Zakaria", "Afghan National Security Forces at the site.", "a Pilgrim costume", "a Welch rabbit", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.546875, "QA-F1": 0.68188074381588}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.5, 1.0, 0.125, 1.0, 1.0, 0.0, 0.8571428571428571, 0.41379310344827586, 0.8571428571428571, 0.7741935483870968, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_hotpotqa-validation-3362", "mrqa_newsqa-validation-1795", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.546875, "CSR": 0.5775, "retrieved_ids": ["mrqa_squad-train-73716", "mrqa_squad-train-37044", "mrqa_squad-train-54998", "mrqa_squad-train-20495", "mrqa_squad-train-16285", "mrqa_squad-train-82717", "mrqa_squad-train-80773", "mrqa_squad-train-53123", "mrqa_squad-train-56555", "mrqa_squad-train-48263", "mrqa_squad-train-5171", "mrqa_squad-train-12997", "mrqa_squad-train-41843", "mrqa_squad-train-79283", "mrqa_squad-train-53690", "mrqa_squad-train-20512", "mrqa_searchqa-validation-3644", "mrqa_newsqa-validation-4178", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-544", "mrqa_squad-validation-5824", "mrqa_newsqa-validation-2042", "mrqa_squad-validation-3718", "mrqa_newsqa-validation-817", "mrqa_searchqa-validation-1747", "mrqa_newsqa-validation-373", "mrqa_searchqa-validation-14601", "mrqa_newsqa-validation-1041", "mrqa_naturalquestions-validation-4552", "mrqa_triviaqa-validation-7311", "mrqa_newsqa-validation-267", "mrqa_searchqa-validation-2214"], "EFR": 1.0, "Overall": 0.74346875}, {"timecode": 25, "before_eval_results": {"predictions": ["infinite number of primes", "8:10 p.m.", "6.4 nanometers", "1894", "un earned property income", "Atlanta, Georgia", "Thunder Road", "Acid rain", "Bette Midler", "gathering money from the public", "the pyloric valve", "Ella Kenion", "Julia Ormond", "anvil", "The Satavahanas", "March 16, 2018", "Hathi Jr", "by capillary action", "twice", "Asuka", "in the pachytene stage of prophase I of meiosis", "Hathi Jr", "the Lower Mainland in Vancouver", "electronic computers", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway", "Wisconsin", "neutrality", "2018", "1981", "USS Chesapeake", "arcade mode", "atransformation of the Greek \u03bc\u03b5\u03c4\u03ac\u03bd\u03bf\u03b9\u03b1", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Mad - Eye Moody", "Lee Mack", "to acquire an advantage without deviating from basic strategy", "Burnham Beeches in Buckinghamshire", "1898", "Clarence Anglin", "April 1st", "9.7 m ( 31.82 ft ) and 9 t ( 20,000 lb )", "the Northeast Monsoon", "Michael Crawford", "1930s", "Thomas Mundy Peterson", "She cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "the 17th episode in the third season of the television series How I Met Your Mother", "The Parlement de Bretagne", "Joe Davis", "phosphorus", "Spencer Perceval", "ancient herding dogs", "Chief of the Operations Staff of the Armed Forces High Command", "Jack Kilby", "Prince George's County police Cpl. Richard Findley,", "Venezuela", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Stark County", "Prince Edward Tudor", "New Orleans"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5599030084445138}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [0.25, 0.28571428571428575, 1.0, 1.0, 0.30769230769230765, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.3636363636363636, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.09090909090909093, 0.5555555555555556, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9019", "mrqa_squad-validation-1583", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-15996"], "SR": 0.4375, "CSR": 0.5721153846153846, "retrieved_ids": ["mrqa_squad-train-69160", "mrqa_squad-train-84172", "mrqa_squad-train-57721", "mrqa_squad-train-9329", "mrqa_squad-train-44115", "mrqa_squad-train-37816", "mrqa_squad-train-16033", "mrqa_squad-train-63855", "mrqa_squad-train-69117", "mrqa_squad-train-2621", "mrqa_squad-train-16264", "mrqa_squad-train-4279", "mrqa_squad-train-39456", "mrqa_squad-train-15111", "mrqa_squad-train-80347", "mrqa_squad-train-17573", "mrqa_squad-validation-9528", "mrqa_naturalquestions-validation-5928", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-162", "mrqa_searchqa-validation-15795", "mrqa_squad-validation-10274", "mrqa_squad-validation-4462", "mrqa_naturalquestions-validation-495", "mrqa_newsqa-validation-3151", "mrqa_searchqa-validation-5298", "mrqa_triviaqa-validation-5052", "mrqa_squad-validation-9533", "mrqa_newsqa-validation-1137"], "EFR": 0.9722222222222222, "Overall": 0.7368362713675214}, {"timecode": 26, "before_eval_results": {"predictions": ["deterministic Turing machine", "99", "already-wealthy individuals or entities", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "England", "virtual reality simulator", "the five - year time jump", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18 by Frankie Laine's `` I Believe '' in 1953", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions", "the outside world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "Ben Rosenbaum", "Zilphia Horton", "Richard Stallman, founder of the Free Software Foundation", "Santa Monica", "Afghanistan, Bangladesh, Bhutan, Maldives, Nepal, India, Pakistan", "December 15, 2017", "Ed Sheeran", "Johnson, a lifelong Democrat and the Republican majority in Congress", "the liver and kidneys", "the lumbar cistern, a subarachnoid space inferior to the conus medullaris", "tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the Indian Hockey Federation ( IHF ) had also been established and it sent a hockey team to the Summer Olympics", "Geoffrey Zakarian", "Tommy James and the Shondells", "Georgia, to an area east of Atlanta", "Bonnie Aarons", "March 31, 2018", "Jay Baruchel", "De Wayne Warren", "2004", "rear - view mirror", "Portuguese and Spanish - French origins", "1990", "The terrestrial biosphere", "1937", "2017", "Beijing", "the court from its members", "to convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "5\u00d75 cards", "Famous Players-Lasky Corporation", "Tiffany & Company", "an American politician and environmentalist", "villanelle", "lifeless, naked body", "man's lifeless, naked body", "four months ago", "magnesium", "Captain Christopher Newport", "rotunda"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5754508347487013}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.5283018867924527, 0.0, 0.0, 0.4444444444444445, 0.5714285714285715, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.42857142857142855, 0.9, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.3333333333333333, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.7272727272727273, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7547", "mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.421875, "CSR": 0.5665509259259259, "retrieved_ids": ["mrqa_squad-train-78435", "mrqa_squad-train-53512", "mrqa_squad-train-58549", "mrqa_squad-train-32830", "mrqa_squad-train-85252", "mrqa_squad-train-20422", "mrqa_squad-train-77245", "mrqa_squad-train-28972", "mrqa_squad-train-9652", "mrqa_squad-train-55117", "mrqa_squad-train-20945", "mrqa_squad-train-9378", "mrqa_squad-train-72786", "mrqa_squad-train-71409", "mrqa_squad-train-33222", "mrqa_squad-train-9190", "mrqa_newsqa-validation-4203", "mrqa_naturalquestions-validation-3698", "mrqa_newsqa-validation-3172", "mrqa_squad-validation-1407", "mrqa_squad-validation-825", "mrqa_squad-validation-3119", "mrqa_hotpotqa-validation-1239", "mrqa_newsqa-validation-2473", "mrqa_triviaqa-validation-4782", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-373", "mrqa_triviaqa-validation-5766", "mrqa_squad-validation-6559", "mrqa_squad-validation-7307", "mrqa_newsqa-validation-1157", "mrqa_searchqa-validation-3735"], "EFR": 0.918918918918919, "Overall": 0.7250627189689689}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous", "Dane", "Albert C. Outler", "Henry Young Darracott Scott", "Seminole Tribe", "One out of every 17 children under 3 years old", "Tuesday", "Dan Parris, 25, and Rob Lehr, 26,", "Mount Vernon Estate & Gardens", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "\"we have more work to do,\"", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "step up", "helping to plan the September 11, 2001, terror attacks,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "Juliet", "Little Rock military recruiting center", "Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "blew up an ice jam Wednesday evening south of  Bismarck, according to CNN affiliate KXMB.", "Michelle Rounds", "telephone survey", "not speak", "African National Congress Deputy President Kgalema Motlanthe", "Turkey", "Bill Stanton", "humans", "Herman Thomas", "Bayern", "a lightning strike", "Deputy Treasury Secretary", "Columbia, Illinois,", "Utah", "The student, whose identity was not released, admitted Friday to police at the University of California San Diego that she hung a noose Thursday night in the library,", "Al-Shabaab", "Tom Hanks", "outside his house in Najaf's Adala neighborhood", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "Both men were hospitalized and expected to survive", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Carl Froch", "Abdullah Gul", "1979", "Lynne Tracy.", "Richard Masur", "Jughead Jones", "Sarah Josepha Hale", "1998", "violin", "to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Johnny Torrio", "furniture", "Sh shrimp", "cnidarians"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5824005752470002}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 0.5, 0.2857142857142857, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.9152542372881356, 0.2608695652173913, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-778", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1604", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.46875, "CSR": 0.5630580357142857, "retrieved_ids": ["mrqa_squad-train-4248", "mrqa_squad-train-7516", "mrqa_squad-train-38364", "mrqa_squad-train-11120", "mrqa_squad-train-59333", "mrqa_squad-train-47964", "mrqa_squad-train-53838", "mrqa_squad-train-68010", "mrqa_squad-train-83794", "mrqa_squad-train-78915", "mrqa_squad-train-83388", "mrqa_squad-train-59124", "mrqa_squad-train-69864", "mrqa_squad-train-84922", "mrqa_squad-train-27952", "mrqa_squad-train-32951", "mrqa_triviaqa-validation-3423", "mrqa_searchqa-validation-1162", "mrqa_naturalquestions-validation-1015", "mrqa_newsqa-validation-43", "mrqa_triviaqa-validation-3232", "mrqa_newsqa-validation-3534", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-7352", "mrqa_searchqa-validation-13939", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3168", "mrqa_newsqa-validation-2068", "mrqa_triviaqa-validation-7536", "mrqa_searchqa-validation-3267", "mrqa_squad-validation-1583"], "EFR": 1.0, "Overall": 0.7405803571428571}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars", "Nepali", "German", "President Sheikh Sharif Sheikh Ahmed", "Africa", "Thursday and Friday", "Rod Blagojevich", "gasoline", "Denver, Colorado.", "Dolgorsuren Dagvadorj,", "it does not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain", "Peshawar", "The Casalesi Camorra clan", "President Clinton", "he regretted describing her as \"wacko.\"", "Adenhart", "The nation's foremost concert producer, Charles Jubert,", "education", "environmental", "2010", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "the Louvre", "More than 15,000", "He won it with a clear strategy that was stuck to with remarkably little internal drama.", "0-0 draw", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam,", "Kim Clijsters", "Misty Croslin", "Zed's", "Nazi Germany", "Sharon Bialek", "the Kurdish militant group", "military veterans", "41,", "the job bill's controversial millionaire's surtax", "Sabina Guzzanti", "Columbia, Missouri.", "15,000", "21 percent", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "necessary, but not sufficient", "the town of Acolman, just north of Mexico City", "1973", "rugby", "rabies", "Parkinson's", "seventh", "Disha Patani", "Anah\u00ed", "Gordon Brown", "Excalibur", "witchcraft"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6139331004140787}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8333333333333334, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.08695652173913043, 0.4, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1139", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-3301", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.515625, "CSR": 0.5614224137931034, "retrieved_ids": ["mrqa_squad-train-53488", "mrqa_squad-train-13338", "mrqa_squad-train-23838", "mrqa_squad-train-26842", "mrqa_squad-train-70952", "mrqa_squad-train-22028", "mrqa_squad-train-83315", "mrqa_squad-train-6828", "mrqa_squad-train-15056", "mrqa_squad-train-26421", "mrqa_squad-train-84017", "mrqa_squad-train-65355", "mrqa_squad-train-83466", "mrqa_squad-train-3747", "mrqa_squad-train-1721", "mrqa_squad-train-2170", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1392", "mrqa_squad-validation-3543", "mrqa_searchqa-validation-11139", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-10653", "mrqa_triviaqa-validation-1363", "mrqa_newsqa-validation-2228", "mrqa_squad-validation-2932", "mrqa_squad-validation-490", "mrqa_newsqa-validation-334", "mrqa_triviaqa-validation-7563", "mrqa_searchqa-validation-6992", "mrqa_naturalquestions-validation-4124"], "EFR": 0.967741935483871, "Overall": 0.7338016198553948}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "Denver Broncos quarterback Denver Broncos", "teach by rote", "\"A good vegan cupcake has the power to transform everything for the better,\"", "Oxygen Channel's \"Dance Your Ass Off\"", "George H.W. Bush", "business dealings", "Almost all British troops in Iraq are being pulled out because the agreement that allows them to be there expires", "Jacob Zuma", "Susan Boyle", "jazz", "\"falling space debris,\"", "Obama's", "30", "Monday night", "prison inmates.", "Franklin, Tennessee,", "The BBC", "the coalition", "that triggered a nationwide manhunt and search for the girl", "Brian David Mitchell,", "Christmas parade", "football", "consumer confidence", "Republican Party", "not intercepted any Haitians attempting illegal crossings into U.S. waters.", "Dean Martin, Katharine Hepburn and Spencer Tracy", "The \"placenta pack\" is said to help rejuvenate and ease muscle stiffness.", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "Paul Ryan (R-WI)", "top designers", "about 5:20 p.m. at Terminal C", "a group called the \"Mata Zetas,\" or Zeta Killers.", "Darrel Mohler", "Casalesi Camorra clan", "Obama and McCain camps", "Sen. Barack Obama", "heavy brush,", "more than 30", "Empire of the Sun", "30-minute recorded message", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "deployed in major cities aross Italy since the early summer.", "Monday,", "Ghana", "small child", "reached an agreement late Thursday to form a government of national reconciliation.", "eight years of age, with a wonderful life and career ahead of him.", "between Weston Road and Highway 11 ( Yonge Street ) ; Highway 2A between West Hill and Newcastle ; and the Scenic Highway between Gananoque and Brockville, now known as the Thousand Islands Parkway", "they each supported major regional wars known as proxy wars", "late January or early February", "Galileo Galilei", "Zeus", "paper sales company", "Christian Kern", "Indianola", "Wayne County, Michigan", "willis", "Akihito", "Dorothy Parker"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5880119559198805}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.34782608695652173, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23255813953488372, 0.0, 0.5000000000000001, 1.0, 0.0, 0.5, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614"], "SR": 0.46875, "CSR": 0.5583333333333333, "retrieved_ids": ["mrqa_squad-train-54647", "mrqa_squad-train-62400", "mrqa_squad-train-46562", "mrqa_squad-train-26743", "mrqa_squad-train-78941", "mrqa_squad-train-60783", "mrqa_squad-train-46116", "mrqa_squad-train-61903", "mrqa_squad-train-35636", "mrqa_squad-train-2428", "mrqa_squad-train-78193", "mrqa_squad-train-72462", "mrqa_squad-train-23788", "mrqa_squad-train-2050", "mrqa_squad-train-83422", "mrqa_squad-train-79580", "mrqa_squad-validation-3130", "mrqa_triviaqa-validation-6854", "mrqa_searchqa-validation-5679", "mrqa_hotpotqa-validation-1756", "mrqa_squad-validation-4015", "mrqa_naturalquestions-validation-5185", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3891", "mrqa_triviaqa-validation-4973", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-4117", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1003", "mrqa_squad-validation-4452", "mrqa_newsqa-validation-3860", "mrqa_squad-validation-7307"], "EFR": 1.0, "Overall": 0.7396354166666667}, {"timecode": 30, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.8984375, "KG": 0.51015625, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "67.9", "letters received or written", "Wilbur & Orville", "Queen Mary II", "the Pula Arena", "Maggie", "Google", "ionization energy", "HIV", "a chela", "1942's", "The Last Starfighter", "prone", "Romanov", "a mirror", "zymurgy", "waiting for Lefty", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Earhart", "Minnesota", "Muriel Pritchett", "Han Solo", "Charles Martel", "Catherine of Aragon", "Paris", "St Mark", "Oklahoma", "Salman Rushdie", "the United Nations", "Tycho Brahe", "comedy", "the Interior", "elephants", "cloister", "Stamp", "Pakistan", "Idiot's", "Clue", "Heath", "radiant Rita", "Ellen Wilson", "herbicides", "tornado", "Omaha, Nebraska", "\"The Greatest Gift''", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "gda\u0144sk", "Bobby Kennedy", "Mercury", "Nardwuar the Human Serviette", "Niveda Thomas", "1967", "illegal immigrants", "CEO", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6216517857142857}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-4424", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3688"], "SR": 0.546875, "CSR": 0.5579637096774194, "retrieved_ids": ["mrqa_squad-train-12256", "mrqa_squad-train-13872", "mrqa_squad-train-82042", "mrqa_squad-train-75916", "mrqa_squad-train-16738", "mrqa_squad-train-54369", "mrqa_squad-train-21005", "mrqa_squad-train-9423", "mrqa_squad-train-80289", "mrqa_squad-train-17081", "mrqa_squad-train-44246", "mrqa_squad-train-44257", "mrqa_squad-train-17321", "mrqa_squad-train-1358", "mrqa_squad-train-19917", "mrqa_squad-train-3657", "mrqa_searchqa-validation-5915", "mrqa_squad-validation-5525", "mrqa_naturalquestions-validation-916", "mrqa_searchqa-validation-3735", "mrqa_squad-validation-117", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1458", "mrqa_naturalquestions-validation-7021", "mrqa_newsqa-validation-3453", "mrqa_searchqa-validation-5128", "mrqa_triviaqa-validation-2250", "mrqa_squad-validation-3118", "mrqa_newsqa-validation-990", "mrqa_squad-validation-4462", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-1137"], "EFR": 1.0, "Overall": 0.7495614919354839}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "the quotient", "Carson Palmer", "hail", "Cordillera de Merida", "Chiefland", "the Hippocratic Oath", "Queen Latifah", "a Golden Retriever", "Shropshire", "the Mediterranean", "fingernails", "a eagle", "Trump", "a crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a conscientious objector", "the oil pipeline", "trout", "Friday the 13th", "Dixie Chicks", "Carl Bernstein", "a buffalo", "America", "Istanbul", "Sitting Bull", "a look", "\"Rehab\"", "the Golden Hind", "Administrative Professionals' Day", "Nasser", "Van Halen", "a black bear, moose", "dams", "Djibouti", "pyrite", "a cyclone", "Ted Morgan", "cashmere", "Princess Diana", "Spilt milk", "grasshopper", "carat", "Robin Hood", "the chalk cliffs", "Tom", "September 29, 2017", "Franklin and Wake counties", "December 1800", "Nicolas Sarkozy", "Democrats", "a quarter", "Rabies", "Environmental Protection Agency", "Robert Gibson", "Mogadishu", "cardio", "400 years ago"], "metric_results": {"EM": 0.59375, "QA-F1": 0.686734068627451}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-12318", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-11136", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.59375, "CSR": 0.55908203125, "retrieved_ids": ["mrqa_squad-train-67606", "mrqa_squad-train-53253", "mrqa_squad-train-57349", "mrqa_squad-train-31510", "mrqa_squad-train-31697", "mrqa_squad-train-45070", "mrqa_squad-train-39625", "mrqa_squad-train-48442", "mrqa_squad-train-29429", "mrqa_squad-train-64092", "mrqa_squad-train-68460", "mrqa_squad-train-76756", "mrqa_squad-train-36393", "mrqa_squad-train-53936", "mrqa_squad-train-30429", "mrqa_squad-train-9139", "mrqa_searchqa-validation-11053", "mrqa_triviaqa-validation-4255", "mrqa_newsqa-validation-1425", "mrqa_searchqa-validation-5510", "mrqa_triviaqa-validation-1630", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-5939", "mrqa_squad-validation-9320", "mrqa_newsqa-validation-893", "mrqa_hotpotqa-validation-1127", "mrqa_searchqa-validation-9915", "mrqa_triviaqa-validation-5252", "mrqa_squad-validation-4462"], "EFR": 1.0, "Overall": 0.74978515625}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuro immune system", "to", "Madrid", "sign", "Jackie Moon", "a tornado", "Trump Taj Mahal", "a banana", "fried", "John", "Liverpool", "Andy Taylor", "Nassau", "the Mediterranean", "Celsius", "Janet Reno", "Spanish American War", "Seinfeld", "steroids", "Atlantic City", "\"Who is John Galt?\"", "Clinton", "Iraq", "the taro root", "Sanssouci", "Frozone", "Chaikovsky", "Malle Babbe", "the Stone Age", "Paul Gauguin", "Billy Pilgrim", "Louis XIII", "Cain's offering was not accepted by God", "Charles", "the Heart", "whiskers", "a cigarette lighter", "Elmer", "the dinosaurs", "Peggy Fleming", "Panama", "dalton", "Sweden", "Castle Rock", "fuchsia", "the Yangtze", "Barbara Bush", "Michelle Pfeiffer", "Sinclair Lewis", "Daphne du Maurier", "Starsky & Hutch", "King Willem - Alexander", "New England Patriots", "comprehend and formulate language", "Damon Albarn", "duchy of Mazovia", "Ken Burns", "Wabanaki", "Flashback", "Manchester United", "the Yemeni port city of Aden", "along the equator between South America and Africa.", "four decades"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5401870265151515}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 0.0, 1.0, 0.4, 0.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.8750000000000001, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-7455", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-14396", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-4697", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-4806", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.4375, "CSR": 0.5553977272727273, "retrieved_ids": ["mrqa_squad-train-85877", "mrqa_squad-train-57788", "mrqa_squad-train-3097", "mrqa_squad-train-79729", "mrqa_squad-train-42560", "mrqa_squad-train-43277", "mrqa_squad-train-20537", "mrqa_squad-train-7738", "mrqa_squad-train-39660", "mrqa_squad-train-2453", "mrqa_squad-train-34243", "mrqa_squad-train-69426", "mrqa_squad-train-54276", "mrqa_squad-train-56169", "mrqa_squad-train-26308", "mrqa_squad-train-68436", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-7635", "mrqa_squad-validation-3559", "mrqa_newsqa-validation-3733", "mrqa_searchqa-validation-16831", "mrqa_newsqa-validation-5", "mrqa_triviaqa-validation-5052", "mrqa_naturalquestions-validation-4413", "mrqa_hotpotqa-validation-1127", "mrqa_naturalquestions-validation-508", "mrqa_squad-validation-10011", "mrqa_searchqa-validation-9777", "mrqa_triviaqa-validation-5499", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-3993", "mrqa_searchqa-validation-12302"], "EFR": 1.0, "Overall": 0.7490482954545455}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual", "echinacea", "poker", "tuna", "kinshasa", "the Bronze Age", "Japan", "Thomas Merton", "ex-wife", "the phantom", "Crystal Car Fathers Day Auto Show", "The Hoboken Five", "77.7", "donut chain", "the magma", "a deer", "German", "volcanoes", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "ducks, hummingbirds", "Columbia University", "Jack O'Lanterns", "Sexuality", "Greece", "the Inca Empire", "contagious", "Vin Diesel", "the mob", "New Mexico", "the French Revolution", "a Purple Heart", "Arkansas", "the CPU", "Lasky", "katana", "Elvis Presley", "Jean Lafitte", "the Komodo Dragon", "Italian", "Churchill", "knitting", "Atonement", "receipt", "Damascus", "Bhuddha", "Innsbruck", "Noah\\'s", "SeaWorld", "on the chest, back, shoulders, torso and / or legs", "Article Two", "Newcastle United", "Genghis Khan.", "Roy Rogers", "African violet", "Northern Transcon", "25 October 1921", "Sonja Henie", "\"The Orchid Thief\"", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "cardiac arrest"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5565972222222222}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13753", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-11208", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-10032", "mrqa_triviaqa-validation-7627", "mrqa_hotpotqa-validation-3417", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.484375, "CSR": 0.5533088235294117, "retrieved_ids": ["mrqa_squad-train-71670", "mrqa_squad-train-30762", "mrqa_squad-train-16490", "mrqa_squad-train-20468", "mrqa_squad-train-70845", "mrqa_squad-train-47061", "mrqa_squad-train-20987", "mrqa_squad-train-33435", "mrqa_squad-train-78140", "mrqa_squad-train-85038", "mrqa_squad-train-76244", "mrqa_squad-train-60508", "mrqa_squad-train-20838", "mrqa_squad-train-55858", "mrqa_squad-train-60348", "mrqa_squad-train-12352", "mrqa_squad-validation-4999", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-1298", "mrqa_squad-validation-6757", "mrqa_triviaqa-validation-7536", "mrqa_searchqa-validation-12750", "mrqa_naturalquestions-validation-3962", "mrqa_squad-validation-8749", "mrqa_searchqa-validation-3497", "mrqa_naturalquestions-validation-10615", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-2480", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-3348", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-2871"], "EFR": 0.9696969696969697, "Overall": 0.7425699086452763}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "Moon River", "Mighty Joe Young", "Robert II", "the Dutch West India Company", "Hans Christian Andersen", "luffa", "Goelitz Confectionery Company", "a snail", "a crossword", "Muhammad Ali", "a deodorant", "the Supreme Court", "the north magnetic pole", "Putin", "thunderstorms", "Kennebunkport", "satellite", "the Black Death", "Devon", "elia Earhart", "Hoover Dam", "Panty Raid", "French", "cricket", "The Pythian Games", "The \"NFL HQ\"", "The Lone Ranger", "The viscacha", "white", "Flying to Africa", "a keypunch", "the Amazons", "The Fugitive", "China", "a blacksmith", "Harpers Ferry", "(a) Find the angle q the ray makes with the normal when it reaches the curved... is erect but enlarged to 1.5 times its actual size", "lilac", "a double curve", "Tampa", "ductile", "Shakespeare and Opera", "Leo", "first anniversary", "a nautilus", "a hand gesture", "Bigfoot", "Juris Doctorate", "put options", "The Thing", "Special Agent Dwayne Cassius Pride ( Scott Bakula )", "Stephen Curry", "Kusha", "Jupiter", "Captain America", "the Great Depression", "South America", "1998", "Picric acid", "Nineteen", "housing, business and infrastructure repairs", "Siri."], "metric_results": {"EM": 0.515625, "QA-F1": 0.5819444444444445}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-8683", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-3737", "mrqa_triviaqa-validation-7740"], "SR": 0.515625, "CSR": 0.5522321428571428, "retrieved_ids": ["mrqa_squad-train-15825", "mrqa_squad-train-40472", "mrqa_squad-train-77296", "mrqa_squad-train-53534", "mrqa_squad-train-30596", "mrqa_squad-train-72798", "mrqa_squad-train-50041", "mrqa_squad-train-52201", "mrqa_squad-train-62234", "mrqa_squad-train-6405", "mrqa_squad-train-53285", "mrqa_squad-train-27827", "mrqa_squad-train-63864", "mrqa_squad-train-58930", "mrqa_squad-train-57877", "mrqa_squad-train-14954", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1021", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-5301", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-5185", "mrqa_newsqa-validation-2338", "mrqa_squad-validation-4462", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-11361", "mrqa_newsqa-validation-463", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-10604", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-886"], "EFR": 0.967741935483871, "Overall": 0.7419635656682029}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "three", "\"How I Met Your Mother,\"", "the two-state solution", "in-cabin lighting system", "patrolling the pavement in protective shoes", "forgery and flying without a valid license,", "Kurdish militant group in Turkey", "Lee Myung-Bak", "end of a biology department faculty meeting at the University of Alabama in Huntsville,", "Malawi", "\"fusion teams,\"", "James Whitehouse,", "all buses, subways and trolleys that carry almost a million people daily.", "Muslim", "a Muslim festival", "Caster Semenya", "Fiona MacKeown", "GospelToday", "death of cardiac arrest", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "rural Tennessee.", "The BBC", "Plymouth Rock", "\"Watchmen\" (No. 1)", "seven", "Karen Floyd", "Expedia", "Robert Redford", "\"wider relationship\"", "death squad killings", "hand-painted Swedish wooden clogs", "July for A Country Christmas", "down a steep embankment in the Angeles National Forest", "piano", "\"Good lord, y'all,\"", "They decided to use a surrogate", "Lisa Brown", "tax incentives for businesses hiring veterans as well as job training", "State Department", "two years,", "Operation Cast Lead", "Diego Maradona", "21-year-old", "bartering -- trading goods and services without exchanging money", "Rawalpindi", "the need for reconciliation in a country that endured a brutal civil war lasting nearly three decades.", "Leo Frank", "Port-au-Prince", "Buddhism", "Russian", "President George Bush", "in different parts of the globe", "Sophocles", "a charbagh", "(Brando)", "Caribbean", "Valletta", "Eisenhower Executive Office Building", "Tottenham Hotspur", "February 22, 1968", "the Palatine hill", "petrol", "(Evan Almighty)"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6063231906981906}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 1.0, 0.3333333333333333, 0.36363636363636365, 1.0, 0.0, 0.0, 0.3333333333333333, 0.3076923076923077, 0.4, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.05555555555555555, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265", "mrqa_searchqa-validation-5633"], "SR": 0.515625, "CSR": 0.5512152777777778, "retrieved_ids": ["mrqa_squad-train-5350", "mrqa_squad-train-77772", "mrqa_squad-train-27202", "mrqa_squad-train-26031", "mrqa_squad-train-68469", "mrqa_squad-train-6592", "mrqa_squad-train-24703", "mrqa_squad-train-40514", "mrqa_squad-train-65019", "mrqa_squad-train-80472", "mrqa_squad-train-70862", "mrqa_squad-train-3809", "mrqa_squad-train-1144", "mrqa_squad-train-80998", "mrqa_squad-train-42969", "mrqa_squad-train-78891", "mrqa_searchqa-validation-10799", "mrqa_newsqa-validation-4028", "mrqa_triviaqa-validation-5492", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-16971", "mrqa_triviaqa-validation-7233", "mrqa_newsqa-validation-2473", "mrqa_naturalquestions-validation-2781", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2175", "mrqa_squad-validation-4462", "mrqa_searchqa-validation-1843", "mrqa_newsqa-validation-697", "mrqa_squad-validation-1583", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-9777"], "EFR": 1.0, "Overall": 0.7482118055555556}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "future exploration of the moon and beyond.", "\"Nothing But Love\"", "Mississippi", "Vernon Forrest,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million.", "\"Top Gun\"", "us to step up.", "Too many glass shards", "one Iraqi soldier,", "Jaipur", "Mahmoud Ahmadinejad", "April 6, 1994", "the Democratic VP candidate", "together with two other buildings on March 3.", "34", "20,000-capacity O2 Arena.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Immigration Minister Eric Besson.", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Kevin Evans", "Kingdom City project", "the Revolutionary Armed Forces of Colombia,", "Dan Brown", "The pilot,", "Paul McCartney and Ringo Starr", "Booches Billiard Hall,", "air support.", "\"She was focused so much on learning that she didn't notice,\"", "in a Starbucks", "finance", "month of three people with ties to the U.S. Consulate in Ciudad Juarez,", "he was diagnosed with skin cancer.", "as he exercised in a park in a residential area of Mexico City,", "in the mountains around Deutschneudorf,", "5,600", "House Democrats had tried", "Nearly eight in 10", "Former Beatles", "at least $20 million to $30 million,", "a vigilante group", "the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them", "in the fovea centralis", "10 years", "Jeffrey Archer", "a peplos", "Jack Nicholson", "Flatbush Zombies", "Louis King", "Venice", "a bagpipes", "reconnaissance", "Earvin \"Magic\" Johnson Jr.", "Fix You"], "metric_results": {"EM": 0.515625, "QA-F1": 0.670844860548808}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.8, 0.8421052631578948, 0.4, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.6799999999999999, 1.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-1127"], "SR": 0.515625, "CSR": 0.5502533783783784, "retrieved_ids": ["mrqa_squad-train-2374", "mrqa_squad-train-27937", "mrqa_squad-train-84590", "mrqa_squad-train-36645", "mrqa_squad-train-67165", "mrqa_squad-train-69506", "mrqa_squad-train-84617", "mrqa_squad-train-63859", "mrqa_squad-train-29664", "mrqa_squad-train-84240", "mrqa_squad-train-19432", "mrqa_squad-train-46142", "mrqa_squad-train-74330", "mrqa_squad-train-63739", "mrqa_squad-train-82336", "mrqa_squad-train-14193", "mrqa_triviaqa-validation-86", "mrqa_newsqa-validation-464", "mrqa_triviaqa-validation-4573", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-4134", "mrqa_searchqa-validation-12261", "mrqa_triviaqa-validation-1315", "mrqa_naturalquestions-validation-519", "mrqa_newsqa-validation-1519", "mrqa_searchqa-validation-4207", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-1137", "mrqa_hotpotqa-validation-5831", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1947"], "EFR": 1.0, "Overall": 0.7480194256756757}, {"timecode": 37, "before_eval_results": {"predictions": ["physicians and other healthcare professionals", "Ricardo Valles de la Rosa,", "six", "Sunni Arab and Shiite tribal leaders", "Capitol Records,", "Kgalema Motlanthe,", "a ferry", "1994,", "Belfast, Northern Ireland", "Cain", "Dan Parris, 25, and Rob Lehr,", "Lana Clarkson", "CEO of an engineering and construction company", "the British capital's other two airports, Stansted and Gatwick,", "40 lashings", "breathe through her nose, smell, eat solid foods and drink out of a cup,", "almost 9 million", "16 Indiana National Guard soldiers", "NATO fighters", "low-calorie", "1,500", "Grayback forest-firefighters", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "Bergdahl, was captured June 30 from Paktika province in southeastern Afghanistan,", "\"Steamboat Bill, Jr.\"", "Brian Smith.", "Urbina", "Swansea Crown Court,", "Virgin America", "the Kirchners", "3,000 kilometers (1,900 miles)", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court", "nuclear weapon", "Iran's parliament speaker", "No 4", "a CBE", "they know what is important in life,", "10 years", "artificial intelligence.", "There's no chance", "10", "April 13,", "Juri Kibuishi,", "London", "Obama", "16", "Ralph Lauren", "$10 billion", "2,800", "three", "David Ben - Gurion", "Kiss", "maintenance fees", "james stewart", "Noises Off", "aeoline", "dachau and Mauthausen", "Delilah Rene", "Tampa Bay Storm", "Pope John Paul II", "art deco", "the Invisible Man", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6167140151515151}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 1.0, 0.2727272727272727, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.484375, "CSR": 0.5485197368421053, "retrieved_ids": ["mrqa_squad-train-2966", "mrqa_squad-train-18899", "mrqa_squad-train-41250", "mrqa_squad-train-3380", "mrqa_squad-train-28293", "mrqa_squad-train-51110", "mrqa_squad-train-81682", "mrqa_squad-train-62035", "mrqa_squad-train-27542", "mrqa_squad-train-61245", "mrqa_squad-train-74138", "mrqa_squad-train-78169", "mrqa_squad-train-21076", "mrqa_squad-train-55827", "mrqa_squad-train-65672", "mrqa_squad-train-41839", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-15795", "mrqa_triviaqa-validation-7777", "mrqa_searchqa-validation-5456", "mrqa_naturalquestions-validation-495", "mrqa_newsqa-validation-2792", "mrqa_squad-validation-6489", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-11809", "mrqa_naturalquestions-validation-6234", "mrqa_searchqa-validation-1747", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2945", "mrqa_triviaqa-validation-644", "mrqa_searchqa-validation-5963"], "EFR": 0.9393939393939394, "Overall": 0.7355514852472089}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\" is primarily responsible for the reduction of violence in Iraq.", "environmental and political events", "the U.S. Holocaust Memorial Museum", "Ireland.", "33", "2007", "heavy turbulence", "Liza Murphy", "frozen world located in the Gaslight Theater.", "Brett Cummins,", "Rod Blagojevich", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "1980,", "Haiti", "Israeli Navy", "Desmond Tutu", "84-year-old", "Hayden", "President Bill Clinton", "humans", "Sylt", "chairman of the House Budget Committee", "broadband television network.", "President Robert Mugabe's", "he rejected the option of committing more forces for an undefined mission of nation-building without any deadlines.", "30", "Lisa Brown", "28", "it would", "A growing percentage of the Somali population has become dependent on humanitarian aid.", "Italian Serie A", "Superman brought down the Ku Klux Klan,", "He tall 34-year-old, slouching exhausted in a Johannesburg church that has become a de facto transit camp,", "mental health and recovery.", "National Infrastructure Program,", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "Russian bombers have landed at a Venezuelan airfield where they will carry out training flights for several days,", "President Pervez Musharraf", "two", "first grand Slam,", "the MS Columbus", "a psychoticic killer who preys on a group of young people at the fictitious Camp Crystal Lake.", "The local Republican Party", "1 October 2006", "1834", "caveolae internalization", "guitar", "Scafell Pike", "caffeine", "Keele University", "9,984", "Smithfield, Rhode Island, U.S.", "a vacuum flask", "Donna Rice Hughes", "albatross", "actor"], "metric_results": {"EM": 0.53125, "QA-F1": 0.633990346961153}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.06666666666666667, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.1142857142857143, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3468", "mrqa_hotpotqa-validation-3644", "mrqa_hotpotqa-validation-5393", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185"], "SR": 0.53125, "CSR": 0.5480769230769231, "retrieved_ids": ["mrqa_squad-train-29306", "mrqa_squad-train-63829", "mrqa_squad-train-41650", "mrqa_squad-train-33208", "mrqa_squad-train-53520", "mrqa_squad-train-60145", "mrqa_squad-train-30067", "mrqa_squad-train-34528", "mrqa_squad-train-19478", "mrqa_squad-train-41067", "mrqa_squad-train-76378", "mrqa_squad-train-55778", "mrqa_squad-train-9172", "mrqa_squad-train-22396", "mrqa_squad-train-21732", "mrqa_squad-train-45777", "mrqa_naturalquestions-validation-9284", "mrqa_hotpotqa-validation-4463", "mrqa_newsqa-validation-1967", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-15996", "mrqa_squad-validation-2226", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-276", "mrqa_newsqa-validation-240", "mrqa_searchqa-validation-3222", "mrqa_newsqa-validation-3184", "mrqa_naturalquestions-validation-8062", "mrqa_searchqa-validation-8117", "mrqa_newsqa-validation-2042", "mrqa_searchqa-validation-8845", "mrqa_naturalquestions-validation-5317"], "EFR": 0.9333333333333333, "Overall": 0.7342508012820513}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "Arroyo and her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week", "ties", "Mediterranean Sea.", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "Vivek Wadhwa,", "\"It appears that they just made those numbers up,\"", "Harlem,", "the fact that the teens were charged as adults.", "Palestinian-Israeli issue", "one-of-a-kind navy dress with red lining", "Saturday", "ensuring that all prescription drugs on the market are FDA approved", "Robert", "suicides", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes", "talk show queen Oprah Winfrey.", "Too many glass shards left by beer drinkers", "1,000 pounds", "two satellites", "the most gigantic pumpkins in the world,", "college campus.", "Sunni Arab and Shiite tribal leaders", "three out of four", "$50 less", "Lindsey oil refinery,", "1,300 meters in the Mediterranean Sea.", "\"he puts more heart and more passion in what he's doing than some of the other dancers", "Pakistan", "he will be captured,", "he wants a \"happy ending\" to the case.", "fluoroquinolone", "to ensure that detainees are not drugged unless there is a medical reason to do so.", "mpire of the Sun", "digging", "1000 square meters", "President Obama", "North Korea", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001 -- 2002 season", "786 -- 802", "31 March 2018", "Muhammad Ali", "the tallest building in the world", "81st", "Premier League", "the Secret Intelligence Service", "75 mi", "chef salads", "grasshopper", "Kneset", "the Secretary of the Interior"], "metric_results": {"EM": 0.5, "QA-F1": 0.6213798088181312}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.07407407407407408, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8421052631578948, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.4444444444444445, 0.6666666666666666, 0.6666666666666666, 0.7142857142857143, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-1702", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-6954"], "SR": 0.5, "CSR": 0.546875, "retrieved_ids": ["mrqa_squad-train-50249", "mrqa_squad-train-49862", "mrqa_squad-train-74514", "mrqa_squad-train-19036", "mrqa_squad-train-67217", "mrqa_squad-train-58542", "mrqa_squad-train-77411", "mrqa_squad-train-13707", "mrqa_squad-train-12373", "mrqa_squad-train-35722", "mrqa_squad-train-5283", "mrqa_squad-train-26325", "mrqa_squad-train-85532", "mrqa_squad-train-22663", "mrqa_squad-train-76611", "mrqa_squad-train-49139", "mrqa_newsqa-validation-3666", "mrqa_searchqa-validation-14720", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-541", "mrqa_triviaqa-validation-2250", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-12438", "mrqa_squad-validation-7473", "mrqa_squad-validation-10320", "mrqa_newsqa-validation-3480", "mrqa_triviaqa-validation-7311", "mrqa_naturalquestions-validation-8119", "mrqa_squad-validation-1941", "mrqa_naturalquestions-validation-4552", "mrqa_triviaqa-validation-6334", "mrqa_squad-validation-7687"], "EFR": 1.0, "Overall": 0.74734375}, {"timecode": 40, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.8828125, "KG": 0.475, "before_eval_results": {"predictions": ["1985", "a nurse who tried to treat Jackson's insomnia with natural remedies", "eight", "Austin Wuennenberg,", "canyon", "machine guns and two silencers", "Matthew Fisher,", "Barack Obama", "NATO", "Joe Lieberman", "police", "the Gulf", "Petionville, Haiti,", "northwest Pakistan", "Basel", "Pyongyang and Seoul", "\"I'm not afraid to say it, sometimes she was a pain in the ass,\"", "Kurt Cobain's", "using recreational drugs", "1983", "22-10.", "Egypt.", "Rima Fakih", "delivers a big speech", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "Justicialist Party, or PJ by its Spanish acronym,", "between 3 million and 4 million fossilized bones.", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "contraband cell phones", "Six", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "19-year-old", "alternative-energy vehicles", "Iraq", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "may become impractical unless we can build large structures to keep the waters at bay,", "Communist Party", "the journalists and the flight crew will be freed,", "Haitians", "Tamil insurgents", "telling CNN his comments had been taken out of context.", "summer", "Rev. Alberto Cutie", "since 1983.", "the child might still be alive,", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "two easily observed features", "Introduced in 1957", "Jack Ruby", "Altamont Speedway Free Festival", "Trainspotting", "Nicol Williamson", "29, 2009", "Latin American culture", "Dolly Parton", "People of the Book", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6201722909443498}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.25, 0.5, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9411764705882353, 0.0, 0.25, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-1870", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.46875, "CSR": 0.5449695121951219, "retrieved_ids": ["mrqa_squad-train-86585", "mrqa_squad-train-25921", "mrqa_squad-train-65695", "mrqa_squad-train-19977", "mrqa_squad-train-84596", "mrqa_squad-train-57890", "mrqa_squad-train-34859", "mrqa_squad-train-40813", "mrqa_squad-train-13009", "mrqa_squad-train-45321", "mrqa_squad-train-80802", "mrqa_squad-train-39381", "mrqa_squad-train-57039", "mrqa_squad-train-79390", "mrqa_squad-train-56583", "mrqa_squad-train-58425", "mrqa_searchqa-validation-5939", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-25", "mrqa_triviaqa-validation-4453", "mrqa_squad-validation-2513", "mrqa_newsqa-validation-2613", "mrqa_searchqa-validation-3735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-198", "mrqa_newsqa-validation-334", "mrqa_squad-validation-1092", "mrqa_triviaqa-validation-5433", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-3222"], "EFR": 1.0, "Overall": 0.7375876524390244}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "in the mouth.", "reportedly opened the door for the man police say was his killer.", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "the company", "the same drama that pulls in the crowds", "across Greece", "a monthly allowance,", "U.S. Navy helicopter crew", "their \"Freshman Year\" experience", "Bastian Schweinsteiger", "not guilty of affray by a court in his home city on Friday.", "businessman Ross Perot.", "outside the municipal building of Abu Ghraib in western Baghdad", "Al Nisr Al Saudi", "two years ago.", "He also said it was important to provide alternative work for poor Afghan farmers to encourage them to give up opium production.", "sailboat", "The FBI's Baltimore field office", "Tuesday in Los Angeles.", "Honduran", "curfew in Jaipur", "The group also has also been linked to the March attack on the Sri Lankan cricket team in the Pakistani city of Lahore.", "Robert Barnett,", "in a park in a residential area of Mexico City,", "40 years ago", "iTunes,", "at the shop at the Form Design Center.", "the Russian air force,", "an Italian and six Africans", "three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist police characterized as \"spectacular.\"", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre", "Missouri.", "the Dalai Lama", "ketamine,", "Haleigh Cummings,", "two and a half hours.", "Bobby Darin,", "Queen Elizabeth's", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama on October 23, 2008,", "an obscure story of flowers", "Schalke", "Kris Allen,", "between raw descriptions and expressions of hope:\"... covered in dust and debris... we saw a few bodies that had been pulled out of the rubble laying dead in the sidewalk... many others injured.\"", "2 total", "Supplemental oxygen", "Iran", "Harley", "Tom Mix", "Harriet Tubman", "lion", "German", "Forbes", "black magic", "cholesterol", "Stockholm", "Italian Agostino Bassi"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5716870527846138}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 0.7272727272727272, 0.4, 0.4878048780487806, 0.4, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.2857142857142857, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_naturalquestions-validation-8733"], "SR": 0.4375, "CSR": 0.5424107142857143, "retrieved_ids": ["mrqa_squad-train-63382", "mrqa_squad-train-61044", "mrqa_squad-train-39927", "mrqa_squad-train-55355", "mrqa_squad-train-72941", "mrqa_squad-train-28750", "mrqa_squad-train-53537", "mrqa_squad-train-34284", "mrqa_squad-train-24076", "mrqa_squad-train-60414", "mrqa_squad-train-43209", "mrqa_squad-train-16042", "mrqa_squad-train-51474", "mrqa_squad-train-82274", "mrqa_squad-train-16565", "mrqa_squad-train-54622", "mrqa_newsqa-validation-1392", "mrqa_squad-validation-809", "mrqa_naturalquestions-validation-519", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-5879", "mrqa_newsqa-validation-2752", "mrqa_searchqa-validation-11136", "mrqa_searchqa-validation-2462", "mrqa_triviaqa-validation-7334", "mrqa_newsqa-validation-1396", "mrqa_triviaqa-validation-3232", "mrqa_naturalquestions-validation-2552", "mrqa_searchqa-validation-4535", "mrqa_squad-validation-5589", "mrqa_newsqa-validation-692", "mrqa_squad-validation-627"], "EFR": 0.9722222222222222, "Overall": 0.7315203373015874}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product / market fit", "Freddie Highmore", "Elvis Presley", "tectonic", "Stefanie Scott", "Tanvi Shah", "Kida", "in Eurasia", "Sam Waterston", "Bobby Beathard, Robert Brazile", "Palmer Williams Jr. as Floyd", "Chicago metropolitan area", "Coldplay", "$19.8 trillion", "2,050 metres ( 6,730 ft )", "Ann Gillespie", "Brooklyn Heights", "Emmett Lathrop `` Doc '' Brown", "the chryselephantine statue of Athena Parthenos", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Joe Young", "accounting Standards Board", "2012", "Bette Midler", "push the food down the esophageal muscle", "Walter Mondale", "Nick Sager", "long - standing policy of neutrality was tested on many occasions during the 1930s", "18th century", "Graham McTavish", "1962", "Julie Adams", "Odoacer", "Seton - Karr", "one", "Bill Belichick", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Brobee", "January 15, 2007", "John Garfield as Al Schmid", "plant", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "Geophysicists", "Billy Colman", "360", "November 17", "Lulu", "MercyMe", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay", "1932", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Evey's mother", "The Screening Room", "model", "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "surrogate", "salt", "Rocky Marciano", "consumer confidence"], "metric_results": {"EM": 0.5, "QA-F1": 0.615048144257703}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.19999999999999998, 0.5, 0.4, 1.0, 0.33333333333333337, 1.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666665, 0.0, 1.0, 0.5882352941176471, 0.16, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-10077", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.5, "CSR": 0.5414244186046512, "retrieved_ids": ["mrqa_squad-train-19257", "mrqa_squad-train-2169", "mrqa_squad-train-48199", "mrqa_squad-train-36314", "mrqa_squad-train-76754", "mrqa_squad-train-27190", "mrqa_squad-train-80045", "mrqa_squad-train-39636", "mrqa_squad-train-29968", "mrqa_squad-train-51037", "mrqa_squad-train-43927", "mrqa_squad-train-27943", "mrqa_squad-train-3526", "mrqa_squad-train-11036", "mrqa_squad-train-39019", "mrqa_squad-train-36435", "mrqa_triviaqa-validation-3450", "mrqa_searchqa-validation-10297", "mrqa_naturalquestions-validation-4200", "mrqa_newsqa-validation-3088", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11086", "mrqa_triviaqa-validation-1993", "mrqa_searchqa-validation-2394", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-3354", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-8839", "mrqa_hotpotqa-validation-3417", "mrqa_newsqa-validation-3655", "mrqa_searchqa-validation-3381", "mrqa_newsqa-validation-2098"], "EFR": 1.0, "Overall": 0.7368786337209302}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational", "A witness", "34", "Miami Beach, Florida,", "eight surgeons", "Somalia's piracy problem was fueled by environmental and political events.(CNN)", "Cash for Clunkers", "Justine Henin", "it has witnessed only normal maritime traffic around Haiti,", "California-based Current TV", "It is I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Tim Clark, Matt Kuchar and Bubba Watson", "Columbia", "Omar Bongo,", "the outdoors, particularly if they have a garden to eat from,", "mother.", "Madrid", "1940's", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "ketamine.", "a Ford F-150 work truck (a plain, regular-cab model), is an employed man. A Toyota Camry", "up three", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "not guilty by reason of insanity that would have resulted in psychiatric custody.", "Swiss", "Mexican military", "Sporting", "The Kirchners", "\"I really hope that what I did will enable other women to come forward in similar situations,\"", "The child was released after five days but her mom, a 33-year-old school teacher,", "CNN's \"Piers Morgan Tonight\"", "\"I hope for the sake of our kids that he gets the psychological help for himself and the safety of others.\"", "London's O2 arena", "90", "Col. Elspeth Cameron-Ritchie,", "Most of those who managed to survive the incident hid in a boiler room and storage closets during the rampage.", "his parents", "nearly 28 years.", "above zero (3 degrees Fahrenheit), but the wind chill (minus 14 degrees) was cold enough to make your skin burn,\"", "Claude Monet", "Princess Diana", "Consumer Reports", "Cash for Clunkers", "nine-wicket", "Des Moines, Iowa,", "the Caribbean", "a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Michael Schumacher", "the right to peaceably assemble, or to petition for a governmental redress of grievances", "Medicare", "Jacob Tremblay", "line code", "Harry Bailley", "The Muffin Man", "Clovis I", "Roots: The Saga of an American Family", "Almeda Mall", "a cheese made from unpasteurised sheep milk with no more than 20% goats milk mixed in", "FRAM", "the Ross Ice Shelf", "james james"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5374513922469375}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.9, 1.0, 0.0, 0.19999999999999998, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.21276595744680854, 0.0, 1.0, 0.6666666666666666, 1.0, 0.22222222222222224, 0.0, 0.4, 0.18181818181818185, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.09999999999999999, 1.0, 0.5106382978723404, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.4375, "CSR": 0.5390625, "retrieved_ids": ["mrqa_squad-train-38788", "mrqa_squad-train-47646", "mrqa_squad-train-42407", "mrqa_squad-train-19837", "mrqa_squad-train-51467", "mrqa_squad-train-21020", "mrqa_squad-train-76634", "mrqa_squad-train-26101", "mrqa_squad-train-3485", "mrqa_squad-train-30823", "mrqa_squad-train-7966", "mrqa_squad-train-37792", "mrqa_squad-train-43991", "mrqa_squad-train-83981", "mrqa_squad-train-1133", "mrqa_squad-train-36243", "mrqa_newsqa-validation-1062", "mrqa_searchqa-validation-15505", "mrqa_squad-validation-7547", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-1701", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-667", "mrqa_naturalquestions-validation-4552", "mrqa_newsqa-validation-939", "mrqa_hotpotqa-validation-4791", "mrqa_naturalquestions-validation-1549", "mrqa_triviaqa-validation-5338", "mrqa_squad-validation-9533", "mrqa_searchqa-validation-7010", "mrqa_triviaqa-validation-5681", "mrqa_hotpotqa-validation-1239"], "EFR": 0.9722222222222222, "Overall": 0.7308506944444445}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40", "700", "Mandi Hamlin", "early detection and helping other women cope with the disease.", "Alfredo Astiz,", "$5.5 billion to build.", "Her husband and attorney, James Whitehouse,", "3.5 percent", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27", "45 minutes,", "14 years", "Chesley", "did not", "repression and dire economic circumstances.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "Ma Khin Khin Leh,", "a federal judge in Mississippi", "give detainees greater latitude in selecting legal representation", "he fears a desperate country with a potential power vacuum that could lash out.", "123 pounds of cocaine and 4.5 pounds of heroin,", "3-0", "70,000 or so", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "two weeks after Black History Month", "G Chat away message.", "Barzee", "pro-democracy activists", "Kim Jong Un", "3,000 kilometers (1,900 miles),", "\"While the FDA remains committed to ultimately ensuring that all prescription drugs on the market are FDA approved, we have to balance that goal with flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "from late - September through early January", "the euro", "a member of the family Sturnidae ( starlings and mynas ) native to Asia", "piscina", "Book of Proverbs", "Douglas MacArthur", "PlayStation 4", "ITV", "cricket fighting", "Patty Duke", "Galileo Galilei", "Carson McCullers", "fearful man, all in coarse gray with a great iron on his leg"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7471410533910534}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2181818181818182, 0.9090909090909091, 1.0, 0.18181818181818182, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-5687", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-1685", "mrqa_searchqa-validation-10531", "mrqa_triviaqa-validation-3284"], "SR": 0.640625, "CSR": 0.5413194444444445, "retrieved_ids": ["mrqa_squad-train-47688", "mrqa_squad-train-28382", "mrqa_squad-train-49949", "mrqa_squad-train-75558", "mrqa_squad-train-61966", "mrqa_squad-train-50198", "mrqa_squad-train-19468", "mrqa_squad-train-27194", "mrqa_squad-train-84316", "mrqa_squad-train-44746", "mrqa_squad-train-47576", "mrqa_squad-train-24105", "mrqa_squad-train-41331", "mrqa_squad-train-30683", "mrqa_squad-train-84771", "mrqa_squad-train-6962", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-1843", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2378", "mrqa_searchqa-validation-12318", "mrqa_triviaqa-validation-4580", "mrqa_squad-validation-2289", "mrqa_searchqa-validation-13939", "mrqa_naturalquestions-validation-10015", "mrqa_newsqa-validation-367", "mrqa_searchqa-validation-10964", "mrqa_hotpotqa-validation-721", "mrqa_squad-validation-6185", "mrqa_searchqa-validation-1088"], "EFR": 1.0, "Overall": 0.7368576388888889}, {"timecode": 45, "before_eval_results": {"predictions": ["sports tourism", "0-0", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first weekend.", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab,", "a treadmill", "Uzbekistan.", "Piers Morgan", "Mary Phagan,", "well over two decades.", "83 eggs.", "drowning death,", "more than a million residents who have been displaced by", "9-1", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her", "\"a hooligan bereft of any personality as a human being,", "15-year-old's", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "it really like to be a new member of the world's most powerful legislature?", "participate in Iraq's government.", "The Rosie Show", "helicopters and unmanned aerial vehicles", "racial intolerance.", "\"Big Three\"", "Rolling Stone", "dogs who walk on ice in Alaska.", "Ralph Lauren,", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"The North could delay the launch if they experience problems with the weather, or within the leadership,", "\"a striking blow to due process and the rule of law.\"", "his brother, Julio Cesar Godoy Toscano,", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Lindsey Vonn", "last month's Mumbai terror attacks", "Rwanda", "cancer", "Roberto Micheletti,", "October 3,", "onto the college campus.", "200", "the aft of the vessel,", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "it was split 10-2.", "July", "December 2, 2013, and the third season concluded on October 1, 2017", "the North Atlantic Ocean", "Christopher Lloyd", "Nero", "Ethiopia", "Andes Mountains of Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Douglas Fairbanks, Jr.", "P.M.S. Blackett", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6411954365079364}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.05714285714285714, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3907", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.5625, "CSR": 0.5417798913043479, "retrieved_ids": ["mrqa_squad-train-30241", "mrqa_squad-train-55795", "mrqa_squad-train-41288", "mrqa_squad-train-40470", "mrqa_squad-train-921", "mrqa_squad-train-33850", "mrqa_squad-train-79841", "mrqa_squad-train-62439", "mrqa_squad-train-6460", "mrqa_squad-train-27871", "mrqa_squad-train-32721", "mrqa_squad-train-3768", "mrqa_squad-train-17059", "mrqa_squad-train-59739", "mrqa_squad-train-29891", "mrqa_squad-train-37531", "mrqa_newsqa-validation-2399", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5633", "mrqa_hotpotqa-validation-4791", "mrqa_naturalquestions-validation-9824", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-3554", "mrqa_newsqa-validation-125", "mrqa_hotpotqa-validation-2731", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-44", "mrqa_searchqa-validation-3644", "mrqa_newsqa-validation-268", "mrqa_triviaqa-validation-6620", "mrqa_naturalquestions-validation-3555"], "EFR": 0.9642857142857143, "Overall": 0.7298068711180125}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam", "taking a medicine that contained the banned substance cortisone.", "Dodi Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "war years,", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight.", "Ciudad Juarez,", "former U.S. secretary of state.", "Sri Lanka", "Communist", "Gainsbourg", "U.N.", "Ike", "The ACLU", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "the Taliban's", "debris", "8,", "new materials -- including ultra-high-strength steel and boron", "a president who understands the world today, the future we seek and the change we need.", "Djibouti,", "in the mouth.", "over 1000 square meters in forward deck space,", "Alfredo Astiz,", "\"He was there before Tiger Woods,", "14 years", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "EU naval force", "the highest ranking former member of Saddam Hussein's regime still at large,", "Michelle Obama", "a fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "black, red or white,", "Seoul.", "try to make life a little easier for these families by organizing the distribution of wheelchair,", "Muqtada al-Sadr", "a house party in Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$81,8709.", "Hungary", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Christmas", "clare", "n\u00famero", "Ellie Kemper", "President's Volunteer Service Award", "nursery rhyme", "the North Pole", "St. Mary's", "Holly", "Lundy"], "metric_results": {"EM": 0.5, "QA-F1": 0.6429721395754004}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727273, 0.9565217391304348, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.28571428571428575, 1.0, 0.8, 0.0, 1.0, 0.5, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-7714", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-1315", "mrqa_searchqa-validation-12477"], "SR": 0.5, "CSR": 0.5408909574468085, "retrieved_ids": ["mrqa_squad-train-54273", "mrqa_squad-train-80500", "mrqa_squad-train-53508", "mrqa_squad-train-31619", "mrqa_squad-train-69480", "mrqa_squad-train-44613", "mrqa_squad-train-33978", "mrqa_squad-train-76426", "mrqa_squad-train-74627", "mrqa_squad-train-80237", "mrqa_squad-train-62935", "mrqa_squad-train-62014", "mrqa_squad-train-14842", "mrqa_squad-train-32749", "mrqa_squad-train-20553", "mrqa_squad-train-22045", "mrqa_naturalquestions-validation-10077", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-1916", "mrqa_naturalquestions-validation-3381", "mrqa_hotpotqa-validation-5199", "mrqa_newsqa-validation-240", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-13434", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-5", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-3348", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-409"], "EFR": 1.0, "Overall": 0.7367719414893618}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\" Body Works\"", "\"a striking blow to due process and the rule of law.\"", "make the new truck safer,", "200", "Alexey Pajitnov,", "1959.", "a lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "\"The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "a mammoth", "$3 billion,", "Les Bleus", "Samoa", "more than 100.", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\"", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "Portuguese water dog", "Long Island", "arrested, arraigned and jailed,", "Damon Bankston", "Authorities in Fayetteville, North Carolina,", "clogs", "\"The Rough Guide to Climate Change\"", "guard in the jails of Washington, D.C.", "Ventures", "energy-efficient light-emitting diodes", "Deputy Treasury Secretary", "an Italian and six Africans", "supply vessel Damon Bankston", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "London and Buenos Aires", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "art fair,", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday", "she's in love,", "Miguel Cotto", "Zac Efron", "US Airways Flight 1549", "269,000", "rearview mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "National Football League", "Turkey", "czarevitch", "auk", "Tennessee", "from 1993 to 1996", "Minette Walters", "Tom Sennett", "Linda's", "photoelectric", "April 13, 2018"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6867321608130432}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.888888888888889, 1.0, 0.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.4, 0.8, 0.0, 0.7555555555555554, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.22222222222222218, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-3217", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_naturalquestions-validation-177"], "SR": 0.5625, "CSR": 0.5413411458333333, "retrieved_ids": ["mrqa_squad-train-30196", "mrqa_squad-train-85606", "mrqa_squad-train-125", "mrqa_squad-train-30989", "mrqa_squad-train-7407", "mrqa_squad-train-56225", "mrqa_squad-train-60843", "mrqa_squad-train-14476", "mrqa_squad-train-68219", "mrqa_squad-train-44484", "mrqa_squad-train-73731", "mrqa_squad-train-3795", "mrqa_squad-train-79149", "mrqa_squad-train-18215", "mrqa_squad-train-45138", "mrqa_squad-train-33501", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-464", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-7233", "mrqa_naturalquestions-validation-922", "mrqa_hotpotqa-validation-4271", "mrqa_naturalquestions-validation-4552", "mrqa_searchqa-validation-9553", "mrqa_searchqa-validation-4624", "mrqa_newsqa-validation-3903", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1606", "mrqa_searchqa-validation-11427", "mrqa_newsqa-validation-1382"], "EFR": 0.9285714285714286, "Overall": 0.7225762648809524}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "does not believe North Korea intends to launch a long-range missile in the near future,", "Lindsey Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "a traditional form of lounge music that flourished in 1940's Japan.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "peppermint oil, soluble fiber, and antispasmodic", "fake his own death", "David Beckham", "Aryan Airlines Flight 1625", "pizza, the other for the drug ketamine.", "Kris Allen,", "death", "Sunday", "Haiti's", "\"He tried", "1981,", "in terms of the country's most-wanted list, Mejia Munera was one of Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Bob Bogle,", "the FDA is silent on our request that it also send a warning letter to physicians clearly describing possible adverse reactions, such as tendon pain, so that patients can be switched to alternative treatments before tendons rupture,\"", "Iran test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "Navy F-14 fighter pilot", "Form Design Center.", "it really like to be a new member of the world's most powerful legislature?", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "three", "$250,000", "WBO welterweight title from Miguel Cotto", "Courtney Love,", "Hu Jintao", "Bahrain", "54", "\"Slumdog Millionaire,\"", "murder in the beating death of a company boss who fired them.", "African National Congress", "$89", "Russell", "maintain an \"aesthetic environment\" and ensure public safety,", "30.3 %", "the Behavioral Analysis Unit", "BeBe Winans", "Pickwick", "Claire Goose", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "lethal", "Zenith Jones Brown", "Fortune And Men's", "Shep Meyers"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6960963031298668}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.8695652173913044, 1.0, 0.75, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 1.0, 0.21052631578947367, 0.5, 0.0, 1.0, 0.33333333333333337, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07407407407407407, 0.8, 0.2222222222222222, 1.0, 0.0909090909090909, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-212", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_searchqa-validation-11020", "mrqa_searchqa-validation-11614"], "SR": 0.546875, "CSR": 0.5414540816326531, "retrieved_ids": ["mrqa_squad-train-62554", "mrqa_squad-train-23649", "mrqa_squad-train-41579", "mrqa_squad-train-42681", "mrqa_squad-train-64998", "mrqa_squad-train-64178", "mrqa_squad-train-58196", "mrqa_squad-train-80705", "mrqa_squad-train-13025", "mrqa_squad-train-40442", "mrqa_squad-train-83690", "mrqa_squad-train-14735", "mrqa_squad-train-3415", "mrqa_squad-train-69196", "mrqa_squad-train-65815", "mrqa_squad-train-6801", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-3063", "mrqa_searchqa-validation-12974", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1392", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-2871", "mrqa_newsqa-validation-3502", "mrqa_searchqa-validation-2252", "mrqa_naturalquestions-validation-6857", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-2836", "mrqa_triviaqa-validation-4742", "mrqa_searchqa-validation-1162"], "EFR": 0.9655172413793104, "Overall": 0.7299880146023927}, {"timecode": 49, "before_eval_results": {"predictions": ["a delegation of American Muslim and Christian leaders", "\"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.\"", "35,000.", "curfew", "Martin Luther King Jr.", "Four", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "in Japan", "aboard a freighter seized by pirates off east  Africa", "Haiti", "the world's poorest children.", "a lump in Henry's nether regions", "Mark Hampton", "\"It was a wrong thing to say, something that we both acknowledge,\"", "racially-tinged remark made by his former caddy,", "David McKenzie", "\"If we're going to revise our policies here, we need to make it so for all the camps,\"", "Daniel Radcliffe", "The Da Vinci Code", "sports cars", "The Da Vinci Code", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "Rwanda declared a cease-fire", "$60 million", "some of the members that have purchased our publications and does not comprise a list of individuals or entities that have a relationship with Stratfor beyond their purchase of our subscription-based publications,\"", "Alice Horton", "At least 33 people", "in the Carrousel du Louvre,", "137", "bartering", "Austin Wuennenberg,", "wanted to change the music on the CD player and the 34-year-old McGee said the football star had acted aggressively in trying to grab the device.", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "teenage", "almost 100 vessels", "Matthew Fisher,", "to the southern city of Naples", "Cyber devices are innovating at an extremely rapid rate and hold tremendous promise for the future,", "Saturday", "Both women", "Andy Serkis", "in the very late 1980s", "in Davos", "Malm\u00f6", "Richard Attenborough", "eclipse", "\"novel with a key\"", "London", "Oklahoma", "Kevin Nealon", "the Roman Catholic Church", "Tammy Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.59375, "QA-F1": 0.710567860958486}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.625, 0.4444444444444445, 1.0, 0.09523809523809523, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4, 0.0, 0.8, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3953", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2370", "mrqa_naturalquestions-validation-6564", "mrqa_searchqa-validation-1891"], "SR": 0.59375, "CSR": 0.5425, "retrieved_ids": ["mrqa_squad-train-79623", "mrqa_squad-train-31986", "mrqa_squad-train-81825", "mrqa_squad-train-49902", "mrqa_squad-train-28275", "mrqa_squad-train-71604", "mrqa_squad-train-10391", "mrqa_squad-train-69652", "mrqa_squad-train-83281", "mrqa_squad-train-13770", "mrqa_squad-train-34271", "mrqa_squad-train-13081", "mrqa_squad-train-84385", "mrqa_squad-train-63790", "mrqa_squad-train-24752", "mrqa_squad-train-21093", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-115", "mrqa_searchqa-validation-7396", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-6506", "mrqa_hotpotqa-validation-5707", "mrqa_searchqa-validation-7229", "mrqa_squad-validation-7547", "mrqa_naturalquestions-validation-10265", "mrqa_newsqa-validation-1443", "mrqa_searchqa-validation-8756", "mrqa_newsqa-validation-3967", "mrqa_hotpotqa-validation-2379", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-9687", "mrqa_naturalquestions-validation-5180"], "EFR": 1.0, "Overall": 0.73709375}, {"timecode": 50, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.884765625, "KG": 0.503125, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria", "11", "July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Shenzhen in southern China.", "\"revolution of values\"", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "second", "Islamabad", "March 8", "female soldier, missing", "remote highway in Michoacan state,", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "Carlotta Walls LaNier", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "10 municipal police officers", "$15 billion in 2008 and is projected to grow by 10 percent,", "12", "Arabic, French and English,", "40", "Johannesburg", "L'Aquila", "\"The Cycle of Life,\"", "North Korea,", "at least 27", "racially-tinged remark", "Amsterdam,", "burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "George Washington", "Madonna", "\"We are doing our best to dissuade the North Koreans from going forward,", "posting a $1,725 bail,", "Cal Ripken Jr.", "78,000 parents", "Apple Inc.", "London's", "U.S. program to assassinate terrorists in Iraq.", "martial arts,", "Jennifer Arnold and husband Bill Klein,", "Operation Crank Call,\"", "Orwell", "Guwahati", "the winter solstice", "Frenchman", "sheep", "daisy", "1853", "musical research", "1902", "Folly", "All's Well That ends Well", "a trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7163289835164836}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9743589743589743, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.8181818181818181, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1384", "mrqa_triviaqa-validation-7329", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-14319"], "SR": 0.59375, "CSR": 0.5435049019607843, "retrieved_ids": ["mrqa_squad-train-83213", "mrqa_squad-train-22584", "mrqa_squad-train-23126", "mrqa_squad-train-70836", "mrqa_squad-train-3193", "mrqa_squad-train-33203", "mrqa_squad-train-25079", "mrqa_squad-train-57887", "mrqa_squad-train-39911", "mrqa_squad-train-81049", "mrqa_squad-train-41440", "mrqa_squad-train-85469", "mrqa_squad-train-51799", "mrqa_squad-train-47258", "mrqa_squad-train-49939", "mrqa_squad-train-76580", "mrqa_searchqa-validation-1784", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-5970", "mrqa_newsqa-validation-1968", "mrqa_squad-validation-3165", "mrqa_triviaqa-validation-3699", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-2315", "mrqa_searchqa-validation-8845", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-916", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-2083"], "EFR": 1.0, "Overall": 0.7440916053921569}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan Defense Minister Yusuf Haji", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "more than 1.2 million", "Arizona", "Kenyan and Somali governments", "meter reader", "Diego Maradona", "London", "near Grand Ronde, Oregon.", "in rural Tennessee.", "Fakih", "as many as 50,000", "14", "Former Mobile County Circuit Judge", "Monday,", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook tablet", "Vicente Carrillo Leyva,", "Dolgorsuren Dagvadorj,", "they are \"still trying to absorb the impact of this week's stunning events.\"", "41,", "Anil Kapoor", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "the pirates", "the estate", "Isabella", "March 22,", "Hamas,", "about 3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "\"Draquila", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "more than 90 percent", "Charlton Heston", "administrative supervision over all courts and the personnel thereof", "l\u00ea L\u1ee3i", "a Nazi endeavor to overcome their enemies' military strength through force of will", "The Landlord's Game", "News Corporation", "Kentucky, Virginia, and Tennessee", "1999", "Brazil", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.625, "QA-F1": 0.738401411701044}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true], "QA-F1": [0.25, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8571428571428571, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-631", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-2485", "mrqa_hotpotqa-validation-2623", "mrqa_searchqa-validation-12036"], "SR": 0.625, "CSR": 0.5450721153846154, "retrieved_ids": ["mrqa_squad-train-62334", "mrqa_squad-train-41744", "mrqa_squad-train-42758", "mrqa_squad-train-51534", "mrqa_squad-train-63969", "mrqa_squad-train-55371", "mrqa_squad-train-52608", "mrqa_squad-train-27616", "mrqa_squad-train-69891", "mrqa_squad-train-17590", "mrqa_squad-train-42407", "mrqa_squad-train-31604", "mrqa_squad-train-63282", "mrqa_squad-train-10719", "mrqa_squad-train-27910", "mrqa_squad-train-44451", "mrqa_newsqa-validation-44", "mrqa_squad-validation-2372", "mrqa_hotpotqa-validation-3902", "mrqa_searchqa-validation-33", "mrqa_newsqa-validation-2799", "mrqa_naturalquestions-validation-7021", "mrqa_newsqa-validation-876", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-3735", "mrqa_hotpotqa-validation-3417", "mrqa_squad-validation-1456", "mrqa_newsqa-validation-3860", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-5298", "mrqa_triviaqa-validation-2251", "mrqa_newsqa-validation-2854"], "EFR": 1.0, "Overall": 0.7444050480769231}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "pregnant soldier", "St. Louis,", "Honduran President Jose Manuel Zelaya", "mother.", "unemployment benefits", "No. 4", "Pfc. Bowe Bergdahl", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male.", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any person who has been abused by any priest of the Diocese of Cloyne during my time as bishop or at any time,\"", "United", "planned attacks", "\"falling space debris,\"", "Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Kingman Regional Medical Center,", "bronze medal in the women's figure skating final,", "Long Island", "5,600", "in 2007, only 9 percent of Turks polled by the Pew Research Center held favorable views of America,", "Sharon Bialek", "prisoners", "two", "\"We get a signal prior to violence,\"", "Muslim", "The three-judge panel was unanimous in its decision, citing evidentiary errors that likely influenced the outcome of the trial.", "Evans", "near the Somali coast to use extreme caution because of the recent pirate attacks.", "used-luxury", "2008,", "killing rampage.", "\"Twilight\"", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "Obama and McCain camps", "flooding was so fast that the thing flipped over,\"", "five", "Trevor Rees", "Dubai", "June 6, 1944,", "the \"surge\" strategy he implemented last year.", "the schools are fighting just as hard to lure in top applicants.", "Greek name Berenice, \u0392\u03b5\u03c1\u03b5\u03bd\u03af\u03ba\u03b7", "reproductive system", "Aidan Gallagher", "Rebecca Adlington", "Buckinghamshire", "15", "Consigliere", "2007", "The entity", "The Suite Life of Zack & Cody", "erotic thriller", "launch one ship", "northern Europe"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5828569173881674}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.23999999999999996, 0.4444444444444445, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.53125, "CSR": 0.5448113207547169, "retrieved_ids": ["mrqa_squad-train-28717", "mrqa_squad-train-53513", "mrqa_squad-train-83978", "mrqa_squad-train-75633", "mrqa_squad-train-44912", "mrqa_squad-train-82745", "mrqa_squad-train-42508", "mrqa_squad-train-5916", "mrqa_squad-train-52291", "mrqa_squad-train-71410", "mrqa_squad-train-72397", "mrqa_squad-train-81799", "mrqa_squad-train-16015", "mrqa_squad-train-76474", "mrqa_squad-train-31827", "mrqa_squad-train-72269", "mrqa_newsqa-validation-2209", "mrqa_searchqa-validation-10017", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2392", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-916", "mrqa_squad-validation-3456", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-1654", "mrqa_squad-validation-7574", "mrqa_triviaqa-validation-5038", "mrqa_newsqa-validation-2371", "mrqa_triviaqa-validation-6827", "mrqa_newsqa-validation-1458", "mrqa_triviaqa-validation-7383", "mrqa_newsqa-validation-1062"], "EFR": 1.0, "Overall": 0.7443528891509434}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lance Cpl. Maria Lauterbach and her fetus", "The 29-year-old was the only one of the seven defendants in the case to be cleared after an incident which was described by judge Henry Globe as an \"explosion of violence.\"", "Argentine", "Ferraris, a Lamborghini and an Acura NSX", "Laurean killed Lauterbach", "1983", "the simple puzzle video game,", "\"Dancing With the Stars\"", "Time's Most Influential People", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "The plane famously landed with 155 people aboard in the frigid river waters by Capt. Chesley \"Sully\" Sullenberger", "he failed to return home,", "\"Vaughn,\"", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday's strike", "help nations trapped by hunger and extreme poverty,", "$10 billion", "prosecutors of buckling under pressure from the ruling party.", "April 22.", "Mitt Romney", "twice.", "seeking help", "Mary Phagan,", "pesos", "judge", "Herman Cain,", "60 euros", "Barack Obama", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "an independent homeland since 1983.", "Nafees A. Syed,", "Sunday", "a share in the royalties for the tune.", "drug cartels", "in a canyon in the path of the blaze Thursday.", "number of calls,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "Professor of phonetics Henry Higgins", "shoes", "herbert lom", "Battle of Prome", "east\u2013west United States", "Bruce Almighty", "a chance/community chest card", "the American League's", "Tom Osborne", "Kwame Nkrumah"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6764945652173913}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1813", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037"], "SR": 0.59375, "CSR": 0.5457175925925926, "retrieved_ids": ["mrqa_squad-train-60167", "mrqa_squad-train-76918", "mrqa_squad-train-80936", "mrqa_squad-train-11385", "mrqa_squad-train-33196", "mrqa_squad-train-49905", "mrqa_squad-train-85046", "mrqa_squad-train-40914", "mrqa_squad-train-44667", "mrqa_squad-train-25676", "mrqa_squad-train-16582", "mrqa_squad-train-51640", "mrqa_squad-train-42976", "mrqa_squad-train-16881", "mrqa_squad-train-54812", "mrqa_squad-train-56425", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-1895", "mrqa_searchqa-validation-1863", "mrqa_squad-validation-664", "mrqa_newsqa-validation-2753", "mrqa_naturalquestions-validation-4079", "mrqa_newsqa-validation-2413", "mrqa_naturalquestions-validation-1415", "mrqa_hotpotqa-validation-1127", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-11770", "mrqa_newsqa-validation-2519", "mrqa_searchqa-validation-14783"], "EFR": 1.0, "Overall": 0.7445341435185184}, {"timecode": 54, "before_eval_results": {"predictions": ["$50", "diabetes and hypertension,", "Jet Republic,", "many different", "at least 27", "last week,", "Peru's", "Joan Rivers", "\"Watchmen\"", "sovereignty over them.", "NATO", "Bangladesh", "as", "a complicated and deeply flawed man", "The Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "The e-mails", "would slow economic growth with higher taxes.", "voluntary manslaughter", "dancing", "South Africa", "The noose incident", "poorest children.", "propofol,", "Catholic church sex abuse scandal,", "head injury.", "down a steep embankment in the Angeles National Forest", "Marxist guerrillas", "1918-1919.", "Rwanda", "Osama bin Laden's sons", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "58 minutes.", "see my kids graduate from this school district.", "CNN", "Jobs", "bribing other wrestlers to lose bouts,", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40 ships", "a five-year weekend residency", "President Obama", "Tuesday", "henry higgins", "The UNHCR recommended against granting asylum,", "Kenyan forces", "Pop star Michael Jackson", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "2017", "The 2018 Major League Baseball season began on March 29, 2018", "quartz or feldspar", "Kursk", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club", "London Review of Books", "Eudora Welty", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6404345654345653}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-655", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.5625, "CSR": 0.5460227272727273, "retrieved_ids": ["mrqa_squad-train-84434", "mrqa_squad-train-40490", "mrqa_squad-train-24766", "mrqa_squad-train-29640", "mrqa_squad-train-4259", "mrqa_squad-train-51965", "mrqa_squad-train-21536", "mrqa_squad-train-34211", "mrqa_squad-train-18067", "mrqa_squad-train-46141", "mrqa_squad-train-45955", "mrqa_squad-train-49220", "mrqa_squad-train-43916", "mrqa_squad-train-35347", "mrqa_squad-train-23803", "mrqa_squad-train-31325", "mrqa_searchqa-validation-14868", "mrqa_newsqa-validation-3194", "mrqa_searchqa-validation-3222", "mrqa_triviaqa-validation-5289", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2680", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-3229", "mrqa_searchqa-validation-9831", "mrqa_squad-validation-2372", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-10823", "mrqa_triviaqa-validation-3725", "mrqa_newsqa-validation-125"], "EFR": 1.0, "Overall": 0.7445951704545454}, {"timecode": 55, "before_eval_results": {"predictions": ["his weekend arrest on gun charges,", "without the", "Mexico", "\"I know England does not have the infrastructure to remove snow like we do in Minnesota,\"", "five", "customers are lining up for vitamin injections that promise", "a thorough understanding of the dogs' needs,", "writing and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "\"We want to reset our relationship and so we will do it together.'\"", "Preah Vihear temple", "general astonishment", "June 6, 1944,", "a lightning strike", "2-1", "Sen. Barack Obama", "money or other discreet aid", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Tiger Woods", "200 human bodies at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "Elisabeth", "21", "The paper said the trip had caused fury among some in the military who saw", "the 3rd District of Utah.", "Bridgestone Invitational in Ohio,", "organizing the distribution of wheelchairs,", "shock,", "\"She was focused so much on learning that she didn't notice,\"", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "the piracy incident", "Alaska or Hawaii.", "Robert Park", "in the neighboring country of Djibouti,", "The National Cancer Institute", "Six", "Bahrain.", "delivers a big speech", "Facebook and Google,", "Sheikh Sharif Sheikh Ahmed", "2006,", "18th", "March 24,", "his wife,", "a senior at Stetson University studying computer science.", "Saturday,", "NATO fighters", "\"Empire of the Sun,\"", "New Zealand", "a model of sustainability.", "Mutt Lange", "summer", "79", "neoclassic", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort in Lake Buena Vista, Florida", "The Night", "mass", "a snout beetle", "siegfried"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6108865093240093}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-2810", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-7008", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.5625, "CSR": 0.5463169642857143, "retrieved_ids": ["mrqa_squad-train-34305", "mrqa_squad-train-48025", "mrqa_squad-train-5635", "mrqa_squad-train-61977", "mrqa_squad-train-41543", "mrqa_squad-train-49303", "mrqa_squad-train-82746", "mrqa_squad-train-36772", "mrqa_squad-train-40837", "mrqa_squad-train-83852", "mrqa_squad-train-50525", "mrqa_squad-train-37276", "mrqa_squad-train-28042", "mrqa_squad-train-27241", "mrqa_squad-train-74379", "mrqa_squad-train-28434", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-373", "mrqa_searchqa-validation-8291", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-226", "mrqa_newsqa-validation-2461", "mrqa_squad-validation-639", "mrqa_triviaqa-validation-3908", "mrqa_newsqa-validation-3502", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-2617", "mrqa_newsqa-validation-450", "mrqa_searchqa-validation-2394", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-2473"], "EFR": 0.9642857142857143, "Overall": 0.7375111607142857}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "those traveling near the Somali coast", "\"A Child's Garden of Verses,\"", "billboards with an image of the burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "immunotherapy", "Rod Blagojevich,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Dodi Fayed,", "the most-wanted man in the world", "Carrousel du Louvre,", "Ansar ul Islam.", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "Washington State's", "shows the world that you love the environment and hate using fuel,\"", "The apartment building collapsed together with two other buildings on March 3.", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Milan", "strife in Somalia,", "cancerous tumor.", "provided Syria and Iraq 500 cubic meters of water", "Abdullah Gul,", "alcohol and drug abuse", "11th year in a row.", "the journalists and the flight crew will be freed,", "Gov. Rod Blagojevich", "national telephone", "Allred", "the AR-15 and two other rifles and left the cabin.", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "was a city of romance, of incredible architecture and history.", "gasoline", "Santaquin City, Utah,", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "the fighters", "Friday night", "Celtic people living in northern Asia Minor", "diastema", "a hollow plastic sphere, approximately 3 cm in diameter ( similar in appearance to table tennis ball, but smaller ) with at least one small hole and a seam", "cryonics", "Cambridge", "Mercury", "13 October 1958", "bassline", "omnisexuality", "the Invisible Man", "Zachary Taylor", "Battlestar Galactica", "Marilyn Monroe"], "metric_results": {"EM": 0.625, "QA-F1": 0.7523330201548323}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.07142857142857142, 0.9411764705882353, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2727272727272727, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.9473684210526316, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0625, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_hotpotqa-validation-2826"], "SR": 0.625, "CSR": 0.5476973684210527, "retrieved_ids": ["mrqa_squad-train-49787", "mrqa_squad-train-24210", "mrqa_squad-train-79496", "mrqa_squad-train-66470", "mrqa_squad-train-3408", "mrqa_squad-train-61536", "mrqa_squad-train-84605", "mrqa_squad-train-40289", "mrqa_squad-train-58604", "mrqa_squad-train-26053", "mrqa_squad-train-16993", "mrqa_squad-train-61741", "mrqa_squad-train-44167", "mrqa_squad-train-50296", "mrqa_squad-train-67586", "mrqa_squad-train-6148", "mrqa_hotpotqa-validation-3806", "mrqa_triviaqa-validation-5681", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-426", "mrqa_searchqa-validation-14398", "mrqa_triviaqa-validation-2363", "mrqa_squad-validation-2976", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1420", "mrqa_squad-validation-5465", "mrqa_naturalquestions-validation-1930", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-5633"], "EFR": 0.9583333333333334, "Overall": 0.7365967653508771}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "Mad Men", "5,600", "Microsoft.", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "Alina Cho", "did not speak", "\"Draquila", "al Qaeda,", "U.S. Chamber of Commerce", "U.S. President-elect Barack Obama", "U.N. Security Council resolution in 2006", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "an acid attack", "Congress", "southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "Petra Nemcova", "South Africa", "Somali", "returning combat veterans could be recruited by right-wing extremist groups.", "opposition supporters in Libreville, Gabon.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "the iPods", "a gym", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "Cologne", "free milk.", "tennis", "No. 1 slot", "Gov. Jan Brewer.", "remote part of northwestern Montana", "securities", "$150 billion", "Gestaltism", "Michael Crawford", "the beginning", "Coconut shy", "Fenn Street School", "the inner ear", "Australian", "Argentinian", "TOSLINK", "rap", "inducere", "Harvard", "129,007"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7264320710991907}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5217391304347826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1522", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174"], "SR": 0.65625, "CSR": 0.5495689655172413, "retrieved_ids": ["mrqa_squad-train-58962", "mrqa_squad-train-44008", "mrqa_squad-train-80695", "mrqa_squad-train-31318", "mrqa_squad-train-36624", "mrqa_squad-train-85085", "mrqa_squad-train-24898", "mrqa_squad-train-28937", "mrqa_squad-train-81530", "mrqa_squad-train-53228", "mrqa_squad-train-1478", "mrqa_squad-train-78742", "mrqa_squad-train-67429", "mrqa_squad-train-83667", "mrqa_squad-train-86289", "mrqa_squad-train-20943", "mrqa_newsqa-validation-3301", "mrqa_triviaqa-validation-115", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1458", "mrqa_triviaqa-validation-3725", "mrqa_newsqa-validation-3062", "mrqa_searchqa-validation-10116", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-2983", "mrqa_triviaqa-validation-2250", "mrqa_hotpotqa-validation-1239", "mrqa_triviaqa-validation-2251", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3029"], "EFR": 1.0, "Overall": 0.7453044181034482}, {"timecode": 58, "before_eval_results": {"predictions": ["Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "The painting shows Jackson sitting in Renaissance-era clothes and holding a book.", "\u00a320 million ($41.1 million)", "40 militants and six Pakistan soldiers", "5 season", "Arthur E. Morgan III,", "Jason Chaffetz", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "a bronze medal in the women's figure skating final,", "No 4,", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"a hooligan bereft of any personality as a human being,", "President Obama.", "Jacob Zuma,", "1937,", "freezing gasoline prices for the rest of the year and lowering natural gas prices by 10 percent.", "18", "the Southeast,", "\"Up,\"", "people who want, or need, to \"move down\" from the new-car market because a new model is simply out of their reach.", "work on six giant pumpkins, specially delivered from nearby Half Moon Bay (some weighing well over 1,000 pounds).", "school,", "a motor scooter", "learn in safer surroundings.", "$50", "J.Crew", "$106.5 million", "Nearly eight in 10", "4,000 credit cards and the company's \"private client\" list,", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\"", "\"black box\" label warning", "drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\" the U.N. drug chief said.", "cancer awareness", "Virgin America", "her overriding priority is to protect her children.", "\"Oh you're, you're really nice,\" but... on the other side of me there are other Dereks that are angry, and like I said before, have a little bit of issues.", "Kenyan and Somali", "opium trade", "Wednesday", "was a thief trying to steal people's money Friday amid the chaos from last week's earthquake.", "Africa", "the most-wanted man in the world", "left - sided heart failure", "After Shawn's kidnapping, Juliet goes to his apartment with Gus to search for clues", "Devastator, who destroys one of the pyramids to reveal the Sun Harvester inside, before he is killed by a destroyer's railgun called in by Simmons", "Madness", "Jelly Roll Morton", "vice-admiral", "George Lawrence Mikan, Jr. (June 18, 1924 \u2013 June 1, 2005), nicknamed Mr. Basketball,", "Kait Parker", "Centre-du-Qu\u00e9bec area", "Nguyen", "doughboy", "United We Stand, Divided We Fall", "professor henry Higgins"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6708721532091098}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.08695652173913045, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.2, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2193", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951"], "SR": 0.5625, "CSR": 0.5497881355932204, "retrieved_ids": ["mrqa_squad-train-67473", "mrqa_squad-train-73710", "mrqa_squad-train-84433", "mrqa_squad-train-65109", "mrqa_squad-train-1044", "mrqa_squad-train-77705", "mrqa_squad-train-26086", "mrqa_squad-train-68143", "mrqa_squad-train-43503", "mrqa_squad-train-44040", "mrqa_squad-train-57936", "mrqa_squad-train-12420", "mrqa_squad-train-60445", "mrqa_squad-train-23501", "mrqa_squad-train-72705", "mrqa_squad-train-68342", "mrqa_newsqa-validation-4184", "mrqa_triviaqa-validation-3468", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2617", "mrqa_triviaqa-validation-2858", "mrqa_searchqa-validation-409", "mrqa_triviaqa-validation-3087", "mrqa_newsqa-validation-2168", "mrqa_searchqa-validation-14366", "mrqa_squad-validation-2919", "mrqa_searchqa-validation-2260", "mrqa_triviaqa-validation-5659", "mrqa_newsqa-validation-1968", "mrqa_searchqa-validation-10308"], "EFR": 0.9642857142857143, "Overall": 0.738205394975787}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings", "1913.", "$40 and a loaf of bread.", "14-day", "U Win Tin,", "543", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "11 healthy eggs", "nine", "64,", "The skull", "at least two and a half hours.", "shark River Park in Monmouth County", "improve the environment", "gift to the Obama girls from Sen. Ted Kennedy.", "\"I miss your beautiful face and voice,\"", "More than 15,000", "0300", "Muslim countries,", "Piers Morgan Tonight", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty", "sumo wrestling", "10 below", "has a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall notices", "Roy", "VBS.TV", "Kerstin", "Marxist guerrillas", "Greeley, Colorado,", "\"Five victims (clockwise from top left): Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "opened considerably higher", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "Muslim north of Sudan", "at least 18 federal agents and two soldiers", "Bahrain", "33", "Kenneth Cole", "the Devastator", "Brazil", "Theodore Roosevelt", "vice-admiral", "Braves", "the Big Bopper", "Greek-American", "feats of exploration", "uncle", "Monarch", "the American Repertory Theater", "Truman", "Briton Allan McNish"], "metric_results": {"EM": 0.671875, "QA-F1": 0.739381105006105}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2817", "mrqa_triviaqa-validation-105", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.671875, "CSR": 0.5518229166666666, "retrieved_ids": ["mrqa_squad-train-66795", "mrqa_squad-train-1993", "mrqa_squad-train-25131", "mrqa_squad-train-14799", "mrqa_squad-train-18076", "mrqa_squad-train-18258", "mrqa_squad-train-74055", "mrqa_squad-train-13571", "mrqa_squad-train-25573", "mrqa_squad-train-31541", "mrqa_squad-train-58279", "mrqa_squad-train-24156", "mrqa_squad-train-32975", "mrqa_squad-train-45155", "mrqa_squad-train-20733", "mrqa_squad-train-85552", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-396", "mrqa_triviaqa-validation-2972", "mrqa_newsqa-validation-2735", "mrqa_naturalquestions-validation-6289", "mrqa_triviaqa-validation-5425", "mrqa_squad-validation-664", "mrqa_newsqa-validation-1968", "mrqa_hotpotqa-validation-4271", "mrqa_searchqa-validation-14838", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-2779", "mrqa_triviaqa-validation-5052", "mrqa_naturalquestions-validation-8591"], "EFR": 1.0, "Overall": 0.7457552083333333}, {"timecode": 60, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.861328125, "KG": 0.49765625, "before_eval_results": {"predictions": ["183", "Carson", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"Even though I moved a slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "Iran's", "in the Philippines", "Sixteen", "Obama", "Matthew Chance", "34", "five victims by helicopter,", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "the Russian air company Vertikal-T,", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Michael Brewer,", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "authorizing killings and kidnappings by paramilitary death squads.", "Ozzy Osbourne", "it is not just $3 billion of new money into the economy.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "\"I pretty much asked me if she was depressed,... how she acted around the baby, if she seemed stressed out,\"", "Obama and McCain camps", "Africa", "in Fayetteville, North Carolina,", "the only goal of the game", "French", "100,000 workers", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "1991-1993,", "a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "Operation Crank Call,\"", "Islamabad", "a man's lifeless, naked body", "Kris Allen,", "ConAgra Foods plant", "Lalo Schifrin", "April 17, 1982", "Billy Idol", "false positives", "Theresa May", "every ten years", "five", "The Dragon", "1994", "a magnolia", "1st", "Jupiter", "mural"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7445071647614973}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975609756097561, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0, 0.4210526315789474, 0.0, 0.0, 1.0, 0.2105263157894737, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-3930", "mrqa_triviaqa-validation-7704", "mrqa_hotpotqa-validation-1812", "mrqa_searchqa-validation-16357"], "SR": 0.671875, "CSR": 0.5537909836065573, "retrieved_ids": ["mrqa_squad-train-9184", "mrqa_squad-train-40159", "mrqa_squad-train-70136", "mrqa_squad-train-66784", "mrqa_squad-train-63450", "mrqa_squad-train-22795", "mrqa_squad-train-26760", "mrqa_squad-train-24471", "mrqa_squad-train-81867", "mrqa_squad-train-52880", "mrqa_squad-train-47043", "mrqa_squad-train-21233", "mrqa_squad-train-45415", "mrqa_squad-train-73312", "mrqa_squad-train-37532", "mrqa_squad-train-71085", "mrqa_newsqa-validation-98", "mrqa_hotpotqa-validation-2237", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_newsqa-validation-2983", "mrqa_naturalquestions-validation-2414", "mrqa_squad-validation-3130", "mrqa_triviaqa-validation-495", "mrqa_searchqa-validation-9687", "mrqa_triviaqa-validation-2250", "mrqa_searchqa-validation-1747", "mrqa_squad-validation-3456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-14425", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-848"], "EFR": 0.9047619047619048, "Overall": 0.7213199526736924}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "South Dakota State Penitentiary", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "The leftist guerilla group, which goes by its Spanish acronym FARC, holds about 750 hostages in the jungles of Colombia", "a baseball bat", "six", "a book.", "Venezuela", "Kerstin", "$1.45 billion", "Iranian consulate,", "Apple Inc.", "Janet Napolitano", "Malawi,", "Daniel Radcliffe", "In the attacks that started in April 1994, Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "The Sopranos", "artificial intelligence.", "sculptures", "Shanghai mayor", "the foyer of the BBC building in Glasgow, Scotland", "take up their tour buses, as well as their road crew and traveling with their own equipment.", "an engineering and construction company", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "The TNT series resume Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "9:20 p.m. ET Wednesday.", "Some truly mind-blowing structures", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three out of four questioned say that things are going well for them personally.", "The island's dining scene", "carving a pumpkin.", "prisoners at the South Dakota State Penitentiary", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "he spent the first night in his car.", "businesses hiring veterans as well as job training for all service members leaving the military.", "The port won't be back for a while. Roads have been split apart and buckled, fences have fallen over.", "UK", "bipartisan", "has a thicker consistency and a deeper flavour than sauce", "skeletal muscle and the brain", "1985", "Dublin", "Goldfinger", "Lidice", "Columbia", "Wynonna Judd", "youngest publicly documented", "the Italian occupation of Libya", "a great horned type", "Canada", "Bolton"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6517113095238096}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-6242", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.5625, "CSR": 0.5539314516129032, "retrieved_ids": ["mrqa_squad-train-39175", "mrqa_squad-train-72556", "mrqa_squad-train-274", "mrqa_squad-train-26543", "mrqa_squad-train-63243", "mrqa_squad-train-22909", "mrqa_squad-train-18041", "mrqa_squad-train-56352", "mrqa_squad-train-1431", "mrqa_squad-train-63211", "mrqa_squad-train-56001", "mrqa_squad-train-21787", "mrqa_squad-train-61585", "mrqa_squad-train-85559", "mrqa_squad-train-59886", "mrqa_squad-train-12504", "mrqa_hotpotqa-validation-4069", "mrqa_squad-validation-9640", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-4888", "mrqa_newsqa-validation-1138", "mrqa_searchqa-validation-11614", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1483", "mrqa_squad-validation-1232", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-467", "mrqa_naturalquestions-validation-3342"], "EFR": 0.9642857142857143, "Overall": 0.7332528081797235}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "sperm and ova", "Michael Buffer", "greater than 14", "16,801 students", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "The first Old World equid fossil was found in the gypsum quarries in Montmartre, Paris, in the 1820s", "the Tigris and Euphrates rivers", "third", "Andrew Garfield", "The Fixx", "The acid plays a key role in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "7.6 mm", "March 8, 2018", "all - star game", "George Harrison", "Kristy Swanson", "nominally a civil service post", "mathematical model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "the coiled tube on the back of each testicle where sperm matures", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "multiple copies of three different types of gene segments", "Universal Pictures", "Tbilisi", "people raced modified cars on dry lake beds northeast of Los Angeles under the rules of the Southern California Timing Association ( SCTA )", "tolled ( quota ) highways", "the federal government", "outside cultivated areas", "Buffalo Bill", "IIII", "extremely slowly in the absence of a catalyst", "The Maginot Line", "the new government", "James Watson and Francis Crick", "the person compelled to pay for reformist programs", "card verification data", "unbiased relationships between exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Sondheim", "the Northern Territory", "Laura Robson", "Afghanistan", "Todd McFarlane", "Massachusetts", "one", "\"significant skeletal remains\" consistent with those of a small child on the outer perimeter of the", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Wally", "Syrup", "the mouth", "locoweed", "December 1974"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6049005681818181}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 0.125, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.9818181818181818, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-11479"], "SR": 0.515625, "CSR": 0.5533234126984127, "retrieved_ids": ["mrqa_squad-train-34190", "mrqa_squad-train-12786", "mrqa_squad-train-68937", "mrqa_squad-train-74206", "mrqa_squad-train-84015", "mrqa_squad-train-34750", "mrqa_squad-train-81262", "mrqa_squad-train-46303", "mrqa_squad-train-44597", "mrqa_squad-train-48791", "mrqa_squad-train-28220", "mrqa_squad-train-39332", "mrqa_squad-train-45164", "mrqa_squad-train-66351", "mrqa_squad-train-44076", "mrqa_squad-train-66960", "mrqa_newsqa-validation-1962", "mrqa_squad-validation-2226", "mrqa_naturalquestions-validation-7598", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-3967", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-1206", "mrqa_naturalquestions-validation-3285", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3431", "mrqa_searchqa-validation-14631", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3987", "mrqa_squad-validation-6809", "mrqa_searchqa-validation-13813"], "EFR": 1.0, "Overall": 0.7402740575396825}, {"timecode": 63, "before_eval_results": {"predictions": ["Keeley Clare Julia Hawes", "Coriolis effect", "1776", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile ( 1.6 km )", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Jessica Sanders", "Article 1, Section 2, Clause 3", "Rick Rude", "November 2, 2010", "Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Michael English", "1877", "31", "1000 AD", "bow bridge", "Dick Rutan and Jeana Yeager", "on a stretch of Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "December 1800", "King Saud University", "Hugo Weaving", "Book of Exodus", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Andy Serkis", "1078", "Simon Peter", "Stefanie Scott", "glycine and arginine", "the art of the book and architecture ; and also including ceramics, metal, glass, and gardens", "Stephen A. Douglas", "Dolby Theatre in Hollywood, Los Angeles, California", "24 -- 3", "The Republic of Tecala", "during meiosis", "2009", "Andy Serkis", "the priests and virgins", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "danzig", "Marjorie McGinnis", "Saxe-Coburg and Gotha", "United States Army National Guard warrant officer", "Anne Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone: The Movie", "The Benchwarmers", "No Child Left Behind", "part of the proceeds"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6604473566515007}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7272727272727273, 0.16666666666666669, 1.0, 0.0, 0.0, 0.8, 1.0, 0.5714285714285715, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8405797101449275, 1.0, 0.15384615384615383, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_triviaqa-validation-2963", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-311"], "SR": 0.53125, "CSR": 0.552978515625, "retrieved_ids": ["mrqa_squad-train-65413", "mrqa_squad-train-81236", "mrqa_squad-train-83948", "mrqa_squad-train-65449", "mrqa_squad-train-12926", "mrqa_squad-train-12101", "mrqa_squad-train-47128", "mrqa_squad-train-33806", "mrqa_squad-train-64723", "mrqa_squad-train-30263", "mrqa_squad-train-64199", "mrqa_squad-train-53278", "mrqa_squad-train-27514", "mrqa_squad-train-14016", "mrqa_squad-train-73486", "mrqa_squad-train-77511", "mrqa_searchqa-validation-11361", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1570", "mrqa_squad-validation-7527", "mrqa_squad-validation-8597", "mrqa_triviaqa-validation-6939", "mrqa_newsqa-validation-1375", "mrqa_triviaqa-validation-2856", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3317", "mrqa_naturalquestions-validation-6577"], "EFR": 0.9666666666666667, "Overall": 0.7335384114583333}, {"timecode": 64, "before_eval_results": {"predictions": ["the winter solstice", "19 July 1990", "senators", "Rex Harrison", "a manufacturing operation", "Turducken", "Patrick Warburton", "Judas Iscariot", "1936", "the President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "Camarillo, California", "Yuzuru Hanyu", "Tracy McConnell", "Randy Goodrum", "the small intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "Rumplestiltskin", "Sylvester Stallone", "35 to 40 hours per week", "Naomi", "a faith, one baptism, one God and Father of all, who is over all and through all and in all", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "1996", "mid - to late 1920s", "Far Away", "Jane Lynch", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves", "Brenda", "1792", "Cyanea capillata", "Bonnie Lipton", "2002", "Charles Haley", "Dawn French", "translator", "Ut\u00f8ya", "125 lb", "Old World fossil representatives", "1964", "pesos", "North Korea", "\"E! News\"", "carbon fiber", "current congressmen", "The Greatest Show on Earth", "Mary Stuart"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7138652544351074}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 1.0, 0.0, 0.17647058823529413, 1.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_hotpotqa-validation-1810", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.640625, "CSR": 0.554326923076923, "retrieved_ids": ["mrqa_squad-train-9590", "mrqa_squad-train-80426", "mrqa_squad-train-36166", "mrqa_squad-train-59117", "mrqa_squad-train-26561", "mrqa_squad-train-74711", "mrqa_squad-train-30533", "mrqa_squad-train-84475", "mrqa_squad-train-69712", "mrqa_squad-train-45398", "mrqa_squad-train-73459", "mrqa_squad-train-80621", "mrqa_squad-train-73339", "mrqa_squad-train-41240", "mrqa_squad-train-73441", "mrqa_squad-train-50261", "mrqa_newsqa-validation-1565", "mrqa_hotpotqa-validation-1296", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3732", "mrqa_squad-validation-10011", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3614", "mrqa_searchqa-validation-16971", "mrqa_triviaqa-validation-3284", "mrqa_newsqa-validation-1641", "mrqa_squad-validation-5525", "mrqa_hotpotqa-validation-2459", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-3194", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-802"], "EFR": 1.0, "Overall": 0.7404747596153846}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "In `` The Crossing ''", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "lithium", "Pebe Sebert and Hugh Moffatt", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae", "Lesley Gore", "Paul", "Robert Kirkman", "Radiotelegraphy", "ingredients", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "December 1, 2009", "four", "com TLD", "Neil Young", "Hans Raffert", "`` It's My Party ''", "the Director of National Intelligence", "Liam Cunningham", "Jim Capaldi, Paul Carrack, and Peter Vale", "a cylinder of glass or plastic that runs along the fiber's length", "Ace", "Goths", "H CO", "StubHub Center in Carson, California", "the Maryland Senate's", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea", "a surname of Norman", "1888", "Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province )", "performance marker", "in Super Bowl LII", "Seattle Center, including the Seattle Center Monorail and the Space Needle", "Columbia River Gorge", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "John Joseph Patrick Ryan", "1912", "Luke 6 : 12 -- 16, and Acts 1 : 13", "Ric Flair", "Around 1200", "Pangaea", "2008", "Adam Werritty", "the Jets", "her white halter dress", "Jonghyun", "King Edward II", "Harrods", "\"Most of my friends have put in at least a couple hours,\"", "job training", "Arnold Drummond", "Nixon", "Great Expectations", "cathode", "No Surprises"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6606301910649737}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428572, 1.0, 1.0, 0.5714285714285715, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.4, 1.0, 1.0, 0.8695652173913044, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-1851", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-1697"], "SR": 0.546875, "CSR": 0.5542140151515151, "retrieved_ids": ["mrqa_squad-train-63399", "mrqa_squad-train-43648", "mrqa_squad-train-32910", "mrqa_squad-train-57036", "mrqa_squad-train-76223", "mrqa_squad-train-31127", "mrqa_squad-train-45106", "mrqa_squad-train-13162", "mrqa_squad-train-16509", "mrqa_squad-train-67920", "mrqa_squad-train-79238", "mrqa_squad-train-68020", "mrqa_squad-train-31672", "mrqa_squad-train-51338", "mrqa_squad-train-4214", "mrqa_squad-train-16771", "mrqa_newsqa-validation-2617", "mrqa_naturalquestions-validation-3004", "mrqa_triviaqa-validation-3547", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-3086", "mrqa_searchqa-validation-12119", "mrqa_newsqa-validation-860", "mrqa_searchqa-validation-12036", "mrqa_newsqa-validation-3860", "mrqa_naturalquestions-validation-4112", "mrqa_searchqa-validation-409", "mrqa_triviaqa-validation-5499", "mrqa_squad-validation-4528", "mrqa_newsqa-validation-3907", "mrqa_triviaqa-validation-7281", "mrqa_newsqa-validation-3175"], "EFR": 0.9655172413793104, "Overall": 0.7335556263061651}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "May 1980", "IX", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "tourneys or slow wheels", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "is most often measured in millimetres per hour or inches per hour", "Kathy Najimy", "Nicole Gale Anderson", "Daya Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "stress", "Richard Crispin Armitage", "the Himalayas", "students", "volcanic activity", "1837", "late - September through early January", "1991", "Joseph Sherrard Kearns", "Union forces", "3 September", "loop", "Carroll O'Connor", "fictional town of West Egg on prosperous Long Island", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "a certified question or proposition of law from one of the United States Courts of Appeals", "after World War II", "Guwahati", "The chief city, Salamina, lies in the west - facing core of the crescent on Salamis Bay, which opens into the Saronic Gulf", "Todd Griffin", "October 29, 2015", "Pir Panjal Railway Tunnel", "16", "~ 3.5 million years old", "federal", "Tigris and Euphrates", "the executive, consisting of the Supreme Court and other federal courts", "In the year 2026", "Holly Marie Combs", "utopian novels of H.G. Wells", "Sarah Brightman", "Microsoft Windows", "at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Los Angeles", "moral", "Lana Del Rey", "NBA", "a greyhound", "Aristotle", "Northwest Mall", "Supergirl", "Field Marshal Lord Gort", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "gun", "Swat Valley.", "( Odysseus)", "Louisiana", "Boy Scouting", "three empty vodka bottles,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5745579275989392}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.45454545454545453, 0.11764705882352941, 0.0, 1.0, 0.25, 1.0, 0.0, 0.36363636363636365, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.8620689655172413, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_searchqa-validation-4320", "mrqa_newsqa-validation-3067"], "SR": 0.46875, "CSR": 0.5529384328358209, "retrieved_ids": ["mrqa_squad-train-787", "mrqa_squad-train-74958", "mrqa_squad-train-7730", "mrqa_squad-train-32565", "mrqa_squad-train-2558", "mrqa_squad-train-79927", "mrqa_squad-train-18492", "mrqa_squad-train-53969", "mrqa_squad-train-5532", "mrqa_squad-train-20158", "mrqa_squad-train-24377", "mrqa_squad-train-46541", "mrqa_squad-train-6610", "mrqa_squad-train-26358", "mrqa_squad-train-627", "mrqa_squad-train-84188", "mrqa_hotpotqa-validation-1297", "mrqa_naturalquestions-validation-8161", "mrqa_newsqa-validation-1224", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-11933", "mrqa_squad-validation-1441", "mrqa_naturalquestions-validation-655", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3625", "mrqa_naturalquestions-validation-7165", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-5857", "mrqa_naturalquestions-validation-9877", "mrqa_newsqa-validation-4182"], "EFR": 0.9411764705882353, "Overall": 0.7284323556848112}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "2010", "Clarence Darrow", "B.F. Skinner", "Spanish", "Tara / Ghost of Christmas Past", "a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "March 6, 2018", "Erica Rivera", "Bill Irwin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "Fishtown neighborhood", "Rabindranath Tagore", "Georgia", "Domhnall Gleeson", "Alex Drake", "March 11, 2016", "March 11, 2018", "Thomas Mundy Peterson", "Augustus Waters", "boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "consistency", "Nucleotides", "a homodimer of 37 - kDa subunits", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "Bonnie Hunt", "the intermembrane space", "February 25, 2004 ( Ash Wednesday, the beginning of Lent )", "the breast or lower chest of beef or veal", "a nearly - identical `` non-drivers identification card ''", "Dr. Hartwell Carver", "two", "in Super Bowl LII, following the 2017 season", "Dadra and Nagar Haveli", "Bonanza Creek Ranch", "a work of social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "the narrator", "Washington metropolitan area", "the euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking song", "tissues of the outer third of the vagina", "Bergen County", "John R. Dilworth", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "Hippos & baboons", "Russia", "Tommy Hilfiger", "a jug or pitcher with a wide mouth"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6214960341193894}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.3636363636363636, 0.9824561403508771, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6, 0.0, 0.4, 0.9166666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.1818181818181818, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-7922", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-3449", "mrqa_searchqa-validation-5472", "mrqa_triviaqa-validation-2358"], "SR": 0.484375, "CSR": 0.5519301470588236, "retrieved_ids": ["mrqa_squad-train-72170", "mrqa_squad-train-75060", "mrqa_squad-train-1187", "mrqa_squad-train-56755", "mrqa_squad-train-28544", "mrqa_squad-train-74524", "mrqa_squad-train-21011", "mrqa_squad-train-81674", "mrqa_squad-train-15797", "mrqa_squad-train-65402", "mrqa_squad-train-25779", "mrqa_squad-train-66674", "mrqa_squad-train-74810", "mrqa_squad-train-36782", "mrqa_squad-train-36579", "mrqa_squad-train-16621", "mrqa_newsqa-validation-536", "mrqa_squad-validation-2564", "mrqa_naturalquestions-validation-3381", "mrqa_squad-validation-9533", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-3450", "mrqa_naturalquestions-validation-5093", "mrqa_searchqa-validation-10045", "mrqa_newsqa-validation-1961", "mrqa_triviaqa-validation-412", "mrqa_naturalquestions-validation-2552", "mrqa_searchqa-validation-3222", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-3794", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-9548"], "EFR": 1.0, "Overall": 0.7399954044117647}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Danny Elfman", "Olivia Olson", "21 June 2007", "Peter Klaven ( Paul Rudd )", "Madison Pettis", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Bindusara", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "personnel directors", "Davos", "Neil Patrick Harris", "1900 to 1946", "Joel", "stems and roots", "either late 2018 or early 2019", "R.E.M.", "the Jews", "lustrous, purple - black metallic solid", "the Ark of the Covenant", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "primarily in Polk County, Florida", "between 11000 and 9000 BC", "the beginning of the Wizarding World shared media franchise", "Christian missionaries", "Elected Emperor of the Romans", "1799", "Kid Creole & The Coconuts", "a god of the Ammonites", "late - night", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "1770 BC", "Australia's Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "September 27 1825", "Miracle", "Dumfries and Galloway", "High Knob", "President Obama and Britain's Prince Charles", "NATO fighters", "19,", "a lighthouse", "lullaby", "Cummings", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6413951358718801}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.8837209302325582, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9816", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.53125, "CSR": 0.5516304347826086, "retrieved_ids": ["mrqa_squad-train-32784", "mrqa_squad-train-12209", "mrqa_squad-train-11262", "mrqa_squad-train-67550", "mrqa_squad-train-15351", "mrqa_squad-train-53431", "mrqa_squad-train-18706", "mrqa_squad-train-17985", "mrqa_squad-train-10199", "mrqa_squad-train-54584", "mrqa_squad-train-15461", "mrqa_squad-train-17926", "mrqa_squad-train-81926", "mrqa_squad-train-8865", "mrqa_squad-train-53689", "mrqa_squad-train-40200", "mrqa_naturalquestions-validation-3285", "mrqa_searchqa-validation-14601", "mrqa_naturalquestions-validation-7356", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-5963", "mrqa_naturalquestions-validation-7490", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1680", "mrqa_searchqa-validation-9777", "mrqa_newsqa-validation-3899", "mrqa_triviaqa-validation-5973", "mrqa_searchqa-validation-13951", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-802", "mrqa_triviaqa-validation-5492", "mrqa_newsqa-validation-2375"], "EFR": 0.9333333333333333, "Overall": 0.7266021286231884}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Old Trafford, Greater Manchester, England", "The Intolerable Acts", "skeletal muscle and the brain", "libretto", "prophets and beloved religious leaders", "2015", "the Detroit Tigers", "Toby Kebbell", "Panning", "May 2017", "Bob Dylan", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "Eastern Redbud", "eleven", "10.5 %", "Roger Dean Stadium", "Blood is the New Black", "Otis Timson", "four", "Benjamin Franklin", "a routing table, or routing information base ( RIB ), is a networked computer that lists the routes to particular network destinations, and in some cases, metrics ( distances ) associated with those routes", "James Rodr\u00edguez", "in Ephesus in AD 95 -- 110", "Johnson", "more than 2,500 locations in all states except Alaska, Hawaii, Connecticut, Maine, New Hampshire, and Vermont", "the lower back", "Forney Hull ( James Frain ), the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "Ashoka", "epidermis", "Hodel", "October 27, 2017", "Wolfgang Hochstetter", "one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Aegisthus", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the courts", "September 29, 2017", "10 : 30am", "Angola", "Norway", "Manley", "December 15, 2017", "Wyatt", "Wednesday", "In God We Trust", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "Herman Cain,", "At least 40", "Juan Martin Del Potro.", "the Caspian Sea", "Sweden", "photoelectric", "South-West Africa"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6744520890716317}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.22222222222222224, 0.896551724137931, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25806451612903225, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843"], "SR": 0.5625, "CSR": 0.5517857142857143, "retrieved_ids": ["mrqa_squad-train-81624", "mrqa_squad-train-32715", "mrqa_squad-train-56477", "mrqa_squad-train-11814", "mrqa_squad-train-38769", "mrqa_squad-train-69415", "mrqa_squad-train-1243", "mrqa_squad-train-4619", "mrqa_squad-train-58967", "mrqa_squad-train-58415", "mrqa_squad-train-25237", "mrqa_squad-train-42108", "mrqa_squad-train-9341", "mrqa_squad-train-36554", "mrqa_squad-train-14059", "mrqa_squad-train-15334", "mrqa_naturalquestions-validation-1555", "mrqa_newsqa-validation-3320", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-3644", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-1271", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3284", "mrqa_newsqa-validation-3614", "mrqa_triviaqa-validation-4493", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-11961", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-652", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-10823"], "EFR": 0.9285714285714286, "Overall": 0.7256808035714285}, {"timecode": 70, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.876953125, "KG": 0.5078125, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as well as at the Gol", "Lagaan ( English : Taxation", "Super Bowl XXXIX", "almost exclusively land based powers", "September 2017", "Kanawha River", "12.65 m", "1820s", "the customer's account", "D\u00e1in", "punk rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "prison", "Supreme Court of Canada", "July 1, 1923", "Firoz Shah Tughlaq", "October 2008", "4 January 2011", "Yul Brynner", "the present Indian constitutive state of Meghalaya ( formerly Assam ), which includes the present districts of East Jaintia Hills district", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "irsten Simone Vangsness", "Frankie Laine's `` I Believe ''", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Ramanaa", "RAF Coningsby in Lincolnshire", "the Speaker", "de pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Ferrari", "Tiger Woods", "2018", "Speaker of the House of Representatives", "the final scene of the fourth season", "Lord's, on 15 July 2004 between Middlesex and Surrey", "mid-size four - wheel drive luxury SUVs", "Ingrid Bergman", "Malayalam", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "The terrestrial biosphere", "Kitty Softpaws", "Austria - Hungary", "on a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "eye", "Vietnam", "Rutger Hauer", "Canada", "Robert Jenrick", "Srinagar", "Jewish", "Tibetan exile leaders,", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "William", "electric"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6417564891249101}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.9538461538461539, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5964912280701754, 0.8363636363636363, 0.6666666666666666, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-477", "mrqa_hotpotqa-validation-820"], "SR": 0.515625, "CSR": 0.5512764084507042, "retrieved_ids": ["mrqa_squad-train-65436", "mrqa_squad-train-73871", "mrqa_squad-train-79065", "mrqa_squad-train-66173", "mrqa_squad-train-58666", "mrqa_squad-train-64492", "mrqa_squad-train-41130", "mrqa_squad-train-5607", "mrqa_squad-train-79469", "mrqa_squad-train-28782", "mrqa_squad-train-6220", "mrqa_squad-train-70317", "mrqa_squad-train-77731", "mrqa_squad-train-1532", "mrqa_squad-train-76901", "mrqa_squad-train-41798", "mrqa_searchqa-validation-2963", "mrqa_newsqa-validation-3264", "mrqa_naturalquestions-validation-686", "mrqa_searchqa-validation-16357", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-11427", "mrqa_naturalquestions-validation-10114", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2947", "mrqa_searchqa-validation-1162", "mrqa_naturalquestions-validation-5579", "mrqa_searchqa-validation-3194", "mrqa_triviaqa-validation-3725", "mrqa_newsqa-validation-1660"], "EFR": 0.7741935483870968, "Overall": 0.6963439913675602}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "National Football League ( NFL )", "the following day", "Labour", "Judi Dench", "a scuffle with the Beast Folk", "six", "Spanish moss", "John Barry", "1990", "Friedman Billings Ramsey", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "drivers who meet more exclusive criteria", "Charles Carroll", "1959", "many forested parts", "Hermia", "in and around an unnamed village", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "V\u1e5bksayurveda", "the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Benzod Benzines", "April 1", "its absolute temperature", "electron donors", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "biscuit", "1890", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the pulmonary arteries", "Steve Russell", "August 21", "1799", "a boy name", "Zachary Taylor", "oscar Wilde", "Galaxy S7", "The New Yorker", "Citgo Petroleum Corporation", "South Africa", "\"The e-mails\"", "Rolling Stone", "a lump of native gold", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6660689958798511}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.6, 0.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 0.4, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4196", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-199", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5766", "mrqa_newsqa-validation-3799", "mrqa_searchqa-validation-10641"], "SR": 0.53125, "CSR": 0.5509982638888888, "retrieved_ids": ["mrqa_squad-train-41576", "mrqa_squad-train-44470", "mrqa_squad-train-13249", "mrqa_squad-train-44727", "mrqa_squad-train-78834", "mrqa_squad-train-24113", "mrqa_squad-train-70297", "mrqa_squad-train-8648", "mrqa_squad-train-20828", "mrqa_squad-train-83498", "mrqa_squad-train-40808", "mrqa_squad-train-59442", "mrqa_squad-train-17456", "mrqa_squad-train-83518", "mrqa_squad-train-5197", "mrqa_squad-train-5252", "mrqa_newsqa-validation-1973", "mrqa_squad-validation-5042", "mrqa_newsqa-validation-2331", "mrqa_naturalquestions-validation-1198", "mrqa_triviaqa-validation-7474", "mrqa_searchqa-validation-4197", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-2060", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-1350", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-198", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-5004"], "EFR": 1.0, "Overall": 0.7414496527777777}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Edward Furlong", "Toby Keith", "General George Washington", "architect Louis Le Vau", "Shenzi", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "142", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "First Lieutenant Israel Greene", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "Kansas City Chiefs", "Yuzuru Hanyu", "Owen Hunt ( Kevin McKidd )", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "The House of Representatives", "Lisa Stelly", "Ali", "Meg Autobot", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a great deal on location", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates men's basketball team from SetonHall University", "13 episodes", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Melinda Dillon", "Empire of Japan", "Djokovic", "White was almost out of competition, scoring only 37.7", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "2002", "Georgia Groome as Georgia Nic Nicholson", "Incudomalleolar joint ( more correctly called incudomallear joint ) or articulatio incudomerlearis", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "in York", "Hamburger Sport-Verein e.V.", "2", "The Los Angeles Dance Theater", "100 meter", "President Sheikh Sharif Sheikh Ahmed", "Brooklyn, New York,", "Suntory", "Victoria", "a yoke", "Funcom game, \"The Secret World\""], "metric_results": {"EM": 0.640625, "QA-F1": 0.7355623543123544}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6153846153846153, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7999999999999999, 1.0, 0.7272727272727272, 0.9, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.2]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-7312", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1335", "mrqa_searchqa-validation-13611", "mrqa_hotpotqa-validation-1074"], "SR": 0.640625, "CSR": 0.5522260273972603, "retrieved_ids": ["mrqa_squad-train-14334", "mrqa_squad-train-31213", "mrqa_squad-train-62896", "mrqa_squad-train-4550", "mrqa_squad-train-21813", "mrqa_squad-train-78361", "mrqa_squad-train-35972", "mrqa_squad-train-29918", "mrqa_squad-train-66261", "mrqa_squad-train-68841", "mrqa_squad-train-63620", "mrqa_squad-train-54699", "mrqa_squad-train-69610", "mrqa_squad-train-74313", "mrqa_squad-train-66935", "mrqa_squad-train-70599", "mrqa_squad-validation-6614", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-3953", "mrqa_naturalquestions-validation-1199", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3662", "mrqa_triviaqa-validation-6939", "mrqa_searchqa-validation-541", "mrqa_triviaqa-validation-5492", "mrqa_searchqa-validation-9687", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-7035", "mrqa_squad-validation-927", "mrqa_newsqa-validation-629", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-2519"], "EFR": 0.9130434782608695, "Overall": 0.7243039011316259}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "the lower motor neurons, the efferent nerves that directly innervate muscles", "Johannes Gutenberg of Mainz", "Shawn Wayans", "the United States is the world's third - or fourth - largest country by total area and the third-most populous", "regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Germany, who beat Argentina 1 -- 0 after extra time", "March 31 to April 8, 2018", "American Indian allies", "radius R of the turntable", "Britain ( German : Luftschlacht um England, literally `` The Air Battle for England '' )", "after World War II", "CeCe Drake", "April 12, 2017", "post translational modification", "1960", "in 1790 passed the first naturalization law for the United States, the Naturalization Act of 1790", "September 6, 2019", "Bulgaria", "Michael Douglas", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff / 26.617", "Werner Ruchti", "Brooklyn, New York", "British singer - songwriter Chris Rea", "Julie Adams", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "the king's army", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350 at the 2010 census", "Uruguay", "to ordain presbyters / bishops and to exercise general oversight", "All the world's a stage", "2002", "Anna Faris", "Cress", "montreal", "wyatt marie", "Gerald Ford", "Bank of China Tower", "Tata Consultancy Services Limited (TCS)", "Corendon Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "Egypt", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6660180382676786}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.6666666666666666, 0.0, 0.125, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.16666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.17391304347826086, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.9411764705882353, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.5625, "CSR": 0.5523648648648649, "retrieved_ids": ["mrqa_squad-train-43972", "mrqa_squad-train-37592", "mrqa_squad-train-22307", "mrqa_squad-train-62051", "mrqa_squad-train-53936", "mrqa_squad-train-23363", "mrqa_squad-train-971", "mrqa_squad-train-30907", "mrqa_squad-train-71411", "mrqa_squad-train-38448", "mrqa_squad-train-36338", "mrqa_squad-train-47822", "mrqa_squad-train-46468", "mrqa_squad-train-57036", "mrqa_squad-train-2936", "mrqa_squad-train-72516", "mrqa_naturalquestions-validation-854", "mrqa_newsqa-validation-820", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-4271", "mrqa_searchqa-validation-13347", "mrqa_hotpotqa-validation-5850", "mrqa_naturalquestions-validation-4200", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-2027", "mrqa_naturalquestions-validation-2414", "mrqa_squad-validation-6470", "mrqa_newsqa-validation-3625", "mrqa_searchqa-validation-14508", "mrqa_hotpotqa-validation-1028"], "EFR": 0.9285714285714286, "Overall": 0.7274372586872587}, {"timecode": 74, "before_eval_results": {"predictions": ["Lgion d'honneur", "Shaft", "\"Kinetic warfare.\"", "Cleopatra", "pharaoh", "Tony Dungy", "Queen", "cadenza", "chili", "thrombocytes", "universal and equal suffrage", "60", "Enigma", "a tornado", "day", "Lord Tennyson", "throat cancer", "Gentle Ben", "terraces", "a voodoo sorcerer", "aquiline", "Hair", "a cozy", "Jalisco", "Davenport", "Sammy Sosa", "Suzuki", "One thousand", "the green-eyed monster", "Mount Olympus", "haematoma", "Death", "Coral", "General William Tecumseh Sherman", "Fess Parker", "a duvet", "The Summer Youth Conservatory", "a crayfish", "Japan", "\"Liberty, Equality, Fraternity\"", "the African Union", "William Wrigley Jr.", "Nepal", "USDA", "cat scratch fever", "freezing", "(Prince) Albert", "a kangaro court", "Whatchamacallit", "Little Richard", "Fantasy Island", "pigs", "lying between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "benjamin franklin", "Sororicide", "saint aidan", "Sulla", "Appenzell Alps", "Parlophone Records", "keyboardist and", "150", "a real person to talk to,\"", "the contestant"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5905913978494624}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_naturalquestions-validation-2572", "mrqa_triviaqa-validation-1931", "mrqa_hotpotqa-validation-4525", "mrqa_newsqa-validation-1890", "mrqa_naturalquestions-validation-5636"], "SR": 0.515625, "CSR": 0.551875, "retrieved_ids": ["mrqa_squad-train-26253", "mrqa_squad-train-18952", "mrqa_squad-train-17653", "mrqa_squad-train-84480", "mrqa_squad-train-48451", "mrqa_squad-train-35569", "mrqa_squad-train-21157", "mrqa_squad-train-68043", "mrqa_squad-train-6160", "mrqa_squad-train-26010", "mrqa_squad-train-16201", "mrqa_squad-train-55276", "mrqa_squad-train-4024", "mrqa_squad-train-85238", "mrqa_squad-train-46235", "mrqa_squad-train-39789", "mrqa_searchqa-validation-6975", "mrqa_squad-validation-6759", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-2022", "mrqa_naturalquestions-validation-1925", "mrqa_triviaqa-validation-5425", "mrqa_naturalquestions-validation-6917", "mrqa_newsqa-validation-1428", "mrqa_searchqa-validation-10233", "mrqa_newsqa-validation-455", "mrqa_triviaqa-validation-111", "mrqa_newsqa-validation-530", "mrqa_naturalquestions-validation-952", "mrqa_squad-validation-2372", "mrqa_triviaqa-validation-2333"], "EFR": 1.0, "Overall": 0.741625}, {"timecode": 75, "before_eval_results": {"predictions": ["8 Mile", "paul newman", "Louisiana", "a rabbit", "Tombs of Kobol", "The Sound and the Fury", "a sandwich", "six", "Cosmo Kramer", "Poetic Justice", "the guillitine", "the Colossus", "(Hugh) Jackman", "silver", "Lebanon", "the eagle", "the Communist Party", "\"The King's Speech,\"", "(Prince) Claudius", "Mussolini", "Margot Fonteyn", "Alfred Nobel", "lifejackets", "exterus", "General Mills", "Emmitt Smith", "clay", "a black hole", "Uganda", "Department of Department", "Heisenberg", "(Prince) Willis", "(David) Hyde Pierce", "The period is named for Friedrich Maximilian Klinger", "Old North Church", "spinal column", "energy drinks", "a pirate ship", "the North West Territories", "Alaska", "the Electric Company", "Vienna", "140000", "Red", "roots", "Eleanor Roosevelt", "Esau", "a skull", "Agatha Christie", "( Ronald) Reagan", "Ford Motor Company", "1947", "Moira Kelly", "Zuzu & Zaza Zebra", "Mt Kenya", "Christian Wulff", "Mata Hari", "Princess Aisha bint Hussein", "French", "England", "Kaka", "133", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6095653044871795}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-5993", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-11498", "mrqa_searchqa-validation-14906", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.546875, "CSR": 0.5518092105263157, "retrieved_ids": ["mrqa_squad-train-66275", "mrqa_squad-train-51035", "mrqa_squad-train-60690", "mrqa_squad-train-54755", "mrqa_squad-train-32097", "mrqa_squad-train-79711", "mrqa_squad-train-37282", "mrqa_squad-train-71991", "mrqa_squad-train-18434", "mrqa_squad-train-61942", "mrqa_squad-train-79039", "mrqa_squad-train-38810", "mrqa_squad-train-20098", "mrqa_squad-train-84049", "mrqa_squad-train-72697", "mrqa_squad-train-64207", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3614", "mrqa_naturalquestions-validation-359", "mrqa_newsqa-validation-3687", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2333", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1565", "mrqa_naturalquestions-validation-4572", "mrqa_newsqa-validation-2587", "mrqa_triviaqa-validation-237", "mrqa_newsqa-validation-1021"], "EFR": 0.9655172413793104, "Overall": 0.7347152903811252}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "the Treasury", "Montserrat", "a cyclone", "Starland Vocal Band", "the gallows", "ohm", "Mildred Taylor", "earthquakes", "the Potomac", "Indiana", "Mary Stuart", "Hulk", "air pressure", "Russia", "Adam Sandler", "Ted Koppel", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "a sack dress", "Bobby McFerrin", "Fore River Shipyard", "Capitol Hill", "a glider", "a heart", "Guyana", "jelly", "camels", "drought", "qu\u00e9", "Jonathan Winters", "P!nk", "Rhode Island", "Isaac Newton", "the African continent", "Cody", "Theodore Roosevelt", "gold", "Joshua", "Jamestown", "a Lignite", "Seymour Cray", "Private Practice", "corticosteroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "a hunch", "Scotland", "Pullman Brown", "chalk quarry", "SBS", "\"Eternal Flame\"", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "Benzodiazepines"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8057291666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-8302", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-216", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-15581", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7709", "mrqa_hotpotqa-validation-512"], "SR": 0.765625, "CSR": 0.554586038961039, "retrieved_ids": ["mrqa_squad-train-15930", "mrqa_squad-train-14250", "mrqa_squad-train-3273", "mrqa_squad-train-20500", "mrqa_squad-train-9001", "mrqa_squad-train-28426", "mrqa_squad-train-67713", "mrqa_squad-train-72530", "mrqa_squad-train-16462", "mrqa_squad-train-39638", "mrqa_squad-train-764", "mrqa_squad-train-70279", "mrqa_squad-train-79469", "mrqa_squad-train-15112", "mrqa_squad-train-61514", "mrqa_squad-train-67773", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3662", "mrqa_triviaqa-validation-2406", "mrqa_naturalquestions-validation-2781", "mrqa_hotpotqa-validation-5394", "mrqa_newsqa-validation-1016", "mrqa_triviaqa-validation-5789", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1827", "mrqa_searchqa-validation-16428"], "EFR": 1.0, "Overall": 0.7421672077922078}, {"timecode": 77, "before_eval_results": {"predictions": ["Charles Darwin", "Inuit", "Bologna", "Billy the Kid", "Rudyard Kipling", "Cheers", "Tarzan", "Edward VI", "Leon Trotsky", "Belgium", "Wendy Beckett", "1066", "ibuprofen", "legislator", "Carver", "Bulldog", "Spooky Salem, MA", "the Saraswati River", "the Baltic Sea", "nolo contendere", "gum", "Abel", "Louis XV", "Wayne Gretzky", "Anna Karenina", "Sacramento", "the Andes mountain range", "jury", "Dreams", "Pantaloons", "Muhammad", "Paul Newman", "... Brian C. Wimes", "champagne", "Rhode Island", "The Simple Life", "Laos", "Agent Orange", "the Philippines", "Kellogg's", "...Hear Good' Music", "Luxor", "Latin", "Venus", "the Hawthorne", "the Congo", "Charles", "Horatio Nelson", "a caiman", "Ferrari", "iris", "John Adams", "1886", "Ali", "tahrir Square", "1914", "robot danzig", "ESPN College Football Friday Primetime", "R&B vocal group", "Kansas Joe McCoy", "protective shoes", "Diego Maradona", "Transportation Security Administration", "silver"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6774553571428571}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-5367", "mrqa_searchqa-validation-7197", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-4756", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319"], "SR": 0.546875, "CSR": 0.5544871794871795, "retrieved_ids": ["mrqa_squad-train-19624", "mrqa_squad-train-320", "mrqa_squad-train-1868", "mrqa_squad-train-70228", "mrqa_squad-train-64465", "mrqa_squad-train-79465", "mrqa_squad-train-829", "mrqa_squad-train-20941", "mrqa_squad-train-10577", "mrqa_squad-train-2436", "mrqa_squad-train-32784", "mrqa_squad-train-77554", "mrqa_squad-train-41548", "mrqa_squad-train-54832", "mrqa_squad-train-7563", "mrqa_squad-train-52224", "mrqa_squad-validation-6185", "mrqa_naturalquestions-validation-2092", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-4146", "mrqa_searchqa-validation-12588", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1310", "mrqa_naturalquestions-validation-6832", "mrqa_squad-validation-4068", "mrqa_newsqa-validation-162", "mrqa_triviaqa-validation-5775", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3557", "mrqa_triviaqa-validation-5052", "mrqa_hotpotqa-validation-550"], "EFR": 0.9310344827586207, "Overall": 0.72835433244916}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "Christmas Eve", "The Firm", "Schwalbe", "Circumnavigate", "Marilyn Monroe", "Cheddar", "comet", "wings", "the Enigma", "surface-to-air missile", "the igloo", "Pluto", "a dermatologist", "Kramer", "The Tempest", "yellow", "Annie's Song", "tire", "Schwarzenegger", "Lafayette", "Iris Murdoch", "triathlon", "the Swahili", "NHL", "gown skirts", "a horse", "Ramses the Great", "Na Me U", "Scott McClellan", "Jeremiah", "Thomas Edison", "The Chorus Line", "Guadalajara", "Sydney", "flavor", "Dutchman", "\"The Janeites\"", "the Alamo", "cereal", "Ronaldo", "college grants", "Rush", "being buried alive", "Swan", "Kansas Jayhawks", "Helsinki", "the kidney", "One Flew Over the Cuckoo's Nest", "Nobel Prize in Literature", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "Standard Motor Company", "Portugal", "cooperative", "Double Agent", "Juan Manuel Mata Garc\u00eda", "Madeleine L' Engle", "Almost all British troops in Iraq", "three", "$3 billion,", "Tom Ewell"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6578125}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-12844", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_searchqa-validation-5300", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-5569", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010"], "SR": 0.578125, "CSR": 0.5547863924050633, "retrieved_ids": ["mrqa_squad-train-8080", "mrqa_squad-train-23993", "mrqa_squad-train-77305", "mrqa_squad-train-43042", "mrqa_squad-train-80089", "mrqa_squad-train-63252", "mrqa_squad-train-79926", "mrqa_squad-train-12042", "mrqa_squad-train-70277", "mrqa_squad-train-13883", "mrqa_squad-train-71640", "mrqa_squad-train-14230", "mrqa_squad-train-25171", "mrqa_squad-train-71684", "mrqa_squad-train-51283", "mrqa_squad-train-51058", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-3872", "mrqa_naturalquestions-validation-3", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-10297", "mrqa_newsqa-validation-4144", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-1945", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-14127", "mrqa_newsqa-validation-3433", "mrqa_squad-validation-1640", "mrqa_newsqa-validation-3178"], "EFR": 0.9629629629629629, "Overall": 0.7347998710736052}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "New Zealand", "Peter", "litter", "New Zealand", "fontanels", "California", "Augustus", "the Dalmatians", "(Daniel) Day Lewis", "cotton", "Bridget Fonda", "South Africa", "a truncheon", "the Mediterranean", "Catherine", "bacon", "the adder", "(the Hold-Up)", "the Thames", "(PIE) FLINGING", "Pitcairn", "Adam Sandler", "Mayo", "\"You had me at greeting\"", "arrested Development", "(aissance)", "(the Indo-European)", "Rodeo", "repent", "Denzel Washington", "Bonn", "nougat", "Jeopary", "ranee", "(Louis) Comfort Tiffany", "Louise", "conk", "Hillary Clinton", "globalization", "Van Halen", "the (the) Rhine", "sodium", "Samsonite", "Chile", "salaam", "Michael Faraday", "pearls", "Norse", "Niagara Falls", "the Bronx", "the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Steelers", "Ethel Merman", "Chung", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "237", "over two decades.", "does not", "14", "8th and 16th"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6016401397515528}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8000000000000002, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.34782608695652173, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-11493", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-15867", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5256", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.53125, "CSR": 0.5544921875, "retrieved_ids": ["mrqa_squad-train-83077", "mrqa_squad-train-12437", "mrqa_squad-train-41286", "mrqa_squad-train-19706", "mrqa_squad-train-58784", "mrqa_squad-train-19045", "mrqa_squad-train-78925", "mrqa_squad-train-86399", "mrqa_squad-train-56421", "mrqa_squad-train-857", "mrqa_squad-train-48911", "mrqa_squad-train-25270", "mrqa_squad-train-67142", "mrqa_squad-train-4164", "mrqa_squad-train-60602", "mrqa_squad-train-76566", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-3953", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-1425", "mrqa_searchqa-validation-5283", "mrqa_triviaqa-validation-4639", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-9766", "mrqa_triviaqa-validation-3423", "mrqa_naturalquestions-validation-4981", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-13939", "mrqa_naturalquestions-validation-366", "mrqa_searchqa-validation-16625"], "EFR": 0.9666666666666667, "Overall": 0.7354817708333333}, {"timecode": 80, "UKR": 0.80078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.896484375, "KG": 0.5046875, "before_eval_results": {"predictions": ["(George) Washington", "the National Hockey League (NHL)", "Homeland Security", "Georgia", "Major General William Devereaux", "scalpels", "the English Channel", "Shakespeare", "Cameroon", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "pardon", "Bartholomew", "tyrosine", "Target", "Frank Sinatra's \"The Impossible Dream, My Way, and That's Life\"", "a possum", "\"Three\"", "Pamplona", "Rapa Nui National Park", "Frans", "Madonna", "drought", "a staycation", "safer than sorry", "Makkedah", "Yogi Bear", "Idaho", "Georgia O'Keeffe", "a passenger vehicle", "1215", "Benjamin Harrison", "the skyscraper", "BILLY THE KID", "The Killing Fields", "Oliver Twist", "a landmark", "lamb", "bread", "Boston", "Martinique", "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb", "the Yangtze River", "Sons of Liberty", "a telescope", "Catholic", "the tuba", "a deep pass route", "a tetrahedron", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II.", "16", "dragonflies", "acidic", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "2006,", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted."], "metric_results": {"EM": 0.609375, "QA-F1": 0.6696428571428572}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-6748", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-2608", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.609375, "CSR": 0.5551697530864197, "retrieved_ids": ["mrqa_squad-train-8238", "mrqa_squad-train-2256", "mrqa_squad-train-34598", "mrqa_squad-train-85764", "mrqa_squad-train-40164", "mrqa_squad-train-75216", "mrqa_squad-train-80772", "mrqa_squad-train-57316", "mrqa_squad-train-4495", "mrqa_squad-train-27376", "mrqa_squad-train-64149", "mrqa_squad-train-62585", "mrqa_squad-train-86373", "mrqa_squad-train-2382", "mrqa_squad-train-63851", "mrqa_squad-train-71179", "mrqa_naturalquestions-validation-553", "mrqa_hotpotqa-validation-2820", "mrqa_newsqa-validation-1147", "mrqa_searchqa-validation-1088", "mrqa_squad-validation-1441", "mrqa_triviaqa-validation-3931", "mrqa_newsqa-validation-1720", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-11095", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-2389", "mrqa_triviaqa-validation-7312", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-2656", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-7095"], "EFR": 1.0, "Overall": 0.7514245756172839}, {"timecode": 81, "before_eval_results": {"predictions": ["species", "Warsaw", "Katrina And The Waves", "the French & Indian War", "Brady", "philosophy", "the American Red Cross", "harm", "Bonnie Raitt", "Titanic", "pickles", "Artemis", "the spinal cord", "Evian", "a goose", "The Life and Death of a Man of Character", "the olfactory nerve", "a bay window", "Newton", "YouTube", "Alexander Hamilton", "the Colorado", "Dune", "a duel", "YouTube", "heresy", "Dwight", "Charlie Watts", "a black widow spider", "the cord", "Virginia", "abundant", "Albert Schweitzer", "the right hemisphere", "a dive bomber", "Henri de Toulouse-Lautrec", "Helen Hayes", "the Punk movement", "biddy", "Wells", "\"Sex In Crazy Places\"", "Bill & Melinda Gates", "the Hippopotamus", "Friedrich Nietzsche", "a dog eat dog world", "Alexander Hamilton", "American", "Niagara Falls", "a rudder", "orange corn", "the Flintstones", "Abanindranath Tagore", "when viewed from different points on Earth", "the trunk", "Carrefour", "Barack Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano,", "national telephone", "the Catholic League.", "ennio Morricone"], "metric_results": {"EM": 0.5625, "QA-F1": 0.669678776683087}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.06896551724137931, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-2805", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-14639", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846"], "SR": 0.5625, "CSR": 0.5552591463414633, "retrieved_ids": ["mrqa_squad-train-33640", "mrqa_squad-train-58584", "mrqa_squad-train-80957", "mrqa_squad-train-52813", "mrqa_squad-train-53171", "mrqa_squad-train-81224", "mrqa_squad-train-18146", "mrqa_squad-train-48328", "mrqa_squad-train-14151", "mrqa_squad-train-55842", "mrqa_squad-train-52528", "mrqa_squad-train-57196", "mrqa_squad-train-25763", "mrqa_squad-train-42827", "mrqa_squad-train-45410", "mrqa_squad-train-23651", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-validation-7549", "mrqa_searchqa-validation-8756", "mrqa_naturalquestions-validation-519", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12974", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-367", "mrqa_naturalquestions-validation-5256", "mrqa_newsqa-validation-2813", "mrqa_triviaqa-validation-7611", "mrqa_naturalquestions-validation-3483", "mrqa_newsqa-validation-421", "mrqa_naturalquestions-validation-10411"], "EFR": 0.9285714285714286, "Overall": 0.7371567399825784}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "Anna", "the beaver", "Dorothy", "Survivor: Fiji", "James West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Calvin Klein Eternity", "Marvell", "Quiz Show", "Portland", "acetone", "Heart of Darkness", "Psycho", "Napoleon", "lullaby", "objects", "Napoleon", "the Sahara", "the reticulated snake", "Munich", "digestif", "a jeopardy development", "Pope Benedict XVI", "Los Alamos", "Somerset Maugham", "sapphire", "Three Coins in the Fountain", "ER", "the Goldenrod", "Luke", "the rectum", "WATERS OF THE BODY", "frequency", "Grease", "the salamander", "Solzhenitsyn", "eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "Vanity Fair", "the Big Sky Conference", "the beaver", "Boston", "a high school girlfriend of an alcoholic student", "a goatherd", "Sweden", "Ajay Tyagi", "the 17th episode in the third season", "94 by 50", "salix", "Dm", "Utsire", "Karl- Anthony Towns", "Love at First Sting", "1988", "Hollywood", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "$10 billion", "her boyfriend, Dodi Fayed, and their driver, Henri Paul."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6730677308802309}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.3636363636363636]}}, "before_error_ids": ["mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-10537", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.578125, "CSR": 0.5555346385542168, "retrieved_ids": ["mrqa_squad-train-32310", "mrqa_squad-train-51152", "mrqa_squad-train-70640", "mrqa_squad-train-41613", "mrqa_squad-train-71054", "mrqa_squad-train-75639", "mrqa_squad-train-32429", "mrqa_squad-train-45826", "mrqa_squad-train-21762", "mrqa_squad-train-85691", "mrqa_squad-train-81031", "mrqa_squad-train-71186", "mrqa_squad-train-61692", "mrqa_squad-train-75746", "mrqa_squad-train-11788", "mrqa_squad-train-37676", "mrqa_searchqa-validation-6857", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2587", "mrqa_searchqa-validation-3222", "mrqa_naturalquestions-validation-3515", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-15007", "mrqa_hotpotqa-validation-1572", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-3298", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-1432", "mrqa_naturalquestions-validation-4196"], "EFR": 1.0, "Overall": 0.7514975527108433}, {"timecode": 83, "before_eval_results": {"predictions": ["the East Sea", "Stitch", "( Joe) Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafarianism", "cinnamon", "I Am the Very Model of a Modern Major-General", "extreme", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Asklepios", "troll", "The Flying Dutchman", "Dan Quayle", "Ruth the Moabitess", "William Faulkner", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "Kiribati", "Nepal", "Palladio", "names of God", "Where were you in '62", "Hair", "cicadas", "Asbury Park", "In darkness", "the white blossoms", "Zappa", "Hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "a loaf of bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Porridge", "Thermopylae", "Magdalene Laundries", "$10,000 Kelly", "\u00c6thelred I", "(William) Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.640625, "QA-F1": 0.721875}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-1991", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-1509"], "SR": 0.640625, "CSR": 0.5565476190476191, "retrieved_ids": ["mrqa_squad-train-55512", "mrqa_squad-train-15988", "mrqa_squad-train-9606", "mrqa_squad-train-11570", "mrqa_squad-train-30415", "mrqa_squad-train-73851", "mrqa_squad-train-82931", "mrqa_squad-train-29318", "mrqa_squad-train-23684", "mrqa_squad-train-52548", "mrqa_squad-train-55096", "mrqa_squad-train-84615", "mrqa_squad-train-12994", "mrqa_squad-train-21323", "mrqa_squad-train-85753", "mrqa_squad-train-53998", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-1813", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-10368", "mrqa_searchqa-validation-11888", "mrqa_newsqa-validation-1039", "mrqa_squad-validation-490", "mrqa_triviaqa-validation-2349", "mrqa_naturalquestions-validation-1870", "mrqa_searchqa-validation-2271", "mrqa_newsqa-validation-3662", "mrqa_triviaqa-validation-1122", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-93", "mrqa_naturalquestions-validation-7490", "mrqa_searchqa-validation-12117"], "EFR": 1.0, "Overall": 0.7517001488095237}, {"timecode": 84, "before_eval_results": {"predictions": ["typing speed", "a crescent", "a trident", "Abercrombie & Fitch", "Jefferson Davis", "Standard Oil", "Crustacea", "Laura Ingalls Wilder", "a carriage", "Monet", "chemicals", "Gerald R. Ford", "Louis Rukeyser", "Jupiter", "Clinton", "Truisms", "Tin", "Stephen Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia Bulldogs", "Giacomo Puccini", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "Montego Bay", "a relic", "cyclosporine", "the Northern Mockingbird", "RESTRICTIVE TYPE OF THIS, CLAUSE", "comedy", "the Owls", "length", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "a star", "1924", "741 weeks", "January 17, 1899", "Douglas MacArthur", "Project Gutenberg", "Indonesia", "Latin American culture", "a co-op of grape growers", "David Naughton", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7491319444444444}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1148"], "SR": 0.6875, "CSR": 0.5580882352941177, "retrieved_ids": ["mrqa_squad-train-5612", "mrqa_squad-train-15462", "mrqa_squad-train-43662", "mrqa_squad-train-682", "mrqa_squad-train-35195", "mrqa_squad-train-75887", "mrqa_squad-train-69034", "mrqa_squad-train-60908", "mrqa_squad-train-28182", "mrqa_squad-train-11975", "mrqa_squad-train-29543", "mrqa_squad-train-73599", "mrqa_squad-train-28345", "mrqa_squad-train-83829", "mrqa_squad-train-80737", "mrqa_squad-train-74572", "mrqa_searchqa-validation-15479", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1028", "mrqa_naturalquestions-validation-10512", "mrqa_triviaqa-validation-3715", "mrqa_naturalquestions-validation-5170", "mrqa_squad-validation-6470", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2403", "mrqa_triviaqa-validation-3452", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-3174", "mrqa_squad-validation-2289"], "EFR": 1.0, "Overall": 0.7520082720588235}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Ellen Holly", "the Prince & the Pauper", "Pushing Daisies", "July", "a reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Lake County, Indiana, United States", "Cleopatra", "a northern pike", "Krispy Kreme", "NYC's Trump Tower", "Luther", "rice", "Frasier", "Kansas City", "arteries", "\"Chinatown.\"", "improvised", "Hamlet", "lime", "The Aviator", "alkaline nedir, ne demek, alkaline anlam", "Robert Duvall", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "Chapter 6", "Fiddler On The Roof", "Pitcairn Island", "hockey", "etching", "Mars", "bone", "David", "potato", "a cookie jar", "Babe Ruth", "New England", "Conrad Hilton Jr.", "he was unable to wrest", "2016", "Jessica Simpson", "wyatt sowerby", "the rose bush", "Robert Plant", "Oklahoma", "138,535 people", "Terence Winter", "her son has strong values.", "\"Python Patrol\"", "The eye of Hurricane Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6951293498168498}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.7692307692307693, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13590", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-3222", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2753", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.578125, "CSR": 0.5583212209302326, "retrieved_ids": ["mrqa_squad-train-10979", "mrqa_squad-train-25873", "mrqa_squad-train-69316", "mrqa_squad-train-78752", "mrqa_squad-train-34510", "mrqa_squad-train-50832", "mrqa_squad-train-6657", "mrqa_squad-train-56237", "mrqa_squad-train-66897", "mrqa_squad-train-67469", "mrqa_squad-train-68314", "mrqa_squad-train-18935", "mrqa_squad-train-86002", "mrqa_squad-train-13704", "mrqa_squad-train-51234", "mrqa_squad-train-48376", "mrqa_naturalquestions-validation-7468", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1804", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-7605", "mrqa_newsqa-validation-3172", "mrqa_naturalquestions-validation-1003", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2753", "mrqa_naturalquestions-validation-3505", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-10285", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9824"], "EFR": 1.0, "Overall": 0.7520548691860466}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "president of Nicaragua", "Chastity", "Frank Sinatra Jr.", "Dmitri Mendeleev", "Kathleen Winsor", "poison gas", "luminous intensity", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "(John Paul) Jones", "The Rolling Stones", "Bridge to Terabithia", "Samuel A. Alito", "shoes", "Civis", "Hermann Hesse", "(Nicolaus) Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "\"Rich Girl\"", "Yogi Berra", "courage", "a jigger", "calcium", "a constitution", "the eastern Mediterranean", "virtual reality", "bass", "The Last Remake of Beau Geste", "hot air balloons", "Tarzan & Jane", "RBIs", "David Berkowitz", "oblique", "pies", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "Keanu Reeves", "the Bolsheviks", "April 17, 1982", "the Garden of Gethsemane", "the Vi\u1ec7t Minh and France", "James Cameron", "Everyhit", "Japan", "Major Charles White Whittlesey", "Kingdom of Dalmatia", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7510569852941177}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.4, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-4669", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-2007", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-4669"], "SR": 0.671875, "CSR": 0.5596264367816092, "retrieved_ids": ["mrqa_squad-train-609", "mrqa_squad-train-41587", "mrqa_squad-train-4215", "mrqa_squad-train-55571", "mrqa_squad-train-61589", "mrqa_squad-train-75419", "mrqa_squad-train-18537", "mrqa_squad-train-79868", "mrqa_squad-train-42729", "mrqa_squad-train-80266", "mrqa_squad-train-79196", "mrqa_squad-train-78815", "mrqa_squad-train-36210", "mrqa_squad-train-40444", "mrqa_squad-train-71130", "mrqa_squad-train-44230", "mrqa_searchqa-validation-12477", "mrqa_squad-validation-8598", "mrqa_newsqa-validation-346", "mrqa_naturalquestions-validation-6931", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-3478", "mrqa_newsqa-validation-530", "mrqa_searchqa-validation-4893", "mrqa_newsqa-validation-4144", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-276", "mrqa_newsqa-validation-2640", "mrqa_searchqa-validation-2394", "mrqa_naturalquestions-validation-9149", "mrqa_newsqa-validation-3930"], "EFR": 0.8571428571428571, "Overall": 0.7237444837848933}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "the spinning mule", "onerous", "the Clown portrait", "Fargo", "rushes", "fibreboard", "the River Thames", "Napster", "a member of the musical Partridge family", "Coors Field", "Elizabeth I", "Wicked", "mental", "exposure", "The lowest point", "the Golden Fleece", "satisfaction", "caution", "Macaulay Culkin", "a locomotive", "Edwards", "Hawaii", "the JFK assassination", "Daniel Boone", "a million", "hemoglobin", "Nancy Sinatra", "ear infection", "the fox", "tabby", "the New World", "Wisconsin", "the Kingdom of Iraq", "Canada", "bipolar", "a brownie", "anvil", "Alexander Calder", "honey", "(Matthew) Broderick", "Christopher Columbus", "a super villain", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "Detroit", "a statement that is taken to be true", "electors", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "kosher", "shark", "Meta", "Agent Carter", "the Parthian Empire", "\"Kill Your Darlings\"", "planning processes are urgently needed", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6692708333333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-14457", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-2933", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-16734", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10767", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6150", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725"], "SR": 0.640625, "CSR": 0.560546875, "retrieved_ids": ["mrqa_squad-train-68693", "mrqa_squad-train-21981", "mrqa_squad-train-39422", "mrqa_squad-train-35543", "mrqa_squad-train-13127", "mrqa_squad-train-39333", "mrqa_squad-train-85090", "mrqa_squad-train-36829", "mrqa_squad-train-62601", "mrqa_squad-train-25157", "mrqa_squad-train-18376", "mrqa_squad-train-61506", "mrqa_squad-train-9118", "mrqa_squad-train-42188", "mrqa_squad-train-35180", "mrqa_squad-train-65602", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-8177", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2562", "mrqa_searchqa-validation-10823", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-6009", "mrqa_searchqa-validation-429", "mrqa_squad-validation-8927", "mrqa_newsqa-validation-3662", "mrqa_naturalquestions-validation-7468", "mrqa_newsqa-validation-2616", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-8439", "mrqa_searchqa-validation-13787"], "EFR": 1.0, "Overall": 0.7525000000000001}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "Booster", "Biggie", "John the Baptist", "John Paul II", "Hillary Clinton", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "( James) Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "the NFL", "Twister", "Spain", "Scrabble", "the Caspian Sea", "football", "the Angels", "Cardiff", "the Ten", "12:11", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "the member who sits in the stern (except in bowloaders) facing the bow", "Transamerica", "China", "Jimmy Carter", "Central Park", "Henry Clay", "the bottom", "Petsmart", "Charles Robert Darwin", "Electric Avenue", "a bibliography", "Jerusalem", "Vanna White", "Toyota", "a temenos", "Istanbul", "Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "Elsa", "purification", "the following day", "early 1980s", "Taron Egerton", "a linesider", "wyatt de Valence", "The Undertones", "Groupe PSA", "Premier Division", "The Five", "stabbed Tate,", "Herman Cain,", "a grizzly bear", "james nolte"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6851562499999999}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5520", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_newsqa-validation-3714", "mrqa_triviaqa-validation-7327"], "SR": 0.609375, "CSR": 0.5610955056179776, "retrieved_ids": ["mrqa_squad-train-45651", "mrqa_squad-train-21548", "mrqa_squad-train-12654", "mrqa_squad-train-38321", "mrqa_squad-train-22738", "mrqa_squad-train-72768", "mrqa_squad-train-13896", "mrqa_squad-train-66584", "mrqa_squad-train-27667", "mrqa_squad-train-66813", "mrqa_squad-train-71885", "mrqa_squad-train-14971", "mrqa_squad-train-57453", "mrqa_squad-train-11876", "mrqa_squad-train-68813", "mrqa_squad-train-25599", "mrqa_newsqa-validation-2197", "mrqa_hotpotqa-validation-4424", "mrqa_naturalquestions-validation-3491", "mrqa_newsqa-validation-2041", "mrqa_searchqa-validation-5456", "mrqa_naturalquestions-validation-276", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-6663", "mrqa_naturalquestions-validation-6931", "mrqa_searchqa-validation-3800", "mrqa_hotpotqa-validation-820", "mrqa_triviaqa-validation-5467", "mrqa_searchqa-validation-8368", "mrqa_triviaqa-validation-7391", "mrqa_naturalquestions-validation-9789", "mrqa_newsqa-validation-1654"], "EFR": 0.92, "Overall": 0.7366097261235955}, {"timecode": 89, "before_eval_results": {"predictions": ["ermine", "Finding Nemo", "easel", "Grant Evans", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "England", "Denmark", "the saguaro", "Saigon", "Shinto", "\"reshit\"", "Venus", "iris", "carrie bradshaw", "Armistice", "toilet paper", "the Panama Canal", "Cesare Borgia", "pearl", "liqueur", "Hangman", "Bleak House", "October", "Gwine to Run All Night", "(George Bernard) Shaw", "Linkin Park", "doggy", "a storm", "the lungs", "gravity", "Captain James Cook", "Robert Bruce", "Marlon Brando", "the United States", "Lana Turner", "a bolt", "Othello", "Viva Zapata", "Bone Thugs-N-Harmony", "zebras", "Helio Castroneves", "Richard III", "Hugh Grant", "Godot", "voyeurism", "the Articles of Confederation", "Pavlov", "a hull", "Hot Wings", "England", "James Madison", "The Firm", "Harriet Tubman", "Hebrew", "\" Finding Nemo\"", "his superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\" and", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7530813834154352}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3448275862068966, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6]}}, "before_error_ids": ["mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-12554", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099", "mrqa_naturalquestions-validation-1848"], "SR": 0.671875, "CSR": 0.5623263888888889, "retrieved_ids": ["mrqa_squad-train-81986", "mrqa_squad-train-75015", "mrqa_squad-train-64246", "mrqa_squad-train-25857", "mrqa_squad-train-37880", "mrqa_squad-train-33712", "mrqa_squad-train-20913", "mrqa_squad-train-64425", "mrqa_squad-train-9434", "mrqa_squad-train-8520", "mrqa_squad-train-65175", "mrqa_squad-train-39182", "mrqa_squad-train-48899", "mrqa_squad-train-57652", "mrqa_squad-train-31010", "mrqa_squad-train-51455", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-16625", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8355", "mrqa_newsqa-validation-1563", "mrqa_naturalquestions-validation-2782", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-11559", "mrqa_naturalquestions-validation-2007", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-6353", "mrqa_searchqa-validation-266", "mrqa_newsqa-validation-1565"], "EFR": 0.9523809523809523, "Overall": 0.7433320932539683}, {"timecode": 90, "UKR": 0.814453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.896484375, "KG": 0.52578125, "before_eval_results": {"predictions": ["Wisconsin", "the nose of Gonzo", "a stagecoach", "Fonzie", "faction", "Hasta la vista", "New York", "the pastry", "fruit", "Tunisia", "a plexus", "a rattlesnake", "Nicholas", "absinthe", "(John) Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "Eternity", "Catherine of Aragon", "leader", "Ravi Shankar", "Bangkok", "Spain", "archery", "oblique", "( Joe) Torre", "meatballs", "Kennedy Space Center", "Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "sake", "Matt Leinart", "Alabama", "a drink", "Elizabeth", "the banjo", "a second feature", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Columbus", "prime", "Fi", "Telma Hopkins", "AD 95 -- 110", "pepsinogen", "Lorenzo", "1919", "Paris", "Point Place", "11", "National Aviation Hall of Fame", "Thursday", "78,000 parents of children ages 3 to 17.iReport.com:", "prisoners at the South Dakota State Penitentiary", "Anne Boleyn"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7791666666666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-2670", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3089", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-3063", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-10419", "mrqa_triviaqa-validation-3485"], "SR": 0.703125, "CSR": 0.5638736263736264, "retrieved_ids": ["mrqa_squad-train-16823", "mrqa_squad-train-39977", "mrqa_squad-train-40899", "mrqa_squad-train-8442", "mrqa_squad-train-52125", "mrqa_squad-train-9963", "mrqa_squad-train-65335", "mrqa_squad-train-56979", "mrqa_squad-train-16672", "mrqa_squad-train-28877", "mrqa_squad-train-51734", "mrqa_squad-train-73261", "mrqa_squad-train-20634", "mrqa_squad-train-13432", "mrqa_squad-train-643", "mrqa_squad-train-37022", "mrqa_triviaqa-validation-1363", "mrqa_naturalquestions-validation-8794", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3732", "mrqa_naturalquestions-validation-10615", "mrqa_squad-validation-10011", "mrqa_squad-validation-4908", "mrqa_newsqa-validation-3666", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2146", "mrqa_hotpotqa-validation-5556", "mrqa_squad-validation-7687", "mrqa_hotpotqa-validation-1028", "mrqa_newsqa-validation-4089"], "EFR": 0.9473684210526315, "Overall": 0.7495921594852516}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a chile", "Oliver Twist", "supernatural", "the Vistula", "Coriolanus", "Fort Worth", "an aide-de-camp", "an oblique fracture", "Roman Polanski", "Court TV", "sharia", "Jake La Motta", "blog", "Pan Am", "Athens", "Holiday Inn", "Buffalo Bills", "Bret Harte", "Islam", "(Madeleine) Albright", "Tian Shan", "the Harlem Renaissance", "Martha Cannary", "John Lennon", "(Richard) Branson", "leader", "lights", "Tarzan", "Once", "Warren G. Harding", "Berrigan", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Man Friday", "Lord North", "Wm. Wrigley", "the euro", "the narwhal", "the wall", "(John) Keats", "Wyatt Earp", "Punjabi", "Macedonia", "Department of Agriculture", "heels", "Frottage", "a triangle", "1999", "cheated on Miley", "2012", "jorge goodall", "(Henry) Hunt", "Tallinn", "Jane Mayer", "1993 to 2001", "Reverend Timothy \"Tim\" Lovejoy", "about 12 million in America,", "Charlotte Gainsbourg", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "Audrey Roberts"], "metric_results": {"EM": 0.625, "QA-F1": 0.6799400252525253}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.06060606060606061, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10428", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.625, "CSR": 0.5645380434782609, "retrieved_ids": ["mrqa_squad-train-74580", "mrqa_squad-train-17650", "mrqa_squad-train-71188", "mrqa_squad-train-72347", "mrqa_squad-train-56027", "mrqa_squad-train-65969", "mrqa_squad-train-57665", "mrqa_squad-train-23951", "mrqa_squad-train-10814", "mrqa_squad-train-24675", "mrqa_squad-train-59520", "mrqa_squad-train-10940", "mrqa_squad-train-44786", "mrqa_squad-train-15280", "mrqa_squad-train-55856", "mrqa_squad-train-38260", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3601", "mrqa_hotpotqa-validation-5394", "mrqa_newsqa-validation-25", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-5757", "mrqa_newsqa-validation-1392", "mrqa_hotpotqa-validation-4069", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3543", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-7581", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-3872", "mrqa_hotpotqa-validation-2623"], "EFR": 0.9583333333333334, "Overall": 0.7519180253623189}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Fiddler On The Roof", "Muhammad Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Sinclair Lewis", "Captains Courageous", "handles", "Central Park", "the nave", "The Tyger", "Chinese", "( Howard) Hughes", "Pablo Escobar", "a conifer", "Clinton", "an asteroid", "first base", "leather", "Ichabod Crane", "T. rex", "\"Chinatown.\"", "a butterfly", "Lolita", "Nibelung", "tango", "( Wesley) Clark", "Filet Mignon", "a penitent", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "his second visit to Galatia", "Lewis Carroll", "meters", "ribs", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "manet", "sons", "The Hairy Ape", "Jason Flemyng", "Eight full seasons", "British citizens", "estonia artist Nicholas Garland", "harrison ford", "France", "1968", "Vytautas \u0160apranauskas", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides,", "September 1947"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7230902777777777}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_searchqa-validation-2035", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-2156", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236", "mrqa_naturalquestions-validation-2586"], "SR": 0.6875, "CSR": 0.5658602150537635, "retrieved_ids": ["mrqa_squad-train-9949", "mrqa_squad-train-43774", "mrqa_squad-train-29676", "mrqa_squad-train-82930", "mrqa_squad-train-18892", "mrqa_squad-train-57470", "mrqa_squad-train-50008", "mrqa_squad-train-35965", "mrqa_squad-train-82494", "mrqa_squad-train-45061", "mrqa_squad-train-22013", "mrqa_squad-train-57967", "mrqa_squad-train-44355", "mrqa_squad-train-2415", "mrqa_squad-train-29495", "mrqa_squad-train-58172", "mrqa_naturalquestions-validation-1731", "mrqa_hotpotqa-validation-5850", "mrqa_newsqa-validation-1981", "mrqa_hotpotqa-validation-2731", "mrqa_squad-validation-6759", "mrqa_newsqa-validation-1933", "mrqa_triviaqa-validation-6277", "mrqa_searchqa-validation-1256", "mrqa_hotpotqa-validation-3417", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14096", "mrqa_squad-validation-1766", "mrqa_hotpotqa-validation-3921", "mrqa_squad-validation-6559", "mrqa_newsqa-validation-2891", "mrqa_naturalquestions-validation-10232"], "EFR": 0.95, "Overall": 0.7505157930107527}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet On the Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Indiana", "Pinocchio", "a kidney", "Paris", "singing machines", "the Chinese pantheon", "Maine", "Gertrude Stein", "The Sun Also Rises", "a dishwasher", "The Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "Varney Air Lines", "Notre Dame", "Tiberius Nero", "Jupiter", "loverly", "rugby", "the Falkland Islands", "Broadway", "Iceland", "Orwell", "chess", "Uneven Heating", "Jonathan Swift", "Miracle On 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "fuel", "the Mesozoic", "Eisenhower", "\"That Stranger Used to Be My Girl\"", "14 Points", "Freddie Mercury", "Mount Aso", "\"Harry Potter and the Order of the Phoenix\"", "Geronimo", "Post", "theMisty Mountains", "cantaloupe", "London", "Carl Sandburg", "a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "The Necromancer", "Eddie Murphy", "herald", "little Reggie", "the Treaty of Waitangi", "Jessica Lange", "Heinkel Flugzeugwerke", "Kenan & Kel", "304,000", "one", "8 p.m. local time", "working men in the tree business."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6092261904761904}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.1904761904761905, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-4374", "mrqa_searchqa-validation-1263", "mrqa_searchqa-validation-7293", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-15431", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-9026", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-7804", "mrqa_searchqa-validation-10151", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-7166", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-4060"], "SR": 0.546875, "CSR": 0.5656582446808511, "retrieved_ids": ["mrqa_squad-train-34602", "mrqa_squad-train-60412", "mrqa_squad-train-27292", "mrqa_squad-train-62173", "mrqa_squad-train-21332", "mrqa_squad-train-76950", "mrqa_squad-train-73802", "mrqa_squad-train-71811", "mrqa_squad-train-71344", "mrqa_squad-train-26032", "mrqa_squad-train-57064", "mrqa_squad-train-17299", "mrqa_squad-train-13555", "mrqa_squad-train-39315", "mrqa_squad-train-6183", "mrqa_squad-train-70590", "mrqa_triviaqa-validation-5467", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-9998", "mrqa_naturalquestions-validation-9523", "mrqa_newsqa-validation-2628", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-4471", "mrqa_newsqa-validation-1275", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-8417", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-3540", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2782"], "EFR": 0.9655172413793104, "Overall": 0.7535788472120324}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Starfighter", "Muqtada al-Sadr", "garden", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "(Jimmy) Doolittle", "a riot", "Lon Chaney", "New York", "Joel", "Sicily", "the Boston Celtics", "wine", "Enron", "the fulcrum", "the Central African Republic", "Rudolf Hess", "a fight", "the hippopotamus", "an eye", "Bech", "Reagan and Bush", "Washington Irving", "the White Mountains of California", "the Egyptians", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "(Jerry) Mathers", "Nine to Five", "Housing and Urban Development", "extradite", "the head", "Eddie Murphy", "Michael Collins", "The Sopranos", "The Sound And The Fury", "a mother- daughter dyad", "Brazil", "obsessive-compulsive", "Katie Holmes", "oatmeal", "the arteries", "1773", "Joule", "Justice", "20 November 1989", "25 September 2007", "the forces of Andrew Moray and William Wallace", "Nafea Faa Ipoipo", "window", "Crispin", "her translation of and commentary on Isaac Newton's book \"Principia\"", "PETE", "SKUM", "12-hour-plus", "Donald Trump.", "This will be the second", "Mary Rose Foster"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7486979166666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_triviaqa-validation-1700", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-1950", "mrqa_newsqa-validation-1587"], "SR": 0.640625, "CSR": 0.5664473684210527, "retrieved_ids": ["mrqa_squad-train-837", "mrqa_squad-train-28945", "mrqa_squad-train-70705", "mrqa_squad-train-79102", "mrqa_squad-train-76873", "mrqa_squad-train-29385", "mrqa_squad-train-56128", "mrqa_squad-train-86338", "mrqa_squad-train-8809", "mrqa_squad-train-56032", "mrqa_squad-train-68629", "mrqa_squad-train-6724", "mrqa_squad-train-38418", "mrqa_squad-train-54569", "mrqa_squad-train-24220", "mrqa_squad-train-68118", "mrqa_naturalquestions-validation-7901", "mrqa_searchqa-validation-13582", "mrqa_searchqa-validation-7185", "mrqa_naturalquestions-validation-8439", "mrqa_newsqa-validation-2315", "mrqa_triviaqa-validation-5933", "mrqa_newsqa-validation-4022", "mrqa_naturalquestions-validation-5676", "mrqa_hotpotqa-validation-5831", "mrqa_triviaqa-validation-7233", "mrqa_newsqa-validation-697", "mrqa_hotpotqa-validation-2134", "mrqa_naturalquestions-validation-359", "mrqa_newsqa-validation-3872", "mrqa_triviaqa-validation-3931", "mrqa_newsqa-validation-3194"], "EFR": 0.8695652173913043, "Overall": 0.7345462671624714}, {"timecode": 95, "before_eval_results": {"predictions": ["Arseniy Yatsenyuk", "beefcake", "the Kuomindang", "The Goonies", "Velvet Revolver", "the Haunted Mansion", "the Continental Congress", "Robert Johnson", "Mahlemuts", "a shank", "fish", "place", "Casablanca", "The Dutchess", "Detroit River", "Mme. Sand", "Northern Exposure", "Kilimanjaro", "Nebuchadnezzar", "a flip", "the Komodo", "Mordecai Richler", "Roseanne", "The West Wing", "a palton", "ravens", "Mexico", "Wade E. Pickren", "Pocahontas", "viruses", "John Hersey", "Patricia Arquette", "Ernie Banks", "a Grotto", "David H. Petraeus", "height", "Hades", "Whig", "Capone, Alphonse", "Maria Callas", "wakame", "the Tournament of Kings", "Antony", "Tennyson", "National Geographic", "\" song of the South\"", "Jerusalem", "the nativity scene", "the Edict of Nantes", "Achilles", "Omega", "at the end of an interrogative sentence", "Dr. Lexie Grey", "since 3, 1, and 4", "paul esterh\u00e1zy", "exponentiation", "wycestershire", "1754", "49", "Lowe's Companies, Inc.", "the Missouri River", "Fernando Gonzalez", "Chester Arthur Stiles,", "ants"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5833333333333333}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-7180", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-3457", "mrqa_triviaqa-validation-4855"], "SR": 0.53125, "CSR": 0.5660807291666667, "retrieved_ids": ["mrqa_squad-train-8753", "mrqa_squad-train-40187", "mrqa_squad-train-46498", "mrqa_squad-train-65503", "mrqa_squad-train-22939", "mrqa_squad-train-15410", "mrqa_squad-train-26697", "mrqa_squad-train-13815", "mrqa_squad-train-10998", "mrqa_squad-train-18022", "mrqa_squad-train-27479", "mrqa_squad-train-8398", "mrqa_squad-train-10404", "mrqa_squad-train-7766", "mrqa_squad-train-29789", "mrqa_squad-train-68719", "mrqa_naturalquestions-validation-10009", "mrqa_triviaqa-validation-6355", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-16710", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-3517", "mrqa_searchqa-validation-266", "mrqa_naturalquestions-validation-199", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-1380", "mrqa_squad-validation-694", "mrqa_newsqa-validation-1990", "mrqa_naturalquestions-validation-6049", "mrqa_naturalquestions-validation-2794"], "EFR": 0.9333333333333333, "Overall": 0.7472265625000001}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a cartwheel", "assemble", "hot air balloons", "pathetic fallacy", "Nomar Garciaparra", "John Glenn", "a heron", "Apollo 1", "the White Company", "New Balance", "Michael Gentry", "Joan of Arc", "finale", "mollusca", "Camille", "the East", "caricare", "the Seven Years' War", "Meg Ryan, Julia Roberts", "The Wizard of Oz", "madding", "tribal nations", "Ron Sandler", "Argentina", "Woodrow Wilson", "the Osmonds", "sul tuo amore", "Whatchamacallits", "\"Johnny Mathis & Deniece Williams\"", "Wyoming", "Tigger", "Geneva", "Frank Sinatra", "bisket", "an Islamic leadership position", "backstroke", "7", "Sydney", "dermatology", "Solomon", "Look Who\\'s Talking", "Chirac", "20", "Polaris", "Carrie and Irene Miner", "Guiana", "grow", "Slovakia", "Timothy", "dilithium", "the fifth studio album by English rock band the Beatles", "1997", "2010", "1215", "bearded woman", "first President of the United States of America", "Mumbai", "Robert", "three", "skeletal dysplasia,", "\"Steamboat Bill, Jr.\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5157738095238096}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-10078", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-1445", "mrqa_searchqa-validation-12162", "mrqa_naturalquestions-validation-9492", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-3859"], "SR": 0.453125, "CSR": 0.564916237113402, "retrieved_ids": ["mrqa_squad-train-44148", "mrqa_squad-train-60806", "mrqa_squad-train-65546", "mrqa_squad-train-42851", "mrqa_squad-train-61814", "mrqa_squad-train-16866", "mrqa_squad-train-23340", "mrqa_squad-train-16004", "mrqa_squad-train-26700", "mrqa_squad-train-40205", "mrqa_squad-train-73003", "mrqa_squad-train-72558", "mrqa_squad-train-51339", "mrqa_squad-train-55782", "mrqa_squad-train-25307", "mrqa_squad-train-76414", "mrqa_searchqa-validation-1167", "mrqa_squad-validation-4068", "mrqa_searchqa-validation-6798", "mrqa_newsqa-validation-419", "mrqa_naturalquestions-validation-5980", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1911", "mrqa_triviaqa-validation-5338", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-2197", "mrqa_searchqa-validation-3736", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9726", "mrqa_searchqa-validation-1788"], "EFR": 1.0, "Overall": 0.7603269974226804}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomads", "Washington", "tribbles", "the Death Valley", "The Two Gentlemen of Verona", "a Cobb salad", "a Hydra", "Gulliver\\'s Travels", "the DEW Line", "Tordis", "jelly beans", "the Himalayan", "sonic boom", "Fergie", "Sacramento", "emerald", "Swiss Cheese", "Ernest Hemingway", "cola", "Annika Sorenstam", "atoms", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "(Henry) Shrapnel", "Venezuela", "Arethusa", "Oklahoma City", "Brazil", "The Criterion Collection", "the dugong", "rain", "1880", "the French and Indian War", "a checkerboard", "Waterloo", "a waterbed", "a monkey", "a bagel", "propeller", "bonnet", "an acre", "(William) Degas", "a cruller", "Helium", "Tokyo", "cream", "Charles Perrault", "Bali, Indonesia", "c. 1000 AD", "Tony Blair", "alzheimer", "big Dipper", "Sofia the First", "Africa", "Ben Elton", "an annual road trip,", "Hamburg", "shortly before the explosion", "Sugar Ray Robinson"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6904761904761905}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_searchqa-validation-9638", "mrqa_naturalquestions-validation-8823", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3521", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-2203", "mrqa_hotpotqa-validation-3237"], "SR": 0.65625, "CSR": 0.5658482142857143, "retrieved_ids": ["mrqa_squad-train-1787", "mrqa_squad-train-53430", "mrqa_squad-train-45229", "mrqa_squad-train-952", "mrqa_squad-train-75871", "mrqa_squad-train-50515", "mrqa_squad-train-57435", "mrqa_squad-train-65690", "mrqa_squad-train-11356", "mrqa_squad-train-10077", "mrqa_squad-train-54002", "mrqa_squad-train-86150", "mrqa_squad-train-37944", "mrqa_squad-train-35757", "mrqa_squad-train-48934", "mrqa_squad-train-66560", "mrqa_searchqa-validation-3297", "mrqa_naturalquestions-validation-288", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-2371", "mrqa_naturalquestions-validation-8794", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-455", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-3139", "mrqa_naturalquestions-validation-4981", "mrqa_newsqa-validation-3782", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-78"], "EFR": 0.9545454545454546, "Overall": 0.7514224837662338}, {"timecode": 98, "before_eval_results": {"predictions": ["Marley", "Magnum", "the Ottoman Empire", "Helen of Troy", "whale", "New York", "Himalayas", "Wayne's World", "Poland", "Kwanzaa", "nuclear", "Russell Crowe", "'Gump'", "a Shelby GT350", "tears", "roulette", "W. Somerset Maugham", "Christo", "Matisse", "bottom", "All Quiet On The Western Front", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "Spain", "Ford", "Sidney Sheldon", "Surround", "Faraday", "breakfast", "Krispy Kreme", "Spanish dignitary", "( Stanton) Avery", "the Death Valley", "the Cumberland Gap", "yolk", "the Navy", "lap", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Chirac", "proclamation", "Destiny's Child", "Luxor", "Spain", "\"Strawberry Fields Forever\"", "anchovy", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "finger", "james I", "Macbeth", "Carol Ann Duffy", "Ravenna", "travel", "\"We don't disclose or discuss our enhanced security measures and/or procedures that we have in place, may institute at any given time, with others intentionally less noticeable,\"", "\"I'm absolutely ecstatic about the situation. I've got a good group of Marines that are behind me, so I'm real excited about the deployment,\"", "an Iranian court", "make life a little easier"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7243990384615384}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.640625, "CSR": 0.5666035353535354, "retrieved_ids": ["mrqa_squad-train-35705", "mrqa_squad-train-77473", "mrqa_squad-train-2201", "mrqa_squad-train-42871", "mrqa_squad-train-19115", "mrqa_squad-train-74339", "mrqa_squad-train-7159", "mrqa_squad-train-26839", "mrqa_squad-train-14514", "mrqa_squad-train-5487", "mrqa_squad-train-75651", "mrqa_squad-train-35133", "mrqa_squad-train-23580", "mrqa_squad-train-42067", "mrqa_squad-train-54781", "mrqa_squad-train-9539", "mrqa_newsqa-validation-1758", "mrqa_hotpotqa-validation-2236", "mrqa_triviaqa-validation-1833", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-1392", "mrqa_hotpotqa-validation-597", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-1840", "mrqa_searchqa-validation-13590", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-1905", "mrqa_hotpotqa-validation-3644", "mrqa_triviaqa-validation-86", "mrqa_searchqa-validation-9204"], "EFR": 0.9565217391304348, "Overall": 0.7519688048967941}, {"timecode": 99, "UKR": 0.818359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.896484375, "KG": 0.5140625, "before_eval_results": {"predictions": ["the Hundred Years' War", "the backbone", "( Alfred) Binet", "Venial sin", "a caveat", "Ruby slippers", "Frank's", "the Spanish Republic", "Vanessa Hudgens", "Mighty Joe Young", "The Two Towers", "Japan", "Rhiannon", "Scotland", "leave It to Beaver", "Kurdish", "Ann Richards", "half-staff", "Macau", "Langston Hughes", "New Coke", "The Color Purple", "THX surround sound system", "Macbeth", "El Greco", "General Motors", "Dog", "shark", "Frankie Valli", "a blade", "a backpacking route", "pineapple", "Buffalo nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "fondue", "TV", "Schwarzenegger", "(AT&T) Gifford", "Animal Crackers", "oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Russia", "Students for a Democratic Society", "All the King\\'s Men", "(Beniamino) Gigli", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 14, 2017", "James Mason", "a sackbut", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale", "Matamoros, Mexico,", "Florida", "on Capitol Hill,", "775"], "metric_results": {"EM": 0.640625, "QA-F1": 0.696875}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-13935", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-15432", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-4773", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-1302", "mrqa_triviaqa-validation-2452", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2926"], "SR": 0.640625, "CSR": 0.56734375, "retrieved_ids": ["mrqa_squad-train-9095", "mrqa_squad-train-53517", "mrqa_squad-train-83280", "mrqa_squad-train-82196", "mrqa_squad-train-24934", "mrqa_squad-train-65065", "mrqa_squad-train-28885", "mrqa_squad-train-102", "mrqa_squad-train-78539", "mrqa_squad-train-54860", "mrqa_squad-train-46954", "mrqa_squad-train-79638", "mrqa_squad-train-61722", "mrqa_squad-train-9485", "mrqa_squad-train-25380", "mrqa_squad-train-61652", "mrqa_squad-validation-2920", "mrqa_hotpotqa-validation-881", "mrqa_naturalquestions-validation-8119", "mrqa_newsqa-validation-1531", "mrqa_naturalquestions-validation-10114", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-13593", "mrqa_naturalquestions-validation-4134", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-394", "mrqa_searchqa-validation-10017", "mrqa_squad-validation-117", "mrqa_searchqa-validation-9174", "mrqa_newsqa-validation-3799", "mrqa_triviaqa-validation-5766"], "EFR": 0.9130434782608695, "Overall": 0.741858695652174}]}