{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=3e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5280, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.875, "Overall": 0.8125}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state (including the judges)", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "The relationship between some gut flora and humans is not merely commensal ( a non-harmful coexistence )", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7842046957671958}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.734375, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "to synthesize fatty acids, isopentenyl pyrophosphate, iron-sulfur clusters, and carry out part of the heme pathway", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Metropolitan Statistical Areas", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "successfully preventing it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "The increasing use of technology, specifically the rise of the internet over the past decade,", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "breaches of law in protest against international organizations and foreign governments.", "Anglo-Saxon language of their subjects", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Iberia", "1913", "patient compliance issues", "20th century", "ambiguity", "\"Bells\"", "to closing the achievement gap between financially comfortable white stu- dents...... before the Highland Lakes Democratic Women in Kingsland on Dec.", "Abraham Lincoln", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound. These breeds are hunting dogs and are generally regarded as having some... Scent hounds specialize in following a scent without having to...", "Aeneid - Proper Names Flashcards, and more  for free.... Dardanus. son of Zeus and Electra, founder of Dardania in the Troad, and.... a mountain peak in northeast Greece", "the Alleged 9-11 Hijackers - Emerald  Within 24h of the attacks, CNN had this first FBI list of 19.", "City on the south side of the most congested U.S.-Mexico crossing; half the northbound cars wait 90 minutes", "Scandinavians", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7135416666666666}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8761", "mrqa_squad-validation-3497", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-1880", "mrqa_squad-validation-6244", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.65625, "CSR": 0.7135416666666667, "EFR": 1.0, "Overall": 0.8567708333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "oxyacetylene welding", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "his son Duncan", "spreading \"an idealized and systematized version of conservative tribal village customs\" under the label of Sharia", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "rubisco", "\"The Book of Roger\"", "the object's mass", "Africa", "Pierre Bayle", "the strain that caused the Black Death is ancestral to most modern strains of the disease", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "The Shirehorses", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "to be identified as transgender", "672", "the Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "he is telling me to regain the trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8485835416182284}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.782608695652174, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.11764705882352941, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-9740", "mrqa_squad-validation-4631", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5372", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.78125, "CSR": 0.73046875, "retrieved_ids": ["mrqa_squad-train-15712", "mrqa_squad-train-35796", "mrqa_squad-train-48057", "mrqa_squad-train-16046", "mrqa_squad-train-84787", "mrqa_squad-train-41183", "mrqa_squad-train-73400", "mrqa_squad-train-59352", "mrqa_squad-train-7834", "mrqa_squad-train-11026", "mrqa_squad-train-52431", "mrqa_squad-train-12129", "mrqa_squad-train-5771", "mrqa_squad-train-57022", "mrqa_squad-train-43853", "mrqa_squad-train-52840", "mrqa_squad-validation-3497", "mrqa_squad-validation-4206", "mrqa_squad-validation-525", "mrqa_squad-validation-7430", "mrqa_squad-validation-10427", "mrqa_squad-validation-7162", "mrqa_searchqa-validation-7896", "mrqa_squad-validation-9896", "mrqa_squad-validation-3998", "mrqa_squad-validation-809", "mrqa_squad-validation-680", "mrqa_searchqa-validation-3385", "mrqa_squad-validation-8452", "mrqa_squad-validation-3922", "mrqa_squad-validation-1504", "mrqa_searchqa-validation-15243"], "EFR": 0.9285714285714286, "Overall": 0.8295200892857143}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "the West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "25-minute", "captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output", "The Central Region", "March Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "a domestic scale", "force model", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "the ATP is synthesized there, in position to be used in the dark reactions", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "Terry & June", "architecture", "arrows", "Moles", "a complex number raised to the zero", "Mikhail Gorbachev", "A Beautiful Mind", "Quentin Blake", "The History Boys", "a valid passport", "a measure of the relative length of a gun", "Protons", "James Hoban", "elia Earhart", "1963", "making", "Sasha Banks", "The United States of America", "the iPods were largely overshadowed by Tuesday's iPhone 4S news", "Charles M. Schulz"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7057291666666666}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-6197", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.671875, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62 acres", "Maciot de Bethencourt", "Spain", "Carolina's defense", "Cam Newton", "eastwards", "they have lost their phycobilisomes", "his last statement", "Pleistocene epoch", "he published his findings first", "Nurses", "time and space complexity", "1951", "Wales", "black earth", "Nederrijn", "opposite end from the mouth", "Hindu and Buddhist sculptures", "the mid-sixties", "Kuz nets curve hypothesis", "lost chloroplast's existence", "Schr\u00f6dinger equation", "90\u00b0 out of phase with each other", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "within the chloroplast's stroma", "operations requiring constant speed", "10 November 2017", "baeocystin", "\"Krabby Road\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Daniel Roebuck", "Jenn Brown", "1999 Odisha cyclone", "Fat Albert", "\" Frontline\" magazine", "d\u00edsabl\u00f3t", "Tom Kartsotis", "modern genetics", "a person trained to pilot, navigate, or otherwise participate as a crew member of a spacecraft", "it appears that Ali Baashi was also specifically targeted by gunmen", "british", "to help evacuate them"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6132406655844156}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-791", "mrqa_squad-validation-8647", "mrqa_squad-validation-9176", "mrqa_squad-validation-7463", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.5625, "CSR": 0.6927083333333333, "EFR": 0.9642857142857143, "Overall": 0.8284970238095237}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "to mount faster and stronger attacks each time this pathogen is encountered", "Calvin cycle", "Zhenjin", "education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "voters were supposed to line up behind their favoured candidates", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "the courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "velocity", "Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "San Francisco", "The View and The Chew", "the Parliament of the United Kingdom at Westminster", "it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "dam", "1", "Jane", "McKinney", "Spock", "Solomon", "Jane", "geomorphology", "Earth", "krokos", "Richmond in North Yorkshire", "The Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "Ethiopia", "1973", "Duck Soup", "London", "Jane Thompson", "Metro Memphis", "Papua province", "Jay", "Paul Biya", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5807224025974026}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-10287", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_squad-validation-7013", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_hotpotqa-validation-426", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.53125, "CSR": 0.6696428571428572, "retrieved_ids": ["mrqa_squad-train-45648", "mrqa_squad-train-55911", "mrqa_squad-train-84271", "mrqa_squad-train-5253", "mrqa_squad-train-13420", "mrqa_squad-train-72829", "mrqa_squad-train-26077", "mrqa_squad-train-70423", "mrqa_squad-train-45681", "mrqa_squad-train-56195", "mrqa_squad-train-3568", "mrqa_squad-train-15047", "mrqa_squad-train-73426", "mrqa_squad-train-43696", "mrqa_squad-train-12540", "mrqa_squad-train-47467", "mrqa_squad-validation-5758", "mrqa_searchqa-validation-5713", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-961", "mrqa_triviaqa-validation-5194", "mrqa_squad-validation-1880", "mrqa_squad-validation-4206", "mrqa_triviaqa-validation-2758", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-3564", "mrqa_squad-validation-7463", "mrqa_squad-validation-10466", "mrqa_hotpotqa-validation-2905", "mrqa_squad-validation-3497", "mrqa_hotpotqa-validation-3821"], "EFR": 0.8666666666666667, "Overall": 0.768154761904762}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time", "Rhine-kilometers", "14", "150", "North American Aviation", "to register as a professional on the General Pharmaceutical Council (GPhC) register", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "His lab was torn down in 1904", "interacting", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "forts Shirley had erected at the Oneida carry", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "the south", "Geordie", "fuel", "US$10 a week", "the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds (64 kg)", "1806-07", "martin", "a shed", "martin", "a police car", "car show", "Wordsworth", "bos", "denmark", "Athens", "Rookwood", "Prada", "Edward R. Murrow", "denmark", "a wave pump", "brit", "ro", "p", "trophy hunting", "Christopher Marlowe", "iPhone", "all right angles are equal", "Genoa", "twins", "a young mystery brun", "Korean War", "bionic", "eight", "Jane Eyre", "Normandy", "Hussein's Revolutionary Command Council", "police", "cowardly lion", "March 22"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5656622023809523}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6324", "mrqa_squad-validation-1404", "mrqa_squad-validation-2097", "mrqa_squad-validation-10251", "mrqa_squad-validation-6773", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_hotpotqa-validation-1653", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.53125, "CSR": 0.65234375, "EFR": 0.9666666666666667, "Overall": 0.8095052083333334}, {"timecode": 8, "before_eval_results": {"predictions": ["a flour mill Boulton & Watt were building", "directly every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th century", "counterflow", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "late night talk shows", "entertainers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Torchwood: Miracle Day", "November 1979", "linear", "breaches of law in protest against international organizations and foreign governments", "Cobham", "Sir Edward Poynter", "Behind the Sofa", "the Simien Mountains National Park", "Florida State University", "Ear's malleus", "Mao Zedong", "Arroz con Leche", "Hawaii", "the Kiwanis Club", "the log cabin", "saxophones", "a tornado", "George Sand", "the Z", "the Clinica Regina Margherita", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "a balloon", "The Princess Diaries", "the Supreme Court", "Massachusetts", "larynx", "Dagny Taggart", "April", "cuffle", "right angle", "Kentucky", "Henry Clay", "Chinese Exclusion Act", "a cracanthos inermis", "1995", "Harry Nicolaides", "Mineola, New York", "Blender's \"500 Greatest Songs Since You Were Born\"", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6605282738095238}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false], "QA-F1": [0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-964", "mrqa_squad-validation-434", "mrqa_squad-validation-5374", "mrqa_squad-validation-8747", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.59375, "CSR": 0.6458333333333333, "EFR": 1.0, "Overall": 0.8229166666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood", "Dutch law", "Terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "the Red Turban rebels", "Jurassic Period", "Presque Isle", "William S. Paley", "anaerobic bacteria", "more greenish", "eicosanoids and cytokines", "live", "50-yard line", "captured the mermaid", "1/6", "DC traction motor", "the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\"", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "Mel Jones", "North America", "Sachin Tendulkar and Kumar Sangakkara", "Coton", "~ 1 kHz", "flytrap", "last Ice Age", "Allison Janney", "2026", "Georgia", "punishment", "1984", "4 September 1936", "Andrew Moray and William Wallace", "Ali Skovbye", "Pangaea", "Have I Told You Lately", "the atrioventricular node", "September", "the 2013 non-fiction book of the same name by David Finkel", "to prevent further offense", "Bob Dylan", "September of that year", "judges", "Lynda Carter", "virtually no limit to the number of reads", "A substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Meg Foster", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "American Samoa", "around 3.5 percent of global greenhouse emissions", "Billy Budd", "En banc"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6472994662608633}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.5, 0.16666666666666669, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.07692307692307693, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-4460", "mrqa_squad-validation-9764", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-12968"], "SR": 0.546875, "CSR": 0.6359375, "retrieved_ids": ["mrqa_squad-train-58475", "mrqa_squad-train-56619", "mrqa_squad-train-6069", "mrqa_squad-train-53421", "mrqa_squad-train-43018", "mrqa_squad-train-35453", "mrqa_squad-train-23244", "mrqa_squad-train-36160", "mrqa_squad-train-496", "mrqa_squad-train-75149", "mrqa_squad-train-46570", "mrqa_squad-train-29318", "mrqa_squad-train-49952", "mrqa_squad-train-18262", "mrqa_squad-train-76481", "mrqa_squad-train-73202", "mrqa_triviaqa-validation-5803", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-12316", "mrqa_newsqa-validation-3564", "mrqa_hotpotqa-validation-3247", "mrqa_searchqa-validation-5070", "mrqa_squad-validation-1504", "mrqa_searchqa-validation-12648", "mrqa_naturalquestions-validation-6050", "mrqa_squad-validation-6284", "mrqa_squad-validation-8238", "mrqa_naturalquestions-validation-3663", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-5672", "mrqa_newsqa-validation-1718", "mrqa_triviaqa-validation-1441"], "EFR": 0.9655172413793104, "Overall": 0.8007273706896552}, {"timecode": 10, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.923828125, "KG": 0.48515625, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "General Amherst", "pastors and teachers", "Justin Tucker", "1543", "None", "State Route 41", "Switzerland", "unit-dose, or a single doses of medicine", "War of Currents", "the tradition", "continental European", "Roger Goodell", "events", "9", "Adelaide", "once", "Around 200,000", "itty Hawk", "Nidal Hasan", "the University of Maryland", "priest Charles Coughlin", "Sean", "Consigliere", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling", "James Gay-Rees", "Augusta", "1970", "1978", "Barack Obama", "My Cat from Hell", "Mark Sinclair", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West, Florida", "gastrocnemius", "John Roberts", "repechage", "Carl Johan", "two", "Madonna", "Freddie Mercury", "the Sousa Band"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7883814102564102}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-2337", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674", "mrqa_naturalquestions-validation-7608", "mrqa_searchqa-validation-4509"], "SR": 0.671875, "CSR": 0.6392045454545454, "EFR": 1.0, "Overall": 0.7662784090909092}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000 square kilometres", "Bishopsgate", "Mnemiopsis", "tomb and memorial", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308 points", "Queen Victoria", "huge", "the main opposition party, the Orange Democratic Movement (ODM)", "Charlesfort", "adapt", "Battle of the Restigouche", "Boston", "forces", "head writer and executive producer", "elephant Man", "a psychologist", "every ten years", "Conan Doyle", "Batmitten", "Hong Kong", "ambilevous", "Bruce Wayne", "a horse", "Irrawaddy River", "Ed White", "River Hull", "Tet", "Samuel Johnson", "Copenhagen", "Troy", "Amnesty International", "John Gorman", "buffalo", "Edinburgh", "Viking feet", "Paul Gauguin", "Action Comics", "Enigma", "change from a solid", "Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "The Union Gap", "floating ribs", "hosting and organising the annual summit", "golf", "The current Secretary of Homeland Security", "Barry and Robin Gibb", "Adelaide", "Eddie Izzard", "Sabina Guzzanti", "a $13 million global crime ring", "a quark", "Neon", "Glory"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6382733585858585}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.6666666666666666, 1.0, 0.0, 0.7272727272727273, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-216", "mrqa_squad-validation-9255", "mrqa_squad-validation-8421", "mrqa_squad-validation-6449", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-395", "mrqa_newsqa-validation-3199", "mrqa_naturalquestions-validation-9323"], "SR": 0.5625, "CSR": 0.6328125, "EFR": 0.8928571428571429, "Overall": 0.7435714285714285}, {"timecode": 12, "before_eval_results": {"predictions": ["pulmonary fibrosis", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Keraites", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September 1901", "The United States of America", "The Dragon", "psilocin", "the Fundamentalist Church of Jesus Christ of Latter Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "Mary MacLane", "Omega SA", "September 14, 1877", "alternate", "Yasir Hussain", "Malayalam", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Jack Ryan", "Emilia-Romagna Region", "the reigning monarch of the United Kingdom", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame (IBHOF)", "The 1996 PGA Championship", "Revolver", "Jack Nicklaus", "atransformation of the Greek \u03bc\u03b5\u03c4\u03ac\u03bd\u03bf\u03b9\u03b1", "Mussolini", "Oliver", "off the coast of Dubai", "1918", "butter", "Warwick", "p Pleistocene Epoch"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7476641414141414}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-47", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_searchqa-validation-16321", "mrqa_triviaqa-validation-2147"], "SR": 0.6875, "CSR": 0.6370192307692308, "retrieved_ids": ["mrqa_squad-train-72600", "mrqa_squad-train-62379", "mrqa_squad-train-46278", "mrqa_squad-train-42569", "mrqa_squad-train-68436", "mrqa_squad-train-28704", "mrqa_squad-train-27571", "mrqa_squad-train-16004", "mrqa_squad-train-30212", "mrqa_squad-train-24422", "mrqa_squad-train-17781", "mrqa_squad-train-31143", "mrqa_squad-train-84523", "mrqa_squad-train-73178", "mrqa_squad-train-15036", "mrqa_squad-train-56538", "mrqa_searchqa-validation-11392", "mrqa_squad-validation-9740", "mrqa_hotpotqa-validation-2896", "mrqa_naturalquestions-validation-4222", "mrqa_searchqa-validation-5070", "mrqa_triviaqa-validation-1686", "mrqa_squad-validation-2145", "mrqa_squad-validation-10251", "mrqa_searchqa-validation-8139", "mrqa_squad-validation-8747", "mrqa_searchqa-validation-8411", "mrqa_squad-validation-1272", "mrqa_squad-validation-10466", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-9679", "mrqa_hotpotqa-validation-4967"], "EFR": 1.0, "Overall": 0.7658413461538462}, {"timecode": 13, "before_eval_results": {"predictions": ["the riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "\"Guilt implies wrong-doing. I feel I have done no wrong, but I am guilty of doing no wrong. I therefore plead not guilty.\"", "colloblasts", "$20.4 billion", "twelve", "Anglo-Saxons", "Doctor Who Confidential documentary", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise", "Roman law meaning 'empty land'", "Henry Hudson", "chipmunk", "the Red King", "Melbourne", "Albania", "trout", "Mayflower", "Ron Ely", "lacrimal", "George Best", "a rainy voices", "The Great British Bake Off", "Red Lion", "Fenn Street School", "the Smiths", "Peter Crouch", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "beards", "Andes", "Thor", "The Comitium", "Moon River", "Tina Turner", "SW20", "Lancashire", "Pacific Ocean", "racing", "Rustle My Davies", "climatology", "Charlie Brown", "roshi", "aguacate", "Black Sea", "lactic acid", "1933", "Mirror Image", "Abu Dhabi", "Craig William Macneill", "terminal brain cancer", "800,000", "craters", "gigante", "Serie B", "Saoirse Ronan"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6664434523809524}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444444, 1.0, 1.0, 1.0, 1.0, 0.8, 0.2222222222222222, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-4951", "mrqa_naturalquestions-validation-6991", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315"], "SR": 0.59375, "CSR": 0.6339285714285714, "EFR": 0.9615384615384616, "Overall": 0.7575309065934066}, {"timecode": 14, "before_eval_results": {"predictions": ["the Cathedral of Saint John the Divine", "The Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW", "Magnetophon tape recorder", "he explored the mountains in hunter's garb", "Rotterdam", "population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people)", "Charles Dickens", "force", "best teachers", "imperfect", "albatross", "woodlands", "go and save the best for last", "The National Gallery of Art", "Portland", "Boston", "Geneva", "woodlands", "a triangle", "turkeys", "russell", "woodlands", "William", "a woodrot", "a light-year", "netherlands", "russellador de Sevilla", "Tim Russert", "russell", "cocoa butter", "Violent Femmes", "quiz", "mind of the Church", "laser", "James Fenimore Cooper", "Veep", "sparkle", "russell", "lead villain", "woodlands", "Copenhagen", "cape horn", "Jose de San", "Madrid", "quick", "rufino", "Colorado", "blackwell's Island", "fertilization", "world's second most populous country after the People's Republic of China", "Nissan", "wood", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "Honda", "woodlands"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5337053571428572}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7170"], "SR": 0.46875, "CSR": 0.6229166666666667, "EFR": 0.9705882352941176, "Overall": 0.7571384803921569}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "paramagnetic", "criminal investigations", "complexity classes", "1963", "Jamukha", "consultant", "711,988", "Mumbai Rajdhani Express", "Speaker of the House of Representatives", "Hugo Weaving", "the passing of the year", "The Dursley family", "Brobee", "the somatic nervous system and the autonomic nervous system", "Aman Gandotra", "Jethalal Gada", "Kevin Sumlin", "a tree species", "the beginning of the American colonies", "Canada", "two - stroke engines and chain drive", "the Italian / Venetian John Cabot", "a writ of certiorari", "Dan Stevens", "Guant\u00e1namo Bay", "patents", "The Jamestown settlement", "January 2017 patch", "2013", "Tatsumi", "December 15, 2017", "Sunni Muslim family", "Magnavox Odyssey", "the Internet", "Christianity", "India", "the nucleus", "The Occupation of the Ruhr", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "ADP and P", "the 1964 Republican National Convention in San Francisco, California", "Hal Derwin", "to oversee the local church", "2007", "55 - 75", "Robert Boyle", "the solar system", "Wandervogel", "Ludwig van Beethoven", "his former Boca Juniors teammate and national coach", "Akshay Kumar", "Harriet M. Welsch", "Bananas", "nitride", "a bouquet"], "metric_results": {"EM": 0.5, "QA-F1": 0.6013550685425686}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.7499999999999999, 0.0, 0.19999999999999998, 0.13333333333333333, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 0.4, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-397", "mrqa_searchqa-validation-8385"], "SR": 0.5, "CSR": 0.615234375, "retrieved_ids": ["mrqa_squad-train-50519", "mrqa_squad-train-33918", "mrqa_squad-train-85522", "mrqa_squad-train-4262", "mrqa_squad-train-8883", "mrqa_squad-train-14467", "mrqa_squad-train-79781", "mrqa_squad-train-22264", "mrqa_squad-train-58443", "mrqa_squad-train-52820", "mrqa_squad-train-49729", "mrqa_squad-train-49226", "mrqa_squad-train-83258", "mrqa_squad-train-3252", "mrqa_squad-train-57723", "mrqa_squad-train-4798", "mrqa_naturalquestions-validation-801", "mrqa_triviaqa-validation-7742", "mrqa_squad-validation-5758", "mrqa_naturalquestions-validation-2146", "mrqa_squad-validation-9565", "mrqa_searchqa-validation-13012", "mrqa_hotpotqa-validation-454", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-16156", "mrqa_naturalquestions-validation-5798", "mrqa_hotpotqa-validation-2341", "mrqa_squad-validation-6449", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-177", "mrqa_triviaqa-validation-146", "mrqa_searchqa-validation-9133"], "EFR": 0.96875, "Overall": 0.755234375}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit", "the worst-case time complexity T(n)", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "quite complex", "temperatures that are too cold in northern Europe for the survival of fleas", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the leader of the political party or coalition with the most seats", "April 1887", "cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions", "Rock Follies of \u201977", "Montmorency", "Nut & Honey Crunch", "Elton John", "beer", "David Davis", "a double dip recession", "Corfu", "a leaf", "Leopoldville", "8 minutes", "the National Industrial Conference Board", "four", "cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "white spirit", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna", "James Mason", "Hypervelocity star", "West Point", "ostrich", "Moby Dick", "William Golding", "the 5th fret", "the Runaways", "Kim Clijsters", "Max Bygraves", "the A38", "Nicola Walker", "Virgin", "1997", "Port Talbot", "rainfall", "\"The best is yet to come\"", "Team GB", "Sax Rohmer", "the EU Data Protection Directive 1995 protection", "May 2010", "Bruce R. Cook", "the third Viscount", "Obama administration", "bomber in volatile northwestern Pakistan killed at three people and wounded 15 others", "the Equator", "I Don't Want to Miss a Thing", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6302588858509911}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-1206", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-12952", "mrqa_hotpotqa-validation-4298"], "SR": 0.5625, "CSR": 0.6121323529411764, "EFR": 1.0, "Overall": 0.7608639705882354}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "cut the French fortress at Louisbourg off from land-based reinforcements", "journalist", "Seventy percent", "modern hatred of the Jews", "Germany and Austria", "the principle of inclusions and components", "Sweynforkbeard", "the King", "eight", "Sierra Freeway", "Mickey Mouse", "manly characters", "Spain", "may", "Google", "dance", "born into Brothels", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "frosted", "jstor", "Vasco da Gama", "canter", "Musculus gluteus maximus", "1972", "Arbor Day", "Countrywide Financial Corp.", "red light", "Conan O'Brien", "norway", "manav", "nicita Khrushchev", "other voices", "hair", "black forest", "manhattan", "joan", "sepoys", "e", "manhattan", "submarines", "joan la Pucelie", "pea soup", "Trinidad and Tobago", "Vladimir Nabokov", "may", "Peter Pan", "hama", "a laser beam", "Phi Beta fraternity", "Joel", "the angel of the Lord", "vaud", "Prince Philip", "Athenion", "over 12 million", "pilot", "a pregnant soldier", "norway", "manhattan", "paper sales company"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4889322916666667}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.33333333333333337, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-10273", "mrqa_squad-validation-4260", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-230", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-2618", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-6435"], "SR": 0.390625, "CSR": 0.5998263888888888, "EFR": 1.0, "Overall": 0.7584027777777778}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "Rheinbrech", "Alfred Stevens", "state intervention through taxation could boost broader consumption, create wealth, and encourage a peaceful, tolerant, multipolar world order", "algebraic aspects", "eight", "1886/1887", "clerical marriage", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "schoolteacher", "Dunlop India Ltd.", "David Anthony O'Leary", "a family member", "Tranquebar (Tharangambadi)", "Attorney General and as Lord Chancellor of England", "North Dakota", "fennec", "Norwood, Massachusetts", "1993", "switzerland", "liquidambar styraciflua", "the Battle of Chester", "Flashback", "Tennessee", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn", "Clark Gable", "Inklings", "the paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Julia Verdin", "2013", "Guthred", "Centers for Medicare & Medicaid Services (CMS)", "Australian", "1945", "1941", "La Scala", "\"How to Train Your Dragon\"", "pronghorn", "United States ambassador to Ghana", "Life Is a Minestrone", "Monk's", "Sir Ernest Rutherford", "Bump", "Willy", "France", "at least $20 million to $30 million", "Ulysses S. Grant", "Virgil Tibbs", "the eventual closure of Guantanamo Bay prison and CIA \"black site\" prisons", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6460120506535948}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.25, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8333333333333334, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.7058823529411764, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9888", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.515625, "CSR": 0.5953947368421053, "retrieved_ids": ["mrqa_squad-train-86198", "mrqa_squad-train-41593", "mrqa_squad-train-69687", "mrqa_squad-train-62683", "mrqa_squad-train-24172", "mrqa_squad-train-18372", "mrqa_squad-train-45902", "mrqa_squad-train-68485", "mrqa_squad-train-65808", "mrqa_squad-train-79680", "mrqa_squad-train-22210", "mrqa_squad-train-29399", "mrqa_squad-train-56719", "mrqa_squad-train-7774", "mrqa_squad-train-7375", "mrqa_squad-train-27405", "mrqa_searchqa-validation-10011", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-80", "mrqa_newsqa-validation-1664", "mrqa_triviaqa-validation-6643", "mrqa_searchqa-validation-12968", "mrqa_triviaqa-validation-2357", "mrqa_squad-validation-9357", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-7140", "mrqa_hotpotqa-validation-1161", "mrqa_naturalquestions-validation-8889", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-6506", "mrqa_squad-validation-7632"], "EFR": 0.967741935483871, "Overall": 0.7510648344651953}, {"timecode": 19, "before_eval_results": {"predictions": ["1,294 birds, 427 mammals, 428 amphibians, and 378 reptiles", "swimming-plates", "MHC I", "Denver's Executive Vice President of Football Operations", "10", "Continental Edison Company", "Time", "Stan Lebar", "Warszawa", "Troggs", "schizophrenia", "Cressida", "Tom Osborne", "Moses", "a shih tzu", "Golda Meir", "Fiddler on the Roof", "Monopoly", "In 1963 she said, \"I feel as though I'm suddenly on stage for a part I never rehearsed\"", "(Marie) Leszczyska", "mask", "Alien", "The Tower of London", "reptiles", "Cher", "onion", "Walter Lasorda", "Benazir Bhutto", "Coca-Cola", "Red Bull", "Chaillot", "Ibrahim Petrovich Hannibal", "butter", "grow a Beard", "soup Nazi", "Pyrrhic victory", "Guatemala", "bonds", "the Rue Morgue", "huevos rancheros", "August Wilson", "Sacher Torte", "Palestine Peace Not Apartheid", "descend", "Lovebird", "( Leonardo) DiCaprio", "watermelon", "Daisy Miller", "calculators", "give Me Liberty or Give Me Death", "Frank Sinatra", "Sonnets", "South Africa", "the standard for the Navy's commissioned ships while in commission", "the Infamy Speech of US President Franklin D. Roosevelt", "Costa del Sol", "River Stour", "Annales de chimie et de physique", "gull-wing doors", "$420,000", "sexual assault", "1994", "state senators", "38"], "metric_results": {"EM": 0.5, "QA-F1": 0.54921875}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.75, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-4730", "mrqa_squad-validation-375", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.5, "CSR": 0.590625, "EFR": 0.96875, "Overall": 0.7503125}, {"timecode": 20, "UKR": 0.75, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.873046875, "KG": 0.49609375, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "John W. Weeks Bridge", "9th", "provide direct patient care services", "US$3 per barrel", "Trajan's Column", "Indian Ocean", "Pam Anderson", "Golda Meyerson", "xerophyte", "anions", "Uranus", "George III", "Mike Danger", "O'ahu, Hawaii", "Gandalf", "Mungo Park", "Squash", "Hodges", "magnetite", "Sam Mendes", "El Paso, Hudspeth, Presidio, Brewster", "Emeril Lagasse", "Spike", "Karl Marx", "in front of a church", "four and a half hours", "norway", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Zephyros", "Frobisher Bay", "Dumbo", "Dickens", "Botany Bay", "Peterborough United", "Porto", "albedo", "11", "California", "red", "remnants of very massive stars", "Brainy", "Virginia Elliott", "Prince Eddy", "Algerian state", "it was killed accidentally while handling a revolver", "Barry White", "gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Peary", "Simon & Garfunkel", "Alan Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.53125, "QA-F1": 0.58203125}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-6316", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-922", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.53125, "CSR": 0.5877976190476191, "EFR": 1.0, "Overall": 0.7413876488095238}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "the Panic of 1901", "supported modern programming practices and enabled business applications to be developed with Flash", "February 6, 2005", "development of electronic computers", "159", "an Easter egg", "Andhra Pradesh and Odisha", "1975", "John Vincent Calipari", "winter festivals", "Billie Jean King", "Rudolf Virchow", "rocks and minerals", "October 30, 2017", "to avoid the inconvenienceiences of a pure barter system", "four", "Laodicean Church", "Lykan", "in the pachytene stage of prophase I of meiosis", "Baltimore", "lagaan", "Oscar", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen, and Emma Thompson", "moral", "May 19, 2008", "Albert Einstein", "2018", "1992", "restored to life", "Master Christopher Jones", "to solve its problem of lack of food self - sufficiency", "RuPaul", "Spanish surname", "the bloodstream or surrounding tissue", "the church sexton Robert Newman and Captain John Pulling", "Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "spain", "31", "the disputed 1824 presidential election", "12", "local organization of businesses whose goal is to further the interests of businesses", "for control purposes", "twelve", "Paige O'Hara", "ghee", "The Crow", "Corinna and seven-time Formula One World Champion Michael Schumacher", "micronutrient-rich", "spain", "top designers", "Heathrow", "gold", "Lichfield Cathedral", "Lee Harvey Oswald", "liver"], "metric_results": {"EM": 0.5, "QA-F1": 0.6158777683605954}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 0.7058823529411764, 0.0, 0.0, 0.6666666666666666, 0.7368421052631579, 0.0, 0.16666666666666669, 0.0, 0.4615384615384615, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.125, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.5, "CSR": 0.5838068181818181, "retrieved_ids": ["mrqa_squad-train-72933", "mrqa_squad-train-6206", "mrqa_squad-train-4238", "mrqa_squad-train-24997", "mrqa_squad-train-61442", "mrqa_squad-train-53691", "mrqa_squad-train-24793", "mrqa_squad-train-79290", "mrqa_squad-train-22470", "mrqa_squad-train-7834", "mrqa_squad-train-37695", "mrqa_squad-train-73218", "mrqa_squad-train-75185", "mrqa_squad-train-81466", "mrqa_squad-train-85897", "mrqa_squad-train-34820", "mrqa_squad-validation-10204", "mrqa_hotpotqa-validation-674", "mrqa_naturalquestions-validation-7473", "mrqa_hotpotqa-validation-1159", "mrqa_triviaqa-validation-6842", "mrqa_newsqa-validation-2525", "mrqa_searchqa-validation-668", "mrqa_squad-validation-6316", "mrqa_naturalquestions-validation-4222", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-4946", "mrqa_squad-validation-3483", "mrqa_searchqa-validation-3485", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-7060"], "EFR": 0.9375, "Overall": 0.7280894886363637}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "fowl", "lexicographer", "Islamic Republic", "One Flew Over the Cuckoo's Nest", "mustard", "Royal Wives", "Harpers Ferry", "Confeitaria Colombo", "The Canterbury Tales", "Versailles", "Target", "meadow", "Russia", "\"Tom Terrific\"", "magnesium", "The Swamp Fox", "The New York Times", "German Shepherd", "peanuts", "China", "Parker House Rolls", "Damascus", "the Jennies", "a holography", "john connote", "the 1096 quake", "the Buonapartes", "Mother Vineyard", "Virginia Woolf", "apogee", "Cherry Garcia", "in his magic books", "Jim Brady", "axiom", "Princeton", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T.S. Eliot", "Andes", "tutus", "asteroids", "the Nutcracker", "a quake", "Labour Party", "1933", "grated cheese", "Falstaff", "reddish", "Republican", "Wojtek", "Bangor Air Force Base", "1995", "skin-tight black trousers", "12-hour-plus shifts", "start a dialogue of peace based on the conversations she had with Americans along the way"], "metric_results": {"EM": 0.5, "QA-F1": 0.5635416666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666665, 0.0, 0.0, 0.6666666666666666, 0.1]}}, "before_error_ids": ["mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-9255", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.5, "CSR": 0.5801630434782609, "EFR": 1.0, "Overall": 0.7398607336956522}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "Solim\u00f5es Basin", "seven", "the 21st century", "on a lifeboat off the coast of Somalia", "Biden", "18", "Adidas", "her mom would always convince her that she was going to be on the Olympic medals podium.", "the body of the aircraft", "honey", "the United States", "Michigan", "on websites on the 24th.", "Two", "President Obama and Vice President Biden", "The Tinkler", "$106,482,500", "Tuesday in Los Angeles.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three out of four", "tennis", "Disney", "Saturn", "90", "directly involved in an Internet broadband deal with a Chinese firm", "$75", "free laundry service", "the WGC-CA Championship", "Jeffrey Jamaleldine", "an insurgent small arms fire", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "more than 1.2 million", "Mitt Romney", "voters are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "Mother's Day", "near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "Pope refused Wednesday to soften the Vatican's ban on condom use", "allergic reaction to peanuts, her mother, Tara, has worried about her daughter's food whenever they eat out.", "Martin \"Al\" Culhane,", "emeralds", "al-Shabaab", "a long-term goal for reducing\" greenhouse emissions.", "a motor motorcycle accident", "the Isthmus of Corinth", "Gavin DeGraw", "Old Trafford", "Buddha", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Floyd Nathaniel \"Nate\" Hills", "Hong Kong Film Award", "honey", "the midnight ride", "Queen Charlotte Sound"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5037140376984127}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.2222222222222222, 0.0, 0.0, 1.0, 0.125, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.40625, "CSR": 0.5729166666666667, "EFR": 1.0, "Overall": 0.7384114583333334}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS primality test", "Deficiencies existed in Command Module design, workmanship and quality control.", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert A. Iger", "Regional League North", "2002", "Suicide Squad", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei City", "Minneapolis", "lady", "Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "imp My Ride", "April 19, 1994,", "Umar S. Israilov", "Derry City F.C.", "Fort Hood, Texas.", "Bonny Hills, New South Wales", "London", "1999", "2006", "swingman", "Zero Mostel", "October 13, 1980", "Chechen Republic", "House of Commons", "1926", "Nikolai Morozov", "1968", "Berthold Heinrich K\u00e4mpfert,", "Girl Meets World", "January 15, 1975", "Pansexuality, or omnisexuality,", "Javan leopard", "2,463,431", "Paul's letter", "her castle alone, and even more so after informing her that Aslan had come to Narnia.", "Le XXIII Olympic Winter Games ( French : Les XXIII\u00e8mes Jeux olympiques d'hiver ; Hangul : \uc81c 23", "Great Britain", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "tells Larry King her son has strong values.", "FREEGILLESPIE.", "linda", "Strait of Hormuz"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7119171626984127}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-1797", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.640625, "CSR": 0.575625, "retrieved_ids": ["mrqa_squad-train-85532", "mrqa_squad-train-34679", "mrqa_squad-train-58062", "mrqa_squad-train-24141", "mrqa_squad-train-34599", "mrqa_squad-train-85876", "mrqa_squad-train-86345", "mrqa_squad-train-57528", "mrqa_squad-train-29884", "mrqa_squad-train-78505", "mrqa_squad-train-75080", "mrqa_squad-train-84311", "mrqa_squad-train-49340", "mrqa_squad-train-14384", "mrqa_squad-train-52668", "mrqa_squad-train-64178", "mrqa_hotpotqa-validation-2366", "mrqa_triviaqa-validation-3865", "mrqa_squad-validation-5758", "mrqa_hotpotqa-validation-5708", "mrqa_squad-validation-3257", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-677", "mrqa_hotpotqa-validation-893", "mrqa_newsqa-validation-539", "mrqa_triviaqa-validation-4710", "mrqa_searchqa-validation-8607", "mrqa_triviaqa-validation-1320", "mrqa_squad-validation-3863", "mrqa_hotpotqa-validation-4813", "mrqa_searchqa-validation-10097"], "EFR": 1.0, "Overall": 0.738953125}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "the water level drops, such that the temperature of the firebox crown increases significantly", "Northern Rail", "South Korea", "paralysis", "shafted", "Romania", "Pocahontas", "Matlock", "Washington", "Chile and Argentina", "The Blue Boy", "Three Worlds", "Liriope", "Creation", "Delaware", "eastern Pyrenees", "The Mayor of Casterbridge", "Dutch", "Salem witch trials", "Gryffindor", "Sam Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Richard Walter Jenkins", "(b\u0259r\u02c8ki n\u0259 \u02c8f\u0251 so\u028a)", "Billy Cox", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "Richard Ripley", "(Magi)", "ultima", "(BS)", "Ash", "Ian Botham", "squash", "Leander", "(born 17 September 1929)", "Charlotte's Web", "Poland", "full-contact", "albion", "Jesus", "Authority", "1 mile ( 1.6 km )", "Steve Valentine", "Out of Control", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "scribe", "amelia earhart", "Final Cut"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6427083333333334}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2266", "mrqa_triviaqa-validation-2393", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3194", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-5822", "mrqa_searchqa-validation-8379"], "SR": 0.59375, "CSR": 0.5763221153846154, "EFR": 0.9615384615384616, "Overall": 0.7314002403846154}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a nobleman", "the disk", "2016", "the Islamic prophet Muhammad", "Mel Tillis", "Pangaea or Pangea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "two years", "the Great British Bake Off's best amateur bakery", "Romancing the Stone", "Orange Juice", "a photodiode", "September 9, 2010", "Jesse Frederick James Conaway", "bactrian", "Dan Stevens", "Ben Fransham", "the Grey Wardens", "1979", "October 27, 2016", "auctoritas", "patristic authors", "Luther Ingram", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "growing faster than the rate of economic growth", "1936", "British Columbia, Canada", "New York University", "2007", "2001", "the Washington Redskins", "the books of Exodus and Deuteronomy", "September 14, 2008", "the Vital Records Office of the states", "Pasek & Paul", "the Chicago metropolitan area", "Francisco Pizarro", "1930s", "the Norman given name Robert", "Mary Rose Foster", "John Smith", "six episodes", "1603", "neutrality", "he cheated on Miley", "banjo", "anabaptists", "chandler", "Taylor Swift", "Keith Crofford", "Michael Crawford", "$22 million", "five days", "flooding", "pica", "david", "business"], "metric_results": {"EM": 0.515625, "QA-F1": 0.570749080882353}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.125, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.5, 0.0, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.515625, "CSR": 0.5740740740740741, "EFR": 0.967741935483871, "Overall": 0.732191326911589}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions", "mathematical models of computation", "BAFTA Television Award for Best Actor", "around 300", "an anvil", "6th season", "Larry Richard Drake", "(n\u00e9e\" Byron; 10 December 1815 \u2013 27 November 1852)", "London's West End", "currently Ron Kouchi", "Hanford Site", "Native American tradition", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "U.S. Marshals", "churros", "Christies Beach", "eastern", "Arsenal", "Burt Reynolds", "torpedoes", "1953 to 1961", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defensive player whose primary role is to prevent the opposing team from scoring goals", "Henry John Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "football YouTube", "Daniel Andre Sturridge", "USS \"Essex\"", "Ron Cowen and Daniel Lipman", "Isabella Hedgeland (born at Cairnburgh Castle in the Scottish Highlands and baptised on 4 May 1759 \u2013 died on 25 June 1857 in London)", "Captain while retaining the substantive rank of Commodore", "Andrea Maffei", "Tomasz Adamek (] ; born 1 December 1976)", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "Umberto II", "Minnesota", "Jeux", "Justice of the Peace", "John Lennon", "International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster", "1986", "changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "fortieth", "Australian", "amorphria", "denied", "her niece", "one", "brandy", "Sgt. Pepper", "North Carolina"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5698854006008611}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.631578947368421, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.07692307692307691, 1.0, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7819", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3907", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.453125, "CSR": 0.5697544642857143, "retrieved_ids": ["mrqa_squad-train-15238", "mrqa_squad-train-22358", "mrqa_squad-train-55579", "mrqa_squad-train-65163", "mrqa_squad-train-57661", "mrqa_squad-train-10106", "mrqa_squad-train-31261", "mrqa_squad-train-69921", "mrqa_squad-train-13472", "mrqa_squad-train-70042", "mrqa_squad-train-9109", "mrqa_squad-train-48625", "mrqa_squad-train-58408", "mrqa_squad-train-57384", "mrqa_squad-train-15925", "mrqa_squad-train-8894", "mrqa_hotpotqa-validation-454", "mrqa_squad-validation-6091", "mrqa_newsqa-validation-3897", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-808", "mrqa_searchqa-validation-6349", "mrqa_squad-validation-8596", "mrqa_triviaqa-validation-146", "mrqa_newsqa-validation-1275", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-6637", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-7209", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-2590", "mrqa_naturalquestions-validation-2686"], "EFR": 1.0, "Overall": 0.7377790178571428}, {"timecode": 28, "before_eval_results": {"predictions": ["left", "the USSR", "common flagellated protists", "Juliet", "top designers", "Secretary of State Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "around 8 p.m. local time Thursday", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "A witness", "Adriano", "he eventually gave up 70 percent of his father-in-law's farm, which he then owned.", "183", "American Civil Liberties Union", "deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan", "40", "Liverpool Street Station", "137", "54-year-old", "Congressional auditors", "Jacob", "South Africa", "Markland Locks and Dam", "4,000", "Seasons of My Heart", "provided Syria and Iraq 500 cubic meters of water", "the Catholic League", "August 19, 2007", "10 years", "all three pleaded not guilty", "Japanese officials", "she is God-sent", "consumer confidence", "he was mad at the U.S. military because of what they had done to Muslims in the past", "about 300,000", "nearly 28 years", "July 18, 1994", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "soldiers had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "along the Chao Phraya River", "two", "an antihistamine and an epinephrine auto-injector", "4,000", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Jean F Kernel", "around 10 : 30am", "Johannes Gutenberg", "crossword puzzle", "Christian Wulff", "Gardiner", "general secretary", "the George Washington Bridge", "Johns Creek", "junk", "Aristotle", "Colombia"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6342704277571372}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.8, 0.0, 0.0, 0.33333333333333337, 0.5454545454545454, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.9473684210526316, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 0.0, 0.5, 0.3870967741935484, 0.8947368421052632, 0.0, 1.0, 1.0, 1.0, 0.125, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3743", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-4780"], "SR": 0.484375, "CSR": 0.5668103448275862, "EFR": 1.0, "Overall": 0.7371901939655172}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "the end of", "lovebirds", "Chicago", "monk seal", "William II", "inqurere", "Take Me Out to the Ball Game", "an expression used in drinking a person's health.", "\"What hath God wrought\"", "New Zealand", "St. Erasmus", "their supper", "the Time Machine", "Wolly", "illegible", "Scrabble", "Mussolini rose to power.", "valkyries", "rain", "bach", "the Silence of the Lambs", "Elysian Fields", "Five Easy pieces", "Thomas Edison", "Manhattan Project", "Charles", "divorce", "Enchanted", "the Liberty Bell", "USB", "Autobahn", "Destiny's Child", "Byron's", "a spoonful of sugar", "inflammatory", "Margot Fonteyn", "eel", "McMillan & wives", "(Whizzer) White", "professor", "Galileo Galilei", "Existentialism", "John Donne", "Beijing", "Annies", "murder", "Charles Lindbergh", "a queen", "synaptic vesicles", "Vatican City", "James W. Marshall at Sutter's Mill in Coloma, California", "a single, implicitly structured data item in a table", "South Korea", "\"stranger\" or \"foreigner\"", "M*A*S*H TV series", "F/A-18F Super Hornet", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "e-mails", "HPV (human papillomavirus)"], "metric_results": {"EM": 0.546875, "QA-F1": 0.621875}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-7230", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-8563", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-935", "mrqa_hotpotqa-validation-739"], "SR": 0.546875, "CSR": 0.5661458333333333, "EFR": 1.0, "Overall": 0.7370572916666667}, {"timecode": 30, "UKR": 0.767578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.85546875, "KG": 0.51015625, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students", "Nikolai Trubetzkoy", "June 1925", "bushwhackers", "British", "Argentina", "Baudot code", "Jacksonville", "the DTM and its successor \u2014 the International Touring Car Championship", "Switzerland", "Maryland", "Tennessee", "John Ford", "Operation Watchtower", "south-east", "1 December 1948", "omnisexuality", "Tea Tree Plaza", "Avoca Lodge", "Atlanta, Georgia", "Atlanta Braves", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "southern (Dolomitic) Alps", "Angus Brayshaw", "an artist manager or a film or television producer", "Islamic philosophy", "January 30, 1930", "Sulla", "The Australian women's national soccer team", "Jaguar Land Rover", "tempo", "Milk Barn Animation", "McLaren-Honda", "Timothy Dowling", "London", "Jane", "Patricia Arquette", "Otto Hahn and Meitner", "AMC Theatres", "31", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins", "Jude", "twenty-three", "a ethnic, cultural or racial group", "Shreya Ghoshal", "Whoopi Goldberg", "September 8, 2017", "volcanic activity", "on a sound stage in front of a live audience in Burbank, California", "horses", "Werner Heisenberg", "the Kiel Canal", "contract talks", "Eintracht Frankfurt", "Democrat", "a p mash", "rockstar", "Will & Grace"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6311755952380953}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.28571428571428575, 0.8571428571428571, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-286", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_triviaqa-validation-1106", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-9323"], "SR": 0.5625, "CSR": 0.5660282258064516, "retrieved_ids": ["mrqa_squad-train-56477", "mrqa_squad-train-29201", "mrqa_squad-train-46305", "mrqa_squad-train-4122", "mrqa_squad-train-24993", "mrqa_squad-train-77779", "mrqa_squad-train-74308", "mrqa_squad-train-17723", "mrqa_squad-train-40200", "mrqa_squad-train-60989", "mrqa_squad-train-5680", "mrqa_squad-train-30462", "mrqa_squad-train-46008", "mrqa_squad-train-68627", "mrqa_squad-train-45960", "mrqa_squad-train-1756", "mrqa_squad-validation-3730", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-1800", "mrqa_newsqa-validation-3640", "mrqa_hotpotqa-validation-672", "mrqa_squad-validation-216", "mrqa_searchqa-validation-13803", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-1767", "mrqa_newsqa-validation-960", "mrqa_triviaqa-validation-6413", "mrqa_searchqa-validation-12185", "mrqa_hotpotqa-validation-4813", "mrqa_squad-validation-8238", "mrqa_squad-validation-680"], "EFR": 1.0, "Overall": 0.7398462701612903}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\"", "film and short novels", "Carson City", "The Nick Cannon Show", "Mickey's Christmas Carol", "San Antonio", "Bergen County", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Professor Frederick Lindemann, Baron Cherwell", "Rawhide", "astronomer and composer", "Don DeLillo", "The Seduction of Hillary Rodham", "Balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Rosie O'Donnell", "the Kentucky Derby", "Taylor Alison Swift", "Miller Brewing", "Centers for Medicare and Medicaid Services", "Indianapolis Motor Speedway", "Clark County, Nevada", "Jay Michael Gruden", "Jango", "High Court of Admiralty", "An All-Colored Vaudeville Show", "Lutheranism", "Robert John Day", "Valley Falls", "dice", "Nicholas \" Nick\" Offerman", "Jewish", "JackScanlon", "Leonard Bernstein", "62", "France", "carbonic acid", "secretary", "eight", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "a car"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6900545634920635}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 0.5, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-4489", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-5501", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-728", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.546875, "CSR": 0.5654296875, "EFR": 1.0, "Overall": 0.7397265625}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "In Sickness", "Laputa", "Leeds", "Sam Phillips", "LSD", "Stephen of Blois", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Bubba", "Rugby", "multi-user dungeon", "a burner", "Greece", "1932", "Steve Coogan", "Renard", "Boston Marathon", "Carl Smith", "Humble Pie", "Jorge Lorenzo", "Rescue Aid Society", "checkers", "Terry Wogan", "Arthur", "the Grail", "Ronald Reagan", "Charlie Fenton", "climate", "the Coney Island Old Island Pier", "the Sutton Hoo", "the heart", "Guildford Dudley", "the Amoco Cadiz", "John Howard", "2 Samuel", "His Holiness", "12th", "Cornell University", "Flybe", "Woodstock", "a pot or crock", "The Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomatoes", "the senior-most judge of the supreme court", "early Christians of Mesopotamia", "Representatives", "2006", "Central Avenue", "middleweight", "Mokotedi Mpshe", "project work", "comfort those in mourning", "Canterbury", "Harold Macmillan", "a marsh"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6041666666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4060", "mrqa_searchqa-validation-6833"], "SR": 0.578125, "CSR": 0.5658143939393939, "EFR": 1.0, "Overall": 0.7398035037878788}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "its main priority", "Ennis", "Stratfor's website", "in the last few months,", "Jaime Andrade", "children ages 3 to 17", "girls", "possible victims of physical and sexual abuse.", "the island's dining scene", "gasoline", "vivian, an NFL Hall of Famer, the most successful female rapper,", "Airbus A320-214", "he won two Emmys for work on the 'Columbo' series starring Peter Falk.", "ice jam", "ozzy,", "abduction of minors.", "oregon", "j. Crew", "mosteller's brother-in-law", "Florida", "Bhola for the Muslim festival of Eid al-Adha.", "Clifford Harris,", "Pew Research Center", "nirvana", "vivian liberto", "jerry polis", "based on race or its understanding of what the law required it to do.", "he won it with a clear strategy that was stuck to with remarkably little internal drama.", "between the ages of 14 to 17.", "johnny Clarkson", "misdemeanor", "1.2 million", "100,000", "vance, 52, a native of California,", "crossfire by insurgent small arms fire,", "2002", "Noriko Savoie would be given custody of the children and agreed to remain in the United States.", "a \"new chapter\" of improved governance in Afghanistan", "Lee Probert", "when people gathered outside as the conference in the building ended.", "shelling of the compound", "in the mouth.", "Atlantic Ocean", "ameneh Bahrami", "Nepal", "Jiverly Wong,", "girl had been sexually assaulted,", "Carrousel du Louvre", "September 21.", "wildland firefighters", "Aspirin", "Prince Bao", "Narendra Modi", "Tony Meo", "74", "mariah liberto", "musical research", "Randall Boggs", "Mick Jackson", "oregon", "Gary Oldman", "paris"], "metric_results": {"EM": 0.484375, "QA-F1": 0.566253591954023}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.8, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.16666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.20689655172413796, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7079", "mrqa_hotpotqa-validation-4112", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-44"], "SR": 0.484375, "CSR": 0.5634191176470589, "retrieved_ids": ["mrqa_squad-train-27010", "mrqa_squad-train-54608", "mrqa_squad-train-20059", "mrqa_squad-train-43959", "mrqa_squad-train-83916", "mrqa_squad-train-64748", "mrqa_squad-train-60323", "mrqa_squad-train-20724", "mrqa_squad-train-78568", "mrqa_squad-train-10822", "mrqa_squad-train-8763", "mrqa_squad-train-22372", "mrqa_squad-train-22002", "mrqa_squad-train-54924", "mrqa_squad-train-30055", "mrqa_squad-train-51273", "mrqa_triviaqa-validation-192", "mrqa_hotpotqa-validation-5526", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-2408", "mrqa_naturalquestions-validation-6506", "mrqa_triviaqa-validation-6944", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-668", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-305", "mrqa_newsqa-validation-3264", "mrqa_hotpotqa-validation-3777", "mrqa_naturalquestions-validation-10249", "mrqa_triviaqa-validation-3591"], "EFR": 1.0, "Overall": 0.7393244485294118}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "the gloved one", "Laos", "Peter Davison", "Westminster Abbey", "Battle of Agincourt", "white spirit", "King George III", "Kent", "Lady Bracknell", "Diptera", "a turkey", "transuranic elements", "Harold Shipman", "Wyre", "Carson City", "All Things Must Pass", "the People's Republic of China", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City Football Club", "Moscow", "La Guiara", "oil of Olay", "hair and fur", "lacquer", "Bathsheba", "Ennio Morricone", "DitaVon Teese", "collapsible support assembly", "Republican", "Argentina", "French", "Roosevelt", "the internal kidney structures", "bachilla", "Rocky Marciano", "the Benedictine Order", "M69. Coventry to Leicester Motorway", "Margot Fonteyn", "John Uhler Lemmon III", "four", "1965", "2018", "Qutab Ud - Din - Aibak", "Danny Lebern Glover", "Trey Parker and Matt Stone", "140 to 219 passengers", "Hundreds", "Democrats", "31 meters (102 feet)", "the Knight of Ni", "Sutter's Fort", "Hawaii"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7145833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.5333333333333333, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-4398", "mrqa_naturalquestions-validation-8444", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-3920"], "SR": 0.65625, "CSR": 0.5660714285714286, "EFR": 0.9545454545454546, "Overall": 0.7307640016233766}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "daiquiri", "calvary", "armadillos", "joe mercer", "Danielle Steel", "Absalom", "joe joe mercer", "The Goonies", "nee mercer", "Quito", "the Seine", "wineamerica", "Alyssa Milano", "bites a dog", "Spangled Banner", "The Rolling Stones", "London", "knight", "Benjamin Franklin", "Bob Dylan", "a urinal", "lunar Module \"Eagle\"", "Spain", "Cadillac", "Matt Damon", "joe mercer", "shalom", "white", "Arthur James Balfour", "government", "Easton", "Scrabble", "Iceland", "car", "a baby", "joe mercer", "Stephen", "Brooke Bollea", "a war", "Nancy Sinatra", "David", "Pinot noir", "Robert Lowell", "ACTIVE", "Richmond", "joe mercer", "Amy Tan", "Florence", "Pandora", "Grenada", "the Himalayas", "Kusha", "Heroes and Villains", "cami de Repos", "silver", "1", "2015", "October 20, 2017", "Columbus", "Gustav's top winds weakened to 110 mph,", "Piedad Cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6319444444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-9192", "mrqa_naturalquestions-validation-10026", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_newsqa-validation-2307"], "SR": 0.59375, "CSR": 0.5668402777777778, "EFR": 1.0, "Overall": 0.7400086805555556}, {"timecode": 36, "before_eval_results": {"predictions": ["February 1082", "Bergdahl,", "\"As I was walking back through the crowd it was the word on everyone's lips,\"", "the mammoth's skull,", "kidney Anne Soliah", "it will unload what could be its final passengers.\"", "a mechanism at the federal level to ensure that drivers comply.", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile on its launch pad,", "75 percent", "Iraqi Kids.", "veterans and their families", "CNN/Opinion Research Corporation", "Kingdom City", "CEO of an engineering and construction company with a vast personal fortune.", "Ku Klux Klan", "President Felipe Calderon", "137", "86th minute", "ballroom-dance show.\"Woz,\"", "you", "Michael Jackson", "\"a striking blow to due process and the rule of law,\"", "Venezuela", "their business books were being handled.", "the Nazi war crimes suspect who had been ordered deported to Germany,", "calls, and those calls were intriguing, and we're chasing those down now,\"", "Mandi Hamlin", "Iraq", "American Legion National Commander David Rehbein", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "depression or lie about their use of medication for fear of losing their licenses to fly.", "Oklahoma", "\"Dancing With The Stars\" and NBC's \"The Biggest Loser.\"", "Malawi", "246", "the skull's", "six", "al-Douri,", "eight in 10", "one-shot victory in the Bob Hope Classic on the final hole to join his father as a winner of the tournament.", "Muslim north of Sudan", "37", "rapper T.I.", "Kyra and Violet", "Susan Boyle", "Florida", "UNICEF", "United States, NATO member states, Russia and India", "27", "45 %", "you", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "(George) Seuss"], "metric_results": {"EM": 0.390625, "QA-F1": 0.491472376994276}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47058823529411764, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8571428571428571, 0.5217391304347826, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_hotpotqa-validation-2625", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.390625, "CSR": 0.5620777027027026, "retrieved_ids": ["mrqa_squad-train-52544", "mrqa_squad-train-67165", "mrqa_squad-train-78770", "mrqa_squad-train-11805", "mrqa_squad-train-39704", "mrqa_squad-train-83643", "mrqa_squad-train-65588", "mrqa_squad-train-81142", "mrqa_squad-train-64879", "mrqa_squad-train-83562", "mrqa_squad-train-27442", "mrqa_squad-train-74399", "mrqa_squad-train-9313", "mrqa_squad-train-27322", "mrqa_squad-train-59122", "mrqa_squad-train-18850", "mrqa_hotpotqa-validation-2665", "mrqa_newsqa-validation-3640", "mrqa_hotpotqa-validation-1358", "mrqa_searchqa-validation-1076", "mrqa_hotpotqa-validation-5372", "mrqa_newsqa-validation-3222", "mrqa_hotpotqa-validation-1111", "mrqa_searchqa-validation-8139", "mrqa_triviaqa-validation-2474", "mrqa_hotpotqa-validation-4112", "mrqa_newsqa-validation-2167", "mrqa_hotpotqa-validation-4298", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-3681", "mrqa_newsqa-validation-328", "mrqa_searchqa-validation-15009"], "EFR": 1.0, "Overall": 0.7390561655405405}, {"timecode": 37, "before_eval_results": {"predictions": ["over $20 billion", "the Veneto region of Northern Italy", "Preston, Lancashire, UK", "Daniel Auteuil", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "Nineteen Eighty-Four", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Portal", "a chronological collection of critical quotations", "Terrence Alexander Jones", "S&M", "one", "Evey", "O", "The Grandmaster", "Scotland", "1940", "half", "Russian", "Cold Spring", "Hilary Erhard Duff", "Ogallala", "October 21, 2016", "All of the Lights", "Everything Is wrong", "Massapequa", "1988", "Dan Brandon Bilzerian", "Spitsbergen", "1967", "commercial", "Giuseppe Verdi", "band director", "1837", "$10\u201320 million", "Mandarin", "Judge Doom", "March", "The Frog Prince", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "Paul McCartney's over- assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison", "Union", "Alison Krauss", "Steve Hansen", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars", "Diamond", "Simon Legree", "Sideways", "ode"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6447916666666667}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5404", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-65", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_triviaqa-validation-6464", "mrqa_searchqa-validation-12752"], "SR": 0.546875, "CSR": 0.5616776315789473, "EFR": 1.0, "Overall": 0.7389761513157895}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Wang Chung", "Panama", "a gastropod shell", "Thailand", "Abraham Lincoln", "graham Hamilton", "Georgie Porgie", "Mork & Mindy", "Catherine de' Medici", "dressage", "Benito Mussolini", "Southern California", "Fort Leavenworth", "INXS", "graham Friedman", "the wildebeest", "Extra-Terrestrial Intelligence", "Arthur", "Blue Atmosphere", "Clara Barton", "9 to 5", "Coral", "moose", "Winnipeg", "Nicaragua", "Arthur Miller", "Margaret,", "1937", "Crossword Clue", "women's liberation", "Space Coast Convention Center", "the gallbladder", "Good Earth", "midway", "Liechtenstein", "Custer", "Gilead", "salt", "Gloria Steinem", "Catherine de' Medici", "Tonga", "Minos", "Gulliver", "rum", "SeaWorld", "coup de grce", "Tyra Banks", "Richard Gephardt", "Bucharest", "Manley", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "attack on Pearl Harbor", "positive lens", "cade", "Greek, Indian and Muslim savants", "Canterbury", "Lowndes County", "Northern Rhodesia", "actor", "Anjuna beach in Goa", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to his advantage.", "four"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6619791666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.4, 0.9666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-11059", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-2227"], "SR": 0.546875, "CSR": 0.5612980769230769, "EFR": 1.0, "Overall": 0.7389002403846153}, {"timecode": 39, "before_eval_results": {"predictions": ["18", "Nalini Negi", "Blue laws", "1980 Summer Olympics", "the IB Career", "in the brain", "Andreas Vesalius", "the season seven finale", "Nicole DuPort", "Angus Young", "Palmer Williams Jr.", "After World War I", "prospective studies", "Michigan State Spartans", "Wake County", "60 by West All", "( ta\u026a\u02c8t\u00e6n\u026ak / )", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "Texas - style chili con carne", "6 March 1983", "Kevin Michael Richardson", "James Arthur", "James Watson", "Antarctica", "during the American Civil War", "Thomas Middleditch", "slavery", "Ernest Rutherford", "Buddhism", "1889", "parthenogenesis", "Deuteronomy", "Buffalo Bill", "$6.2 trillion", "Sleeping with the Past", "boy", "1820s", "Soviet Union", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "at standard temperature and pressure", "John Ernest Crawford", "July 2014", "Cathy Dennis and Rob Davis", "1924", "Americans", "`` Zhongguo", "metamorphic rock", "Carmen", "a waterfowl", "glass", "Rikki Farr's", "the Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "in the Gaslight Theater", "\"M*A*S*H\"", "Depeche Mode", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6521613383256528}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.7499999999999999, 1.0, 0.0, 0.0, 0.2666666666666667, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5714285714285715, 0.06451612903225806, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-269"], "SR": 0.546875, "CSR": 0.5609375, "retrieved_ids": ["mrqa_squad-train-23167", "mrqa_squad-train-44504", "mrqa_squad-train-33773", "mrqa_squad-train-27706", "mrqa_squad-train-18788", "mrqa_squad-train-82447", "mrqa_squad-train-78285", "mrqa_squad-train-47683", "mrqa_squad-train-24836", "mrqa_squad-train-24618", "mrqa_squad-train-24156", "mrqa_squad-train-50875", "mrqa_squad-train-72938", "mrqa_squad-train-68264", "mrqa_squad-train-8041", "mrqa_squad-train-28476", "mrqa_naturalquestions-validation-1255", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-1457", "mrqa_naturalquestions-validation-3429", "mrqa_searchqa-validation-1236", "mrqa_hotpotqa-validation-5311", "mrqa_squad-validation-525", "mrqa_hotpotqa-validation-4937", "mrqa_triviaqa-validation-935", "mrqa_newsqa-validation-3435", "mrqa_hotpotqa-validation-4967", "mrqa_newsqa-validation-2117", "mrqa_squad-validation-5605", "mrqa_newsqa-validation-2408", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-7742"], "EFR": 0.9655172413793104, "Overall": 0.731931573275862}, {"timecode": 40, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.84765625, "KG": 0.48828125, "before_eval_results": {"predictions": ["the architect or engineer", "Naples", "malaria", "Jefferson", "Rubik's Cube", "the kettledrum", "meringue", "let (department manager) go, but can't do it until I have someone to replace him.", "one-hand", "Department of Justice", "Jimmy Doolittle", "John Brown", "FDR's opponents accused him of trying to \"pack\" this", "One Hundred Years of Solitude", "Frida Kahlo", "the Principality of Antioch", "Jeeves & Wooster", "Sardinia", "(litho)", "William Pitt the Younger", "Popcorn", "Madonna", "welterweight", "yoyo", "Charlotte", "\"There Is Nothin' Like A Dame\"", "Edinburgh, Scotland", "fluoroquinolones", "defensive", "Columbine", "Italy", "Kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "the Fantastical World Around You", "a petition signed by a certain... in 1891, permitting a certain number of citizens to make a request to amend a", "Chicago", "the Great Pyramid", "Herod", "Alaska", "\"more likely to be killed by a terrorist\"", "Asia", "anaphylaxis", "Peter Pan", "Kuwait", "\"1st\"", "Nathanael West", "diamond", "Charlie Sheen", "Call of the Wild", "Spain disputes the legality of the constitution and claims that it does not change the position of Gibraltar as a colony of the UK with only the UK empowered to discuss Gibraltar matters on the international scene.", "the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Syrup,\"", "Larry Eustachy,", "Isabella II", "Stanford University", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5856587998102466}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.06451612903225806, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-13067", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-15319", "mrqa_naturalquestions-validation-3961", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.484375, "CSR": 0.5590701219512195, "EFR": 1.0, "Overall": 0.728220274390244}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "aurochs", "Israel", "Prince Rainier", "Harper", "Fred Astaire", "Humphrey Bogart", "(Honda) CBR1000RR", "Alan Bartlett Shepard", "by burning nitrates and mercuric oxides", "le Carr\u00e9", "jackstones", "Rosslyn Chapel", "Hispaniola", "the Zulu War", "blood", "Ironside", "Aristotle", "(Jefferson) Sachs", "South Sudan", "Tuesday", "Herzegovina", "Secretary of State William H. Seward", "east coast", "Antoine Lavoisier", "NOW Magazine", "Toscana (Tuscany) Region", "the Alamo", "Beaujolais", "Edmund Cartwright", "Stern", "(Jefferson) Rubens", "the popes", "sk\u00e5l", "Barry McGuigan", "Wisconsin", "Kent Nagano", "Eton College", "Harrods", "Charles Dickens", "Ted Hankey", "(Jefferson) Stilwell", "a leaf", "sternum", "Portuguese", "mexico", "Greece", "Ed Miliband", "commitment", "an iron lung", "The Mandate of Heaven", "fascia surrounding skeletal muscle", "Robin", "the Distinguished Service Cross", "Indian classical", "2013", "15", "\"an eye for an eye,\"", "Arabic, French and English", "Schwalbe", "the owl", "Seinfeld", "Cress"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6543650793650793}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-7009", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-3241", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.546875, "CSR": 0.5587797619047619, "EFR": 1.0, "Overall": 0.7281622023809524}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman", "George Strait", "Andrew Gold", "1983", "a virtual reality simulator", "Lord Banquo", "Pakistan", "October 1, 2015", "digital transmission", "Isaiah Amir Mustafa", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "Paracelsus", "John C. Reilly", "Marshall Sahlins", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "1898", "1770 BC", "360", "a single, implicitly structured data item", "1959", "Gunpei Yokoi", "216", "Justin Bieber", "the Red Sea and the east African coast", "ideology", "160km / hour", "Chinese", "Andrew Garfield", "90s", "Gibraltar", "electrons", "cut off close by the hip, and under the left shoulder", "Lulu", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight", "Tokyo for the 2020 Summer Olympics", "1972", "Virgil Tibbs", "Ethel Merman", "1961", "usernames, passwords, commands and data", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Barbara Eve Harris", "Lake Wales, Florida", "1560s", "Johannes Gutenberg", "Wichita", "Tina Turner", "john Galliano", "Henry J. Kaiser", "Phil Collins", "SARS", "tax incentives", "d. Maria Siemionow,", "23 million square meters (248 million square feet)", "neon", "the Prisoner of Azkaban", "the ark", "Basilan"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6389791580968052}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.45454545454545453, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.8333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.15384615384615385, 1.0, 0.6666666666666666, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.53125, "CSR": 0.5581395348837209, "retrieved_ids": ["mrqa_squad-train-49675", "mrqa_squad-train-55128", "mrqa_squad-train-62637", "mrqa_squad-train-1542", "mrqa_squad-train-28938", "mrqa_squad-train-26317", "mrqa_squad-train-31494", "mrqa_squad-train-73061", "mrqa_squad-train-4367", "mrqa_squad-train-29531", "mrqa_squad-train-36124", "mrqa_squad-train-53517", "mrqa_squad-train-69816", "mrqa_squad-train-44587", "mrqa_squad-train-41064", "mrqa_squad-train-13878", "mrqa_triviaqa-validation-5194", "mrqa_naturalquestions-validation-10265", "mrqa_squad-validation-10427", "mrqa_naturalquestions-validation-1327", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-7707", "mrqa_naturalquestions-validation-144", "mrqa_squad-validation-2318", "mrqa_searchqa-validation-8365", "mrqa_hotpotqa-validation-2915", "mrqa_triviaqa-validation-3979", "mrqa_searchqa-validation-5461", "mrqa_naturalquestions-validation-8889", "mrqa_searchqa-validation-4120", "mrqa_triviaqa-validation-935", "mrqa_hotpotqa-validation-3364"], "EFR": 0.9666666666666667, "Overall": 0.7213674903100775}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a cappuccino", "Sheffield United", "Microsoft", "Wat Tyler", "Scout the pinto horse", "Scotland", "Earth", "James Hogg", "Texas", "rhinos", "Pears soap", "Bavarian", "Louis XVI", "Martin Van Buren", "fifty-six", "Uranus", "Plato", "the chord", "Chuck Berry, B.B. King, Dionne Warwick", "Separate Tables", "Wilson", "coal", "Stephen of Blois", "Relpromax Antitrust Inc.", "eukharisti\u0101", "National Football League", "Bear Grylls", "jaws (such as lampreys, and hagfish)", "Tanzania", "Val Doonican", "tittle", "E. T. A. Hoffmann", "the Republic of Upper Volta", "Robert Wright", "an elephant", "German", "New Zealand", "Mendip Hills", "graffiti", "Jane Austen", "God bless America, My home sweet home", "the Bass Red Triangle", "boxing", "Benjamin Disraeli", "The Jungle Book", "YouTube", "Jan van Eyck", "Prime Minister Yitzhak Rabin", "Shania Twain", "John Nash", "electron donors", "`` It Ain't Over'til It's Over ''", "used as a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "Elvis' Christmas Album", "restrictions on nighttime raids of Afghan homes and compounds,", "Robert Park", "Nearly eight in 10", "Cairo", "Jackson Pollock", "an elk", "tax incentives for businesses hiring veterans as well as job training"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6272035256410255}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1551"], "SR": 0.578125, "CSR": 0.55859375, "EFR": 1.0, "Overall": 0.728125}, {"timecode": 44, "before_eval_results": {"predictions": ["Between 1975 and 1990", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "1990", "at the end of the 18th century", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the atmosphere of terror", "from 1995 to 2012", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Rothschild", "Soviet Union", "smith", "Gwyneth Paltrow, Ewan McGregor, Olivia Munn, Paul Bettany and Jeff Goldblum", "alternate uniform", "1874", "Citric acid", "North Dakota and Minnesota", "David Walliams", "Zambia", "The Sun", "Christopher Tin", "Saint Louis", "Chesley Burnett \"Sully\" Sullenberger III", "Francis", "Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore", "only American-born", "2015", "19th", "smith", "Lev Ivanovich Yashin", "Carrefour", "General Sir John Monash", "Benjam\u00edn", "the Bank of China Tower", "the first Spanish conquistadors in the region of North America", "Chickamauga Wars", "9", "Antiochia", "Gatwick Airport", "200,000", "2,140 kilometres ( 1,330 mi )", "It is the oldest city in Highlands County", "honey bees", "squash", "Chicago", "soy", "Nineteen political prisoners", "How I Met Your Mother", "a collapsed ConAgra Foods plant", "Everest", "I.M. Pei", "Florence Nightingale", "the Gibraltar of America"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5598828688672439}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.28571428571428575, 1.0, 1.0, 0.5, 0.0, 0.8571428571428571, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.33333333333333337, 1.0, 0.9090909090909091, 0.0, 0.125, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-1836", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-4599", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.421875, "CSR": 0.5555555555555556, "EFR": 0.972972972972973, "Overall": 0.7221119557057057}, {"timecode": 45, "before_eval_results": {"predictions": ["pamphlets on Islam", "Gibraltar", "Jesse", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Sunset Boulevard", "Duke of Buccleuch and Queensbury", "The Lion King", "show business", "Wyoming", "Benedictus", "La's", "Javier Bardem", "8", "Lee Harvey Oswald", "virtual", "Sherlock Holmes", "Bayern", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine", "Confucius", "Japan", "stewardi", "London", "Christian Dior", "Phoenicia", "(C) Bobby Moore", "The Frighteners", "Jerez", "plac\u0113b\u014d", "\"OUR MUTUAL FRIend\"", "FC Porto", "writing", "an argument", "Rochdale", "Portuguese", "Madagascar", "Helsinki", "Monopoly", "myxoma virus", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Mercedes -Benz G - Class", "Switzerland", "eastern India", "World Famous Gold & Silver Pawn Shop", "1957", "Tim Masters,", "South Africa", "Dental brackets", "ABBA", "Phoenicia", "New York Giants"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6547715053763441}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, true, true], "QA-F1": [0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528"], "SR": 0.578125, "CSR": 0.5560461956521738, "retrieved_ids": ["mrqa_squad-train-69293", "mrqa_squad-train-84952", "mrqa_squad-train-42937", "mrqa_squad-train-43459", "mrqa_squad-train-80173", "mrqa_squad-train-73263", "mrqa_squad-train-46081", "mrqa_squad-train-76207", "mrqa_squad-train-11173", "mrqa_squad-train-54306", "mrqa_squad-train-45413", "mrqa_squad-train-45986", "mrqa_squad-train-26096", "mrqa_squad-train-7891", "mrqa_squad-train-78679", "mrqa_squad-train-66769", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-390", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-685", "mrqa_newsqa-validation-3042", "mrqa_triviaqa-validation-3855", "mrqa_hotpotqa-validation-5651", "mrqa_triviaqa-validation-414", "mrqa_hotpotqa-validation-398", "mrqa_naturalquestions-validation-10012", "mrqa_newsqa-validation-2167", "mrqa_newsqa-validation-1314", "mrqa_searchqa-validation-4914", "mrqa_naturalquestions-validation-7608", "mrqa_triviaqa-validation-4573", "mrqa_squad-validation-9176"], "EFR": 1.0, "Overall": 0.7276154891304347}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby", "a modem", "Clinton", "George H.W. Bush", "Penn State", "Luxor", "Berlusconi", "leviathan", "Mending Wall", "wombat", "a crystal", "thunder", "Josephine", "the Three Musketeers", "the iTunes Store", "Neptune", "Annie", "The Comedy of Humours", "KLM", "Captain Marvel", "X-Men", "the retina", "a goat", "Planet of the Apes", "a knish", "India", "the Reading Railroad", "Trotsky", "Mexican cheese", "the Justice Department", "Melissa Etheridge", "Ignace Jan Paderewski", "james", "Charles Schulz", "bass", "Frida Kahlo", "Jane Austen", "Rikki-Tavi", "an investment fund", "polygons", "country music", "lm", "a ferry", "the Tiananmen Square democracy movement", "Oresteia", "sugar smacks", "Erwin Rommel", "the Dolphins", "Thomas Mundy Peterson", "The Capture of USS Chesapeake", "From 1900 to 1946", "george terrier", "alligator", "Hindi", "London", "John Snow", "Ghana's", "soldiers had not gone anywhere they were not permitted to be.", "Pakistan intelligence institutions and its army", "Tuesday", "1955"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6889136904761904}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1216"], "SR": 0.609375, "CSR": 0.5571808510638299, "EFR": 0.96, "Overall": 0.719842420212766}, {"timecode": 47, "before_eval_results": {"predictions": ["Hawaii", "the Mayor", "Shel Silverstein", "beers", "trolley", "Liverpool", "Mount Rushmore", "Cyrus the Younger", "Greece", "Jim Bunning", "John Fogerty", "a robot", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "drug and alcohol", "the Darfur region", "the bicentennial", "the midway", "George Gershwin", "alpaca", "amelia earhart", "Heredity", "Bicentennial Man", "the rod", "heart attack", "Elke Sommer", "Tsar Ivan IV", "Flav", "Fulgencio Batista", "The Indianapolis 500", "the Twist", "beryl", "a cuckoo", "London", "beetle", "Joan of Arc", "palindrome", "quid", "Vanilla Ice", "A Night at the Roxbury", "John Steinbeck", "Eric Knight", "Heroes", "the Ganges", "Thomas Mann", "The book of Samuel", "Sing Sing", "Rajendra Prasad", "August 18, 1945", "an edited version", "Bedfordshire", "Otto I", "The Lion", "Lord's Resistance Army", "In South Asia and the Middle East", "Netflix", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6733355186480185}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.9090909090909091, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-3452", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-13588", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2504", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.5625, "CSR": 0.5572916666666667, "EFR": 0.9642857142857143, "Overall": 0.7207217261904761}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Deseo", "Mike Todd", "Alex Cross", "The Incredibles", "a Cheetah", "Charlie Brown", "Odin", "Japan", "Sea-Monkeys", "daffodils", "\"24\"", "Neil Simon's", "Voyager 2", "a gull", "Nez Perce", "Eva Peron", "incense", "the Hawkeyes", "the 2016 NBA Draft", "Swiffer", "Huckleberry Hound", "Austria", "Jason Bourne", "Peru", "The Trojan War", "the Chagos", "the Colosseum", "the Cardamom Mountains", "Dr. Hook and the Medicine Show", "Songs of Innocence and of Experience", "Uvula", "the catechism of the Council of Trent", "Ben-Oni", "Scrubs", "Cheyenne", "the Black Sea", "King George", "Frank Sinatra", "the Zambezi river", "saburau", "2 Samuel 5:4", "The Police", "Jamestown", "American funk rock band", "Robert Ford", "St. Francis of Assisi", "the Lemon Meringue pie", "Melville", "Tarzan & Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stieg Larsson", "The Merchant of Venice and The Taming of the Shrew", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "materials -- including ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7138888888888889}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-1131", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-8195", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.640625, "CSR": 0.5589923469387755, "retrieved_ids": ["mrqa_squad-train-45937", "mrqa_squad-train-16597", "mrqa_squad-train-64399", "mrqa_squad-train-83953", "mrqa_squad-train-38144", "mrqa_squad-train-42673", "mrqa_squad-train-28181", "mrqa_squad-train-70732", "mrqa_squad-train-31539", "mrqa_squad-train-60327", "mrqa_squad-train-68068", "mrqa_squad-train-66324", "mrqa_squad-train-10335", "mrqa_squad-train-8468", "mrqa_squad-train-50371", "mrqa_squad-train-5485", "mrqa_triviaqa-validation-7685", "mrqa_hotpotqa-validation-4567", "mrqa_searchqa-validation-4416", "mrqa_newsqa-validation-1836", "mrqa_searchqa-validation-815", "mrqa_hotpotqa-validation-4989", "mrqa_naturalquestions-validation-2990", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-5624", "mrqa_hotpotqa-validation-5651", "mrqa_naturalquestions-validation-1325", "mrqa_squad-validation-964", "mrqa_naturalquestions-validation-2544", "mrqa_newsqa-validation-3743", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-4489"], "EFR": 1.0, "Overall": 0.7282047193877551}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "the Doge's Palace", "Carmen Zapata", "the Isles of Scilly", "the Israel\u2019s", "feminist", "fourteen", "the kidneys", "crabapples", "Thierry Roussel", "Novak Djokovic", "Apollo 11", "five", "Kirk Douglas", "John Ford", "tin", "Longchamp", "Nihon-koku", "Ford", "a joey", "Maine", "USS Missouri", "Pyrenees", "basketball", "Janis Joplin", "Mr. Stringer", "basketball", "South Africa", "\"God Only Knows,", "Ed Miliband", "Scotland", "aeoline", "Margaret Mitchell", "the Republic of Upper Volta", "Fred Perry", "40", "75", "Sir Winston Churchill", "John Masefield", "Rio de Janeiro", "\"Party of God\"", "Bengali", "Claire", "Guatemala", "Carousel", "Leicester", "Frank Saul", "radishes", "james lister", "Downton Abbey", "achlais", "Garfield Sobers", "Herman Hollerith", "September 2017", "Golden Gate National Recreation Area", "Forbes", "English Electric Canberra", "Ford", "pattern matching", "he was one of 10 gunmen who attacked several targets in Mumbai", "a shrews", "tapas", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6741319444444445}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-3654", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-7720", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_searchqa-validation-4559", "mrqa_hotpotqa-validation-3207"], "SR": 0.609375, "CSR": 0.56, "EFR": 0.88, "Overall": 0.70440625}, {"timecode": 50, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.837890625, "KG": 0.45078125, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "The ladies'single figure skating competition of the 2018 Winter Olympics was held at the Gangneung Ice Arena", "The Walking Dead", "in Christian eschatology", "1962", "non-ferrous", "the economy composed of both public services and public enterprises", "the sacroiliac joint or SI joint", "Joudeh Al - Goudia family", "after World War II", "Cheshire", "The Massachusetts Compromise", "L.K. Advani", "30 months", "Jason Marsden", "Louis Le Vau, landscape architect Andr\u00e9 Le N\u00f4tre, and painter - decorator Charles Lebrun", "Ashrita Furman", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "the early 1960s", "623", "the beginning", "2013", "Diego Tinoco", "when each of the variables is a perfect monotone function of the other", "January 2004", "Glenn Close", "the roofs of the choir side - aisles at Durham Cathedral", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Dr. Addison Montgomery", "Carolyn Sue Jones", "De pictura", "a mark that reminds of the Omnipotent Lord, which is formless", "in various submucosal membrane sites of the body", "Article 1, Section 2", "birch", "push the food down the esophagus", "Dolly Parton", "northamptonshire", "durham county", "Jack Murphy Stadium", "\"Black Abbots\"", "Prince Amedeo, 5th Duke of Aosta", "a real person to talk to,\"", "Suba Kampong township", "for a full facial transplant since 2004.", "larynx", "pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6196103358787183}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.13333333333333333, 0.5714285714285715, 0.28571428571428575, 0.0, 0.0, 0.0, 0.14814814814814814, 0.1818181818181818, 1.0, 0.13333333333333336, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6153846153846153, 0.4666666666666667, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-4540", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-1676", "mrqa_searchqa-validation-15123", "mrqa_newsqa-validation-2294"], "SR": 0.515625, "CSR": 0.5591299019607843, "EFR": 0.967741935483871, "Overall": 0.7115462424889311}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander", "Franklin Roosevelt", "Scottish post-punk band Orange Juice", "1837", "Zoe Badwi, Jade Thirlwall's cousin", "22 November 1914", "Shareef Abdur - Rahim", "May 26, 2017", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "near Camarillo, California,", "douglas nambahu", "1994", "Deuteronomy 5 : 4 -- 25", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "Danish - Norwegian patronymic surname meaning `` son of Anders '' ( itself derived from the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew )", "a cliffhanger showing the first few moments of Sam's next leap ( along with him again uttering `` Oh, boy `` '' on discovering his situation ),", "two senators, regardless of its population, serving staggered terms of six years ; with fifty states presently in the Union, there are 100 U.S. Senators", "senior enlisted sailor ( `` E-9 '' )", "1603", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes, OLD, PLH ( born August 26, 1954 ), nicknamed the Magician and Bata, is a Filipino professional pool player", "Jim Carrey", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "Brooklyn, New York", "2015, 2016", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular", "21 June 2007", "chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Germany", "Gamora", "Darlene Cates", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "John Daly", "Matt Monro", "Joe Willie Kirk", "jane virginia", "Vito Corleone", "supply chain management", "Baugur Group", "Venice", "Hyundai Steel", "at Gaylord Opryland", "100 percent", "birmingham", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5781589965510322}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.9090909090909091, 0.5714285714285715, 0.0, 0.5714285714285715, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.10526315789473684, 0.0, 0.13793103448275862, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.4102564102564102, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8205128205128205, 0.9302325581395349, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.4375, "CSR": 0.5567908653846154, "retrieved_ids": ["mrqa_squad-train-24884", "mrqa_squad-train-44299", "mrqa_squad-train-27651", "mrqa_squad-train-13746", "mrqa_squad-train-31996", "mrqa_squad-train-74389", "mrqa_squad-train-1882", "mrqa_squad-train-23186", "mrqa_squad-train-27378", "mrqa_squad-train-23377", "mrqa_squad-train-10131", "mrqa_squad-train-82473", "mrqa_squad-train-58395", "mrqa_squad-train-14421", "mrqa_squad-train-8398", "mrqa_squad-train-44679", "mrqa_naturalquestions-validation-5564", "mrqa_squad-validation-4206", "mrqa_hotpotqa-validation-508", "mrqa_searchqa-validation-4120", "mrqa_naturalquestions-validation-10118", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2167", "mrqa_squad-validation-9255", "mrqa_triviaqa-validation-2758", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-8385", "mrqa_triviaqa-validation-6683", "mrqa_naturalquestions-validation-1165", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-10398"], "EFR": 0.8611111111111112, "Overall": 0.6897522702991453}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius Old Town", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "created the American Land-Grant universities and colleges", "Pacific War", "1949", "\"gunslinger\"", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "S6 Edge+", "the Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "William Harold \"Bill\" Ponsford", "Anatoly Lunacharsky", "Robert Matthew Hurley", "Macbeth", "Brad Silberling", "1987", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "seventh generation", "Len Wiseman", "31 July 1975", "Texas Tech Red Raiders", "Walldorf", "Elvis' Christmas Album", "the sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca Hopkins", "coordinator", "Godiva", "England national team", "\"Futurama\"", "Los Alamos National Laboratory", "land area", "Lush Ltd.", "Telugu", "1952", "Georgia Southern University", "Restoration Hardware", "1942", "Kansas City Chiefs", "Luis Edgardo Resto", "C. H. Greenblatt", "Stephen Graham", "President alone, and the latter grants judicial power solely to the federal judiciary", "introverted Sensing ( Si ), Extroverted Thinking ( Te )", "Belgium", "Jiles Perry, Sr.", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "3,000 kilometers (1,900 miles)", "Casalesi clan", "Linda Darnell", "Scrabble", "Wendell", "a leap year"], "metric_results": {"EM": 0.625, "QA-F1": 0.7020833333333334}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.13333333333333333, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.625, "CSR": 0.5580778301886793, "EFR": 1.0, "Overall": 0.7177874410377358}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine", "eight-day", "9-week-old", "Iran's President Mahmoud Ahmadinejad", "18", "Darrel Mohler", "Spc. Megan Lynn Touma,", "Operation Pipeline Express", "admitting they learned of the death from TV news coverage,", "in the head", "President Obama", "a challenge.\"When you cross to the south of the planet you always find rain, strong winds and big waves,", "Grand Ronde, Oregon.", "a bag", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "14-day", "the fact that the teens were charged as adults.", "Conway", "a practical framework that could help the U.S. government better respond to threats of genocide", "rwanda", "Arsene Wenger", "went second in Serie A with a 5-1 win over Torino in the San Siro", "Genocide Prevention Task Force", "Mohammed Mohsen Zayed", "The U.S. Food and Drug Administration Tuesday ordered the makers of certain antibiotics to add a \"black box\" label warning", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "anti-government protesters", "Saturday", "social networking sites", "for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Sen. Barack Obama", "Bergman", "sheila marie marie ryan", "three", "between June 20 and July 20,\"", "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici", "Sen. Piedad Cordoba", "buddhism", "Bollywood superstar Amitabh Bachchan", "Pakistani territory", "a fight outside of an Atlanta strip club", "Britain's Got Talent", "Sen. Barack Obama", "Swamp Soccer", "the man facing up, with his arms out to the side.", "stand down.", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "The ACLU", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Bruno Mars", "2018", "surfer", "largest group of animals", "white", "2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Monocerotis", "fish", "a crust of mashed potato"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6449971446215053}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6153846153846153, 1.0, 0.0, 0.09090909090909091, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 0.0, 0.1, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.36363636363636365, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.5625, "CSR": 0.5581597222222222, "EFR": 0.9642857142857143, "Overall": 0.7106609623015874}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou", "Squamish, British Columbia, Canada", "2018", "the Gospels of Matthew, Mark, Luke and John", "on a side table", "the illegitimate son of Ned Stark", "Ben Luckey as Rick Dicker, a government agent who is responsible for helping the Parrs stay mundane and undercover", "In the 2016 draft, one of the Top -- 10 players that was declared eligible for NCAA play in 2016, ThonMaker,", "Cristeta Comerford", "31", "Jesse Frederick James Conaway", "The tuatara, a lizard - like reptile native to New Zealand", "from the Eastern front, where Russia had surrendered", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions 14 - 15, 146 - 147 and 148 - 149", "In late - 2011, she announced plans for a clothing and jewelry line called House of Maryse", "on a beach in Malibu, California", "desublimation", "eight", "Scottish", "three mystic apes", "in cell - mediated, cytotoxic innate immunity ), and B cells ( for humoral, antibody - driven adaptive immunity )", "into the intermembrane space", "Kansas", "New England Patriots", "Chesapeake Bay", "Thomas Edison's assistants, Fred Ott", "in a relationship, marriage, or once talked to", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "A Doll's House ( Bokm\u00e5l : Et dukkehjem ; also translated as A Doll House )", "The series of 16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "the topography and the dominant wind direction", "Development of Substitute Materials", "a pagan custom", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung", "the 2005 novel The Book thief by Markus Zusak and adapted by Michael Petroni", "John Garfield as Al Schmid", "the Islamic Community", "Lord Irwin", "its absolute temperature", "a constitutional right", "Robert Gillespie Adamson IV", "the 18th century", "1998", "the left atrium of the heart", "Norman Whitfield and Barrett Strong", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the internal auditory canal of the temporal bone", "1803", "United Parcel Service", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's", "second-degree attempted murder and conspiracy,", "$106,482,500", "introduce legislation Thursday to improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "real estate investment trusts", "Hubert H. Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5165587103743937}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true], "QA-F1": [0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.9411764705882353, 0.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 0.21428571428571425, 0.23529411764705882, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.16666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.878048780487805, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-3624", "mrqa_hotpotqa-validation-574", "mrqa_newsqa-validation-3250", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.390625, "CSR": 0.5551136363636364, "retrieved_ids": ["mrqa_squad-train-83884", "mrqa_squad-train-77693", "mrqa_squad-train-77666", "mrqa_squad-train-83935", "mrqa_squad-train-58151", "mrqa_squad-train-56203", "mrqa_squad-train-76987", "mrqa_squad-train-21531", "mrqa_squad-train-79798", "mrqa_squad-train-2194", "mrqa_squad-train-79642", "mrqa_squad-train-47062", "mrqa_squad-train-70934", "mrqa_squad-train-25016", "mrqa_squad-train-59950", "mrqa_squad-train-47901", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-15243", "mrqa_squad-validation-1708", "mrqa_searchqa-validation-4416", "mrqa_triviaqa-validation-5296", "mrqa_newsqa-validation-3004", "mrqa_hotpotqa-validation-1865", "mrqa_searchqa-validation-5613", "mrqa_triviaqa-validation-3105", "mrqa_searchqa-validation-4120", "mrqa_hotpotqa-validation-1629", "mrqa_searchqa-validation-5715", "mrqa_hotpotqa-validation-1263", "mrqa_triviaqa-validation-2324"], "EFR": 0.9487179487179487, "Overall": 0.7069381920163169}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "Lutherhaven", "clouds", "Makkedah", "swab", "asteroids", "\"plankton\"", "Al Gore", "Eleanor Roosevelt", "BATTLE of LAKE ERIE", "Bangladesh", "The Secret", "medals", "wrestle apatow", "a laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "pH", "the Aladdin", "Nine to Five", "Jan & Dean", "force his", "ice cream", "Huckabee", "catherine the great", "Texas", "constellations", "AILD", "Celia", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "Back to the Future", "an antelope", "Anne", "Guatemala", "Dizzy", "soup", "The ACT", "Enrico Fermi", "Icarus", "suspension bridge", "Tigger", "the breath", "the marathon", "QWERTY", "Renewal of the Covenant", "collect menstrual flow", "13 May 1787", "nasal septum", "Triumph", "Kansas", "the recorder", "UFC 50: The War of '04", "newspapers, television, radio, cable television", "March 17, 2015", "4.6 million", "Dalai Lama", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6558407738095238}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-11807", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-6375", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_triviaqa-validation-4151", "mrqa_hotpotqa-validation-1190", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.546875, "CSR": 0.5549665178571428, "EFR": 1.0, "Overall": 0.7171651785714286}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Tardis", "berry", "The Potteries", "Lorraine", "iron", "Little arrows", "Lorraine", "cats", "Reanne Evans", "the Central African Republic", "The Battle of Camlann", "David Hilbert", "1905", "british", "Lab\u00e8que", "London", "Hard Times", "Muhammad Ali", "carbon", "The Bill", "M65", "Boxing Day", "cheers", "Taliban", "alpestrine", "a toad", "ameliorate", "bokm\u00e5l", "skirts", "Australia", "Blucher", "Atlas", "Sachin Tendulkar", "a black Ferrari", "the River Hull", "tenerife", "South Africa", "bone", "Nutbush", "Jeremy Thorpe", "Shintoism", "Huddersfield", "the Greater Antilles", "malts", "Pluto", "Jim Branning", "cryogenics", "Fleet Street", "Scafell Pike", "baseball", "Speaker of the House of Representatives", "Athens", "iOS, watchOS, and tvOS", "Leslie James \"Les\" Clark", "the fourth season of \"American Idol\" in 2005", "\" Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World", "propofol", "Emmett Kelly", "Lorraine", "Shakespeare in Love", "`` Can't Change Me,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6365838001867414}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0588235294117647, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-2291", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_naturalquestions-validation-7270"], "SR": 0.609375, "CSR": 0.555921052631579, "EFR": 0.96, "Overall": 0.7093560855263157}, {"timecode": 57, "before_eval_results": {"predictions": ["outside influences in next month's run-off election,", "Monday", "eight-week", "which type of guy you should avoid.", "The directive comes as the coalition seeks to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "fritter his cash away on fast cars, drink and celebrity parties.\"", "Stratfor,", "The Taliban has threatened to kill Bergdahl if foreign troops continue targeting civilians in the name of search operations in Ghazni and Paktika provinces,", "Unseeded Frenchwoman Aravane Rezai produced one of the shocks of the year on Sunday by defeating favorite Venus Williams in straight sets to win the final of the Madrid Open.", "murder in the beating death of", "David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern,", "sailing speed record.", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "The opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal", "Saturday", "The Eye on series has travelled around the world visiting France, Russia, India, South Korea, China, South Africa, Brazil, Beirut and Poland.", "Dube, 43, was killed in Johannesburg around 8 p.m. local time Thursday after someone tried to steal his car,", "11", "stuart", "The missile defense system is not aimed at Russia,\"", "Citizens", "refusal or inability to \"turn it off\"", "Janet Napolitano", "bicycles", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials, India said Monday.", "Pakistan from Afghanistan,", "the fact that the teens were charged as adults.", "Siri", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "bingham", "The incident Sunday evening", "Landry", "President Bush of a failure of leadership at a critical moment in the nation's history.\"", "Thebault", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "The 725-mile Veracruz regatta began on Friday", "Grease", "to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "2002 for British broadcaster Channel 4", "The sound of pounding hooves thunders in the high desert air.", "millionaire's surtax,", "seven", "One of Osama bin Laden's sons", "from that of a small potato to over 60 kg ( 130 lb )", "Great Britain's", "Manuel `` Manny '' Heffley is Greg and Rodrick's younger brother", "2004", "foxes", "Ambassador Bridge", "The University of Liverpool", "Alfred von Schlieffen", "Chillingham Castle", "the 400th anniversary", "the Liffey", "scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.5, "QA-F1": 0.5839482342964}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.0, 0.08695652173913045, 0.0, 1.0, 0.1818181818181818, 0.07407407407407407, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.1, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.05555555555555555, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212"], "SR": 0.5, "CSR": 0.5549568965517242, "retrieved_ids": ["mrqa_squad-train-6026", "mrqa_squad-train-10227", "mrqa_squad-train-15387", "mrqa_squad-train-65834", "mrqa_squad-train-12614", "mrqa_squad-train-50921", "mrqa_squad-train-77669", "mrqa_squad-train-27596", "mrqa_squad-train-78199", "mrqa_squad-train-29590", "mrqa_squad-train-53783", "mrqa_squad-train-18236", "mrqa_squad-train-81564", "mrqa_squad-train-39698", "mrqa_squad-train-34372", "mrqa_squad-train-29423", "mrqa_hotpotqa-validation-4064", "mrqa_newsqa-validation-2525", "mrqa_naturalquestions-validation-4222", "mrqa_triviaqa-validation-2264", "mrqa_naturalquestions-validation-4759", "mrqa_squad-validation-10273", "mrqa_newsqa-validation-1039", "mrqa_triviaqa-validation-4951", "mrqa_searchqa-validation-7140", "mrqa_newsqa-validation-539", "mrqa_hotpotqa-validation-1080", "mrqa_searchqa-validation-5936", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-1649", "mrqa_searchqa-validation-1086", "mrqa_hotpotqa-validation-1505"], "EFR": 1.0, "Overall": 0.7171632543103448}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted", "1,467", "1911", "Nicole Kidman, Meryl Streep", "14", "the National Basketball Development League", "Gust Avrakotos", "involuntary euthanasia", "test pilot, and businessman", "a common pochard", "The Summer Olympic Games", "Miami Gardens", "St. Louis Cardinals", "1992", "1993", "University of Vienna", "Jack Ridley", "The Pennsylvania State University", "Willis (Sears) Tower", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australian", "suburb", "hydrogen fueled space rockets, as well as automobiles and other transportation vehicles", "The Savannah River Site", "shooting guard", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Harsh Times", "1883", "23", "Mach number", "Asif Kapadia", "1872", "poetry", "Madonna", "musicologist", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "Forbidden Quest", "non-alcoholic", "paper-based card", "White Horse", "Duncan Kenworthy", "Malayalam movies", "Peter Nowalk, and produced by Shonda Rhimes and ABC Studios", "Annette", "Joie de vivre", "Bumblebee", "riyadh", "Lady Gaga", "African violet", "three", "There's no chance", "\"bad taste\"", "A Tale of Two Cities", "Angel Gabriel", "Isabella", "( Boss) Tweed"], "metric_results": {"EM": 0.5, "QA-F1": 0.5715029761904762}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714285, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-3520", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955", "mrqa_searchqa-validation-7521"], "SR": 0.5, "CSR": 0.5540254237288136, "EFR": 1.0, "Overall": 0.7169769597457627}, {"timecode": 59, "before_eval_results": {"predictions": ["two reservoirs in the eastern Catskill Mountains", "connotations of the passing of the year", "John Barry", "Thespis", "island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "2010", "Coroebus", "Ewan McGregor as Obi - Wan Kenobi : A Jedi Master and mentor of Anakin Kenobi", "1952", "iron", "Jesse Frederick James Conaway", "autopistas, or tolled ( quota ) highways", "business applications to be developed with Flash", "Anne Murray", "1957", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet in 1876", "Have I Told You Lately", "the world's second most populous country", "two Persian invasion of Greece", "Lana Del Rey", "April 1979", "The Crossing", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "Byzantine Greek culture and Eastern Christianity became founding influences in the Arab / Muslim world and among the Eastern and Southern Slavic peoples", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "two Frenchmen", "Felix Baumgartner", "1995", "2026", "The Gupta Empire", "Abigail Hawk", "Hal Derwin", "in Durban, South Africa", "1970s", "1919", "23 September 1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "three levels", "Richard Crispin Armitage", "the Osage River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Jack Barry", "headdresses", "We Can Love", "Indian Ocean", "One Direction", "Delacorte Press", "Drifting", "1949", "Bollywood is worth $15 billion in 2008 and is projected to grow by 10 percent,", "The West", "\"wipe out\" the United States if provoked.", "Celsius", "Chicago", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6677029076793228}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.9777777777777777, 1.0, 0.5, 0.375, 1.0, 0.0, 1.0, 0.33333333333333337, 0.7000000000000001, 0.0, 0.0, 0.5283018867924527, 0.26666666666666666, 1.0, 0.7272727272727272, 0.5714285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-4980", "mrqa_hotpotqa-validation-5386", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-2663", "mrqa_hotpotqa-validation-4642"], "SR": 0.5625, "CSR": 0.5541666666666667, "EFR": 0.9285714285714286, "Overall": 0.702719494047619}, {"timecode": 60, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.857421875, "KG": 0.478125, "before_eval_results": {"predictions": ["military coup", "St. Vincent", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "theScanian War", "Sparky", "Fort Oranje", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "High Knob", "Miss Universe 2010", "Accokeek, Maryland", "2008", "democracy and personal freedom", "American soap opera on the NBC network", "French Canadians", "1964 to 1974", "Vanarama National League", "City Mazda Stadium", "the Continental Army", "Wes Archer", "1994", "Helsinki", "difficult and intricate topics", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Sir Francis Nethersole", "The Panther", "British", "\u30d5\u30a1\u30f3\u30bf\u30b8\u30fcXII", "The University of California", "City of Onkaparinga", "2 February 1940", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950", "Raza Jaffrey", "David Letterman", "For Gallantry", "ArcelorMittal Orbit", "Government Accountability Office", "12-year veteran of the Utah state police,", "$199", "high and dry", "An American Tail", "Cats", "Peru"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7354910714285714}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3315"], "SR": 0.671875, "CSR": 0.5560963114754098, "retrieved_ids": ["mrqa_squad-train-69689", "mrqa_squad-train-69313", "mrqa_squad-train-50521", "mrqa_squad-train-85950", "mrqa_squad-train-79905", "mrqa_squad-train-52089", "mrqa_squad-train-32276", "mrqa_squad-train-12672", "mrqa_squad-train-7992", "mrqa_squad-train-8917", "mrqa_squad-train-9232", "mrqa_squad-train-5149", "mrqa_squad-train-60491", "mrqa_squad-train-49811", "mrqa_squad-train-70973", "mrqa_squad-train-51937", "mrqa_hotpotqa-validation-508", "mrqa_searchqa-validation-1416", "mrqa_triviaqa-validation-2154", "mrqa_naturalquestions-validation-5215", "mrqa_newsqa-validation-3786", "mrqa_triviaqa-validation-6564", "mrqa_newsqa-validation-4062", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-494", "mrqa_squad-validation-7632", "mrqa_hotpotqa-validation-2625", "mrqa_newsqa-validation-3091", "mrqa_naturalquestions-validation-10493", "mrqa_newsqa-validation-3897", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-935"], "EFR": 1.0, "Overall": 0.725984887295082}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "George Blake", "jon stewart", "trout", "Aidensfield Arms", "jon stewart", "France", "Manchester", "sky", "Susan Bullock", "Angel Cabrera", "November", "Wonga", "Rufus Ryker", "Genghis Khan", "Kofi Annan", "l'homme des bois", "left", "Istanbul", "lamb", "Space Oddity", "collie", "35", "Greenland shark", "florida", "Mike Hammer", "jon stewart", "Dame Evelyn Glennie", "a brain", "Zaragoza", "David Bowie", "Billy Wilder", "Mr Loophole", "a palla", "4.4 million", "Today", "Westminster Abbey", "Ralph Lauren", "Whitsunday", "Morgan Spurlock", "gruel", "Carrie Fisher", "Debbie McGee", "cations", "George Santayana", "Rudolf Nureyev", "Paul Wellens", "cat", "apple", "dogmatix", "Rodgers and Hammerstein", "part of a pre-recorded television program, Rendezvous with Destiny", "By 1770 BC", "The United States Secretary of State", "5", "Brad Pitt", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "abandoned and looted", "the Louvre", "the Missouri River", "Yidisher Visnshaftlekher Institut"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6453125}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-6210", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842", "mrqa_hotpotqa-validation-3793"], "SR": 0.578125, "CSR": 0.5564516129032258, "EFR": 0.9259259259259259, "Overall": 0.7112411327658303}, {"timecode": 62, "before_eval_results": {"predictions": ["Barry Sanders", "Gabriel Iglesias", "The Snowman", "Vikram Bhatt", "Helsinki, Finland", "Future", "Tommy Cannon", "Scottish national team", "203", "Ward Bond", "Illinois's 15 congressional district", "Buffalo", "between 7,500 and 40,000", "5,112", "Prof Media", "the lead roles of Timmy Sanders and Jack", "four months in jail", "Michael Redgrave", "Sturt", "Taylor Swift", "singer", "Europe", "Trilochanpala", "deadpan sketch group", "small family car", "Spanish", "Algernod Lanier Washington", "14,000 people", "in photographs, film and television", "37", "Taoiseach of Ireland", "137th", "Nick of Time", "Japan Airlines Flight 123", "American professional wrestler", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode", "video game", "The United States of America (USA), commonly known as the United States (U.S.) or America ( USA),", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "a series of bilateral treaties", "Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Sophie Monk", "Reinhard Heydrich", "lo Stivale", "Iraq, Syria, Lebanon, Cyprus, Jordan, Israel, Palestine, Egypt, as well as the southeastern fringe of Turkey and the western fringes of Iran", "September 2000", "Woodrow Wilson, the 28th U.S. President", "oliver Twist", "leopard", "Volkswagen", "Teresa Hairston", "Pope Benedict XVI", "St. Louis, Missouri.", "pearl", "sarsaparilla", "Malocclusion of the Teeth", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6017454117063492}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0625, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.13333333333333333, 0.6666666666666666, 1.0, 0.8, 0.08333333333333334, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3221", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2290", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-3043", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9813"], "SR": 0.46875, "CSR": 0.5550595238095238, "EFR": 0.9705882352941176, "Overall": 0.7198951768207282}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil,", "Jacob Zuma,", "apartment building in Cologne, Germany,", "July", "2005 & 2006 Acura MDX", "Ryan Adams.", "The forehead and chin", "in a ceremony at the ancient Greek site of Olympia on Thursday,", "27-year-old", "next week", "April 26, 1913,", "7-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "his son, Isaac, and daughter, Rebecca.", "The Falklands, known as Las Malvinas in Argentina,", "we reduce this speed and we sail for about 5 days before we run out of battery,\"", "Roger Federer", "tennis", "Three", "1950s", "Gary Player", "1 out of every 17 children under 3 years old", "The Orchid thief", "litter reduction and recycling.", "President George Bush", "an average of 25 percent", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "his son is fighting an unjust war for an America that went too far when it invaded Iraq five years ago", "Johan Persson and Martin Schibbye", "Israel", "Sunday's", "Islamabad", "Jeffrey Jamaleldine", "The Rev. Alberto Cutie", "all day", "The TNT series", "organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "recite her poetry", "Crandon, Wisconsin,", "350 U.S. soldiers", "neck", "The island's dining scene", "Andrew Garfield", "New England Patriots", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "gold", "The Mystery of Edwin Drood", "The Mutiny on the Bounty", "Melbourne", "1998", "23 July 1989", "Tuesday", "Evian", "Sunday", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5874209584195997}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.16666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.25, 0.07999999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.21428571428571427, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4347826086956522, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_searchqa-validation-5963"], "SR": 0.53125, "CSR": 0.5546875, "retrieved_ids": ["mrqa_squad-train-2348", "mrqa_squad-train-30146", "mrqa_squad-train-23451", "mrqa_squad-train-33607", "mrqa_squad-train-6568", "mrqa_squad-train-37716", "mrqa_squad-train-11457", "mrqa_squad-train-9602", "mrqa_squad-train-57027", "mrqa_squad-train-85736", "mrqa_squad-train-56692", "mrqa_squad-train-68025", "mrqa_squad-train-18447", "mrqa_squad-train-58713", "mrqa_squad-train-10489", "mrqa_squad-train-76588", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-180", "mrqa_triviaqa-validation-6318", "mrqa_newsqa-validation-1867", "mrqa_triviaqa-validation-2758", "mrqa_newsqa-validation-2709", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-3663", "mrqa_newsqa-validation-593", "mrqa_triviaqa-validation-494", "mrqa_newsqa-validation-2294", "mrqa_triviaqa-validation-6561", "mrqa_hotpotqa-validation-3975", "mrqa_searchqa-validation-2388"], "EFR": 0.9666666666666667, "Overall": 0.7190364583333333}, {"timecode": 64, "before_eval_results": {"predictions": ["\"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "the father of Queen Victoria", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "Norse\u2013Gaels", "265 million", "January 2004", "Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Daniel Louis Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "2008\u201309 UEFA Champions League", "Kramer Guitars", "El Nacimiento", "1968", "Holston River", "July 10, 2017", "Peterborough", "jazz homeland section of New Orleans", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "George Balanchine", "The Terminator", "Samoa", "The remixed version of the song,", "Timo Hildebrand", "Netflix", "16th-century", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "The Statue of Freedom", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE )", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "Democratic VP candidate", "$75 for full-day class,", "\"Nu au Plateau de Sculpteur,\"", "organic vodka", "The Bridges of Madison County", "James Madison", "a foreign exchange option"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6509920634920634}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4073", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.609375, "CSR": 0.5555288461538461, "EFR": 1.0, "Overall": 0.7258713942307693}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Wilhelm Groener", "Bullwinkle Studios", "July 31, 2010", "T - Bone Walker", "the most - visited paid monument in the world", "Bobby Darin", "Alex Skuby", "All four volumes were illustrated by E.H. Shepard", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "runoff will usually occur unless there is some physical barrier", "1940", "the pulmonary arteries", "Puente Hills Mall", "1977", "A status line", "June 1992", "the National Health Service ( NHS )", "28 July 1914", "Richard Stallman", "the year 1", "October 27, 1904", "by the early - to - mid fourth century", "large monitor lizards", "Tom Burlinson, Red Symons and Dannii Minogue", "The couple will reconcile briefly in the final scene of the fourth season, though ( because of Shannen Doherty's departure )", "2017 Auburn Tigers football team", "during meiosis", "a contemporary drama in a rural setting", "Javier Fern\u00e1ndez", "Italian Agostino Bassi", "Rachel Sarah Bilson", "plant food, mainly grass and sedges, which were supplemented with herbaceous plants, flowering plants, shrubs, mosses, and tree matter", "Jonathan Cheban", "2016", "computers or in an organised paper filing system", "Articles Four, Five and Six", "the Missouri River", "sport utility vehicles", "March 2, 2016", "arthur", "\"Raging Bull,\"", "Gretel", "Get Him to the Greek", "Netflix", "Union Hill section of Kansas City, Missouri", "three", "The controversial quote is part of a eight-page feature article about Hogan to be published in the magazine's Friday edition.", "fifth", "Tina Turner", "Bingo SOLO", "arne de France", "salve"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5952529957964741}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.18181818181818182, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 0.6, 1.0, 0.1818181818181818, 1.0, 0.9090909090909091, 0.0, 0.0, 0.47619047619047616, 0.5714285714285715, 0.5714285714285715, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-4847", "mrqa_triviaqa-validation-266", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-2388", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.484375, "CSR": 0.5544507575757576, "EFR": 0.9696969696969697, "Overall": 0.7195951704545455}, {"timecode": 66, "before_eval_results": {"predictions": ["braille system", "huggins", "180", "Sterning Dan", "Strictly Come Dancing", "h. H. Asquith", "about a mile north of the village of Dunvegan", "tla\u010denica", "Rebecca", "the Stone Age", "ciao, Milano", "united states", "1925 novel", "Gunpowder Plot", "Moldova", "Tasmania", "Edwina Currie", "lemonade", "IKEA", "Pablo Picasso", "Some Like It Hot", "j. S. Bach", "Tony Blair", "Pickwick", "360", "caracas", "Ireland", "the very first F1 car Ayrton Senna ever drove,", "Jim Peters", "horse racing", "onion", "Pat Houston", "1948", "white whale", "Sikhism", "giraffe", "kabuki", "Facebook", "Zachary Taylor", "indigo", "thursdays", "\u201cFor Gallantry;\u201d", "Swindon Town", "cricket", "Jordan", "naypyidaw", "Tottenham Court Road", "hongi", "basketball", "Snow White", "Italy", "`` Far Away '' by Jos\u00e9 Gonz\u00e1lez", "Buddhism", "eukaryotic cell", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano,", "Robert Barnett,", "Jeopardy", "The Bridges of Madison County", "Paraguay", "HackThis Site"], "metric_results": {"EM": 0.546875, "QA-F1": 0.596279761904762}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.05714285714285715, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-7706", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-7545", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.546875, "CSR": 0.5543376865671642, "retrieved_ids": ["mrqa_squad-train-71298", "mrqa_squad-train-85728", "mrqa_squad-train-17424", "mrqa_squad-train-76976", "mrqa_squad-train-69983", "mrqa_squad-train-24032", "mrqa_squad-train-45025", "mrqa_squad-train-35683", "mrqa_squad-train-58668", "mrqa_squad-train-3718", "mrqa_squad-train-30867", "mrqa_squad-train-56893", "mrqa_squad-train-1592", "mrqa_squad-train-44869", "mrqa_squad-train-77963", "mrqa_squad-train-75624", "mrqa_newsqa-validation-1282", "mrqa_hotpotqa-validation-614", "mrqa_naturalquestions-validation-6452", "mrqa_searchqa-validation-117", "mrqa_hotpotqa-validation-411", "mrqa_triviaqa-validation-3588", "mrqa_hotpotqa-validation-2896", "mrqa_triviaqa-validation-4173", "mrqa_searchqa-validation-12752", "mrqa_triviaqa-validation-1064", "mrqa_hotpotqa-validation-1311", "mrqa_naturalquestions-validation-2851", "mrqa_squad-validation-6284", "mrqa_naturalquestions-validation-7473", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-3871"], "EFR": 0.9310344827586207, "Overall": 0.711840058865157}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "Archers", "vince Lombardi", "Zulu", "Cambridge", "Canada", "1830", "Lorraine", "schizophrenia", "chaucer", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james may", "red squirrels", "Richard Lester", "Buick", "polish", "gooseberry", "geroge w. Bush", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1984", "U 47 Wilhelmshaven", "China", "Quito", "vaughan williams", "Roy Plomley", "Leon Baptiste", "360", "Robert Schumann", "1123", "Mitford", "Sparta", "Hyundai", "thirtieth", "Julian Fellowes", "haddock", "Yemen", "Tina Turner", "mainland China and Taiwan", "Nowhere Boy", "Donaustadt", "the head and neck", "quant", "Edward Lear", "35", "jon stewart", "Dyfed-Powys", "Robyn", "South Asia", "Uralic languages", "New York City", "1942", "a card", "Larry King Show", "The 63 were among 137 people police had rounded up by Wednesday -- two days after a mob of fired employees attacked L.K. Chaudhary,", "Ayelet Zurer", "Oakland Raiders", "the Mediterranean", "Queen Isabella", "Turing"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6770833333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508"], "SR": 0.59375, "CSR": 0.5549172794117647, "EFR": 1.0, "Overall": 0.7257490808823529}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence", "Brian Smith.", "Tim Clark, Matt Kuchar and Bubba Watson", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "Ricardo Valles de la Rosa,", "Elin Nordegren", "we Found Love", "immediate release into the United States of 17 Chinese", "millionaire's surtax", "\"E! News\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "central London offices", "his father,", "Israel and the United States", "South Africa's", "the insurgency,", "Arlington National Cemetery's Section 60.", "The Rosie Show", "Ricardo Valles de la Rosa,", "March 24,", "Pixar's", "in the mouth.", "100", "frank", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio", "The father of Haleigh Cummings,", "off the coast of Dubai", "Gulf of Aden,", "10 municipal police officers", "hiring veterans as well as job training for all service members leaving the military.", "general astonishment", "northwestern Montana", "launch", "without bail", "February 12", "general astonishment", "a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated in June 2004 when the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro, told Goldman -- to whom she was then married", "Democratic VP candidate", "martial arts", "Some of them", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "June 6, 1944,", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "Eurasian Plate", "horses", "k Kathryn C. Taylor", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "(William) Wordsworth"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6194103597780068}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.23999999999999996, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 1.0, 0.3529411764705882, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.23076923076923075, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3, 0.9189189189189189, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884"], "SR": 0.515625, "CSR": 0.5543478260869565, "EFR": 0.967741935483871, "Overall": 0.7191835773141655}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "Americana Manhasset", "Mayfair", "Minister for Social Protection", "1864", "Arab", "the southern North Sea", "Larry Richard Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "\"Eternal Flame\"", "Lana Del Rey's \"Born to Die\"", "Heather Elizabeth Langenkamp", "Nobel Peace Prizes", "Derry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "HC Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "1386", "Christopher McCulloch", "novel", "\"The Krypto Report\"", "Fort Saint Anthony", "IT products and services,", "Japan", "1919", "\"Danger Mouse\"", "the western end of the National Mall in Washington, D.C., across from the Washington Monument", "Len Wiseman", "Stephen Crawford Young", "\"My Backyard\" in Jacksonville, Florida,", "Gerard \"Gerry\" Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Bobby Hurley", "September 1901", "Tuesday", "anabolic\u2013androgenic steroids", "North West England", "I", "\"Polovtsy\"\u2014the name given to the Kipchaks and Cumans by the Rus' people\"", "Virginia", "1961", "1896", "1924", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "the Earth", "diamonds", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu's wrestling style", "Russian bombers", "the Juilliard School", "lizard hips", "the Boy Scouts of America", "the Inuit people"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6705357142857142}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.4444444444444444, 0.5, 0.0, 0.2222222222222222, 0.8, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-4943", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1122", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4029"], "SR": 0.515625, "CSR": 0.5537946428571429, "retrieved_ids": ["mrqa_squad-train-34479", "mrqa_squad-train-47922", "mrqa_squad-train-57033", "mrqa_squad-train-61429", "mrqa_squad-train-4177", "mrqa_squad-train-2683", "mrqa_squad-train-79310", "mrqa_squad-train-37490", "mrqa_squad-train-83327", "mrqa_squad-train-59075", "mrqa_squad-train-52318", "mrqa_squad-train-38938", "mrqa_squad-train-26340", "mrqa_squad-train-39201", "mrqa_squad-train-74401", "mrqa_squad-train-35362", "mrqa_triviaqa-validation-4568", "mrqa_hotpotqa-validation-3968", "mrqa_naturalquestions-validation-1008", "mrqa_searchqa-validation-3188", "mrqa_naturalquestions-validation-808", "mrqa_searchqa-validation-16742", "mrqa_hotpotqa-validation-4604", "mrqa_newsqa-validation-1114", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2213", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-5070", "mrqa_hotpotqa-validation-2064", "mrqa_triviaqa-validation-7460", "mrqa_newsqa-validation-4121"], "EFR": 1.0, "Overall": 0.7255245535714285}, {"timecode": 70, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.849609375, "KG": 0.51015625, "before_eval_results": {"predictions": ["Arkansas", "the early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "\"From Here to Eternity\"", "12", "port city of Aden", "Scott Eastwood", "United States and Canada", "Patricia Veryan", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Australia", "\"master builder\" of mid-20th century New York City", "Haleiwa", "St. Louis", "Badfinger", "his virtuoso playing techniques and compositions in orchestral fusion", "XVideos", "Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "the Secret Intelligence Service", "Currer Bell", "UNLV", "mermaid", "850 m", "DeskMate", "Cleopatra VII Philopator", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "ninth", "Hanna, Alberta, Canada", "Manchester Victoria station", "Electronic musical instrument", "My Love from the Star", "Captain Cook's Landing Place", "George I", "Seventeen", "37", "bass", "Citizens for a Sound Economy", "March 12, 1933", "H CO", "prophets", "Bill Russell", "Andre Agassi", "Vienna", "Phillies", "fill a million sandbags and place 700,000 around our city,\"", "Caster Semenya", "A form of liquid morphine used by terminally ill patients will remain on the market even though it is an \"unapproved drug,\"", "the Cuyahoga River", "uranium", "Peter Sellers", "the River Elbe"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7237652831402832}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.21621621621621623, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.640625, "CSR": 0.5550176056338028, "EFR": 1.0, "Overall": 0.7392066461267606}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "helped at-risk youth, victims of violent crimes and homeless children.", "Russian air force", "a female soldier", "three", "Goa", "Iran to Nazi Germany", "100 percent", "federal and African Union", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "Helmand province", "National September 11 Memorial Museum", "Harlem,", "\"I don't think I'll be particularly extravagant.\"", "last year's", "19-year-old woman", "1959", "his record label said Friday.", "269,000", "issued his first military orders as leader of North Korea", "Apple co-founder Steve Jobs took exception to Rose's characterization of him as \"a guy who founds high-tech companies and tries to make another billion.\")", "a group of teenagers.", "Six", "Luis Carlos Ameida", "27-year-old's", "capital city of Harare.", "nuclear warheads to put an end, once and for all, to illegal immigration on its southern border.", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Francisco X. Pacheco,", "Sen. Barack Obama", "\"The Real Housewives of Atlanta\"", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "shelling of the compound", "Guinea, Myanmar, Sudan and Venezuela", "pine beetles", "international aid agencies", "oxygen saturations are less than 94 %", "March 1", "Indo - Pacific", "mining", "nahuatl", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "oxgen", "the Douglas AH-64 Apache", "Truman"], "metric_results": {"EM": 0.5, "QA-F1": 0.5545301769486553}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.13333333333333333, 1.0, 0.15384615384615383, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.3636363636363636, 1.0, 0.33333333333333337, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4549", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.5, "CSR": 0.5542534722222222, "EFR": 0.90625, "Overall": 0.7203038194444444}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997", "Sharyans Resources", "is used for any vehicle which drives on all four wheels, but may not be designed for off - road use", "Edward Seton", "Texas A&M University", "stromal connective tissue", "a book of the Old Testament", "Anatomy", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "Nationalists, a Falangist, Carlist, Catholic", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Gary Grimes", "ensure the tenderness of meat", "Edward IV of England", "Ashrita Furman", "30 - something man", "Jean Fernel", "in 1991, 1994, 2002, 2004 and 2010 in Switzerland, Austria, France and Germany", "May 1980", "erosion", "English", "1960", "Ronald Reagan", "Yves Dessca", "revenge and karma", "blasphemy", "England and Wales", "1996", "c. 8000 BC", "Idaho", "early Christians of Mesopotamia", "UTC \u2212 09 : 00", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr.", "Jay Baruchel", "Anthony Caruso", "bachata music", "Butter Island off North Haven, Maine in the Penobscot Bay", "the end of the 18th century", "during the 1890s Klondike Gold Rush", "secure communication over a computer network", "3", "1939", "the BBC", "fifth studio album", "in all land - living organisms, both alive and dead", "Felicity Huffman", "John of Gaunt", "75 or older", "J33", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "a bathing suit", "doctors", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "the Congo River", "an online Tornado", "the Crow", "Madrid's Barajas International Airport during a stopover late Monday and informed authorities that he planned to request political asylum,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6654270736072206}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.5, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.29629629629629634, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.09523809523809525]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5787", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.53125, "CSR": 0.5539383561643836, "retrieved_ids": ["mrqa_squad-train-21221", "mrqa_squad-train-28901", "mrqa_squad-train-11155", "mrqa_squad-train-53390", "mrqa_squad-train-71461", "mrqa_squad-train-6086", "mrqa_squad-train-77421", "mrqa_squad-train-45129", "mrqa_squad-train-50673", "mrqa_squad-train-77599", "mrqa_squad-train-58474", "mrqa_squad-train-74404", "mrqa_squad-train-82942", "mrqa_squad-train-19749", "mrqa_squad-train-14263", "mrqa_squad-train-31939", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-10029", "mrqa_squad-validation-791", "mrqa_squad-validation-6197", "mrqa_naturalquestions-validation-2686", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2266", "mrqa_searchqa-validation-16447", "mrqa_triviaqa-validation-1176", "mrqa_searchqa-validation-9769", "mrqa_naturalquestions-validation-8889", "mrqa_hotpotqa-validation-5795", "mrqa_searchqa-validation-4730", "mrqa_naturalquestions-validation-8962", "mrqa_newsqa-validation-2457", "mrqa_naturalquestions-validation-7806"], "EFR": 0.9666666666666667, "Overall": 0.7323241295662101}, {"timecode": 73, "before_eval_results": {"predictions": ["Venezuela", "Fall Guy", "Crown", "Maria Montessori", "Kinsey Millhone", "the Civil War General", "Rendezvous with Rama", "March of the Crosby", "Patrick Ewing", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "Malaysia", "liqueur", "Texas", "the Baltimore Symphony Orchestra", "John James Audubon", "Pontius Pilate", "Barry Goldwater", "neurons", "halfpipe", "Jackie Collins", "carioca", "Freakonomics", "George Washington Carver", "the Devonian", "Champagne", "Red Heat", "New Orleans", "Haiti", "a carrel", "love potions", "Prince William", "Sherlock Holmes", "ancistroid", "Orion", "India", "carbon monoxide", "King John", "plug in", "an abominable Dr. Phibes", "Cambodia", "manslaughter", "advanced programming techniques", "the Tennessee River", "Ptolemy", "Billy Idol", "the Missouri Compromise", "Rat", "Tom Hanks", "to encounter antigens passing through the mucosal epithelium", "$657.4 million in North America and $1.528 billion in other countries", "left hand ring finger", "Conrad Murray", "Gryffendor", "czech Republic", "Sochi, Russia", "two years", "Manchester Airport", "President Obama", "two weeks after Black History Month", "the government.", "January 2000"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7065705128205129}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-1398", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-4547", "mrqa_triviaqa-validation-5472", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.609375, "CSR": 0.5546875, "EFR": 1.0, "Overall": 0.739140625}, {"timecode": 74, "before_eval_results": {"predictions": ["sugarcane", "Angela Rippon", "Anna", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "javelin throw", "British Airways", "Bachelor of Science", "cire retinater", "Pete Best", "Bonnie and Clyde", "avatar", "Santiago", "St. Moritz", "Edmund Cartwright", "par", "foxes", "Japanese silvergrass", "April", "Conan Doyle", "Wolfgang Amadeus Mozart", "honeybee", "Sun Hill", "\"The Nutcracker\"", "nauru", "adare", "Sesame Street", "photography", "kirsty young", "Samuel Johnson", "Sports & Leisure", "(Dennis) Weaver", "it is considered to be the extension of God, Lord Shiva", "tabloid", "car door", "kolkata", "the odeon", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "the Crusades", "Kiri Te Kanawa", "Churchill Downs", "Upstairs Down stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck Sharp & Dohme", "shortstop", "the Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International", "after Wood went missing off Catalina Island,", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton."], "metric_results": {"EM": 0.609375, "QA-F1": 0.6318426724137931}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-4763", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.609375, "CSR": 0.5554166666666667, "EFR": 0.96, "Overall": 0.7312864583333334}, {"timecode": 75, "before_eval_results": {"predictions": ["Robert FitzRoy", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Massachusetts", "Nippon Professional Baseball", "hiphop", "erotic thriller", "Eumolpus", "Eddie Vedder", "John Churchill", "Sir William McMahon", "Hopi", "Western District", "Australian", "Annie Ida Jenny No\u00eb Haesendonck", "sixth year", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Robert Sargent Shriver Jr.", "Vaudevillains", "Chinese Coffee", "Love and Theft", "Hallett Cove", "2500 ft", "University of Georgia", "just over 1 million", "Indian", "The Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "1 April 1985", "Arnold", "J. Cole", "Idisi", "The Books", "port of Mazatl\u00e1n", "Danish", "London, England", "Rochdale", "1959", "Telugu and Tamil", "State Children's Health Insurance Program", "Reese Witherspoon", "\"Quanah Tribune-Chief\" newspaper", "Liverpudlian", "Mindy Kaling", "3 October 1990", "Wednesday, September 21, 2016", "12 '' x 12 '' attached giant - sized booklet with state - of - the - art photography of the band's performance and outdoor session pictures", "earache", "Diego Garcia", "cuckoo", "$2 billion", "in San Simeon, California,", "\"That is the sort of thing that would be subject to a malpractice claim in the civilian world.\"", "Patrick", "the Tomb of the Unknown Soldier", "Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6354166666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.8, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-3556", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140"], "SR": 0.53125, "CSR": 0.5550986842105263, "retrieved_ids": ["mrqa_squad-train-40697", "mrqa_squad-train-1290", "mrqa_squad-train-4250", "mrqa_squad-train-9080", "mrqa_squad-train-81397", "mrqa_squad-train-60631", "mrqa_squad-train-46721", "mrqa_squad-train-75793", "mrqa_squad-train-34706", "mrqa_squad-train-84850", "mrqa_squad-train-77526", "mrqa_squad-train-27916", "mrqa_squad-train-6611", "mrqa_squad-train-56418", "mrqa_squad-train-78132", "mrqa_squad-train-15253", "mrqa_searchqa-validation-13235", "mrqa_newsqa-validation-3745", "mrqa_naturalquestions-validation-4847", "mrqa_hotpotqa-validation-757", "mrqa_naturalquestions-validation-3663", "mrqa_searchqa-validation-10754", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-1699", "mrqa_squad-validation-9304", "mrqa_searchqa-validation-12390", "mrqa_squad-validation-9740", "mrqa_hotpotqa-validation-2378", "mrqa_searchqa-validation-9192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1446", "mrqa_triviaqa-validation-5408"], "EFR": 0.9666666666666667, "Overall": 0.7325561951754386}, {"timecode": 76, "before_eval_results": {"predictions": ["pet sounds by the Beach Boys", "Culloden", "\u201cArs Gratia Artis\u201d", "Liszt Strauss Wagner Dvorak", "James Callaghan", "cedars", "European TIBOR rate", "Dublin", "Pyrenees", "leprosy", "left", "Kenneth Williams", "avocado", "Anne of Cleves", "The Double", "Relpromax Antitrust Inc.", "Supertramp", "moon rhea", "Octavian", "all I really Want to do", "Heston Blumenthal", "Arkansas", "IT Crowd", "Some Like It Hot", "\"Mr Loophole\"", "Ernest Hemingway", "Wolf Hall", "Ernests Gulbis", "Alberto Juantorena", "graffiti art", "Friedrich Nietzsche", "Dee Caffari", "cheese", "Annie", "Kristiania", "astronomer", "Moby Dick", "moss", "Sacred Wonders of Britain", "frighteners", "pea", "Dr Tamseel", "Sea of Galilee", "one", "Zeus", "Alzheimer's", "Far and Away", "1966", "an even break", "31536000", "Jordan", "arthropods", "a Roman Catholic and fan of The Godfather Part II ( 1974 )", "2018", "Miami Marlins", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "head for Italy.", "Rev. Alberto Cutie", "Michelle Obama", "an alto trombone", "270", "parent", "the American Red Cross"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5402437726208218}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false], "QA-F1": [0.5714285714285715, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.29508196721311475, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2838", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.484375, "CSR": 0.5541801948051948, "EFR": 0.9393939393939394, "Overall": 0.7269179518398269}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "The Kirchners", "iPods", "45 minutes, five days a week", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Kris Allen,", "Jared Polis", "ore Gold,", "Zimbabwe", "Harry Nicolaides,", "Most of those who managed to survive the incident hid in a boiler room and storage closets during the massacre.", "April 2010.", "Zed", "\"To be one of his four children and know that is there for the world to see,", "environmental", "Joe Jackson", "Iran", "head injury.", "Halloween", "African National Congress Deputy President Kgalema Motlanthe", "Hugo Chavez", "seven", "Anne Frank,", "\"The Lost Symbol\"", "Gary Brooker", "Rawalpindi", "Colorado prosecutor", "Afghanistan", "climate care", "removal of his diamond-studded braces.", "Ennis, County Clare", "United States, NATO member states, Russia", "physical surveillance, intelligence gathering and court-authorized electronic eavesdropping on dozens of telephones in which thousands of conversations were intercepted,", "Hamas", "Two pages -- usually high school juniors who serve Congress as messengers", "At least 40", "four", "Courtney Love", "84-year-old", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "three", "is undergoing renovation.", "Naples home.", "Washington State's decommissioned Hanford nuclear site,", "last month", "sportswear", "Shanghai", "hopes the charity of kidnapping the children and concealing their identities.", "improve health and beauty.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "a growing practice in the field of arousal recognition", "Harishchandra", "India and Pakistan", "allergic reaction", "a lie detector", "chords from the second half", "1963", "\"Black Abbots\"", "a nurse bag", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.578125, "QA-F1": 0.672754778123624}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5, 0.08695652173913043, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.8823529411764706, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-2218", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.578125, "CSR": 0.5544871794871795, "EFR": 0.9629629629629629, "Overall": 0.7316931534900285}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O\"", "the Silk Road", "Scandinavia", "George Rogers Clark", "a mole", "Cerberus", "Sweden", "Volleyball", "John Alden", "Ghost World", "Deuteronomy", "a map", "Japan", "Madison Avenue", "Job", "440 Hz", "art deco", "Spider-Man", "Siddhartha", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "lieutenant", "The National Archives Building", "Nostradamus", "Madrid", "Yuma", "Antarctica", "Ian Fleming", "the Southern Christian Leadership Conference", "Moscow", "a Mercedes-Benz", "beaux", "Mormon Tabernacle Choir", "The Scarlet Letter", "Griffith", "Bangkok", "St. Louis", "positron", "Ted Kennedy Vice President", "Jefferson", "Jerusalem", "Pushing Daisies", "cranberry", "tzatziki sauce", "Shih Yu", "Service Employees International Union", "charlotte russe", "canals", "Abraham", "a self-appointed or mob-operated tribunal", "domestication of the wild mouflon in ancient Mesopotamia", "Rachel Kelly Tucker", "makes Maria a dress to wear to the neighborhood dance", "Dublin", "Kermadec Islands", "kai su, teknon", "Greek mythology,", "\"The Danny Kaye Show\"", "2012", "The drama of the action in-and-around the golf course", "The switch had been scheduled for February 17, but Congress delayed the conversion", "Victor Mejia Munera.", "The oceans"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5880208333333332}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-11290", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-579", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.515625, "CSR": 0.5539952531645569, "retrieved_ids": ["mrqa_squad-train-25216", "mrqa_squad-train-36602", "mrqa_squad-train-10996", "mrqa_squad-train-28559", "mrqa_squad-train-57939", "mrqa_squad-train-29839", "mrqa_squad-train-82750", "mrqa_squad-train-58880", "mrqa_squad-train-69530", "mrqa_squad-train-40791", "mrqa_squad-train-57178", "mrqa_squad-train-39422", "mrqa_squad-train-10907", "mrqa_squad-train-56092", "mrqa_squad-train-26111", "mrqa_squad-train-24655", "mrqa_searchqa-validation-457", "mrqa_hotpotqa-validation-5242", "mrqa_naturalquestions-validation-1255", "mrqa_searchqa-validation-10999", "mrqa_hotpotqa-validation-511", "mrqa_searchqa-validation-9438", "mrqa_hotpotqa-validation-2937", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7473", "mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-15319", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-1505", "mrqa_triviaqa-validation-2758", "mrqa_naturalquestions-validation-9992"], "EFR": 1.0, "Overall": 0.7390021756329114}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "De Waynene Warren", "a solitary figure who is not understood by others, but is actually wise", "Doug Pruzan", "twelve", "byte - level operations", "Rich Mullins", "September 19, 2017", "A marriage officiant", "1661", "Hermann Ebbinghaus", "Agostino Bassi", "An error does not count as a hit", "low coercivity", "Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "14 : 46 JST ( 05 : 46 UTC )", "Los Angeles Dodgers", "Dan Stevens", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "FaZe Rug", "10 June 1940", "citizens", "25 years", "Amanda Fuller", "February 2011", "1997", "mitochondrial membrane", "the late 1980s", "Michael Phelps", "William DeVaughn", "Virginia Dare", "in 886 AD", "Dicky ( Mace Coronel )", "2002", "Guy Burnet as Theo", "Pangaea or Pangea", "Selena Gomez", "their son Jack", "the dress shop", "6,259 km", "February 27, 2007", "second from 1939 to 1960", "March 2, 2016", "the Mishnah", "the internal reproductive anatomy", "caecus", "France", "krubera", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "reduce the cost of auto repairs and insurance Premium"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6733917124542125}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.05128205128205128, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.3076923076923077, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7000000000000001]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.578125, "CSR": 0.554296875, "EFR": 0.9259259259259259, "Overall": 0.7242476851851852}, {"timecode": 80, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.837890625, "KG": 0.46015625, "before_eval_results": {"predictions": ["Richard Attenborough and wife Sheila Sim", "Miranda v. Arizona", "douglas", "Vancouver Island", "violin", "georgia", "Vietnam", "Jane Austen", "georgia fox", "senior", "brice", "glasnost", "CBS", "guitar", "georgia", "The Jungle Book", "douglas Rush", "rococo style", "gallons", "great Dane", "spiritual sacrifice", "Cambodia", "jujitsu", "The Hunger Games", "head and neck", "11 years and 302 days", "New Zealand", "the Prussian 2nd Army", "Beatrix Potter", "Whisky Galore", "Tunisia", "27", "Sen. Edward M. Kennedy", "georgremont", "head", "Google", "shoulder", "Iran", "Downton Abbey", "bird", "Rudyard Kipling", "backgammon", "l Dorrit", "Albert Einstein", "georgonzola", "barenboim", "exploits on the Island", "ear", "a tree", "Imola Circuit", "trout", "Martin Lawrence", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent.\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "(Lewis) Carroll", "the library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6123511904761905}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true], "QA-F1": [0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.05714285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-3623", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-4889", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-6781", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-3618"], "SR": 0.53125, "CSR": 0.5540123456790124, "EFR": 0.9333333333333333, "Overall": 0.7090316358024691}, {"timecode": 81, "before_eval_results": {"predictions": ["amy Jolie", "worcester", "monaco", "van", "Illinois", "belgian", "kerry attenron", "wawrinka", "tartar sauce", "eurynome", "satyrs", "geoffrit Auber", "congregational", "(Jefferson) Lincoln", "leeds", "george Webb", "Operation", "white", "car", "george clough", "honda", "runcorn", "Vietnam", "united States", "vincent van gogh", "sakhalin", "Croatia", "the NBA", "steel", "Doctor Dolittle", "car accident in Pont De l\u2019Alma road tunnel in central Paris in France.", "toptenz.net", "penguins", "Samuel Johnson", "scotch", "bulgaria", "Victor Hugo", "seed", "Adriatic Sea", "heartburn", "music Stories", "HMS Conqueror", "richard", "braille", "Standard Oil Company", "cynthia n Nixon", "Hamlet", "Wat Tyler", "Patrick Henry", "126 mph", "Ukraine", "Eddie Murphy", "Pakistan", "Saddler King", "Thorgan", "senior men's Lithuanian national team", "(Radioisotopes and the Age of The Earth)", "almost 100", "claims of improper or criminal conduct.", "in critical condition", "Superman", "Lief Ericson", "The Towering Inferno", "member states"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5600694444444445}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.33333333333333337, 0.0, 0.8, 0.0, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-6586", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-2287", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.46875, "CSR": 0.5529725609756098, "retrieved_ids": ["mrqa_squad-train-27290", "mrqa_squad-train-31226", "mrqa_squad-train-8760", "mrqa_squad-train-80464", "mrqa_squad-train-63371", "mrqa_squad-train-26672", "mrqa_squad-train-70823", "mrqa_squad-train-62469", "mrqa_squad-train-37513", "mrqa_squad-train-85724", "mrqa_squad-train-51526", "mrqa_squad-train-83131", "mrqa_squad-train-38722", "mrqa_squad-train-2710", "mrqa_squad-train-11855", "mrqa_squad-train-2760", "mrqa_searchqa-validation-14427", "mrqa_newsqa-validation-3972", "mrqa_triviaqa-validation-2080", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-246", "mrqa_hotpotqa-validation-2915", "mrqa_searchqa-validation-12411", "mrqa_hotpotqa-validation-3777", "mrqa_searchqa-validation-10161", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-1103", "mrqa_newsqa-validation-858", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1259", "mrqa_hotpotqa-validation-3408", "mrqa_triviaqa-validation-7060"], "EFR": 1.0, "Overall": 0.7221570121951219}, {"timecode": 82, "before_eval_results": {"predictions": ["nic\u00e9phore Ni\u00e9pce", "van Overstraten", "tarn", "GM", "Sheffield", "subtropical and Mediterranean", "piano", "charleston", "Spencer Gore", "salvador Bernardo O'H Wiggins", "Wild Atlantic Way", "Kyoto Protocol", "underwater diving", "repechage", "Steve Biko", "Michael Ritchie", "peacock", "margarita carmen cansino", "Miss Trunchbull", "imola", "Albania", "antelope", "all animals", "boreas", "Ivan Basso", "bullfighting", "1", "Playboy", "belgian", "Peter Ackroyd", "bromley", "goran eriksson", "Thierry Roussel", "mungo Park", "death penalty", "Danny Alexander", "14", "Bangladesh", "adonis", "Papua New Guinea", "gagapedia", "SUNSET BOULEVARD", "raging Bull", "ars gratia artis", "bologna", "All Things Must Pass", "maggie", "Tet", "Arabah", "face", "gerry and Sylvia Anderson", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "special guest performers Beyonc\u00e9 and Bruno Mars", "Greg Gorman and Helmut Newton", "American real estate developer, philanthropist and sports team owner", "Isabella II", "Mexico", "Marines and their families", "Arizona", "Frdric Chopin", "Indiana Jones", "Batavia", "The Cosmopolitan of Las Vegas"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5880208333333333}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-2084", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-16678"], "SR": 0.546875, "CSR": 0.5528990963855422, "EFR": 0.9310344827586207, "Overall": 0.7083492158288325}, {"timecode": 83, "before_eval_results": {"predictions": ["Ricky Gervais", "The Green Arrow", "a parable", "Romeo", "Spinal Tap", "Tennessee", "Detroit", "Ferris B Mueller's Day Off", "the United States", "the Pyramid of Khafre", "Ruth Bader Ginsburg", "Article VII", "touch", "the Old Fashioned", "the Osmonds", "Bonnie and Clyde", "Crustaceans", "the College of William and Mary", "a chimp", "Yellowstone", "John Updike", "the Ganges", "vision", "Bright Lights", "Senator", "coelacanth", "Northanger Abbey", "Cheers", "heid", "Crosby, Stills & Nash", "Matt Leinart", "a person with type AB blood", "Charles Edward Stuart", "an eaglejim", "the Falklands", "a taro", "a quip", "a lighthouse", "white", "Dan Rather", "Fraser", "Buffalo Bill", "the Big Bang", "pig", "Harvard", "neurons", "Hawaii", "the Pierian spring", "a hobo", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "pentecost", "humble pie", "foxx", "City and County of Honolulu", "the Australian coast", "1992", "publicly criticized his father's parenting skills.", "Steven Chu", "Stella McCartney", "killing"], "metric_results": {"EM": 0.53125, "QA-F1": 0.634375}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666665, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6019", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-8018", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-6797", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-4", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3660"], "SR": 0.53125, "CSR": 0.5526413690476191, "EFR": 1.0, "Overall": 0.7220907738095238}, {"timecode": 84, "before_eval_results": {"predictions": ["the 1970s", "Steveston Outdoor pool in Richmond, BC", "the 1930s", "Isabella Palmieri", "the status line", "each team has either selected a player or traded its draft position", "a major victory of the Civil Rights Movement, and a model for many future impact litigation cases", "1991", "biscuit - sized cakes", "230 million kilometres", "members of the actual club with the parading permit as well as the brass band", "Ash Wednesday", "Castleford", "the fourth C key from left", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "the wintertime", "is an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "Robber Barons", "2001", "Spencer Treat Clark", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "Xiu Li Dai and Yongge Dai", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg", "gastrocnemius muscle", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "living in Austin, Texas trying to build a stable lifestyle by digging deep holes and caring for his neighbor Jenny McDonald, a ten - year - old girl with cerebral palsy", "1945", "California State Route 1", "Andaman and Nicobar Islands -- Port Blair", "midpiece", "Burj Khalifa", "Pangaea or Pangea", "plasma membrane in bacteria", "Johnny Cash & Willie Nelson", "Rick shoots her in the head", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing", "10 years", "2026", "eleven", "`` Singing the Blues '' by Guy Mitchell in 1957, `` Happy '' by Pharrell Williams in 2014, `` What Do You Mean? '' by Justin Bieber in 2015", "the early 20th century", "Fats Waller", "Joanna Moskawa", "1962", "Loch Ness", "LFL", "griffin", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "the Consumer Product Safety Commission Tuesday,", "\"That's ridiculous!\"", "a saddle bags", "The Tin Drum", "Dwight D. Eisenhower", "Joel \"Taz\" DiGregorio,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.6205405938473573}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.3076923076923077, 0.1379310344827586, 0.0, 0.5, 0.6666666666666666, 0.1, 0.0, 1.0, 0.625, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8181818181818181, 1.0, 0.3333333333333333, 1.0, 0.4444444444444445, 0.0, 0.13793103448275862, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.35294117647058826, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9723", "mrqa_triviaqa-validation-2065", "mrqa_triviaqa-validation-3036", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-4132", "mrqa_newsqa-validation-3992"], "SR": 0.421875, "CSR": 0.5511029411764705, "retrieved_ids": ["mrqa_squad-train-4847", "mrqa_squad-train-55162", "mrqa_squad-train-35605", "mrqa_squad-train-65975", "mrqa_squad-train-59476", "mrqa_squad-train-16757", "mrqa_squad-train-52311", "mrqa_squad-train-29184", "mrqa_squad-train-3477", "mrqa_squad-train-29157", "mrqa_squad-train-25912", "mrqa_squad-train-7906", "mrqa_squad-train-515", "mrqa_squad-train-57524", "mrqa_squad-train-82648", "mrqa_squad-train-34764", "mrqa_newsqa-validation-2491", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-6706", "mrqa_newsqa-validation-1948", "mrqa_hotpotqa-validation-2978", "mrqa_triviaqa-validation-4225", "mrqa_hotpotqa-validation-3120", "mrqa_newsqa-validation-3144", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-610", "mrqa_searchqa-validation-6256", "mrqa_hotpotqa-validation-3360", "mrqa_searchqa-validation-10906", "mrqa_triviaqa-validation-255", "mrqa_naturalquestions-validation-7286"], "EFR": 0.9459459459459459, "Overall": 0.7109722774244832}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "sprint", "ganges", "gerry adams", "mollusks", "Roy Rogers", "Steve Jobs", "Tommy Lee Jones", "Nirvana", "Donna Summer", "frog", "geese", "a special messenger of Jesus Christ", "Sheryl Crow", "lacey", "9801", "germany", "neurons", "Porridge", "oregon", "Swordfish", "eardrum", "george best", "faggot", "11", "parson brown", "Australia", "pascal", "british Airways", "five", "Challenger", "The World is Not Enough", "Giglio", "Vienna", "glee", "david hockney", "iron", "Japan", "bayern munich", "Denise Richards", "Italy", "El Paso", "New Years Day", "chili", "Madagascar", "nundy", "mike martin", "kolkata", "dance", "david bennett", "Candace", "Chung", "Game 1", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "U.S. Open", "propofol,", "bankruptcy", "Treaty of Versailles", "Zidane", "Augustus", "a newt"], "metric_results": {"EM": 0.5, "QA-F1": 0.5687500000000001}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-5173", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5759", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-8849", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-350"], "SR": 0.5, "CSR": 0.5505087209302326, "EFR": 1.0, "Overall": 0.7216642441860465}, {"timecode": 86, "before_eval_results": {"predictions": ["Denmark", "General Sir John Monash", "tempo", "photographs, film and television", "Arthur Freed", "alt-right", "the Runaways", "50 best cities to live in", "La Liga", "season 1", "June 13, 1960", "Iran", "short-interspersed nuclear elements", "death", "London", "South Korean", "quantum mechanics", "mac crinain", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa S\u00e1nchez", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "Fear and Loathing", "Martha Wainwright", "Leafcutter John", "moth", "Final Fantasy XII", "Jim Thorpe", "De La Soul", "\"The Marshall Mathers LP 2\"", "Shropshire Union Canal", "1621", "A skerry", "Oliver Parker", "FX", "Kalokuokamaile", "Pac-12 Conference", "Roots: The Saga of an American Family", "five", "Jack Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "8 August 1907", "Maidstone, Kent", "strings of eight bits ( known as bytes )", "an action role - playing video game developed and published by Nippon Ichi Software for the PlayStation 4", "nathan leopold Jr.", "Bill Haley & His Comets", "george Carey", "Amanda Knox's aunt Janet Huff", "Number Ones", "near Garacad, Somalia,", "(E)B. White", "Andrew Jackson", "Jefferson", "Willa Cather"], "metric_results": {"EM": 0.625, "QA-F1": 0.6852813852813853}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-533", "mrqa_hotpotqa-validation-3231", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_searchqa-validation-1530"], "SR": 0.625, "CSR": 0.5513649425287357, "EFR": 0.9583333333333334, "Overall": 0.7135021551724138}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "Vision of the Future", "in 1754", "May 10, 1976", "Hamlet", "Martin Ingerman", "Milwaukee Bucks", "McLaren-Honda", "Armin Shimerman", "The Spiderwick Chronicles", "American reality documentary television series", "Sarah Kerrigan, the Queen of Blades", "Qualcomm", "water sprite", "10-metre platform event", "Cincinnati Bengals", "Utnapishtim, the far-away", "\"Guardians of the Galaxy Vol. 2\"", "November 15, 1903", "Bury St Edmunds, Suffolk, England", "Rothschild banking dynasty", "Mr. Church", "Bigger Than Both of Us", "Thomas Christopher Ince", "Matt Serra", "public", "Los Angeles", "The Future", "Prussian-Lithuanian teacher, poet, humanist, philosopher and Lithuanian", "al-Qaeda", "the Darling River", "Baldwin", "2 April 1977", "House of Commons", "William Finn", "Love Letter", "Indian", "German submarine U-32 (S182)", "Barnoldswick", "the late 12th Century", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander Lindemann,", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "The Division of Cook", "Baji Rao I", "Prafulla Chandra Ghosh", "the retina", "Confederate forces", "Western Samoa", "delorean dMC-12", "amelia earhart", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Michael Arrington,", "was a nasty, irritable fellow who showed no interest in mating and would attack females when they were introduced.", "a snowmobile", "a snakes", "bone", "vasoconstriction of most blood vessels"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6625496031746032}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6, 0.0, 0.14285714285714285, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.546875, "CSR": 0.5513139204545454, "retrieved_ids": ["mrqa_squad-train-61561", "mrqa_squad-train-46475", "mrqa_squad-train-63312", "mrqa_squad-train-2809", "mrqa_squad-train-35101", "mrqa_squad-train-19148", "mrqa_squad-train-50491", "mrqa_squad-train-23331", "mrqa_squad-train-3184", "mrqa_squad-train-83542", "mrqa_squad-train-4381", "mrqa_squad-train-10590", "mrqa_squad-train-4730", "mrqa_squad-train-45381", "mrqa_squad-train-21218", "mrqa_squad-train-81926", "mrqa_squad-validation-5303", "mrqa_newsqa-validation-1510", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-3787", "mrqa_newsqa-validation-2423", "mrqa_triviaqa-validation-3468", "mrqa_squad-validation-7537", "mrqa_searchqa-validation-1530", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-14453", "mrqa_naturalquestions-validation-3174", "mrqa_searchqa-validation-4261", "mrqa_triviaqa-validation-3610", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-8411"], "EFR": 1.0, "Overall": 0.7218252840909091}, {"timecode": 88, "before_eval_results": {"predictions": ["germania w", "\"La M\u00f4me Piaf\"", "ulysses S. Grant", "Apollo", "Richard Wagner", "Atticus Finch", "Peter Principle", "copper and zinc", "Hammertone", "Dunfermline", "bison bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "black", "Samoa", "John Gorman", "The Daily Mirror", "iron", "Olympus Mons on Mars", "Poland", "caffari", "calos", "belize", "h Humphrey Lyttelton", "clarence", "prawns", "James Hogg", "massively multiplayer", "Fermanagh", "Colombia", "Kevin Painter", "llyn Padarn", "katherine parr", "Muhammad Ali", "carmen Miranda", "Mishal Husain", "tiger tim", "1982", "estonia", "Sarajevo", "gluten", "an enclave", "Robert Louis Stevenson", "muthia Murlitharan", "Ridley Scott", "four", "Futurama", "adrian edmondson", "144 inches", "1925", "September 29, 2017", "Walter Brennan", "13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "the surge", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "devil's food cake", "Michelangelo", "Missouri", "George Jetson"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6546875}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-2725", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-2876", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_newsqa-validation-161"], "SR": 0.609375, "CSR": 0.5519662921348314, "EFR": 1.0, "Overall": 0.7219557584269662}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "Graphical", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "Large", "Nelly", "gladiators", "Nemo", "the European Green Woodpecker's tongue", "the Kite Runner", "a shark", "Uganda", "Oprah Winfrey", "Dixie Chicks", "apple pie", "California", "Best Buy", "the Ionian Sea", "Pope John Paul II", "Lobster Newburg", "Yemen", "David Geffen", "chariots", "Neruda", "the Fifth amendment", "a mite", "Saturn", "Nanny Diaries", "liquid crystal", "Robert Frost", "a Dictum", "Butternut Squash Tortellini", "Crete", "Father Brown", "Reuben", "The Outsiders", "waltz", "Belch", "Jane Austen", "Wisconsin", "Charles Darnay", "Q", "When Harry Met Sally", "Mexico", "pumice", "John Molson", "Jan and Dean", "Celebrity Reporter", "Janis Joplin", "all transmissions", "Sir Hugh Beaver", "Andorra", "mike faraday", "nelson Rockefeller", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "upper respiratory infection", "Second seed Fernando Gonzalez", "At least 14", "more than two years,"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7927083333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-5546", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-13703", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-795"], "SR": 0.703125, "CSR": 0.5536458333333334, "EFR": 1.0, "Overall": 0.7222916666666667}, {"timecode": 90, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.87109375, "KG": 0.5265625, "before_eval_results": {"predictions": ["the Harpe brothers", "McComb, Mississippi", "The Bonnie Banks o' Loch Lomond", "American reality television series", "Gweilo", "\"The Royal Family\"", "The Ninth Gate", "James G. Kiernan", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "Los Angeles Dance Theater", "Johnnie Ray", "Hampton University", "The Clash of Triton", "Jenji Kohan", "1", "the second line", "Scottish Premiership club Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "Hard rock", "uncle of Prince Philip, Duke of Edinburgh", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "Bigfoot", "Cyclic Defrost", "England, Scotland, and Ireland", "\"Coal Miner's daughter\"", "Worcester", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "Ealdorman", "La Scala, Milan", "Orson Welles", "1979", "Scott Mechlowicz", "Ryan Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "Mission Revival Style", "Muhammad", "18", "Harlem River", "Turkish", "dollar", "sulfur dioxide", "1913", "Juan Martin Del Potro", "Amsterdam, in the Netherlands, to Ankara, Turkey", "Lord of the Rings", "Jaguar", "smut", "semi-autonomous organisational units"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7378163197097021}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2730", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-1568", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_naturalquestions-validation-373"], "SR": 0.65625, "CSR": 0.5547733516483517, "retrieved_ids": ["mrqa_squad-train-8957", "mrqa_squad-train-74865", "mrqa_squad-train-47887", "mrqa_squad-train-22563", "mrqa_squad-train-4386", "mrqa_squad-train-59191", "mrqa_squad-train-78805", "mrqa_squad-train-39778", "mrqa_squad-train-73774", "mrqa_squad-train-48415", "mrqa_squad-train-82289", "mrqa_squad-train-30779", "mrqa_squad-train-5517", "mrqa_squad-train-65930", "mrqa_squad-train-58652", "mrqa_squad-train-14510", "mrqa_hotpotqa-validation-2080", "mrqa_searchqa-validation-12411", "mrqa_squad-validation-2506", "mrqa_naturalquestions-validation-8294", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-15863", "mrqa_triviaqa-validation-2725", "mrqa_newsqa-validation-3101", "mrqa_searchqa-validation-14970", "mrqa_hotpotqa-validation-1025", "mrqa_naturalquestions-validation-7553", "mrqa_triviaqa-validation-4872", "mrqa_searchqa-validation-3477", "mrqa_triviaqa-validation-4167", "mrqa_hotpotqa-validation-458"], "EFR": 0.9545454545454546, "Overall": 0.7364731362387612}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Phillip Schofield and Christine Bleakley", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "Gearing is employed in the transmission, which contains a number of different sets of gears that can be changed to allow a wide range of vehicle speeds, and also in the differential", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Skeletal muscle", "USS Chesapeake", "1977", "Russian citadels", "Charles Darwin and Alfred Russel Wallace", "the inverted - drop - shaped icon that marks locations in Google Maps", "Richard Stallman", "2004", "1940", "War Powers Resolution", "an individual noticing that the person in the photograph is attractive, well groomed, and properly attired, assumes, using a mental heuristic", "fuel rocket engines", "Spanish", "either two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "Kansas City Chiefs", "used obscure languages as a means of secret communication during World War II", "Zhu Yuanzhang", "1980 Summer Olympics", "Heather Stebbins", "The soma ( cell body ) of each pseudounipolar neuron is located within a dorsal root ganglion", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017", "Julie Adams", "1881", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street, recorded the film's score with the London Symphony Orchestra and London Philharmonic", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century.", "550 quadrillion Imperial gallons", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Theodore Roosevelt, Robert M. La Follette, Sr., and Charles Evans Hughes on the Republican side, and William Jennings Bryan, Woodrow Wilson and Al Smith on the Democratic side", "August 5, 1937", "voters gathered as a tribe the members would be well known enough to each other that an outsider could be spotted", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings, when Cowboys quarterback Roger Staubach ( a Roman Catholic and fan of The Godfather Part II ( 1974 )", "Payson, Lauren, and Kaylie", "2015, 2017", "Dr. Lexie Grey", "September 6, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton as Johnny, a teenage gorillas who wants to sing", "1990", "smen", "T'Pau", "Fort Nelson near Portsmouth", "dominoes", "Sparta", "World Famous Gold & Silver Pawn Shop", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "death of cardiac arrest", "one day,", "Jefferson", "Babel", "James Bond", "Juan Ponce de Len"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5614570424660035}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.21052631578947367, 1.0, 0.3448275862068966, 1.0, 1.0, 0.0, 0.25, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 0.3333333333333333, 0.3636363636363636, 0.33333333333333337, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.40625, "CSR": 0.5531589673913043, "EFR": 0.8421052631578947, "Overall": 0.7136622211098398}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite beer", "beetle", "the South Saskatchewan", "Kentmere Valley, near Kendal, Lake District, Cumbria, Eng.", "electronic junk mail", "Tahrir Square", "David Frost", "Newbury Racecourse", "detention", "town of Knutsford", "Portugal", "Spongebob", "make Me an offer", "China", "Maine", "Edward VI", "George W. Bush", "Washington, D.C.", "jack Sprat", "Ronnie Kray", "the Council of Constance", "Dublin", "The Mayor of Casterbridge", "heel", "Amsterdam Stock Exchange", "John Lennon", "Lusitania", "Anne Boleyn", "Australia", "antelope", "Portugal", "Swaziland", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Isambard Kingdom Brunel", "Canada", "Vickers Vimy", "Jinnah International Airport", "republic of indus", "Edmund I", "Peter Paul Rubens", "John Ford", "six", "Mendip Hills", "Burma", "Charles Taylor", "Pancho Villa", "for the purpose of changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Jerry Leiber and Mike Stoller", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County", "Brown Mountain Overlook", "Lucky Dube,", "Middle East and North Africa", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "Lobotomy"], "metric_results": {"EM": 0.625, "QA-F1": 0.6886642156862746}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-2600", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-1587", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122"], "SR": 0.625, "CSR": 0.5539314516129032, "EFR": 1.0, "Overall": 0.7453956653225806}, {"timecode": 93, "before_eval_results": {"predictions": ["American", "Keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "Briton Allan McNish", "Coahuila, Mexico", "Atomic Kitten", "Ephedrine", "Colin Vaines", "California", "racehorse breeder", "Jim Kelly", "Australian", "D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "forget to Remember", "1947", "Easter Rising", "Tuesday, January 24, 2012", "John Monash", "\u00c6thelstan", "Middlesbrough F.C.", "rap parts from Darryl,RB Djan", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "recording debut of future AC/DC founders Angus Young and Malcolm Young", "Goddess of Pop", "125 lb", "chocolate-colored", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Anne", "\" Neighbours\"", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "gulls", "chariot", "Louisiana", "\"stressed and tired force\" made vulnerable by multiple deployments,", "Citizens", "Tuesday", "The African Queen", "cats", "Gibraltar", "acidity or basicity of an aqueous solution"], "metric_results": {"EM": 0.578125, "QA-F1": 0.71640552054155}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.578125, "CSR": 0.554188829787234, "retrieved_ids": ["mrqa_squad-train-4862", "mrqa_squad-train-26902", "mrqa_squad-train-63358", "mrqa_squad-train-17371", "mrqa_squad-train-39040", "mrqa_squad-train-48692", "mrqa_squad-train-69699", "mrqa_squad-train-73128", "mrqa_squad-train-60860", "mrqa_squad-train-39626", "mrqa_squad-train-37255", "mrqa_squad-train-34286", "mrqa_squad-train-67495", "mrqa_squad-train-77694", "mrqa_squad-train-10718", "mrqa_squad-train-21753", "mrqa_hotpotqa-validation-2635", "mrqa_triviaqa-validation-2862", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-2863", "mrqa_naturalquestions-validation-309", "mrqa_newsqa-validation-1572", "mrqa_searchqa-validation-9788", "mrqa_hotpotqa-validation-4967", "mrqa_newsqa-validation-2697", "mrqa_triviaqa-validation-5101", "mrqa_squad-validation-8647", "mrqa_searchqa-validation-16311", "mrqa_triviaqa-validation-7390", "mrqa_hotpotqa-validation-3223", "mrqa_newsqa-validation-513"], "EFR": 1.0, "Overall": 0.7454471409574468}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa Park", "Guinea", "mayflower", "four", "Daily Mail", "tartan", "Toy Story", "GM Korea", "lungs", "Periodic Table", "The Left Book Club", "Chile", "Saint Columba", "Donald Sutherland", "Chicago", "egypt", "Cardiff", "sternum", "pressure", "James Murdoch", "Chicago", "Fluids", "bach", "Squeeze", "The Altamont Speedway Free Festival", "Robert Plant", "Jerry Seinfeld", "propeller", "kia", "lemurs", "Sir Robert Walpole", "eight", "Andorra", "a horse collar", "Enoch Powell", "Lone Ranger", "St Paul's Cathedral", "27", "Formula One", "squash", "Mary Decker", "Godwin Austen", "France", "\" Birdman of Alcatraz\"", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "Lady Godiva", "festival of Britain", "rubber sole", "cloaks", "1940s", "0.30 in ( 7.6 mm )", "a hydrolysis reaction", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "5.3 million", "6-4", "al Qaeda", "UNICEF", "Corman", "Lady of the Lamp", "Saturn", "the term globalization"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7302083333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-1929", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-9012", "mrqa_searchqa-validation-11091"], "SR": 0.6875, "CSR": 0.5555921052631578, "EFR": 0.85, "Overall": 0.7157277960526315}, {"timecode": 95, "before_eval_results": {"predictions": ["war drama", "its air-cushioned sole", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Edward Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs", "2004", "Van Diemen's Land", "Jim Kelly", "Stern-Plaza", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three", "Derrick Coleman", "Prussia", "David Wells", "Roslin", "two", "Argentine", "13th century", "Pru Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1993", "Jesus", "Sulla", "Riot Act", "Larry Gatlin & the Gatlin Brothers", "right-hand batsman", "black nationalism", "The Simpsons", "Bayern", "Deftones", "Gangsta's Paradise", "Clitheroe Football Club", "Green Lantern", "\" Cleopatra\"", "The Fault in Our Stars", "Liesl", "how the Grinch Stole Christmas", "twin-faced sheepskin", "White Horse", "banjo player", "Flavivirus", "Elise Marie Stefanik", "Francis Schaeffer", "the South Pacific", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "President pro tempore of the Senate", "william mcknard", "capture of Quebec", "cold comfort farm", "red", "lightning strikes", "murders of his father and brother", "Guernsey", "Southern Christian", "Berlin", "brain and spinal cord"], "metric_results": {"EM": 0.5, "QA-F1": 0.6579427083333333}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false], "QA-F1": [0.0, 0.375, 0.25, 0.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428572, 0.28571428571428575, 0.0, 0.5, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-1673", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-4302", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-731", "mrqa_naturalquestions-validation-7342"], "SR": 0.5, "CSR": 0.5550130208333333, "EFR": 0.90625, "Overall": 0.7268619791666666}, {"timecode": 96, "before_eval_results": {"predictions": ["maximum speed 160 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "pyloric valve", "Taika Waititi", "Ephesus", "Mark Lowry", "Phillip Paley", "Germany", "Electromagnetic radiation", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Baaghi", "Taylor Michel Momsen", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "the epidermis", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization", "pigs", "a `` take it or leave it '' position", "the first of half a dozen Dutch Companies sailed to trade there from 1595, which amalgamated in March 1602 into the United East Indies Company ( VOC )", "The musical premiered on October 16, 2012, at Ars Nova ; directed by Rachel Chavkin the show was staged as an immersive production, with action happening around and among the audience", "American country music duo Brooks & Dunn", "May 31, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "the world", "Lulu", "the NFL", "Steve Russell", "The 50 stars on the flag represent the 50 states of the United States of America", "Profit maximization", "Melbourne", "April 1, 2016", "the Alamodome and city of San Antonio", "801,200", "michael p Phelps", "royal oak", "The Krankies", "France", "Province of Syracuse", "June 11, 1986", "1-0", "200", "Republican Gov. Bobby Jindal", "\"reshit\"", "John Deere", "gusts", "curfew"], "metric_results": {"EM": 0.6875, "QA-F1": 0.754849614054846}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8235294117647058, 1.0, 0.2608695652173913, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 0.2857142857142857, 0.18181818181818182, 0.8, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_triviaqa-validation-800", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252"], "SR": 0.6875, "CSR": 0.5563788659793815, "retrieved_ids": ["mrqa_squad-train-27813", "mrqa_squad-train-75956", "mrqa_squad-train-10948", "mrqa_squad-train-33733", "mrqa_squad-train-12652", "mrqa_squad-train-71231", "mrqa_squad-train-39776", "mrqa_squad-train-18317", "mrqa_squad-train-77173", "mrqa_squad-train-60266", "mrqa_squad-train-33171", "mrqa_squad-train-20725", "mrqa_squad-train-72683", "mrqa_squad-train-58305", "mrqa_squad-train-30782", "mrqa_squad-train-42531", "mrqa_hotpotqa-validation-429", "mrqa_newsqa-validation-2533", "mrqa_searchqa-validation-5817", "mrqa_triviaqa-validation-602", "mrqa_hotpotqa-validation-4797", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-2826", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-3784", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-646", "mrqa_triviaqa-validation-1961", "mrqa_searchqa-validation-15725", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7953", "mrqa_triviaqa-validation-3876"], "EFR": 0.9, "Overall": 0.7258851481958762}, {"timecode": 97, "before_eval_results": {"predictions": ["Tchaikovsky", "dark places", "the Konabar", "the boll weevil", "touchpad", "Wikipedia", "Butch Cassidy", "Buddhism", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Edgar Allan Poe", "(Sergey) Brin", "Sanders", "an American alternative rock band", "bread", "Yale", "Napoleon", "Paris", "Baden- Wrttemberg", "the Stanza della Segnatura", "an ant", "birkenstock", "Firebird", "Hafnium", "flax", "the Muse", "the Wachowski brothers", "Rumpole", "a president lost the popular vote but lost the voting power given to it by the electoral college", "Steve Austin", "Kurt Warner", "40", "a small retail store", "Belle", "Ratatouille", "pro bono", "Gentle Ben", "The Office", "The Oprah Show", "Bigfoot", "Jackson Pollock", "glisten", "Mona Lisa", "Ngi", "Crayola", "The Man in the Gray Flannel Suit", "Assimilation", "bright orange", "Isaiah Amir Mustafa", "1999", "Americans acting under orders", "mike hammer", "The Crow", "L. P. Hartley", "The Berber languages", "the European Champion Clubs' Cup", "second largest", "North Korea", "Al alcohol", "It wasn't appreciated how much of an impact it can have on a patient's quality of life,\"", "Norman Mark Reedus"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.06666666666666667, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-3811", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_searchqa-validation-6988", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1760", "mrqa_newsqa-validation-96", "mrqa_hotpotqa-validation-2138"], "SR": 0.53125, "CSR": 0.5561224489795918, "EFR": 1.0, "Overall": 0.7458338647959184}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Redblush grapefruit", "a quay", "Nathaniel", "the cornea", "Crystal Light", "Rumpole", "the guillotine", "the light bulb", "Spider-Man", "Atlanta", "Chile", "Dick Tracy", "Queen Latifah", "Van Allen", "beer", "Hakuin", "El", "Zenith", "a cynocephalus", "wine", "Frank", "the q- tip", "natural selection", "Massachusetts", "Battle of the Bulge", "a shaft", "W. Somerset Maugham", "the Two Sicilies", "the British", "a republic", "Francis Drake", "the Nisei", "Enrico Fermi", "Nokia", "the pituitary", "Alfred Hitchcock", "Hank Aaron", "reconnaissance", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Columbus", "Joseph Haydn", "Meringue", "Serena", "the Yakuza", "stones", "four", "James Hutton", "961", "michael hartemink", "the god Dionysus", "Mary Seacole", "Orchard Central", "Fort Hood", "OutKast", "iPods", "suspend all", "Wednesday", "Nick Sager"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6489583333333333}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3054", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-3438", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-221", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-6821", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.515625, "CSR": 0.5557133838383839, "EFR": 0.9354838709677419, "Overall": 0.7328488259612251}, {"timecode": 99, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.841796875, "KG": 0.47734375, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017", "2018", "Neuropsychology", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013 ( XLVIII )", "Ozzie Smith", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "the Beatles", "the Persian style of architecture", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014", "Natural - language processing ( NLP )", "six", "HTTP / 1.1", "$2 million in 2011", "three high fantasy adventure films directed by Peter Jackson", "Sohrai", "the Jos Plateau", "Cecil Lockhart", "James Long", "the fifth-most populous city in Florida and the largest in the state that is not a county seat", "April 13, 2018", "quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "`` Killer Within ''", "Disha Vakani", "Nickelback", "1999", "King Willem - Alexander", "based on a Yogiism, or quotation from Yogi Berra", "Deuteronomy 5 : 4 -- 25", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul", "H.L. Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1998", "Manley", "leslie chandler", "Francis Matthews", "HYMENAEUS", "1907", "1776", "Field Marshal Stapleton Cotton", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "eight-day journey", "101", "Spain", "( Clayton) Barbeau", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6812263257575757}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-791", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-2494", "mrqa_searchqa-validation-3524"], "SR": 0.609375, "CSR": 0.55625, "retrieved_ids": ["mrqa_squad-train-48507", "mrqa_squad-train-71329", "mrqa_squad-train-52222", "mrqa_squad-train-74913", "mrqa_squad-train-31134", "mrqa_squad-train-32176", "mrqa_squad-train-11924", "mrqa_squad-train-64832", "mrqa_squad-train-67632", "mrqa_squad-train-65872", "mrqa_squad-train-35166", "mrqa_squad-train-10779", "mrqa_squad-train-68002", "mrqa_squad-train-81129", "mrqa_squad-train-62709", "mrqa_squad-train-83051", "mrqa_squad-validation-9578", "mrqa_hotpotqa-validation-2937", "mrqa_newsqa-validation-960", "mrqa_triviaqa-validation-1179", "mrqa_naturalquestions-validation-10225", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-2242", "mrqa_naturalquestions-validation-10026", "mrqa_hotpotqa-validation-3995", "mrqa_triviaqa-validation-6807", "mrqa_hotpotqa-validation-574", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-2425", "mrqa_triviaqa-validation-3610", "mrqa_searchqa-validation-12151", "mrqa_triviaqa-validation-590"], "EFR": 1.0, "Overall": 0.732890625}]}